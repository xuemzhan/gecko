[1] examples/core/engine_base_demo.py
```python
import asyncio
import time
from typing import Any, AsyncIterator, List, Type, Dict

from pydantic import BaseModel, Field

# Gecko æ ¸å¿ƒç»„ä»¶å¯¼å…¥
from gecko.core.engine.base import CognitiveEngine, AgentOutput
from gecko.core.message import Message
from gecko.core.memory import TokenMemory
from gecko.core.toolbox import ToolBox
from gecko.core.protocols import (
    ModelProtocol, 
    CompletionResponse, 
    CompletionChoice, 
    StreamChunk
)

# ==========================================
# 1. æ¨¡æ‹Ÿç»„ä»¶ (Mock Components)
# ==========================================

class MockModel(ModelProtocol):
    """
    ä¸€ä¸ªç®€å•çš„ Mock æ¨¡å‹ï¼Œå®ç°äº† ModelProtocolã€‚
    å®ƒåªæ˜¯å›æ˜¾ç”¨æˆ·çš„è¾“å…¥ï¼Œæˆ–è€…ç”Ÿæˆé¢„å®šä¹‰çš„æµå¼æ•°æ®ã€‚
    """
    async def acompletion(self, messages: List[Dict[str, Any]], **kwargs) -> CompletionResponse:
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await asyncio.sleep(0.1)
        
        last_content = messages[-1]["content"]
        response_text = f"Mock Response to: {last_content}"
        
        return CompletionResponse(
            choices=[
                CompletionChoice(message={"role": "assistant", "content": response_text})
            ],
            usage={"prompt_tokens": 10, "completion_tokens": 5, "total_tokens": 15}
        )

    async def astream(self, messages: List[Dict[str, Any]], **kwargs) -> AsyncIterator[StreamChunk]:
        # æ¨¡æ‹Ÿæµå¼è¾“å‡º
        full_text = "This is a streaming response from the mock model."
        for word in full_text.split():
            await asyncio.sleep(0.05)
            yield StreamChunk(
                choices=[{"delta": {"content": word + " "}}]
            )

# ==========================================
# 2. è‡ªå®šä¹‰å¼•æ“å®ç° (Custom Engine)
# ==========================================

class DemoEngine(CognitiveEngine):
    """
    ç»§æ‰¿ CognitiveEngine çš„æ¼”ç¤ºå¼•æ“ã€‚
    å¿…é¡»å®ç° step() æ–¹æ³•ã€‚
    """
    
    async def step(self, input_messages: List[Message], **kwargs) -> AgentOutput:
        """
        å®ç°æ ¸å¿ƒæ¨ç†é€»è¾‘ï¼ˆå¸¦ç»Ÿè®¡ä¿®å¤ï¼‰
        """
        # â±ï¸ 1. å¼€å§‹è®¡æ—¶
        start_time = time.time()
        
        # 2. éªŒè¯è¾“å…¥
        self.validate_input(input_messages)
        
        # 3. è§¦å‘ before_step hook
        await self.before_step(input_messages, **kwargs)

        # 4. å‡†å¤‡æ•°æ®
        formatted_msgs = [m.to_openai_format() for m in input_messages]
        
        try:
            # 5. è°ƒç”¨æ¨¡å‹
            response = await self.model.acompletion(formatted_msgs)
            content = response.choices[0].message["content"]
            
            # è·å– token ä½¿ç”¨é‡ (MockModel è¿”å›äº† usage)
            # å¦‚æœ response.usage æ˜¯å¯¹è±¡åˆ™å–å±æ€§ï¼Œå¦‚æœæ˜¯å­—å…¸åˆ™å–é”®å€¼
            usage_info = response.usage
            total_tokens = 0
            if isinstance(usage_info, dict):
                total_tokens = usage_info.get("total_tokens", 0)
            elif hasattr(usage_info, "total_tokens"):
                total_tokens = usage_info.total_tokens

            # 6. æ„å»ºè¾“å‡º
            output = AgentOutput(
                content=content,
                metadata={"finish_reason": "stop"}
            )
            
            # 7. è§¦å‘ after_step hook
            await self.after_step(input_messages, output, **kwargs)
            
            # âœ… 8. [ä¿®å¤ç‚¹] æ‰‹åŠ¨æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            duration = time.time() - start_time
            if self.stats:
                self.stats.add_step(duration, tokens=total_tokens)
            
            return output
            
        except Exception as e:
            # âœ… 9. [ä¿®å¤ç‚¹] è®°å½•é”™è¯¯ç»Ÿè®¡
            if self.stats:
                self.stats.errors += 1
                
            # é”™è¯¯å¤„ç† hook
            await self.on_error(e, input_messages)
            raise

    async def step_stream(self, input_messages: List[Message], **kwargs) -> AsyncIterator[str]:
        """
        è¦†ç›–æµå¼æ¨ç†æ–¹æ³•
        """
        formatted_msgs = [m.to_openai_format() for m in input_messages]
        
        async for chunk in self.model.astream(formatted_msgs):
            content = chunk.content
            if content:
                yield content

    async def step_structured(
        self, 
        input_messages: List[Message], 
        response_model: Type[BaseModel], 
        **kwargs
    ) -> BaseModel:
        """
        è¦†ç›–ç»“æ„åŒ–è¾“å‡ºæ–¹æ³• (æ¨¡æ‹Ÿå®ç°)
        """
        # æ¨¡æ‹Ÿï¼šç›´æ¥è¿”å›ä¸€ä¸ªä¼ªé€ çš„ç»“æ„åŒ–å¯¹è±¡
        # å®é™…åœºæ™¯ä¸­è¿™é‡Œä¼šè°ƒç”¨ StructureEngine
        print(f"   [Engine] Parsing structured output for {response_model.__name__}...")
        await asyncio.sleep(0.1)
        
        return response_model(
            reasoning="Simulated reasoning",
            score=95,
            tags=["demo", "mock"]
        )

# ==========================================
# 3. è¾…åŠ©æ•°æ®ç»“æ„
# ==========================================

class AnalysisResult(BaseModel):
    """ç”¨äºæµ‹è¯•ç»“æ„åŒ–è¾“å‡ºçš„æ¨¡å‹"""
    reasoning: str = Field(description="æ€è€ƒè¿‡ç¨‹")
    score: int = Field(description="è¯„åˆ†")
    tags: List[str] = Field(description="æ ‡ç­¾")

# ==========================================
# 4. ä¸»æ¼”ç¤ºæµç¨‹
# ==========================================

async def main():
    print("ğŸš€ Starting Engine Base Demo...\n")

    # --- åˆå§‹åŒ–ä¾èµ– ---
    model = MockModel()
    toolbox = ToolBox() # ç©ºå·¥å…·ç®±
    # æ³¨æ„ï¼šè¿™é‡Œç®€å• mock memoryï¼Œå®é™…åº”ä¼ å…¥ SessionInterface å®ç°
    memory = TokenMemory(session_id="demo_session", max_tokens=1000)

    # --- ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆå§‹åŒ–å¼•æ“ ---
    print("1ï¸âƒ£  Testing Context Manager & Basic Step")
    async with DemoEngine(model, toolbox, memory) as engine:
        
        # --- è®¾ç½® Hooks ---
        async def my_before_hook(messages, **kwargs):
            print(f"   [Hook] Before step: Processing {len(messages)} messages")

        async def my_after_hook(messages, output, **kwargs):
            print(f"   [Hook] After step: Generated {len(output.content)} chars")

        engine.before_step_hook = my_before_hook
        engine.after_step_hook = my_after_hook

        # --- æµ‹è¯•æ™®é€šæ¨ç† ---
        user_msg = Message.user("Hello Gecko!")
        print(f"   User: {user_msg.content}")
        
        output = await engine.step([user_msg])
        print(f"   Agent: {output.content}\n")

        # --- æµ‹è¯•æµå¼æ¨ç† ---
        print("2ï¸âƒ£  Testing Streaming")
        print("   Agent (Stream): ", end="", flush=True)
        async for token in engine.step_stream([Message.user("Stream me!")]):
            print(token, end="", flush=True)
        print("\n")

        # --- æµ‹è¯•ç»“æ„åŒ–è¾“å‡º ---
        print("3ï¸âƒ£  Testing Structured Output")
        result = await engine.step_structured(
            [Message.user("Analyze this")], 
            response_model=AnalysisResult
        )
        print(f"   Result: {result.model_dump_json()}\n")

        # --- æŸ¥çœ‹ç»Ÿè®¡ ---
        print("4ï¸âƒ£  Execution Stats")
        stats = engine.get_stats()
        print(f"   Total Steps: {stats['total_steps']}")
        print(f"   Total Time:  {stats['total_time']:.4f}s")
        print(f"   Avg Time:    {stats['avg_step_time']:.4f}s")
        print(f"   Errors:      {stats['errors']}")

if __name__ == "__main__":
    asyncio.run(main())
```

[2] examples/core/engine_react_demo.py
```python
import asyncio
import os
from typing import Any, Dict

from pydantic import BaseModel, Field

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from gecko.core.agent import Agent
from gecko.core.message import Message
from gecko.core.memory import TokenMemory
from gecko.core.toolbox import ToolBox
from gecko.core.engine.react import ReActEngine
from gecko.plugins.tools.base import BaseTool
from gecko.plugins.models.zhipu import ZhipuGLM, glm_4_5_air

# ==========================================
# 1. å®šä¹‰ç®€å•çš„å·¥å…·
# ==========================================

class CalculatorTool(BaseTool):
    name: str = "calculator"
    description: str = "Useful for performing basic arithmetic operations. Input should be a math expression string."
    parameters: Dict[str, Any] = {
        "type": "object",
        "properties": {
            "expression": {
                "type": "string",
                "description": "The math expression to evaluate, e.g., '2 + 2'"
            }
        },
        "required": ["expression"]
    }

    async def execute(self, arguments: Dict[str, Any]) -> str:
        expression = arguments.get("expression")
        try:
            # æ³¨æ„ï¼ševal åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ˜¯ä¸å®‰å…¨çš„ï¼Œä»…ç”¨äºæ¼”ç¤º
            return str(eval(expression))
        except Exception as e:
            return f"Error: {str(e)}"

class WeatherTool(BaseTool):
    name: str = "get_current_weather"
    description: str = "Get the current weather in a given location"
    parameters: Dict[str, Any] = {
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA"
            },
            "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
        },
        "required": ["location"]
    }

    async def execute(self, arguments: Dict[str, Any]) -> str:
        location = arguments.get("location")
        return f"The weather in {location} is sunny and 25Â°C."

# ==========================================
# 2. å®šä¹‰ç»“æ„åŒ–è¾“å‡ºæ¨¡å‹
# ==========================================

class AnalysisReport(BaseModel):
    """åˆ†ææŠ¥å‘Šç»“æ„"""
    summary: str = Field(description="å¯¹ç”¨æˆ·é—®é¢˜çš„ç®€çŸ­æ€»ç»“")
    action_items: list[str] = Field(description="å»ºè®®é‡‡å–çš„è¡ŒåŠ¨é¡¹åˆ—è¡¨")
    priority: str = Field(description="ä¼˜å…ˆçº§ (High/Medium/Low)")

# ==========================================
# 3. ä¸»æ¼”ç¤ºæµç¨‹
# ==========================================

async def main():
    # 1. åˆå§‹åŒ–æ¨¡å‹ (ä½¿ç”¨æä¾›çš„ ZhipuGLM å®ç°)
    # è¯·ç¡®ä¿ ZHIPU_API_KEY ç¯å¢ƒå˜é‡å·²è®¾ç½®ï¼Œæˆ–è€…åœ¨æ„é€ å‡½æ•°ä¸­ä¼ å…¥
    api_key = os.getenv("ZHIPU_API_KEY", "3bd5e6fdc377489c80dbb435b84d7560.izN8bDXCVR1FNSYS")
    llm = ZhipuGLM(api_key=api_key, model="glm-4-flash") # ä½¿ç”¨ flash æ¨¡å‹é€Ÿåº¦æ›´å¿«

    # 2. åˆå§‹åŒ–å·¥å…·ç®±
    toolbox = ToolBox(tools=[CalculatorTool(), WeatherTool()])

    # 3. åˆå§‹åŒ–è®°å¿†
    memory = TokenMemory(session_id="react_demo_session", max_tokens=2000)

    # 4. æ„å»º Agent (ä½¿ç”¨ ReActEngine)
    agent = Agent(
        model=llm,
        toolbox=toolbox,
        memory=memory,
        engine_cls=ReActEngine,
        max_turns=5 # é™åˆ¶æœ€å¤§æ€è€ƒè½®æ•°
    )

    print("\nğŸš€ ReAct Agent Demo (Powered by ZhipuGLM)\n")

    # --- åœºæ™¯ 1: éœ€è¦ä½¿ç”¨å·¥å…·çš„å¤æ‚æŸ¥è¯¢ ---
    query1 = "What is 123 * 45? Also, what's the weather in Beijing?"
    print(f"ğŸ‘¤ User: {query1}")
    print("ğŸ¤– Agent (Thinking...):")
    
    # ä½¿ç”¨ run() æ–¹æ³• (éæµå¼)
    response1 = await agent.run(query1)
    print(f"ğŸ’¡ Final Answer: {response1.content}\n")
    
    # æŸ¥çœ‹ç»Ÿè®¡ (ReActEngine ä¼šè®°å½•å·¥å…·è°ƒç”¨)
    if agent.engine.stats:
        print(f"ğŸ“Š Stats: Steps={agent.engine.stats.total_steps}, ToolCalls={agent.engine.stats.tool_calls}")

    print("-" * 50)

    # --- åœºæ™¯ 2: ç»“æ„åŒ–è¾“å‡º ---
    query2 = "Based on the weather in Beijing, suggest a weekend plan."
    print(f"\nğŸ‘¤ User: {query2} (Requesting Structured Output)")
    
    # ä½¿ç”¨ run() å¹¶æŒ‡å®š response_model
    result = await agent.run(query2, response_model=AnalysisReport)
    print(f"ğŸ“¦ Structured Result:\n{result.model_dump_json(indent=2)}\n")

    print("-" * 50)

    # --- åœºæ™¯ 3: æµå¼è¾“å‡º ---
    query3 = "Tell me a short story about a Gecko programmer."
    print(f"\nğŸ‘¤ User: {query3} (Streaming Mode)")
    print("ğŸŒŠ Stream: ", end="", flush=True)
    
    async for chunk in agent.stream(query3):
        print(chunk, end="", flush=True)
    print("\n")

if __name__ == "__main__":
    # ç¡®ä¿å®‰è£…äº† litellm
    # pip install litellm
    asyncio.run(main())
```

[3] examples/core/events_demo.py
```python
import asyncio
import time
from typing import Optional

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from gecko.core.events import EventBus, BaseEvent
from gecko.core.logging import get_logger

logger = get_logger(__name__)

# ==========================================
# 1. å®šä¹‰è‡ªå®šä¹‰äº‹ä»¶
# ==========================================

class UserLoginEvent(BaseEvent):
    """ç”¨æˆ·ç™»å½•äº‹ä»¶"""
    type: str = "user.login"
    
class OrderCreatedEvent(BaseEvent):
    """è®¢å•åˆ›å»ºäº‹ä»¶"""
    type: str = "order.created"

# ==========================================
# 2. å®šä¹‰å¤„ç†å™¨ (Handlers)
# ==========================================

async def async_logger(event: BaseEvent):
    """å¼‚æ­¥æ—¥å¿—å¤„ç†å™¨"""
    # æ¨¡æ‹Ÿè€—æ—¶ I/O
    await asyncio.sleep(0.1)
    print(f"ğŸ“ [Async Logger] {event.type}: {event.data}")

def sync_metrics(event: BaseEvent):
    """åŒæ­¥æŒ‡æ ‡ç»Ÿè®¡å¤„ç†å™¨"""
    print(f"ğŸ“Š [Sync Metrics] Counting event: {event.type}")

async def slow_processor(event: BaseEvent):
    """æ…¢é€Ÿå¤„ç†å™¨ï¼ˆç”¨äºæ¼”ç¤ºåå°ä»»åŠ¡ç­‰å¾…ï¼‰"""
    print(f"â³ [Slow Proc] Start processing {event.type}...")
    await asyncio.sleep(1.0) # æ¨¡æ‹Ÿé•¿ä»»åŠ¡
    print(f"âœ… [Slow Proc] Finished {event.type}")

# ==========================================
# 3. å®šä¹‰ä¸­é—´ä»¶ (Middleware)
# ==========================================

async def audit_middleware(event: BaseEvent) -> Optional[BaseEvent]:
    """å®¡è®¡ä¸­é—´ä»¶ï¼šç»™æ‰€æœ‰äº‹ä»¶æ·»åŠ å®¡è®¡æ—¶é—´æˆ³"""
    event.data["audit_ts"] = time.time()
    return event

async def spam_filter_middleware(event: BaseEvent) -> Optional[BaseEvent]:
    """åƒåœ¾è¿‡æ»¤ä¸­é—´ä»¶ï¼šæ‹¦æˆªåŒ…å« 'spam' çš„äº‹ä»¶"""
    if event.data.get("is_spam"):
        print(f"ğŸš« [Middleware] Blocked spam event: {event.type}")
        return None  # è¿”å› None æ‹¦æˆªäº‹ä»¶
    return event

# ==========================================
# 4. ä¸»æ¼”ç¤ºæµç¨‹
# ==========================================

async def main():
    print("ğŸš€ Starting EventBus Demo...\n")

    # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨å¤„ç† shutdown
    async with EventBus() as bus:
        
        # --- æ³¨å†Œç»„ä»¶ ---
        print("1ï¸âƒ£  Registering Handlers & Middleware")
        
        # è®¢é˜…ç‰¹å®šäº‹ä»¶
        bus.subscribe("user.login", async_logger)
        bus.subscribe("user.login", sync_metrics)
        
        # è®¢é˜…æ‰€æœ‰äº‹ä»¶ (é€šé…ç¬¦)
        bus.subscribe("*", lambda e: print(f"ğŸ‘€ [Global Watcher] Saw {e.type}"))
        
        # æ³¨å†Œä¸­é—´ä»¶
        bus.add_middleware(audit_middleware)
        bus.add_middleware(spam_filter_middleware)
        print("   Done.\n")

        # --- åœºæ™¯ 1: æ­£å¸¸å‘å¸ƒ (ç­‰å¾…æ¨¡å¼) ---
        print("2ï¸âƒ£  Publishing User Login (Wait=True)")
        login_event = UserLoginEvent(data={"user_id": 101, "ip": "127.0.0.1"})
        
        await bus.publish(login_event, wait=True)
        # æ­¤æ—¶ async_logger å·²ç»æ‰§è¡Œå®Œæ¯•
        print("   Event processing completed.\n")

        # --- åœºæ™¯ 2: ä¸­é—´ä»¶æ‹¦æˆª ---
        print("3ï¸âƒ£  Publishing Spam Event")
        spam_event = OrderCreatedEvent(data={"order_id": 999, "is_spam": True})
        
        await bus.publish(spam_event, wait=True)
        print("   Spam event published (should be blocked).\n")

        # --- åœºæ™¯ 3: åå°ä»»åŠ¡ (ä¸ç­‰å¾…) ---
        print("4ï¸âƒ£  Publishing Slow Event (Wait=False)")
        
        # ä¸´æ—¶è®¢é˜…ä¸€ä¸ªæ…¢é€Ÿä»»åŠ¡
        bus.subscribe("order.created", slow_processor)
        
        order_event = OrderCreatedEvent(data={"order_id": 202, "amount": 50.0})
        
        # è¿™é‡Œä¸ä¼šé˜»å¡ 1ç§’ï¼Œè€Œæ˜¯ç«‹å³è¿”å›
        start_time = time.time()
        await bus.publish(order_event, wait=False)
        print(f"   Publish returned in {time.time() - start_time:.4f}s (Non-blocking)")
        print("   Main logic continues doing other work...\n")

    # --- è‡ªåŠ¨ Shutdown ---
    # é€€å‡º async with å—æ—¶ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨ shutdown(wait=True)
    # è¿™å°†ç­‰å¾…ä¸Šé¢çš„ slow_processor æ‰§è¡Œå®Œæ¯•
    print("5ï¸âƒ£  EventBus Shutdown")
    print("   Context manager exited. All background tasks should be finished now.")

if __name__ == "__main__":
    asyncio.run(main())
```

[4] examples/core/memory_demo.py
```python
# examples/memory_demo.py
import asyncio
from gecko.core.memory import TokenMemory
from gecko.core.message import Message
from gecko.plugins.storage.sqlite import SQLiteSessionStorage


async def main():
    # 1. åˆ›å»ºå­˜å‚¨
    storage = SQLiteSessionStorage("sqlite://./test.db")
    
    # 2. åˆ›å»º TokenMemory
    memory = TokenMemory(
        session_id="user_123",
        storage=storage,
        max_tokens=4000,
        model_name="gpt-4",
        cache_size=1000,
        max_message_length=10000
    )
    
    print(memory)
    
    # 3. è®¡ç®—å•æ¡æ¶ˆæ¯
    msg = Message.user("What's the weather today?")
    tokens = memory.count_message_tokens(msg)
    print(f"\nå•æ¡æ¶ˆæ¯ tokens: {tokens}")
    
    # 4. æ‰¹é‡è®¡ç®—
    messages = [
        Message.user("Hello"),
        Message.assistant("Hi there!"),
        Message.user("How are you?"),
    ]
    
    # ä½¿ç”¨ç¼“å­˜
    counts1 = memory.count_messages_batch(messages, use_cache=True)
    print(f"\næ‰¹é‡è®¡æ•°ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰: {counts1}")
    
    # ä¸ä½¿ç”¨ç¼“å­˜ï¼ˆæ›´å¿«ï¼Œä½†ä¸ç¼“å­˜ç»“æœï¼‰
    counts2 = memory.count_messages_batch(messages, use_cache=False)
    print(f"æ‰¹é‡è®¡æ•°ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰: {counts2}")
    
    # 5. æµ‹è¯•ç¼“å­˜æ€§èƒ½
    print("\n=== ç¼“å­˜æ€§èƒ½æµ‹è¯• ===")
    
    # é¦–æ¬¡è®¡æ•°ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
    for _ in range(3):
        memory.count_message_tokens(msg)
    
    # æŸ¥çœ‹ç»Ÿè®¡
    memory.print_cache_stats()
    
    # 6. å†å²åŠ è½½æµ‹è¯•
    print("\n=== å†å²åŠ è½½æµ‹è¯• ===")
    
    # æ„é€ å¤§é‡å†å²æ¶ˆæ¯
    raw_messages = [
        {"role": "system", "content": "You are a helpful assistant."},
    ]
    
    for i in range(100):
        raw_messages.append({
            "role": "user",
            "content": f"Question {i}: Can you help me?"
        })
        raw_messages.append({
            "role": "assistant",
            "content": f"Answer {i}: Of course! I'm here to help."
        })
    
    # åŠ è½½å†å²ï¼ˆè‡ªåŠ¨è£å‰ªï¼‰
    history = await memory.get_history(raw_messages)
    
    print(f"åŸå§‹æ¶ˆæ¯æ•°: {len(raw_messages)}")
    print(f"åŠ è½½åæ¶ˆæ¯æ•°: {len(history)}")
    print(f"æ€» tokens: {sum(memory.count_message_tokens(m) for m in history)}")
    
    # 7. æ¸…ç©ºç¼“å­˜
    memory.clear_cache()
    print("\nç¼“å­˜å·²æ¸…ç©º")
    memory.print_cache_stats()


if __name__ == "__main__":
    asyncio.run(main())
```

[5] examples/core/message_demo.py
```python
# examples/message_demo.py
import asyncio
from gecko.core.message import Message, MediaResource


async def main():
    print("=== Gecko Message ç¤ºä¾‹ ===\n")
    
    # 1. ç®€å•æ–‡æœ¬æ¶ˆæ¯
    print("1. ç®€å•æ–‡æœ¬æ¶ˆæ¯")
    user_msg = Message.user("Hello, how are you?")
    print(f"   {user_msg}")
    print(f"   OpenAI æ ¼å¼: {user_msg.to_openai_format()}\n")
    
    # 2. åŠ©æ‰‹æ¶ˆæ¯
    print("2. åŠ©æ‰‹æ¶ˆæ¯")
    assistant_msg = Message.assistant("I'm doing great, thanks!")
    print(f"   {assistant_msg}\n")
    
    # 3. ç³»ç»Ÿæ¶ˆæ¯
    print("3. ç³»ç»Ÿæ¶ˆæ¯")
    system_msg = Message.system("You are a helpful assistant.")
    print(f"   {system_msg}\n")
    
    # 4. å¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆåŒæ­¥ï¼‰
    print("4. å¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆåŒæ­¥ï¼‰")
    # å‡è®¾æœ‰æœ¬åœ°å›¾ç‰‡
    # multimodal_msg = Message.user(
    #     text="What's in this image?",
    #     images=["./test_image.jpg"]
    # )
    # print(f"   {multimodal_msg}")
    # print(f"   åŒ…å« {multimodal_msg.get_image_count()} å¼ å›¾ç‰‡\n")
    
    # 5. å¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆå¼‚æ­¥ï¼‰
    print("5. å¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆå¼‚æ­¥ï¼‰")
    # async_msg = await Message.user_async(
    #     text="Analyze these images",
    #     images=["./image1.jpg", "./image2.jpg"]
    # )
    # print(f"   {async_msg}\n")
    
    # 6. å·¥å…·è¿”å›æ¶ˆæ¯
    print("6. å·¥å…·è¿”å›æ¶ˆæ¯")
    tool_msg = Message.tool_result(
        tool_call_id="call_123",
        content={"result": "Search completed", "count": 42},
        tool_name="search"
    )
    print(f"   {tool_msg}")
    print(f"   å†…å®¹: {tool_msg.get_text_content()[:50]}\n")
    
    # 7. ä» OpenAI æ ¼å¼è§£æ
    print("7. ä» OpenAI æ ¼å¼è§£æ")
    openai_format = {
        "role": "assistant",
        "content": "Here's what I found...",
        "tool_calls": [
            {
                "id": "call_1",
                "function": {
                    "name": "search",
                    "arguments": '{"query": "test"}'
                }
            }
        ]
    }
    parsed_msg = Message.from_openai(openai_format)
    print(f"   {parsed_msg}\n")
    
    # 8. æ¶ˆæ¯å·¥å…·æ–¹æ³•
    print("8. æ¶ˆæ¯å·¥å…·æ–¹æ³•")
    long_msg = Message.user("This is a very long message " * 20)
    print(f"   åŸå§‹é•¿åº¦: {len(long_msg.get_text_content())}")
    
    truncated = long_msg.truncate_content(50)
    print(f"   æˆªæ–­å: {truncated.get_text_content()}")
    
    print(f"   æ˜¯å¦ä¸ºç©º: {long_msg.is_empty()}")
    print(f"   æ˜¯å¦æœ‰å›¾ç‰‡: {long_msg.has_images()}\n")
    
    # 9. æ¶ˆæ¯å…‹éš†
    print("9. æ¶ˆæ¯å…‹éš†")
    cloned = user_msg.clone()
    print(f"   åŸå§‹: {user_msg}")
    print(f"   å…‹éš†: {cloned}")
    print(f"   æ˜¯å¦ç›¸åŒå¯¹è±¡: {cloned is user_msg}\n")
    
    # 10. MediaResource ç¤ºä¾‹
    print("10. MediaResource ç¤ºä¾‹")
    # ä» URL
    url_resource = MediaResource(
        url="https://example.com/image.jpg",
        detail="high"
    )
    print(f"   URL èµ„æº: {url_resource.to_openai_image_url()}")
    
    # ä» base64
    base64_resource = MediaResource(
        base64_data="iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg==",
        mime_type="image/png"
    )
    print(f"   Base64 å¤§å°ä¼°ç®—: {base64_resource.get_size_estimate()} bytes\n")


if __name__ == "__main__":
    asyncio.run(main())
```

[6] examples/core/output_demo.py
```python
# examples/output_demo.py
from gecko.core.output import (
    AgentOutput,
    TokenUsage,
    create_text_output,
    create_tool_output,
    merge_outputs
)


def main():
    print("=== Gecko AgentOutput ç¤ºä¾‹ ===\n")
    
    # 1. ç®€å•æ–‡æœ¬è¾“å‡º
    print("1. ç®€å•æ–‡æœ¬è¾“å‡º")
    output1 = AgentOutput(content="Hello, how can I help you today?")
    print(f"   {output1}")
    print(f"   æœ‰å†…å®¹: {output1.has_content()}")
    print(f"   æ˜¯å¦ä¸ºç©º: {output1.is_empty()}\n")
    
    # 2. å¸¦ Token ä½¿ç”¨çš„è¾“å‡º
    print("2. å¸¦ Token ä½¿ç”¨ç»Ÿè®¡")
    usage = TokenUsage(
        prompt_tokens=100,
        completion_tokens=50,
        total_tokens=150
    )
    output2 = AgentOutput(
        content="Based on my analysis...",
        usage=usage
    )
    print(f"   {output2}")
    print(f"   Usage: {output2.usage}")
    
    # ä¼°ç®—æˆæœ¬ï¼ˆGPT-4 ä»·æ ¼ç¤ºä¾‹ï¼‰
    cost = usage.get_cost_estimate(
        prompt_price_per_1k=0.03,
        completion_price_per_1k=0.06
    )
    print(f"   ä¼°ç®—æˆæœ¬: ${cost:.4f}\n")
    
    # 3. å¸¦å·¥å…·è°ƒç”¨çš„è¾“å‡º
    print("3. å¸¦å·¥å…·è°ƒç”¨çš„è¾“å‡º")
    output3 = AgentOutput(
        content="I'll search for that information.",
        tool_calls=[
            {
                "id": "call_1",
                "function": {
                    "name": "search",
                    "arguments": '{"query": "AI trends 2024"}'
                }
            },
            {
                "id": "call_2",
                "function": {
                    "name": "calculator",
                    "arguments": '{"expression": "2+2"}'
                }
            }
        ]
    )
    print(f"   {output3}")
    print(f"   å·¥å…·è°ƒç”¨æ•°: {output3.tool_call_count()}")
    print(f"   å·¥å…·åç§°: {output3.get_tool_names()}\n")
    
    # 4. æ ¼å¼åŒ–è¾“å‡º
    print("4. æ ¼å¼åŒ–è¾“å‡º")
    print(output3.format())
    
    # 5. è¾“å‡ºç»Ÿè®¡
    print("5. è¾“å‡ºç»Ÿè®¡")
    stats = output3.get_stats()
    print(f"   ç»Ÿè®¡ä¿¡æ¯: {stats}\n")
    
    # 6. è½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼
    print("6. è½¬æ¢ä¸º OpenAI æ¶ˆæ¯æ ¼å¼")
    msg_dict = output3.to_message_dict()
    print(f"   {msg_dict}\n")
    
    # 7. ä½¿ç”¨å·¥å…·å‡½æ•°å¿«é€Ÿåˆ›å»º
    print("7. ä½¿ç”¨å·¥å…·å‡½æ•°")
    quick_output = create_text_output(
        "Quick response",
        usage=TokenUsage(prompt_tokens=10, completion_tokens=5),
        source="demo"
    )
    print(f"   {quick_output}")
    print(f"   å…ƒæ•°æ®: {quick_output.metadata}\n")
    
    # 8. åˆå¹¶å¤šä¸ªè¾“å‡º
    print("8. åˆå¹¶å¤šä¸ªè¾“å‡º")
    out1 = AgentOutput(content="Part 1", usage=TokenUsage(prompt_tokens=10, completion_tokens=5))
    out2 = AgentOutput(content="Part 2", usage=TokenUsage(prompt_tokens=20, completion_tokens=10))
    
    merged = merge_outputs([out1, out2])
    print(f"   åˆå¹¶å†…å®¹: {merged.content}")
    print(f"   åˆå¹¶ usage: {merged.usage}\n")
    
    # 9. è¾“å‡ºé¢„è§ˆ
    print("9. æ–‡æœ¬é¢„è§ˆ")
    long_output = AgentOutput(content="A" * 200)
    preview = long_output.get_text_preview(50)
    print(f"   é¢„è§ˆ: {preview}\n")
    
    # 10. å¸ƒå°”å€¼è½¬æ¢
    print("10. å¸ƒå°”å€¼è½¬æ¢")
    empty_output = AgentOutput()
    print(f"   ç©ºè¾“å‡º: bool(empty_output) = {bool(empty_output)}")
    print(f"   æœ‰æ•ˆè¾“å‡º: bool(output1) = {bool(output1)}")


if __name__ == "__main__":
    main()
```

[7] examples/core/prompt_demo.py
```python
# examples/prompt_demo.py
from gecko.core.prompt import PromptTemplate, PromptLibrary


def main():
    print("=== Gecko Prompt æ¨¡æ¿ç¤ºä¾‹ ===\n")
    
    # 1. åŸºç¡€æ¨¡æ¿
    print("1. åŸºç¡€æ¨¡æ¿")
    template = PromptTemplate(
        template="Hello, {{ name }}! You are {{ age }} years old.",
        input_variables=["name", "age"]
    )
    result = template.format(name="Alice", age=25)
    print(f"   ç»“æœ: {result}\n")
    
    # 2. å¸¦æ¡ä»¶çš„æ¨¡æ¿
    print("2. å¸¦æ¡ä»¶çš„æ¨¡æ¿")
    conditional = PromptTemplate(
        template="""
{% if premium %}
Welcome, Premium User {{ name }}!
You have access to advanced features.
{% else %}
Welcome, {{ name }}!
Consider upgrading to Premium.
{% endif %}
        """,
        input_variables=["name", "premium"]
    )
    print(conditional.format(name="Bob", premium=True))
    print(conditional.format(name="Charlie", premium=False))
    
    # 3. å¸¦å¾ªç¯çš„æ¨¡æ¿
    print("3. å¸¦å¾ªç¯çš„æ¨¡æ¿")
    loop_template = PromptTemplate(
        template="""
Available tools:
{% for tool in tools %}
- {{ tool.name }}: {{ tool.description }}
{% endfor %}
        """,
        input_variables=["tools"]
    )
    tools = [
        {"name": "search", "description": "Search the web"},
        {"name": "calculator", "description": "Perform calculations"},
    ]
    print(loop_template.format(tools=tools))
    
    # 4. è‡ªåŠ¨æå–å˜é‡
    print("4. è‡ªåŠ¨æå–å˜é‡")
    auto_template = PromptTemplate(
        template="User {{ user }} asked: {{ question }}"
    )
    detected_vars = auto_template.get_variables_from_template()
    print(f"   æ£€æµ‹åˆ°çš„å˜é‡: {detected_vars}")
    auto_template.input_variables = list(detected_vars)
    print(f"   æ ¼å¼åŒ–: {auto_template.format(user='Alice', question='What is AI?')}\n")
    
    # 5. éƒ¨åˆ†å¡«å……
    print("5. éƒ¨åˆ†å¡«å……")
    partial_template = PromptTemplate(
        template="Translate {{ text }} from {{ source }} to {{ target }}",
        input_variables=["text", "source", "target"]
    )
    # å›ºå®šæºè¯­è¨€å’Œç›®æ ‡è¯­è¨€
    partial = partial_template.partial(source="English", target="Chinese")
    print(f"   å‰©ä½™å˜é‡: {partial.input_variables}")
    print(f"   ç»“æœ: {partial.format(text='Hello')}\n")
    
    # 6. å®‰å…¨æ ¼å¼åŒ–ï¼ˆç¼ºå°‘å˜é‡ï¼‰
    print("6. å®‰å…¨æ ¼å¼åŒ–")
    result = template.format_safe(name="David")  # ç¼ºå°‘ age
    print(f"   ç»“æœ: {result}\n")
    
    # 7. ä»æ–‡ä»¶åŠ è½½ï¼ˆç¤ºä¾‹ï¼‰
    # print("7. ä»æ–‡ä»¶åŠ è½½")
    # template = PromptTemplate.from_file("./prompts/system.txt")
    
    # 8. Few-shot æ¨¡æ¿
    print("8. Few-shot å­¦ä¹ æ¨¡æ¿")
    examples = [
        {"input": "2 + 2", "output": "4"},
        {"input": "5 + 3", "output": "8"},
        {"input": "10 - 7", "output": "3"},
    ]
    few_shot = PromptTemplate.from_examples(
        examples,
        template="Q: {{ input }}\nA: {{ output }}"
    )
    print(few_shot.template)
    print()
    
    # 9. ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿
    print("9. ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿åº“")
    react_template = PromptLibrary.get_react_prompt()
    react_prompt = react_template.format(
        tools=[
            {"name": "search", "description": "Search the web"},
        ],
        question="What is the capital of France?"
    )
    print(react_prompt)
    print()
    
    # 10. æ‘˜è¦æ¨¡æ¿
    print("10. æ‘˜è¦æ¨¡æ¿")
    summary_template = PromptLibrary.get_summarization_prompt()
    summary_prompt = summary_template.format(
        text="This is a very long text that needs to be summarized...",
        max_words=20
    )
    print(summary_prompt)


if __name__ == "__main__":
    main()
```

[8] examples/core/protocol_demo.py
```python
# examples/protocol_demo.py
"""
Gecko Protocol æ¼”ç¤ºæ–‡ä»¶

å±•ç¤ºå¦‚ä½•å®ç°å’Œä½¿ç”¨ Gecko æ¡†æ¶ä¸­å®šä¹‰çš„å„ç§åè®®ã€‚

åŒ…å«çš„æ¼”ç¤ºï¼š
1. ModelProtocol - åŸºç¡€æ¨¡å‹å®ç°
2. StreamableModelProtocol - æµå¼æ¨¡å‹å®ç°
3. StorageProtocol - å­˜å‚¨åç«¯å®ç°
4. ToolProtocol - å·¥å…·å®ç°
5. EmbedderProtocol - åµŒå…¥æ¨¡å‹å®ç°
6. RunnableProtocol - å¯è¿è¡Œå¯¹è±¡å®ç°
7. VectorStoreProtocol - å‘é‡å­˜å‚¨å®ç°
8. èƒ½åŠ›æ£€æµ‹å’ŒéªŒè¯

è¿è¡Œæ–¹å¼:
    python examples/protocol_demo.py
"""

import asyncio
from typing import Any, AsyncIterator, Dict, List, Optional

from gecko.core.protocols import (
    # åè®®
    ModelProtocol,
    StreamableModelProtocol,
    StorageProtocol,
    ToolProtocol,
    EmbedderProtocol,
    RunnableProtocol,
    VectorStoreProtocol,
    # å“åº”æ¨¡å‹
    CompletionResponse,
    CompletionChoice,
    CompletionUsage,
    StreamChunk,
    # å·¥å…·å‡½æ•°
    check_protocol,
    supports_streaming,
    supports_function_calling,
    supports_vision,
    get_model_name,
    validate_model,
    validate_storage,
    validate_tool,
)


# ==================== 1. ModelProtocol ç¤ºä¾‹ ====================

class SimpleModel:
    """
    ç®€å•çš„æ¨¡å‹å®ç°ï¼ˆä»…æ”¯æŒè¡¥å…¨ï¼‰
    
    æ¼”ç¤ºï¼š
    - å®ç° ModelProtocol çš„æœ€å°è¦æ±‚
    - è¿”å›æ ‡å‡†çš„ CompletionResponse
    """
    
    def __init__(self, model_name: str = "simple-model-v1"):
        self.model_name = model_name
        self._call_count = 0
    
    async def acompletion(
        self, 
        messages: List[Dict[str, Any]], 
        **kwargs
    ) -> CompletionResponse:
        """å¼‚æ­¥è¡¥å…¨æ¥å£"""
        self._call_count += 1
        
        # æ¨¡æ‹Ÿ API è°ƒç”¨å»¶è¿Ÿ
        await asyncio.sleep(0.1)
        
        # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        user_message = next(
            (msg["content"] for msg in reversed(messages) if msg["role"] == "user"),
            "No user message"
        )
        
        # æ„å»ºå“åº”
        response_content = f"Echo: {user_message} (call #{self._call_count})"
        
        return CompletionResponse(
            id=f"cmpl-{self._call_count}",
            model=self.model_name,
            created=1234567890,
            choices=[
                CompletionChoice(
                    index=0,
                    message={
                        "role": "assistant",
                        "content": response_content
                    },
                    finish_reason="stop"
                )
            ],
            usage=CompletionUsage(
                prompt_tokens=len(user_message),
                completion_tokens=len(response_content),
                total_tokens=len(user_message) + len(response_content)
            )
        )


# ==================== 2. StreamableModelProtocol ç¤ºä¾‹ ====================

class StreamingModel:
    """
    æ”¯æŒæµå¼è¾“å‡ºçš„æ¨¡å‹
    
    æ¼”ç¤ºï¼š
    - å®ç° StreamableModelProtocol
    - æµå¼ç”Ÿæˆå“åº”å†…å®¹
    """
    
    def __init__(self, model_name: str = "streaming-model-v1"):
        self.model_name = model_name
        self._supports_function_calling = True
        self._supports_vision = False
    
    async def acompletion(
        self, 
        messages: List[Dict[str, Any]], 
        **kwargs
    ) -> CompletionResponse:
        """éæµå¼è¡¥å…¨"""
        await asyncio.sleep(0.1)
        
        user_message = next(
            (msg["content"] for msg in reversed(messages) if msg["role"] == "user"),
            ""
        )
        
        response_content = f"Complete response to: {user_message}"
        
        return CompletionResponse(
            id="cmpl-streaming",
            model=self.model_name,
            choices=[
                CompletionChoice(
                    message={"role": "assistant", "content": response_content},
                    finish_reason="stop"
                )
            ]
        )
    
    async def astream(
        self, 
        messages: List[Dict[str, Any]], 
        **kwargs
    ) -> AsyncIterator[StreamChunk]:
        """æµå¼è¡¥å…¨"""
        user_message = next(
            (msg["content"] for msg in reversed(messages) if msg["role"] == "user"),
            ""
        )
        
        response_text = f"Streaming response to: {user_message}"
        words = response_text.split()
        
        for i, word in enumerate(words):
            # æ¨¡æ‹Ÿæµå¼å»¶è¿Ÿ
            await asyncio.sleep(0.05)
            
            # ç”Ÿæˆ chunk
            chunk = StreamChunk(
                id="chunk-streaming",
                model=self.model_name,
                choices=[
                    {
                        "index": 0,
                        "delta": {"content": word + " "},
                        "finish_reason": None if i < len(words) - 1 else "stop"
                    }
                ]
            )
            
            yield chunk


# ==================== 3. StorageProtocol ç¤ºä¾‹ ====================

class MemoryStorage:
    """
    å†…å­˜å­˜å‚¨åç«¯ï¼ˆç”¨äºæ¼”ç¤ºå’Œæµ‹è¯•ï¼‰
    
    æ¼”ç¤ºï¼š
    - å®ç° StorageProtocol
    - æ”¯æŒ TTLï¼ˆè¿‡æœŸæ—¶é—´ï¼‰
    """
    
    def __init__(self):
        self._data: Dict[str, Dict[str, Any]] = {}
    
    async def get(self, key: str) -> Optional[Dict[str, Any]]:
        """è·å–æ•°æ®"""
        if key not in self._data:
            return None
        
        item = self._data[key]
        
        # æ£€æŸ¥ TTL
        if "ttl" in item and "stored_at" in item:
            import time
            age = time.time() - item["stored_at"]
            if age > item["ttl"]:
                # å·²è¿‡æœŸï¼Œåˆ é™¤å¹¶è¿”å› None
                del self._data[key]
                return None
        
        return item.get("value")
    
    async def set(
        self, 
        key: str, 
        value: Dict[str, Any], 
        ttl: Optional[int] = None
    ) -> None:
        """å­˜å‚¨æ•°æ®"""
        import time
        
        self._data[key] = {
            "value": value,
            "stored_at": time.time(),
        }
        
        if ttl is not None:
            self._data[key]["ttl"] = ttl
    
    async def delete(self, key: str) -> bool:
        """åˆ é™¤æ•°æ®"""
        if key in self._data:
            del self._data[key]
            return True
        return False
    
    async def exists(self, key: str) -> bool:
        """æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨ï¼ˆåŒ…å«è¿‡æœŸæ£€æŸ¥ï¼‰"""
        result = await self.get(key)
        return result is not None
    
    async def clear(self) -> None:
        """æ¸…ç©ºæ‰€æœ‰æ•°æ®"""
        self._data.clear()


# ==================== 4. ToolProtocol ç¤ºä¾‹ ====================

class CalculatorTool:
    """
    è®¡ç®—å™¨å·¥å…·
    
    æ¼”ç¤ºï¼š
    - å®ç° ToolProtocol
    - å®šä¹‰å‚æ•° Schema
    - å®‰å…¨æ‰§è¡Œç”¨æˆ·è¾“å…¥
    """
    
    name = "calculator"
    description = "Execute mathematical expressions safely"
    parameters = {
        "type": "object",
        "properties": {
            "expression": {
                "type": "string",
                "description": "The mathematical expression to evaluate (e.g., '2+2', '10*5')"
            }
        },
        "required": ["expression"]
    }
    
    async def execute(self, arguments: Dict[str, Any]) -> str:
        """æ‰§è¡Œè®¡ç®—"""
        expression = arguments.get("expression", "")
        
        # å®‰å…¨æ£€æŸ¥ï¼šä»…å…è®¸æ•°å­—å’ŒåŸºæœ¬è¿ç®—ç¬¦
        allowed_chars = set("0123456789+-*/()., ")
        if not all(c in allowed_chars for c in expression):
            return f"Error: Invalid characters in expression '{expression}'"
        
        try:
            # ä½¿ç”¨ evalï¼ˆåœ¨å—æ§ç¯å¢ƒä¸­ï¼‰
            result = eval(expression, {"__builtins__": {}}, {})
            return f"Result: {result}"
        except Exception as e:
            return f"Error: {str(e)}"


class WebSearchTool:
    """
    ç½‘ç»œæœç´¢å·¥å…·ï¼ˆæ¨¡æ‹Ÿï¼‰
    
    æ¼”ç¤ºï¼š
    - æ›´å¤æ‚çš„å‚æ•° Schema
    - å¼‚æ­¥æ“ä½œ
    """
    
    name = "web_search"
    description = "Search the web for information"
    parameters = {
        "type": "object",
        "properties": {
            "query": {
                "type": "string",
                "description": "The search query"
            },
            "max_results": {
                "type": "integer",
                "description": "Maximum number of results to return",
                "default": 5
            },
            "language": {
                "type": "string",
                "description": "Search language (e.g., 'en', 'zh')",
                "default": "en"
            }
        },
        "required": ["query"]
    }
    
    async def execute(self, arguments: Dict[str, Any]) -> str:
        """æ‰§è¡Œæœç´¢"""
        query = arguments["query"]
        max_results = arguments.get("max_results", 5)
        language = arguments.get("language", "en")
        
        # æ¨¡æ‹Ÿæœç´¢å»¶è¿Ÿ
        await asyncio.sleep(0.2)
        
        # æ¨¡æ‹Ÿæœç´¢ç»“æœ
        results = [
            f"Result {i+1}: Information about '{query}' (lang: {language})"
            for i in range(max_results)
        ]
        
        return "\n".join(results)


# ==================== 5. EmbedderProtocol ç¤ºä¾‹ ====================

class SimpleEmbedder:
    """
    ç®€å•çš„åµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨éšæœºå‘é‡æ¼”ç¤ºï¼‰
    
    æ¼”ç¤ºï¼š
    - å®ç° EmbedderProtocol
    - æ‰¹é‡å¤„ç†æ–‡æœ¬
    """
    
    def __init__(self, dimension: int = 384):
        self._dimension = dimension
    
    async def embed(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡åµŒå…¥æ–‡æœ¬"""
        import random
        
        # æ¨¡æ‹Ÿ API å»¶è¿Ÿ
        await asyncio.sleep(0.1)
        
        # ç”Ÿæˆéšæœºå‘é‡ï¼ˆå®é™…åº”è°ƒç”¨çœŸå®çš„åµŒå…¥æ¨¡å‹ï¼‰
        embeddings = []
        for text in texts:
            # ä½¿ç”¨æ–‡æœ¬é•¿åº¦ä½œä¸ºç§å­ï¼Œä¿æŒä¸€è‡´æ€§
            random.seed(len(text))
            embedding = [random.random() for _ in range(self._dimension)]
            embeddings.append(embedding)
        
        return embeddings
    
    async def embed_single(self, text: str) -> List[float]:
        """åµŒå…¥å•ä¸ªæ–‡æœ¬"""
        results = await self.embed([text])
        return results[0]
    
    def get_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        return self._dimension


# ==================== 6. RunnableProtocol ç¤ºä¾‹ ====================

class SimpleAgent:
    """
    ç®€å•çš„å¯è¿è¡Œ Agent
    
    æ¼”ç¤ºï¼š
    - å®ç° RunnableProtocol
    - ç»Ÿä¸€çš„è¿è¡Œæ¥å£
    """
    
    def __init__(self, name: str = "SimpleAgent"):
        self.name = name
    
    async def run(self, input: Any) -> str:
        """è¿è¡Œ Agent"""
        # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
        if isinstance(input, str):
            query = input
        elif isinstance(input, dict):
            query = input.get("query", str(input))
        else:
            query = str(input)
        
        # æ¨¡æ‹Ÿå¤„ç†
        await asyncio.sleep(0.1)
        
        return f"[{self.name}] Processed: {query}"


# ==================== 7. VectorStoreProtocol ç¤ºä¾‹ ====================

class SimpleVectorStore:
    """
    ç®€å•çš„å‘é‡å­˜å‚¨ï¼ˆå†…å­˜å®ç°ï¼‰
    
    æ¼”ç¤ºï¼š
    - å®ç° VectorStoreProtocol
    - ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢
    """
    
    def __init__(self):
        self._vectors: Dict[str, List[float]] = {}
        self._metadata: Dict[str, Dict[str, Any]] = {}
    
    async def add(
        self,
        ids: List[str],
        vectors: List[List[float]],
        metadata: Optional[List[Dict[str, Any]]] = None,
    ) -> None:
        """æ·»åŠ å‘é‡"""
        for i, vector_id in enumerate(ids):
            self._vectors[vector_id] = vectors[i]
            if metadata and i < len(metadata):
                self._metadata[vector_id] = metadata[i]
    
    async def search(
        self,
        query_vector: List[float],
        top_k: int = 5,
        filters: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸ä¼¼å‘é‡"""
        import math
        
        def cosine_similarity(v1: List[float], v2: List[float]) -> float:
            """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
            dot_product = sum(a * b for a, b in zip(v1, v2))
            magnitude1 = math.sqrt(sum(a * a for a in v1))
            magnitude2 = math.sqrt(sum(b * b for b in v2))
            
            if magnitude1 == 0 or magnitude2 == 0:
                return 0.0
            
            return dot_product / (magnitude1 * magnitude2)
        
        # è®¡ç®—æ‰€æœ‰å‘é‡çš„ç›¸ä¼¼åº¦
        similarities = []
        for vector_id, vector in self._vectors.items():
            # åº”ç”¨è¿‡æ»¤å™¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if filters:
                metadata = self._metadata.get(vector_id, {})
                if not all(metadata.get(k) == v for k, v in filters.items()):
                    continue
            
            score = cosine_similarity(query_vector, vector)
            similarities.append({
                "id": vector_id,
                "score": score,
                "metadata": self._metadata.get(vector_id, {})
            })
        
        # æ’åºå¹¶è¿”å› top_k
        similarities.sort(key=lambda x: x["score"], reverse=True)
        return similarities[:top_k]
    
    async def delete(self, ids: List[str]) -> None:
        """åˆ é™¤å‘é‡"""
        for vector_id in ids:
            self._vectors.pop(vector_id, None)
            self._metadata.pop(vector_id, None)
    
    async def update(
        self,
        ids: List[str],
        metadata: List[Dict[str, Any]],
    ) -> None:
        """æ›´æ–°å…ƒæ•°æ®"""
        for i, vector_id in enumerate(ids):
            if vector_id in self._vectors and i < len(metadata):
                self._metadata[vector_id] = metadata[i]


# ==================== æ¼”ç¤ºå‡½æ•° ====================

async def demo_model_protocol():
    """æ¼”ç¤º ModelProtocol"""
    print("\n" + "=" * 60)
    print("1. ModelProtocol æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleModel()
    
    # éªŒè¯åè®®
    print(f"âœ“ å®ç°äº† ModelProtocol: {check_protocol(model, ModelProtocol)}")
    print(f"âœ“ æ”¯æŒæµå¼: {supports_streaming(model)}")
    print(f"âœ“ æ¨¡å‹åç§°: {get_model_name(model)}")
    
    # è°ƒç”¨æ¨¡å‹
    messages = [
        {"role": "user", "content": "Hello, world!"}
    ]
    
    response = await model.acompletion(messages)
    print(f"\nè¯·æ±‚: {messages[0]['content']}")
    print(f"å“åº”: {response.choices[0].message['content']}")
    print(f"Token ä½¿ç”¨: {response.usage.total_tokens}")


async def demo_streaming_model():
    """æ¼”ç¤º StreamableModelProtocol"""
    print("\n" + "=" * 60)
    print("2. StreamableModelProtocol æ¼”ç¤º")
    print("=" * 60)
    
    model = StreamingModel()
    
    print(f"âœ“ å®ç°äº† StreamableModelProtocol: {check_protocol(model, StreamableModelProtocol)}")
    print(f"âœ“ æ”¯æŒæµå¼: {supports_streaming(model)}")
    print(f"âœ“ æ”¯æŒå‡½æ•°è°ƒç”¨: {supports_function_calling(model)}")
    
    # æµå¼è°ƒç”¨
    messages = [{"role": "user", "content": "Tell me a story"}]
    
    print("\næµå¼è¾“å‡º: ", end="")
    async for chunk in model.astream(messages):
        if chunk.content:
            print(chunk.content, end="", flush=True)
    print()


async def demo_storage_protocol():
    """æ¼”ç¤º StorageProtocol"""
    print("\n" + "=" * 60)
    print("3. StorageProtocol æ¼”ç¤º")
    print("=" * 60)
    
    storage = MemoryStorage()
    
    print(f"âœ“ å®ç°äº† StorageProtocol: {check_protocol(storage, StorageProtocol)}")
    validate_storage(storage)
    
    # å­˜å‚¨æ•°æ®
    await storage.set("user:123", {"name": "Alice", "age": 25})
    print("\nâœ“ å­˜å‚¨æ•°æ®: user:123")
    
    # è·å–æ•°æ®
    data = await storage.get("user:123")
    print(f"âœ“ è·å–æ•°æ®: {data}")
    
    # å¸¦ TTL çš„å­˜å‚¨
    await storage.set("temp:session", {"token": "abc123"}, ttl=2)
    print("\nâœ“ å­˜å‚¨ä¸´æ—¶æ•°æ® (TTL: 2ç§’)")
    
    print("âœ“ ç«‹å³è·å–:", await storage.get("temp:session"))
    
    print("âœ“ ç­‰å¾… 3 ç§’...")
    await asyncio.sleep(3)
    
    print("âœ“ å†æ¬¡è·å–:", await storage.get("temp:session"))


async def demo_tool_protocol():
    """æ¼”ç¤º ToolProtocol"""
    print("\n" + "=" * 60)
    print("4. ToolProtocol æ¼”ç¤º")
    print("=" * 60)
    
    # è®¡ç®—å™¨å·¥å…·
    calc = CalculatorTool()
    
    print(f"âœ“ å·¥å…·åç§°: {calc.name}")
    print(f"âœ“ å·¥å…·æè¿°: {calc.description}")
    print(f"âœ“ å®ç°äº† ToolProtocol: {check_protocol(calc, ToolProtocol)}")
    
    validate_tool(calc)
    
    # æ‰§è¡Œå·¥å…·
    result1 = await calc.execute({"expression": "2 + 2"})
    print(f"\nè®¡ç®— '2 + 2': {result1}")
    
    result2 = await calc.execute({"expression": "10 * 5 + 3"})
    print(f"è®¡ç®— '10 * 5 + 3': {result2}")
    
    # æœç´¢å·¥å…·
    print("\n" + "-" * 60)
    search = WebSearchTool()
    
    print(f"âœ“ å·¥å…·åç§°: {search.name}")
    validate_tool(search)
    
    result = await search.execute({
        "query": "Python asyncio",
        "max_results": 3,
        "language": "en"
    })
    print(f"\næœç´¢ç»“æœ:\n{result}")


async def demo_embedder_protocol():
    """æ¼”ç¤º EmbedderProtocol"""
    print("\n" + "=" * 60)
    print("5. EmbedderProtocol æ¼”ç¤º")
    print("=" * 60)
    
    embedder = SimpleEmbedder(dimension=8)  # ä½¿ç”¨å°ç»´åº¦ä¾¿äºå±•ç¤º
    
    print(f"âœ“ å®ç°äº† EmbedderProtocol: {check_protocol(embedder, EmbedderProtocol)}")
    print(f"âœ“ å‘é‡ç»´åº¦: {embedder.get_dimension()}")
    
    # åµŒå…¥å•ä¸ªæ–‡æœ¬
    text = "Hello, world!"
    embedding = await embedder.embed_single(text)
    print(f"\næ–‡æœ¬: '{text}'")
    print(f"å‘é‡ (å‰5ç»´): {[f'{x:.4f}' for x in embedding[:5]]}")
    
    # æ‰¹é‡åµŒå…¥
    texts = ["Python", "JavaScript", "Rust"]
    embeddings = await embedder.embed(texts)
    print(f"\næ‰¹é‡åµŒå…¥ {len(texts)} ä¸ªæ–‡æœ¬:")
    for text, emb in zip(texts, embeddings):
        print(f"  '{text}': {[f'{x:.4f}' for x in emb[:5]]}")


async def demo_runnable_protocol():
    """æ¼”ç¤º RunnableProtocol"""
    print("\n" + "=" * 60)
    print("6. RunnableProtocol æ¼”ç¤º")
    print("=" * 60)
    
    agent = SimpleAgent(name="DemoAgent")
    
    print(f"âœ“ å®ç°äº† RunnableProtocol: {check_protocol(agent, RunnableProtocol)}")
    
    # ä¸åŒç±»å‹çš„è¾“å…¥
    result1 = await agent.run("What is AI?")
    print(f"\nå­—ç¬¦ä¸²è¾“å…¥: {result1}")
    
    result2 = await agent.run({"query": "Explain Python", "context": "beginner"})
    print(f"å­—å…¸è¾“å…¥: {result2}")


async def demo_vector_store_protocol():
    """æ¼”ç¤º VectorStoreProtocol"""
    print("\n" + "=" * 60)
    print("7. VectorStoreProtocol æ¼”ç¤º")
    print("=" * 60)
    
    store = SimpleVectorStore()
    embedder = SimpleEmbedder(dimension=8)
    
    print(f"âœ“ å®ç°äº† VectorStoreProtocol: {check_protocol(store, VectorStoreProtocol)}")
    
    # å‡†å¤‡æ–‡æ¡£
    documents = [
        "Python is a programming language",
        "JavaScript is used for web development",
        "Machine learning is a subset of AI",
        "Deep learning uses neural networks"
    ]
    
    # ç”ŸæˆåµŒå…¥
    print(f"\næ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°å‘é‡åº“...")
    embeddings = await embedder.embed(documents)
    
    await store.add(
        ids=[f"doc_{i}" for i in range(len(documents))],
        vectors=embeddings,
        metadata=[{"text": doc, "index": i} for i, doc in enumerate(documents)]
    )
    
    # æœç´¢
    query = "What is Python?"
    query_embedding = await embedder.embed_single(query)
    
    print(f"\næŸ¥è¯¢: '{query}'")
    results = await store.search(query_embedding, top_k=2)
    
    print("\næœ€ç›¸ä¼¼çš„æ–‡æ¡£:")
    for i, result in enumerate(results, 1):
        print(f"  {i}. {result['metadata']['text']} (ç›¸ä¼¼åº¦: {result['score']:.4f})")


async def demo_protocol_validation():
    """æ¼”ç¤ºåè®®éªŒè¯"""
    print("\n" + "=" * 60)
    print("8. åè®®éªŒè¯æ¼”ç¤º")
    print("=" * 60)
    
    # æœ‰æ•ˆçš„æ¨¡å‹
    valid_model = SimpleModel()
    try:
        validate_model(valid_model)
        print("âœ“ æœ‰æ•ˆæ¨¡å‹éªŒè¯é€šè¿‡")
    except Exception as e:
        print(f"âœ— éªŒè¯å¤±è´¥: {e}")
    
    # æ— æ•ˆçš„æ¨¡å‹
    class InvalidModel:
        pass
    
    invalid_model = InvalidModel()
    try:
        validate_model(invalid_model)
        print("âœ— æ— æ•ˆæ¨¡å‹åº”è¯¥éªŒè¯å¤±è´¥")
    except TypeError as e:
        print(f"âœ“ æ— æ•ˆæ¨¡å‹æ­£ç¡®æ‹’ç»: {str(e)[:50]}...")
    
    # æ— æ•ˆçš„å·¥å…·ï¼ˆç¼ºå°‘å±æ€§ï¼‰
    class InvalidTool:
        async def execute(self, arguments):
            return "result"
    
    invalid_tool = InvalidTool()
    try:
        validate_tool(invalid_tool)
        print("âœ— æ— æ•ˆå·¥å…·åº”è¯¥éªŒè¯å¤±è´¥")
    except ValueError as e:
        print(f"âœ“ æ— æ•ˆå·¥å…·æ­£ç¡®æ‹’ç»: {str(e)[:50]}...")


# ==================== ä¸»å‡½æ•° ====================

async def main():
    """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("Gecko Protocol æ¼”ç¤º")
    print("=" * 60)
    print("\næœ¬æ¼”ç¤ºå°†å±•ç¤ºå¦‚ä½•å®ç°å’Œä½¿ç”¨ Gecko æ¡†æ¶ä¸­çš„å„ç§åè®®ã€‚")
    
    # è¿è¡Œæ‰€æœ‰æ¼”ç¤º
    await demo_model_protocol()
    await demo_streaming_model()
    await demo_storage_protocol()
    await demo_tool_protocol()
    await demo_embedder_protocol()
    await demo_runnable_protocol()
    await demo_vector_store_protocol()
    await demo_protocol_validation()
    
    print("\n" + "=" * 60)
    print("æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)
    print("\næç¤º:")
    print("- æ‰€æœ‰åè®®éƒ½ä½¿ç”¨ Protocol å®šä¹‰ï¼Œæ”¯æŒé¸­å­ç±»å‹")
    print("- ä½¿ç”¨ check_protocol() è¿›è¡Œè¿è¡Œæ—¶ç±»å‹æ£€æŸ¥")
    print("- ä½¿ç”¨ validate_*() å‡½æ•°è¿›è¡Œå®Œæ•´éªŒè¯")
    print("- å®ç°åè®®æ—¶åªéœ€å®ç°å¿…éœ€çš„æ–¹æ³•å’Œå±æ€§")
    print("\nè¯¦ç»†æ–‡æ¡£è¯·å‚è€ƒ: gecko/core/protocols.py")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())
```

[9] examples/core/session_demo.py
```python
# examples/session_demo.py
import asyncio
from gecko.core.session import Session, SessionManager
from gecko.plugins.storage.sqlite import SQLiteSessionStorage


async def main():
    print("=== Gecko Session ç¤ºä¾‹ ===\n")
    
    # 1. åŸºç¡€ä¼šè¯ä½¿ç”¨
    print("1. åŸºç¡€ä¼šè¯")
    session = Session(session_id="demo_session")
    
    # è®¾ç½®çŠ¶æ€
    session.set("user_name", "Alice")
    session.set("preferences", {"theme": "dark", "language": "zh"})
    
    # è·å–çŠ¶æ€
    name = session.get("user_name")
    prefs = session.get("preferences")
    print(f"   ç”¨æˆ·: {name}")
    print(f"   åå¥½: {prefs}\n")
    
    # 2. ä½¿ç”¨å­—å…¸è¯­æ³•
    print("2. å­—å…¸è¯­æ³•")
    session["score"] = 100
    session["level"] = 5
    
    print(f"   Score: {session['score']}")
    print(f"   Level: {session['level']}")
    print(f"   Keys: {session.keys()}\n")
    
    # 3. TTL å’Œè¿‡æœŸ
    print("3. TTL å’Œè¿‡æœŸ")
    temp_session = Session(session_id="temp", ttl=5)  # 5 ç§’åè¿‡æœŸ
    temp_session.set("data", "temporary")
    
    print(f"   æ˜¯å¦è¿‡æœŸ: {temp_session.is_expired()}")
    print(f"   å‰©ä½™æ—¶é—´: {temp_session.metadata.time_to_expire():.1f}s")
    
    # å»¶é•¿ TTL
    temp_session.extend_ttl(10)
    print(f"   å»¶é•¿åå‰©ä½™: {temp_session.metadata.time_to_expire():.1f}s\n")
    
    # 4. æ ‡ç­¾ç®¡ç†
    print("4. æ ‡ç­¾ç®¡ç†")
    session.add_tag("premium")
    session.add_tag("verified")
    
    print(f"   æ ‡ç­¾: {session.metadata.tags}")
    print(f"   æ˜¯å¦ premium: {session.has_tag('premium')}\n")
    
    # 5. ä¼šè¯ä¿¡æ¯
    print("5. ä¼šè¯ä¿¡æ¯")
    info = session.get_info()
    print(f"   è®¿é—®æ¬¡æ•°: {info['access_count']}")
    print(f"   çŠ¶æ€é”®æ•°: {info['state_keys']}")
    print(f"   åˆ›å»ºæ—¶é—´: {info['created_at']}\n")
    
    # 6. ä¼šè¯å…‹éš†
    print("6. ä¼šè¯å…‹éš†")
    cloned = session.clone(new_id="cloned_session")
    print(f"   åŸå§‹: {session.session_id}")
    print(f"   å…‹éš†: {cloned.session_id}")
    print(f"   å…‹éš†çš„æ•°æ®: {cloned.get('user_name')}\n")
    
    # 7. æŒä¹…åŒ–
    print("7. æŒä¹…åŒ–åˆ°å­˜å‚¨")
    storage = SQLiteSessionStorage("sqlite://:memory:")
    
    persistent_session = Session(
        session_id="persistent",
        storage=storage,
        auto_save=True
    )
    persistent_session.set("important_data", "must be saved")
    
    await persistent_session.save()
    print("   ä¼šè¯å·²ä¿å­˜\n")
    
    # 8. ä¼šè¯ç®¡ç†å™¨
    print("8. ä¼šè¯ç®¡ç†å™¨")
    manager = SessionManager(
        storage=storage,
        default_ttl=3600,  # 1 å°æ—¶
        auto_cleanup=True
    )
    
    # åˆ›å»ºå¤šä¸ªä¼šè¯
    s1 = await manager.create_session(user="Alice", score=100)
    s2 = await manager.create_session(user="Bob", score=200)
    
    print(f"   æ´»è·ƒä¼šè¯æ•°: {manager.get_active_count()}")
    
    # è·å–ä¼šè¯
    retrieved = await manager.get_session(s1.session_id)
    print(f"   æ£€ç´¢åˆ°çš„ç”¨æˆ·: {retrieved.get('user')}\n")
    
    # 9. æ‰¹é‡æ›´æ–°
    print("9. æ‰¹é‡æ›´æ–°")
    session.update({
        "last_login": "2024-01-01",
        "login_count": 42,
        "status": "active"
    })
    print(f"   æ›´æ–°åçš„é”®: {session.keys()}\n")
    
    # 10. æ¸…ç†
    print("10. ä¼šè¯æ¸…ç†")
    await manager.shutdown()
    print("   ç®¡ç†å™¨å·²å…³é—­ï¼Œæ‰€æœ‰ä¼šè¯å·²ä¿å­˜")


if __name__ == "__main__":
    asyncio.run(main())
```

[10] examples/core/structure_demo.py
```python
# examples/structure_demo.py
import asyncio
from pydantic import BaseModel, Field
from gecko.core.structure import StructureEngine, StructureParseError


# å®šä¹‰æ•°æ®æ¨¡å‹
class UserProfile(BaseModel):
    name: str = Field(description="ç”¨æˆ·å")
    age: int = Field(description="å¹´é¾„", ge=0, le=150)
    email: str = Field(description="é‚®ç®±åœ°å€")
    interests: list[str] = Field(default_factory=list, description="å…´è¶£çˆ±å¥½")


class SearchQuery(BaseModel):
    query: str = Field(description="æœç´¢å…³é”®è¯")
    max_results: int = Field(default=5, description="æœ€å¤§ç»“æœæ•°")
    filters: dict = Field(default_factory=dict, description="è¿‡æ»¤æ¡ä»¶")


async def main():
    print("=== Gecko ç»“æ„åŒ–è¾“å‡ºç¤ºä¾‹ ===\n")
    
    # 1. ç”Ÿæˆ OpenAI Tool Schema
    print("1. ç”Ÿæˆ OpenAI Tool Schema")
    schema = StructureEngine.to_openai_tool(SearchQuery)
    print(f"   å·¥å…·å: {schema['function']['name']}")
    print(f"   å‚æ•°: {list(schema['function']['parameters']['properties'].keys())}\n")
    
    # 2. ä» JSON æ–‡æœ¬è§£æ
    print("2. ä»çº¯ JSON æ–‡æœ¬è§£æ")
    json_text = '''
    {
        "name": "Alice",
        "age": 25,
        "email": "alice@example.com",
        "interests": ["AI", "Python", "Music"]
    }
    '''
    
    try:
        user = await StructureEngine.parse(json_text, UserProfile)
        print(f"   è§£ææˆåŠŸ: {user.name}, {user.age} å²")
        print(f"   å…´è¶£: {', '.join(user.interests)}\n")
    except StructureParseError as e:
        print(f"   è§£æå¤±è´¥: {e}\n")
    
    # 3. ä» Markdown ä»£ç å—è§£æ
    print("3. ä» Markdown ä»£ç å—è§£æ")
    markdown_text = '''
    è¿™æ˜¯ç”¨æˆ·ä¿¡æ¯ï¼š
    
    ```json
    {
        "name": "Bob",
        "age": 30,
        "email": "bob@example.com"
    }
    ```
    
    ä»¥ä¸Šæ˜¯æå–çš„æ•°æ®ã€‚
    '''
    
    try:
        user = await StructureEngine.parse(markdown_text, UserProfile)
        print(f"   è§£ææˆåŠŸ: {user.name}\n")
    except StructureParseError as e:
        print(f"   è§£æå¤±è´¥: {e}\n")
    
    # 4. ä»å·¥å…·è°ƒç”¨è§£æ
    print("4. ä»å·¥å…·è°ƒç”¨è§£æ")
    tool_calls = [
        {
            "id": "call_123",
            "function": {
                "name": "search",
                "arguments": '{"query": "Python tutorials", "max_results": 10}'
            }
        }
    ]
    
    try:
        query = await StructureEngine.parse(
            content="",
            model_class=SearchQuery,
            raw_tool_calls=tool_calls
        )
        print(f"   æŸ¥è¯¢: {query.query}")
        print(f"   æœ€å¤§ç»“æœæ•°: {query.max_results}\n")
    except StructureParseError as e:
        print(f"   è§£æå¤±è´¥: {e}\n")
    
    # 5. å¤„ç†æ ¼å¼é—®é¢˜ï¼ˆè‡ªåŠ¨æ¸…ç†ï¼‰
    print("5. å¤„ç†æ ¼å¼é—®é¢˜ï¼ˆè‡ªåŠ¨æ¸…ç†ï¼‰")
    dirty_json = '''
    {
        "name": "Charlie",
        "age": 35,
        "email": "charlie@example.com",  // è¿™æ˜¯æ³¨é‡Š
        "interests": ["reading", "coding",],  // å°¾éƒ¨é€—å·
    }
    '''
    
    try:
        user = await StructureEngine.parse(
            dirty_json,
            UserProfile,
            auto_fix=True
        )
        print(f"   æ¸…ç†åè§£ææˆåŠŸ: {user.name}\n")
    except StructureParseError as e:
        print(f"   è§£æå¤±è´¥: {e.get_detailed_error()}\n")
    
    # 6. éªŒè¯æ•°æ®
    print("6. éªŒè¯æ•°æ®")
    data = {
        "name": "David",
        "age": 28,
        "email": "david@example.com"
    }
    
    try:
        user = StructureEngine.validate(data, UserProfile)
        print(f"   éªŒè¯æˆåŠŸ: {user}\n")
    except Exception as e:
        print(f"   éªŒè¯å¤±è´¥: {e}\n")
    
    # 7. Schema å·®å¼‚æ£€æŸ¥
    print("7. Schema å·®å¼‚æ£€æŸ¥")
    incomplete_data = {
        "name": "Eve",
        # ç¼ºå°‘ age å’Œ email
        "extra_field": "should not be here"
    }
    
    diff = StructureEngine.get_schema_diff(incomplete_data, UserProfile)
    print(f"   ç¼ºå°‘å­—æ®µ: {diff['missing_required']}")
    print(f"   é¢å¤–å­—æ®µ: {diff['extra_fields']}\n")
    
    # 8. è¯¦ç»†é”™è¯¯ä¿¡æ¯
    print("8. è¯¦ç»†é”™è¯¯ä¿¡æ¯")
    invalid_text = "This is not JSON at all, just plain text."
    
    try:
        user = await StructureEngine.parse(invalid_text, UserProfile)
    except StructureParseError as e:
        print("   æ•è·åˆ°è¯¦ç»†é”™è¯¯:")
        print(e.get_detailed_error())


if __name__ == "__main__":
    asyncio.run(main())
```

[11] examples/core/toolbox_demo.py
```python
# examples/toolbox_demo.py
import asyncio
from gecko.core.toolbox import ToolBox
from gecko.plugins.tools.standard.calculator import CalculatorTool
from gecko.plugins.tools.standard.duckduckgo import DuckDuckGoSearch


async def main():
    # 1. åˆ›å»ºå·¥å…·ç®±
    toolbox = ToolBox(
        tools=[CalculatorTool(), DuckDuckGoSearch()],
        max_concurrent=3,
        default_timeout=10.0,
        enable_retry=True,
        max_retries=2,
    )
    
    print(f"å·¥å…·ç®±åˆå§‹åŒ–å®Œæˆ: {toolbox}")
    print(f"å·²æ³¨å†Œå·¥å…·: {[t.name for t in toolbox.list_tools()]}\n")
    
    # 2. å•ä¸ªå·¥å…·æ‰§è¡Œ
    print("=== å•ä¸ªå·¥å…·æ‰§è¡Œ ===")
    try:
        result = await toolbox.execute(
            "calculator",
            {"expression": "(10 + 5) * 2"},
            call_id="calc_001"
        )
        print(f"è®¡ç®—ç»“æœ: {result}\n")
    except Exception as e:
        print(f"æ‰§è¡Œå¤±è´¥: {e}\n")
    
    # 3. æ‰¹é‡å¹¶å‘æ‰§è¡Œ
    print("=== æ‰¹é‡å¹¶å‘æ‰§è¡Œ ===")
    tool_calls = [
        {
            "id": "call_1",
            "name": "calculator",
            "arguments": {"expression": "2 + 2"}
        },
        {
            "id": "call_2",
            "name": "duckduckgo_search",
            "arguments": {"query": "Python asyncio"}
        },
        {
            "id": "call_3",
            "name": "calculator",
            "arguments": {"expression": "100 / 5"}
        },
    ]
    
    results = await toolbox.execute_many(tool_calls)
    
    for r in results:
        status = "âŒ" if r.is_error else "âœ…"
        print(f"{status} {r.tool_name} ({r.call_id})")
        print(f"   ç»“æœ: {r.result[:100]}")
        print(f"   è€—æ—¶: {r.duration:.3f}s\n")
    
    # 4. æŸ¥çœ‹ç»Ÿè®¡
    print("=== æ‰§è¡Œç»Ÿè®¡ ===")
    toolbox.print_stats()
    
    # 5. è·å–æ‘˜è¦
    summary = toolbox.get_summary()
    print("å…¨å±€æ‘˜è¦:")
    print(f"  æ€»æ‰§è¡Œæ¬¡æ•°: {summary['total_executions']}")
    print(f"  æ€»é”™è¯¯æ¬¡æ•°: {summary['total_errors']}")
    print(f"  æ•´ä½“æˆåŠŸç‡: {summary['overall_success_rate']:.1%}")
    print(f"  å¹³å‡è€—æ—¶: {summary['avg_time_per_call']:.3f}s")


if __name__ == "__main__":
    asyncio.run(main())
```

[12] examples/core/utils_demo.py
```python
# examples/utils_demo.py
import asyncio
from gecko.core.utils import (
    ensure_awaitable,
    retry,
    safe_dict,
    merge_dicts,
    truncate,
    format_size,
    format_duration,
    Timer,
    timing,
    chunk_list,
    deduplicate,
)


# ç¤ºä¾‹å‡½æ•°
def sync_function(x):
    return x * 2


async def async_function(x):
    await asyncio.sleep(0.1)
    return x * 3


@retry(max_attempts=3, delay=0.5)
async def unstable_function(fail_count=0):
    """æ¨¡æ‹Ÿä¸ç¨³å®šçš„å‡½æ•°"""
    if fail_count > 0:
        raise ValueError("Intentional failure")
    return "Success"


@timing
async def slow_function():
    """å¸¦è®¡æ—¶çš„å‡½æ•°"""
    await asyncio.sleep(1)
    return "Done"


async def main():
    print("=== Gecko Utils ç¤ºä¾‹ ===\n")
    
    # 1. ensure_awaitable
    print("1. ç»Ÿä¸€å¼‚æ­¥/åŒæ­¥è°ƒç”¨")
    result1 = await ensure_awaitable(sync_function, 5)
    result2 = await ensure_awaitable(async_function, 5)
    print(f"   åŒæ­¥å‡½æ•°ç»“æœ: {result1}")
    print(f"   å¼‚æ­¥å‡½æ•°ç»“æœ: {result2}\n")
    
    # 2. é‡è¯•æœºåˆ¶
    print("2. é‡è¯•æœºåˆ¶")
    try:
        result = await unstable_function(fail_count=0)
        print(f"   æˆåŠŸ: {result}\n")
    except Exception as e:
        print(f"   å¤±è´¥: {e}\n")
    
    # 3. æ•°æ®è½¬æ¢
    print("3. æ•°æ®è½¬æ¢")
    class TestClass:
        def __init__(self):
            self.name = "test"
            self.value = 123
    
    obj = TestClass()
    data = safe_dict(obj)
    print(f"   å¯¹è±¡è½¬å­—å…¸: {data}\n")
    
    # 4. å­—å…¸åˆå¹¶
    print("4. å­—å…¸åˆå¹¶")
    d1 = {"a": 1, "b": {"x": 1}}
    d2 = {"b": {"y": 2}, "c": 3}
    merged = merge_dicts(d1, d2, deep=True)
    print(f"   æ·±åº¦åˆå¹¶: {merged}\n")
    
    # 5. å­—ç¬¦ä¸²å¤„ç†
    print("5. å­—ç¬¦ä¸²å¤„ç†")
    long_text = "A" * 200
    truncated = truncate(long_text, max_length=50)
    print(f"   æˆªæ–­æ–‡æœ¬: {truncated}\n")
    
    print(f"   æ–‡ä»¶å¤§å°: {format_size(1048576)}")
    print(f"   æ—¶é•¿: {format_duration(3665)}\n")
    
    # 6. è®¡æ—¶å™¨
    print("6. è®¡æ—¶å™¨")
    with Timer("æµ‹è¯•æ“ä½œ") as t:
        await asyncio.sleep(0.5)
    print(f"   è€—æ—¶: {t.elapsed:.2f}s\n")
    
    # 7. è®¡æ—¶è£…é¥°å™¨
    print("7. è®¡æ—¶è£…é¥°å™¨")
    result = await slow_function()
    print(f"   ç»“æœ: {result}\n")
    
    # 8. åˆ—è¡¨å·¥å…·
    print("8. åˆ—è¡¨å·¥å…·")
    chunks = chunk_list([1, 2, 3, 4, 5, 6, 7], chunk_size=3)
    print(f"   åˆ†å—: {chunks}")
    
    unique = deduplicate([1, 2, 2, 3, 1, 4, 3])
    print(f"   å»é‡: {unique}\n")


if __name__ == "__main__":
    asyncio.run(main())
```

[13] examples/fast_dev_demo.py
```python
# examples/fast_dev_demo.py
import asyncio
from gecko.core.builder import AgentBuilder
from gecko.core.message import Message
from gecko.plugins.models.zhipu import glm_4_5_air
from gecko.plugins.storage.sqlite import SQLiteSessionStorage # [æ–°å¢] æ˜¾å¼å¼•å…¥å­˜å‚¨åç«¯

async def main():
    # [ä¿®æ”¹] åˆå§‹åŒ–å­˜å‚¨å®ä¾‹ (ä¸å†åªä¼  URL å­—ç¬¦ä¸²)
    storage = SQLiteSessionStorage("sqlite://./dev_sessions.db")

    agent = (AgentBuilder()
             .with_model(glm_4_5_air(temperature=0.3))
             .with_session_id("user_123")              # [æ–°å¢] æŒ‡å®šä¼šè¯ ID
             .with_storage(storage)                    # [ä¿®æ”¹] æ³¨å…¥å­˜å‚¨å®ä¾‹
             # .with_tools([KnowledgeTool(...)])       # [è¯´æ˜] Vector RAG ç°åœ¨å»ºè®®ä½œä¸º Tool æ³¨å…¥
             .build())

    # ç¬¬ä¸€æ¬¡è¿è¡Œï¼šè®°ä½åå­—
    print("--- Round 1 ---")
    output1 = await agent.run([Message(role="user", content="æˆ‘å«å¼ ä¸‰ï¼Œä»¥åå«æˆ‘è€å¼ ")])
    print("AI:", output1.content)

    # ç¬¬äºŒæ¬¡è¿è¡Œï¼šéªŒè¯è®°å¿†æ¢å¤ (TokenMemory ä¼šè‡ªåŠ¨ä» SQLite åŠ è½½å†å²)
    print("\n--- Round 2 ---")
    output2 = await agent.run([Message(role="user", content="æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")])
    print("AI:", output2.content)

if __name__ == "__main__":
    asyncio.run(main())
```

[14] examples/full_tool_test.py
```python
# examples/full_tool_test.py
import asyncio
from gecko.core.builder import AgentBuilder
from gecko.core.message import Message
from gecko.plugins.models.zhipu import glm_4_5_air

# è‡ªåŠ¨åŠ è½½æ‰€æœ‰å·¥å…·ï¼ˆåªéœ€ importï¼‰
from gecko.plugins.tools.standard.calculator import CalculatorTool  # noqa: F401
from gecko.plugins.tools.standard.duckduckgo import DuckDuckGoSearchTool  # noqa: F401

async def main():
    agent = AgentBuilder()\
        .with_model(glm_4_5_air(temperature=0.3))\
        .build()

    output = await agent.run([
        Message(role="user", content="è¯·åŒæ—¶å®Œæˆä¸¤ä»¶äº‹ï¼š"
                 "1. è®¡ç®— (12345 + 67890) * 2.5"
                 "2. æœç´¢ä»Šå¤©åŒ—äº¬çš„å¤©æ°”é¢„æŠ¥"
                 "æœ€åç”¨ä¸­æ–‡æ€»ç»“")
    ])
    print("\n=== æœ€ç»ˆå›ç­” ===\n")
    print(output.content)

if __name__ == "__main__":
    asyncio.run(main())
```

[15] examples/mock_test.py
```python
```

[16] examples/models/models_demo.py
```python
# examples/models/models_demo.py
import asyncio
import os
import sys
from dotenv import load_dotenv

# ç¡®ä¿ gecko åŒ…å¯å¯¼å…¥
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))

from gecko.plugins.models.presets.zhipu import ZhipuChat
from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.factory import create_model
from gecko.core.message import Message

load_dotenv()

async def demo_mock():
    """æ¼”ç¤ºï¼šMock æ¨¡å¼"""
    print("\n--- Mock Demo ---")
    # [ä¿®å¤] ä½¿ç”¨æ ‡å‡†çš„æ¨¡å‹åç§°ï¼Œä»¥ä¾¿ LiteLLM è¯†åˆ« Provider (å³ä¾¿æ˜¯åœ¨ Mock æ¨¡å¼ä¸‹)
    config = ModelConfig(
        model_name="gpt-3.5-turbo",  # æ”¹ä¸ºæ ‡å‡†åç§°
        api_key="mock",
        extra_kwargs={"mock_response": "This is a mocked response."}
    )
    # ä½¿ç”¨å·¥å‚åˆ›å»º
    model = create_model(config)
    
    resp = await model.acompletion([{"role": "user", "content": "hi"}])
    print(f"AI: {resp.choices[0].message['content']}")

async def demo_zhipu():
    """æ¼”ç¤ºï¼šæ™ºè°±çœŸå®è°ƒç”¨"""
    print("\n--- Zhipu Live Demo ---")
    key = os.getenv("ZHIPU_API_KEY")
    if not key:
        print("Skipping: No ZHIPU_API_KEY found.")
        return

    model = ZhipuChat(api_key=key, model="glm-4-flash")
    
    # 1. æ–‡æœ¬
    msg = Message.user("ç®€è¿°AIæ™ºèƒ½ä½“çš„ä¼˜åŠ¿")
    resp = await model.acompletion([msg.to_openai_format()])
    # æ³¨æ„ï¼šLiteLLM æœ‰æ—¶è¿”å› content ä¸º None (å¦‚æœè¢«è¿‡æ»¤)ï¼Œéœ€é˜²å¾¡
    content = resp.choices[0].message.get("content", "") or ""
    print(f"AI: {content[:50]}...")

    # 2. æµå¼
    print("Streaming: ", end="")
    async for chunk in model.astream([msg.to_openai_format()]):
        if chunk.content:
            print(chunk.content, end="", flush=True)
    print()

async def main():
    await demo_mock()
    await demo_zhipu()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass
```

[17] examples/multi_tool.py
```python
```

[18] examples/rag_agent.py
```python
```

[19] examples/storage/custom_storage_demo.py
```python
# examples/custom_storage_demo.py
import asyncio
import os
import json
import tempfile
import shutil
from typing import Dict, Any
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.mixins import ThreadOffloadMixin, AtomicWriteMixin, JSONSerializerMixin
from gecko.plugins.storage.interfaces import SessionInterface

class JsonFileStorage(
    AbstractStorage, 
    SessionInterface, 
    ThreadOffloadMixin, 
    AtomicWriteMixin, 
    JSONSerializerMixin
):
    """
    æ¼”ç¤ºï¼šä¸€ä¸ªç®€å•çš„åŸºäº JSON æ–‡ä»¶çš„å­˜å‚¨å®ç°
    
    ç‰¹æ€§ï¼š
    1. çº¿ç¨‹å®‰å…¨ï¼ˆThreadOffloadï¼‰
    2. è¿›ç¨‹å†…å†™é”ï¼ˆAtomicWriteï¼‰
    3. è‡ªåŠ¨åºåˆ—åŒ–
    4. [ä¿®å¤] åŸå­æ–‡ä»¶å†™å…¥ï¼Œé˜²æ­¢å¹¶å‘è¯»å†™æ—¶çš„ç«æ€å´©æºƒ
    """
    
    def __init__(self, url: str, **kwargs):
        super().__init__(url, **kwargs)
        # è§£æ path: json://./data.json -> ./data.json
        self.file_path = url.replace("json://", "")
    
    async def initialize(self) -> None:
        print(f"[Init] Checking file: {self.file_path}")
        # åœ¨çº¿ç¨‹ä¸­æ£€æŸ¥æ–‡ä»¶
        await self._run_sync(self._ensure_file)

    async def shutdown(self) -> None:
        print("[Shutdown] Storage closed")

    def _ensure_file(self):
        """åŒæ­¥æ–‡ä»¶æ£€æŸ¥é€»è¾‘"""
        if not os.path.exists(self.file_path):
            self._write_json_atomically({})

    def _write_json_atomically(self, data: Dict[str, Any]):
        """
        [æ ¸å¿ƒä¿®å¤] åŸå­å†™å…¥æ–‡ä»¶
        
        æ­¥éª¤ï¼š
        1. å†™å…¥ä¸´æ—¶æ–‡ä»¶
        2. åŸå­é‡å‘½åè¦†ç›–åŸæ–‡ä»¶
        
        è¿™é˜²æ­¢äº† reader åœ¨ writer æˆªæ–­æ–‡ä»¶ä½†æœªå®Œæˆå†™å…¥æ—¶è¯»å–åˆ°ç©ºæ–‡ä»¶ã€‚
        """
        dir_name = os.path.dirname(self.file_path) or "."
        # åœ¨åŒä¸€ç›®å½•ä¸‹åˆ›å»ºä¸´æ—¶æ–‡ä»¶ï¼Œç¡®ä¿åŸå­é‡å‘½åå¯è¡Œï¼ˆè·¨åˆ†åŒºæ— æ³•åŸå­é‡å‘½åï¼‰
        with tempfile.NamedTemporaryFile("w", dir=dir_name, delete=False, encoding="utf-8", suffix=".tmp") as tmp_f:
            json.dump(data, tmp_f, ensure_ascii=False, indent=2)
            tmp_name = tmp_f.name
        
        # åŸå­æ›¿æ¢
        try:
            shutil.move(tmp_name, self.file_path)
        except Exception:
            # æ¸…ç†æ®‹ä½™
            if os.path.exists(tmp_name):
                os.remove(tmp_name)
            raise

    async def get(self, session_id: str) -> Dict[str, Any] | None:
        def _read():
            try:
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get(session_id)
            except (FileNotFoundError, json.JSONDecodeError):
                return None
        
        # å¸è½½åˆ°çº¿ç¨‹æ± è¯»å–
        return await self._run_sync(_read)

    async def set(self, session_id: str, state: Dict[str, Any]) -> None:
        def _write():
            # 1. è¯»å–æœ€æ–°æ•°æ® (åœ¨å†™é”ä¿æŠ¤ä¸‹ï¼Œç¡®ä¿è¯»-æ”¹-å†™çš„ä¸€è‡´æ€§)
            try:
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            except (FileNotFoundError, json.JSONDecodeError):
                data = {}
            
            # 2. æ›´æ–°å†…å­˜
            data[session_id] = state
            
            # 3. åŸå­å†™å…¥ç£ç›˜
            self._write_json_atomically(data)

        # åŠ é” + çº¿ç¨‹å¸è½½
        async with self.write_guard():
            await self._run_sync(_write)

    async def delete(self, session_id: str) -> None:
        def _delete():
            try:
                with open(self.file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            except (FileNotFoundError, json.JSONDecodeError):
                return

            if session_id in data:
                del data[session_id]
                self._write_json_atomically(data)
        
        async with self.write_guard():
            await self._run_sync(_delete)

async def main():
    file_path = "demo_sessions.json"
    # æ¸…ç†æ—§æ–‡ä»¶é˜²æ­¢å¹²æ‰°
    if os.path.exists(file_path):
        os.remove(file_path)

    storage = JsonFileStorage(f"json://{file_path}")
    
    try:
        # 1. åˆå§‹åŒ–
        await storage.initialize()
        
        # 2. å†™å…¥æ•°æ®
        print("Writing session...")
        await storage.set("user_1", {"name": "Alice", "history": ["Hi"]})
        
        # 3. è¯»å–æ•°æ®
        data = await storage.get("user_1")
        print(f"Read session: {data}")
        
        # 4. å¹¶å‘å†™å…¥æµ‹è¯•
        print("Testing concurrent writes...")
        async def update_age(age):
            # æ¨¡æ‹Ÿä¸šåŠ¡é€»è¾‘ï¼šè¯» -> æ”¹ -> å†™
            # æ³¨æ„ï¼šè¿™é‡Œçš„ä¸šåŠ¡é€»è¾‘æœ¬èº«åœ¨æç«¯å¹¶å‘ä¸‹å­˜åœ¨"æ›´æ–°ä¸¢å¤±"é£é™©ï¼Œ
            # ä½†æˆ‘ä»¬è¦æµ‹è¯•çš„æ˜¯ storage å±‚ä¸ä¼šå´©æºƒã€‚
            s = await storage.get("user_1") or {}
            s["age"] = age
            await storage.set("user_1", s)
            print(f"Updated age to {age}")

        # å¹¶å‘æ‰§è¡Œ 5 æ¬¡ï¼Œå¢åŠ å‹åŠ›
        await asyncio.gather(
            update_age(20),
            update_age(25),
            update_age(30),
            update_age(35),
            update_age(40)
        )
        
        final_data = await storage.get("user_1")
        print(f"Final data: {final_data}")
        
    finally:
        await storage.shutdown()
        # æ¸…ç†æ–‡ä»¶
        if os.path.exists(file_path):
            os.remove(file_path)

if __name__ == "__main__":
    asyncio.run(main())
```

[20] examples/storage/redis_storage_demo.py
```python
# examples/redis_storage_demo.py
import asyncio
import os
from gecko.plugins.storage.factory import create_storage
from gecko.core.logging import setup_logging

# éœ€è¦è¿è¡ŒçœŸå®çš„ Redis: docker run -p 6379:6379 redis
REDIS_URL = os.getenv("GECKO_REDIS_URL", "redis://localhost:6379/0")

async def main():
    setup_logging(level="INFO")
    
    print(f"ğŸ”Œ Connecting to {REDIS_URL}...")
    
    try:
        # ä½¿ç”¨å·¥å‚åˆ›å»º
        storage = await create_storage(REDIS_URL)
        print("âœ… Storage initialized")
        
        session_id = "demo_user_007"
        
        # å†™å…¥
        print("ğŸ’¾ Writing data...")
        await storage.set(session_id, {
            "user": "Bond",
            "mission": "Secret",
            "active": True
        })
        
        # è¯»å–
        print("ğŸ“– Reading data...")
        data = await storage.get(session_id)
        print(f"   Result: {data}")
        
        # æ¸…ç†
        print("ğŸ§¹ Deleting data...")
        await storage.delete(session_id)
        
        # å†æ¬¡è¯»å–
        data = await storage.get(session_id)
        print(f"   After delete: {data}")
        
        await storage.shutdown()
        print("ğŸ‘‹ Shutdown complete")
        
    except Exception as e:
        print(f"âŒ Demo failed: {e}")
        print("Tip: Ensure Redis is running or set GECKO_REDIS_URL")

if __name__ == "__main__":
    asyncio.run(main())
```

[21] examples/storage/sqlite_storage_demo.py
```python
# examples/sqlite_storage_demo.py
import asyncio
import os
import time
from gecko.plugins.storage.backends.sqlite import SQLiteStorage

async def main():
    db_path = "./demo_sqlite.db"
    url = f"sqlite:///{db_path}"
    
    print(f"ğŸš€ Initializing SQLite Storage at {url}")
    
    # 1. åˆ›å»ºå®ä¾‹
    storage = SQLiteStorage(url)
    
    try:
        # 2. åˆå§‹åŒ– (å»ºè¡¨, WAL)
        await storage.initialize()
        
        # 3. å†™å…¥æµ‹è¯•
        print("\nğŸ’¾ Saving session data...")
        session_id = "user_session_123"
        data = {
            "name": "Gecko Agent",
            "role": "Assistant",
            "history": [
                {"role": "user", "content": "Hello"},
                {"role": "assistant", "content": "Hi there!"}
            ],
            "metadata": {"timestamp": time.time()}
        }
        await storage.set(session_id, data)
        print("âœ… Saved.")
        
        # 4. è¯»å–æµ‹è¯•
        print("\nğŸ“– Reading session data...")
        loaded_data = await storage.get(session_id)
        print(f"âœ… Loaded: {loaded_data['name']} (History: {len(loaded_data['history'])} msgs)")
        
        # 5. æ›´æ–°æµ‹è¯•
        print("\nğŸ”„ Updating session data...")
        loaded_data["metadata"]["updated"] = True
        await storage.set(session_id, loaded_data)
        
        # 6. å¹¶å‘æµ‹è¯• (éªŒè¯æ˜¯å¦é˜»å¡)
        print("\nâš¡ Testing concurrency (Non-blocking check)...")
        start_time = time.time()
        
        async def background_writer(idx):
            # æ¨¡æ‹Ÿå†™å…¥
            await storage.set(f"bg_sess_{idx}", {"idx": idx})
            return idx

        # åŒæ—¶å‘èµ· 10 ä¸ªå†™æ“ä½œ
        tasks = [background_writer(i) for i in range(10)]
        # åŒæ—¶åšä¸€ä¸ª Sleep æ¨¡æ‹Ÿ Event Loop å…¶ä»–ä»»åŠ¡
        tasks.append(asyncio.sleep(0.1))
        
        await asyncio.gather(*tasks)
        duration = time.time() - start_time
        print(f"âœ… Concurrency test passed in {duration:.3f}s")
        
    finally:
        # 7. å…³é—­
        await storage.shutdown()
        # æ¸…ç†
        if os.path.exists(db_path):
            os.remove(db_path)
            # WAL æ¨¡å¼ä¼šäº§ç”Ÿ .wal å’Œ .shm æ–‡ä»¶
            if os.path.exists(db_path + "-wal"): os.remove(db_path + "-wal")
            if os.path.exists(db_path + "-shm"): os.remove(db_path + "-shm")
        print("\nğŸ‘‹ Cleanup done.")

if __name__ == "__main__":
    asyncio.run(main())
```

[22] examples/storage/vector_storage_demo.py
```python
# examples/vector_storage_demo.py
import asyncio
import os
import shutil
import random
from gecko.plugins.storage.backends.chroma import ChromaStorage
from gecko.plugins.storage.backends.lancedb import LanceDBStorage

async def run_demo(storage_cls, url, name):
    print(f"\n--- Demo: {name} ---")
    
    # æ¸…ç†æ—§æ•°æ®
    path = url.split("://")[1]
    if os.path.exists(path):
        shutil.rmtree(path)
        
    store = storage_cls(url)
    
    try:
        print(f"1. Initializing {name}...")
        await store.initialize()
        
        # ç”Ÿæˆå‡æ•°æ® (100 ä¸ªå‘é‡ï¼Œç»´åº¦ 128)
        dim = 128
        count = 100
        print(f"2. Generating {count} vectors (dim={dim})...")
        
        documents = []
        for i in range(count):
            vec = [random.random() for _ in range(dim)]
            documents.append({
                "id": f"doc_{i}",
                "embedding": vec,
                "text": f"This is document number {i}",
                "metadata": {"index": i, "group": "A" if i < 50 else "B"}
            })
            
        print("3. Upserting documents (Thread Offloaded)...")
        start_t = asyncio.get_running_loop().time()
        await store.upsert(documents)
        end_t = asyncio.get_running_loop().time()
        print(f"   Done in {end_t - start_t:.4f}s")
        
        print("4. Searching...")
        query_vec = [random.random() for _ in range(dim)]
        results = await store.search(query_vec, top_k=3)
        
        for i, res in enumerate(results):
            print(f"   Rank {i+1}: {res['id']} (Score: {res['score']:.4f}) - {res['text']}")
            
    except ImportError:
        print(f"âš ï¸  Skipping {name}: Library not installed.")
    except Exception as e:
        print(f"âŒ Error in {name}: {e}")
    finally:
        await store.shutdown()
        if os.path.exists(path):
            shutil.rmtree(path)

async def main():
    # æ¼”ç¤º Chroma
    try:
        import chromadb
        await run_demo(ChromaStorage, "chroma://./demo_chroma", "ChromaDB")
    except ImportError:
        print("\n[SKIP] ChromaDB not installed")

    # æ¼”ç¤º LanceDB
    try:
        import lancedb
        # LanceDB Demo éœ€è¦è®¾ç½®æ­£ç¡®çš„ç»´åº¦ï¼Œæˆ‘ä»¬åœ¨ demo ç±»é‡Œæ²¡æœ‰åŠ¨æ€æ”¹ï¼Œ
        # ä½† LanceDBStorage å®ç°é‡Œæ˜¯åŠ¨æ€è¯» params çš„ã€‚
        # è¿™é‡Œæˆ‘ä»¬åœ¨ä»£ç é‡Œ hardcode äº† demo æ•°æ®çš„ç»´åº¦æ˜¯ 128
        # LanceDB Storage é»˜è®¤è¯» dim=1536ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ URL ä¼ å‚è¦†ç›–
        await run_demo(LanceDBStorage, "lancedb://./demo_lance?dim=128", "LanceDB")
    except ImportError:
        print("\n[SKIP] LanceDB not installed")

if __name__ == "__main__":
    asyncio.run(main())
```

[23] examples/team_demo.py
```python
# examples/team_demo.py
"""
Team å¤šæ™ºèƒ½ä½“åä½œç¤ºä¾‹

å±•ç¤º Gecko Team å¼•æ“çš„å¹¶è¡Œå¤„ç†èƒ½åŠ›ï¼š
1. ä¸“å®¶è¯„å®¡å›¢æ¨¡å¼ (Panel of Experts)
2. å¹¶å‘æ§åˆ¶ (Rate Limiting)
3. å®¹é”™æœºåˆ¶ (Partial Failure Handling)
4. ç»“æœèšåˆ (Aggregation)

è¿è¡Œå‰æï¼š
    export ZHIPU_API_KEY="your_api_key"
"""
from __future__ import annotations

import asyncio
import os
from typing import List

from gecko.compose.team import Team
from gecko.core.agent import Agent
from gecko.core.builder import AgentBuilder
from gecko.core.logging import get_logger
from gecko.plugins.models.zhipu import glm_4_5_air

logger = get_logger(__name__)


# ========================= 1. è¾…åŠ©å‡½æ•° =========================

def create_expert(role: str, prompt: str, api_key: str) -> Agent:
    """
    åˆ›å»ºä¸€ä¸ªç‰¹å®šè§’è‰²çš„ä¸“å®¶ Agent
    """
    model = glm_4_5_air(api_key=api_key, temperature=0.8)
    
    return (
        AgentBuilder()
        .with_model(model)
        .with_session_id(f"expert_{role}")
        .with_system_prompt(f"ä½ æ˜¯ä¸€ä½{role}ã€‚{prompt} è¯·ç®€çŸ­å›ç­”ï¼ˆ50å­—ä»¥å†…ï¼‰ã€‚")
        .build()
    )


async def aggregate_results(results: List[str]) -> str:
    """
    èšåˆå‡½æ•°ï¼šå°†å›¢é˜Ÿçš„æ„è§æ±‡æ€»
    """
    summary = []
    for i, res in enumerate(results, 1):
        # å¤„ç†å¯èƒ½çš„é”™è¯¯ä¿¡æ¯ï¼ˆTeam çš„å®¹é”™æœºåˆ¶ï¼‰
        if str(res).startswith("Error:"):
            summary.append(f"ä¸“å®¶ {i}: [ç¼ºå¸­] ({res})")
        else:
            summary.append(f"ä¸“å®¶ {i}: {res}")
            
    return "\n".join(summary)


# ========================= 2. ä¸»æµç¨‹ =========================

async def main():
    api_key = os.environ.get("ZHIPU_API_KEY")
    if not api_key:
        logger.error("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ ZHIPU_API_KEY")
        return

    logger.info("ğŸš€ åˆå§‹åŒ–ä¸“å®¶è¯„å®¡å›¢...")

    # 1. ç»„å»ºå›¢é˜Ÿ
    # å®šä¹‰ä¸‰ä¸ªä¸åŒè§†è§’çš„ä¸“å®¶
    optimist = create_expert(
        "ä¹è§‚ä¸»ä¹‰æœªæ¥å­¦å®¶", 
        "è¯·å¯¹æœªæ¥çš„ AI å‘å±•ç»™å‡ºä¸€ä¸ªæå…¶ä¹è§‚çš„é¢„æµ‹ã€‚", 
        api_key
    )
    
    pessimist = create_expert(
        "æ‚²è§‚ä¸»ä¹‰å®‰å…¨ä¸“å®¶", 
        "è¯·è­¦å‘Šäººç±» AI å¯èƒ½å¸¦æ¥çš„æœ€å¤§ç”Ÿå­˜é£é™©ã€‚", 
        api_key
    )
    
    realist = create_expert(
        "åŠ¡å®å·¥ç¨‹å¸ˆ", 
        "è¯·ä»æŠ€æœ¯è½åœ°è§’åº¦è¯„ä¼°æœªæ¥ 5 å¹´ AI çš„å®é™…åº”ç”¨ã€‚", 
        api_key
    )

    # 2. åˆ›å»º Team å¼•æ“
    # è®¾ç½® max_concurrent=2ï¼Œæ¼”ç¤ºæµé‡æ•´å½¢ï¼ˆè™½ç„¶æœ‰3ä¸ªä¸“å®¶ï¼Œä½†åŒä¸€æ—¶é—´åªå¹¶å‘è¯·æ±‚2ä¸ªï¼‰
    team = Team(
        members=[optimist, pessimist, realist],
        name="AI_Review_Board",
        max_concurrent=2
    )

    topic = "æˆ‘ä»¬åº”è¯¥å¦‚ä½•çœ‹å¾… AGI çš„åˆ°æ¥ï¼Ÿ"
    print(f"\nğŸ™ï¸ è®®é¢˜: {topic}\n")

    # 3. å¹¶è¡Œæ‰§è¡Œ
    # Team.run ä¼šè‡ªåŠ¨å¤„ç†å¹¶å‘ã€ç­‰å¾…æ‰€æœ‰ç»“æœã€å¹¶æ•è·å•ä¸ª Agent çš„å¼‚å¸¸
    raw_results = await team.run(topic)

    # 4. ç»“æœå±•ç¤º
    print("-" * 20 + " è¯„å®¡ç»“æœ " + "-" * 20)
    final_report = await aggregate_results(raw_results)
    print(final_report)
    print("-" * 50)


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—çº§åˆ«ä»¥ä¾¿è§‚å¯Ÿ Team çš„å¹¶å‘æ‰§è¡Œæ—¥å¿—
    import logging
    logging.basicConfig(level=logging.INFO)
    
    asyncio.run(main())
```

[24] examples/team_of_agents.py
```python
```

[25] examples/tools/tools_demo.py
```python
# examples/tools/tools_demo.py
"""
Gecko Tool Demo (ä¿®å¤ç‰ˆ)

å±•ç¤ºå¦‚ä½•ç»“åˆ ZhipuGLM æ¨¡å‹ä¸ ToolBox æ„å»ºå…·å¤‡å·¥å…·è°ƒç”¨èƒ½åŠ›çš„ Agentã€‚
"""
import asyncio
import os
import sys
from typing import Type

from pydantic import BaseModel, Field

# ç¡®ä¿å½“å‰ç›®å½•åœ¨ sys.path ä¸­ï¼Œä»¥ä¾¿èƒ½å¯¼å…¥ gecko åŒ… (å¦‚æœåœ¨å¼€å‘æ¨¡å¼ä¸‹)
sys.path.append(os.path.join(os.path.dirname(__file__), "../../"))

from gecko.core.builder import AgentBuilder
from gecko.core.logging import setup_logging

# å¯¼å…¥æ’ä»¶ç³»ç»Ÿ
from gecko.plugins.models.zhipu import ZhipuGLM
from gecko.plugins.tools.base import BaseTool, ToolResult
from gecko.plugins.tools.registry import register_tool

# å¯¼å…¥æ ‡å‡†å·¥å…·åº“ä»¥è§¦å‘è‡ªåŠ¨æ³¨å†Œ
import gecko.plugins.tools.standard

setup_logging(level="INFO")


# ==========================================
# è‡ªå®šä¹‰å·¥å…·å®šä¹‰
# ==========================================

class WeatherArgs(BaseModel):
    city: str = Field(..., description="åŸå¸‚åç§°ï¼Œä¾‹å¦‚: 'Beijing', 'Shanghai'")

@register_tool("weather_query")
class WeatherTool(BaseTool):
    name: str = "weather_query"
    description: str = "æŸ¥è¯¢ç‰¹å®šåŸå¸‚çš„å½“å‰å¤©æ°”çŠ¶å†µã€‚"
    args_schema: Type[BaseModel] = WeatherArgs

    async def _run(self, args: WeatherArgs) -> ToolResult:
        print(f"\n[Mock API] Querying weather for {args.city}...")
        mock_data = {
            "Beijing": "Sunny, 25Â°C, Wind: NW 3km/h",
            "Shanghai": "Rainy, 22Â°C, Wind: SE 5km/h",
            "New York": "Cloudy, 18Â°C, Wind: NE 10km/h"
        }
        result = mock_data.get(args.city, "Unknown location")
        return ToolResult(content=result)


# ==========================================
# ä¸»ç¨‹åº
# ==========================================

async def main():
    print("ğŸš€ åˆå§‹åŒ– Gecko Agent...")

    # 1. è·å– API Key
    api_key = os.environ.get("ZHIPU_API_KEY")
    
    # [Fix] æ„é€ å‚æ•°å­—å…¸ï¼Œé¿å…æ˜¾å¼ä¼ é€’ None è¦†ç›–é»˜è®¤å€¼
    llm_kwargs = {
        "model": "glm-4-air",
        "temperature": 0.1,
    }
    
    if api_key:
        print(f"âœ… æ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ API Key: {api_key[:6]}******")
        llm_kwargs["api_key"] = api_key
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ° ZHIPU_API_KEYï¼Œå°†å°è¯•ä½¿ç”¨æ¨¡å‹å†…ç½®çš„é»˜è®¤ Key (ä»…ä¾›æµ‹è¯•)...")
        # ä¸åœ¨ llm_kwargs ä¸­è®¾ç½® 'api_key'ï¼Œè®© ZhipuGLM ä½¿ç”¨ default å€¼

    # 2. åˆå§‹åŒ–æ¨¡å‹
    try:
        llm = ZhipuGLM(**llm_kwargs)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        print("è¯·è¿è¡Œ: export ZHIPU_API_KEY='your_key_here'")
        return

    # 3. æ„å»º Agent
    try:
        agent = (
            AgentBuilder()
            .with_model(llm)
            .with_tools([
                "calculator",        # æ ‡å‡†åº“ï¼šå®‰å…¨è®¡ç®—å™¨
                "duckduckgo_search", # æ ‡å‡†åº“ï¼šè”ç½‘æœç´¢
                "weather_query"      # è‡ªå®šä¹‰ï¼šå¤©æ°”æŸ¥è¯¢
            ])
            .with_max_tokens(4000)
            .build()
        )
    except Exception as e:
        print(f"âŒ Agent æ„å»ºå¤±è´¥: {e}")
        return

    print(f"ğŸ“¦ å·²åŠ è½½å·¥å…·: {[t.name for t in agent.toolbox.list_tools()]}")
    
    # ==========================================
    # æ‰§è¡Œæµ‹è¯•
    # ==========================================
    
    # åœºæ™¯ A: æ•°å­¦è®¡ç®—
    query_math = "è®¡ç®— (123 * 45) + sqrt(1024) çš„ç»“æœæ˜¯å¤šå°‘ï¼Ÿ"
    print(f"\nUser: {query_math}")
    try:
        response = await agent.run(query_math)
        print(f"Agent: {response.content}")
    except Exception as e:
        print(f"Execution failed: {e}")

    # åœºæ™¯ B: è”ç½‘æœç´¢
    # åªæœ‰åœ¨å®‰è£…äº† duckduckgo-search ä¸”ç½‘ç»œé€šç•…æ—¶æ‰æ‰§è¡Œ
    try:
        import duckduckgo_search
        query_search = "2024å¹´å·´é»å¥¥è¿ä¼šé‡‘ç‰Œæ¦œç¬¬ä¸€åæ˜¯å“ªä¸ªå›½å®¶ï¼Ÿ"
        print(f"\nUser: {query_search}")
        response = await agent.run(query_search)
        print(f"Agent: {response.content}")
    except ImportError:
        print("\nâš ï¸ è·³è¿‡æœç´¢æµ‹è¯•ï¼šæœªå®‰è£… duckduckgo-search")
    except Exception as e:
        print(f"\nâš ï¸ æœç´¢æµ‹è¯•å‡ºé”™ (ç½‘ç»œé—®é¢˜?): {e}")

    # åœºæ™¯ C: è‡ªå®šä¹‰å·¥å…·
    query_weather = "åŒ—äº¬å’Œä¸Šæµ·ç°åœ¨çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
    print(f"\nUser: {query_weather}")
    await agent.run(query_weather)

if __name__ == "__main__":
    asyncio.run(main())
```

[26] examples/workflow_dag.py
```python
# examples/workflow_dag.py
import asyncio
from gecko.compose.workflow import Workflow
from gecko.compose.nodes import step, Next
from gecko.compose.team import Team
from gecko.core.builder import AgentBuilder
from gecko.core.message import Message
from gecko.plugins.models.zhipu import glm_4_5_air 

# å·¥å‚å‡½æ•°ï¼šåˆ›å»ºæ–°çš„ Agent å®ä¾‹
def make_agent():
    # è°ƒé«˜ temperature å¢åŠ å˜åŒ–
    return AgentBuilder().with_model(glm_4_5_air(temperature=0.7)).build()

@step("research")
async def research(context):
    """
    è°ƒç ”èŠ‚ç‚¹ï¼šä¼˜å…ˆä½¿ç”¨ä¸Šä¸€è½®çš„åé¦ˆï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨åˆå§‹è¾“å…¥
    """
    # 1. å°è¯•è·å–ä¸Šä¸€æ­¥ä¼ æ¥çš„â€œä¿®æ­£æŒ‡ä»¤â€ï¼ˆç”± Next.input æ³¨å…¥åˆ° last_outputï¼‰
    # 2. å¦‚æœæ²¡æœ‰ï¼Œä½¿ç”¨å…¨å±€åˆå§‹è¾“å…¥ context.input
    topic = context.history.get("last_output") or context.input
    
    # å¦‚æœä¸Šä¸€æ­¥æ˜¯ Team çš„ç»“æœï¼ˆListï¼‰ï¼Œè¯´æ˜æ˜¯åˆšä» Team è¿‡æ¥ä½†æ²¡ç»è¿‡ Check ä¿®æ”¹ï¼ˆç†è®ºä¸Šä¸ä¼šï¼Œä½†ä¸ºäº†å¥å£®æ€§ï¼‰
    if isinstance(topic, list): 
        topic = context.input

    print(f"\nğŸ” [Research] æ­£åœ¨è°ƒç ”: {topic}")
    
    agent = make_agent()
    # è¿™é‡Œçš„ prompt å†³å®šäº†è¾“å‡ºé•¿åº¦
    output = await agent.run([Message(role="user", content=f"{topic}")])
    return output.content

# å®šä¹‰ Team èŠ‚ç‚¹
team_node = Team(members=[make_agent(), make_agent()])

@step("check_quality")
async def check_quality(context):
    """
    è´¨æ£€èŠ‚ç‚¹ï¼šå†³å®šæ˜¯å¦é€šè¿‡ï¼Œæˆ–è€…æ‰“å›é‡åš
    """
    # Team çš„è¾“å‡ºåœ¨ history ä¸­ï¼Œkey æ˜¯èŠ‚ç‚¹å "team_review"
    raw_result = context.history.get("team_review")
    
    # [ä¿®å¤ 1] å°† List[str] åˆå¹¶ä¸ºå•ä¸ª String
    if isinstance(raw_result, list):
        combined_text = "\n---\n".join(str(r) for r in raw_result)
    else:
        combined_text = str(raw_result)
        
    text_len = len(combined_text)
    print(f"ğŸ§ [Check] å½“å‰å†…å®¹é•¿åº¦: {text_len} å­—ç¬¦")

    # [ä¿®å¤ 2] è·å–æˆ–åˆå§‹åŒ–å¾ªç¯è®¡æ•°å™¨ (é˜²æ­¢æ­»å¾ªç¯)
    loop_count = context.state.get("loop_count", 0)
    
    # è®¾å®šé˜ˆå€¼ï¼šæ¯”å¦‚é•¿åº¦å°äº 100 ä¸”é‡è¯•æ¬¡æ•°å°‘äº 3 æ¬¡
    if text_len < 100 and loop_count < 2:
        new_count = loop_count + 1
        context.state["loop_count"] = new_count
        print(f"âš ï¸ [Check] å†…å®¹å¤ªçŸ­ï¼Œç¬¬ {new_count} æ¬¡æ‰“å›é‡åš...")
        
        # [ä¿®å¤ 3] ä¿®æ”¹ Promptï¼Œå¼ºåˆ¶è¦æ±‚é•¿æ–‡ï¼Œæ”¹å˜ Agent çš„è¡Œä¸º
        new_prompt = f"ä¹‹å‰çš„å†…å®¹å¤ªçŸ­äº†ï¼ˆåªæœ‰{text_len}å­—ï¼‰ã€‚è¯·é’ˆå¯¹ '{context.input}' å†™ä¸€ç¯‡ä¸å°‘äº 200 å­—çš„è¯¦ç»†åˆ†ææŠ¥å‘Šã€‚"
        
        # è¿”å› Next æŒ‡ä»¤ï¼š
        # - node: è·³è½¬å› research èŠ‚ç‚¹
        # - input: å°† new_prompt ä¼ é€’ç»™ research èŠ‚ç‚¹
        return Next(node="research", input=new_prompt)
    
    print("âœ… [Check] è´¨é‡è¾¾æ ‡ (æˆ–å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°)")
    return f"æœ€ç»ˆæŠ¥å‘Š (ç»è¿‡ {loop_count} æ¬¡ä¿®æ­£):\n{combined_text}"

async def main():
    workflow = Workflow("ResearchLoop")
    
    # 1. æ³¨å†ŒèŠ‚ç‚¹
    workflow.add_node("research", research)
    workflow.add_node("team_review", team_node)
    workflow.add_node("check", check_quality)
    
    # 2. å®šä¹‰æµå‘
    workflow.set_entry_point("research")
    workflow.add_edge("research", "team_review")
    workflow.add_edge("team_review", "check")
    # check -> research çš„è¾¹ç”±ä»£ç é€»è¾‘åŠ¨æ€æ§åˆ¶
    
    print("ğŸš€ å¯åŠ¨å·¥ä½œæµ...")
    # åˆå§‹è¾“å…¥ç®€å•ä¸€ç‚¹ï¼Œæ•…æ„è¯±å¯¼ç¬¬ä¸€æ¬¡ç”Ÿæˆè¾ƒçŸ­çš„å†…å®¹
    output = await workflow.execute("ç®€è¿° AI Agent")
    print("\nğŸ‰ å·¥ä½œæµç»“æŸ Result:\n", output)

if __name__ == "__main__":
    asyncio.run(main())
```

[27] examples/workflow_demo.py
```python
# examples/workflow_demo.py
"""
Workflow ç¼–æ’ç¤ºä¾‹

å±•ç¤º Gecko Workflow å¼•æ“çš„æ ¸å¿ƒç‰¹æ€§ï¼š
1. æ¡ä»¶åˆ†æ”¯ (Conditional Branching)
2. å¾ªç¯ä¸è·³è½¬ (Next Instruction)
3. ä¸Šä¸‹æ–‡çŠ¶æ€å…±äº« (Context State)
4. æ··åˆèŠ‚ç‚¹ç¼–æ’ (Agent + Function)

è¿è¡Œå‰æï¼š
    export ZHIPU_API_KEY="your_api_key"
"""
from __future__ import annotations

import asyncio
import os
from typing import Any

from gecko.compose.nodes import Next, step
from gecko.compose.workflow import Workflow, WorkflowContext
from gecko.core.agent import Agent
from gecko.core.builder import AgentBuilder
from gecko.core.logging import get_logger
from gecko.plugins.models.zhipu import glm_4_5_air

logger = get_logger(__name__)


# ========================= 1. å®šä¹‰èŠ‚ç‚¹ =========================

@step(name="InputAnalyzer")
async def analyze_input(user_input: str, context: WorkflowContext):
    """
    èŠ‚ç‚¹ 1: åˆ†æç”¨æˆ·è¾“å…¥
    å°†è¾“å…¥å­˜å…¥ contextï¼Œå¹¶è¿”å›è¾“å…¥é•¿åº¦ä¾›åç»­åˆ¤æ–­
    """
    logger.info(f"ğŸ” åˆ†æè¾“å…¥: {user_input}")
    
    # åœ¨ Context ä¸­å­˜å‚¨çŠ¶æ€
    context.state["original_query"] = user_input
    context.state["loop_count"] = 0
    
    return len(user_input)


@step(name="QuickResponse")
def quick_response(length: int):
    """
    èŠ‚ç‚¹ 2A: å¿«é€Ÿå›å¤ (åˆ†æ”¯ A)
    å½“è¾“å…¥è¾ƒçŸ­æ—¶ï¼Œç›´æ¥è¿”å›ç®€å•è§„åˆ™å›å¤
    """
    logger.info("âš¡ï¸ æ‰§è¡Œå¿«é€Ÿå›å¤è·¯å¾„")
    return f"è¾“å…¥å¤ªçŸ­ ({length} å­—ç¬¦)ï¼Œè¯·æä¾›æ›´å¤šç»†èŠ‚ã€‚"


@step(name="DeepThinking")
async def deep_thinking_agent(context: WorkflowContext):
    """
    èŠ‚ç‚¹ 2B: æ·±åº¦æ€è€ƒ (åˆ†æ”¯ B) - ä½¿ç”¨ Agent
    å½“è¾“å…¥è¾ƒé•¿æ—¶ï¼Œè°ƒç”¨ LLM è¿›è¡Œåˆ†æ
    """
    logger.info("ğŸ§  æ‰§è¡Œæ·±åº¦æ€è€ƒè·¯å¾„ (Agent)")
    query = context.state["original_query"]
    
    # æ„å»ºä¸€ä¸ªç®€å•çš„ Zhipu Agent
    api_key = os.environ.get("ZHIPU_API_KEY")
    if not api_key:
        return "Error: No API Key found"

    model = glm_4_5_air(api_key=api_key)
    agent = (
        AgentBuilder()
        .with_model(model)
        .with_session_id("demo_session")
        .build()
    )
    
    # æ‰§è¡Œ Agent
    result = await agent.run(f"è¯·ç®€è¦åˆ†æè¿™å¥è¯çš„æƒ…æ„Ÿï¼š{query}")
    return result.content


@step(name="RefinementLoop")
def refinement_loop(context: WorkflowContext):
    """
    èŠ‚ç‚¹ 3: ä¼˜åŒ–å¾ªç¯ (Loop)
    æ¨¡æ‹Ÿä¸€ä¸ªè‡ªæˆ‘ä¿®æ­£å¾ªç¯ï¼šå¦‚æœç»“æœåŒ…å« "Error"ï¼Œé‡è¯•æœ€å¤š 3 æ¬¡
    """
    last_output = context.get_last_output()
    loop_count = context.state["loop_count"]
    
    logger.info(f"ğŸ”„ æ£€æŸ¥ç»“æœ (Loop {loop_count}): {str(last_output)[:20]}...")
    
    # æ¨¡æ‹Ÿï¼šå¦‚æœæ˜¯ Error ä¸”é‡è¯•æ¬¡æ•°æœªåˆ°ï¼Œé€šè¿‡ Next è·³è½¬å› DeepThinking
    if "Error" in str(last_output) and loop_count < 2:
        context.state["loop_count"] += 1
        logger.warning("âš ï¸ æ£€æµ‹åˆ°é”™è¯¯ï¼Œè§¦å‘é‡è¯•å¾ªç¯...")
        
        # [Fix] ç›®æ ‡èŠ‚ç‚¹åç§°å¿…é¡»ä¸ Workflow.add_node ä¸­æ³¨å†Œçš„åç§°ä¸€è‡´ ("Deep")
        return Next(node="Deep", input=context.state["original_query"])
    
    return last_output


@step(name="FinalSummary")
async def final_summary(result: Any):
    """
    èŠ‚ç‚¹ 4: æœ€ç»ˆæ±‡æ€»
    """
    logger.info("âœ… ç”Ÿæˆæœ€ç»ˆæ‘˜è¦")
    return f"=== Workflow Result ===\n{result}"


# ========================= 2. æ„å»ºä¸è¿è¡Œ =========================

async def main():
    # 1. åˆ›å»º Workflow
    wf = Workflow(name="DemoFlow", max_steps=20)
    
    # 2. æ·»åŠ èŠ‚ç‚¹
    # æ³¨æ„ï¼šWorkflow æ³¨å†Œçš„åç§°æ˜¯ keyï¼ŒNext æŒ‡ä»¤è·³è½¬å¿…é¡»ä½¿ç”¨è¿™ä¸ª key
    wf.add_node("Analyze", analyze_input)
    wf.add_node("Quick", quick_response)
    wf.add_node("Deep", deep_thinking_agent)  # æ³¨å†Œåä¸º "Deep"
    wf.add_node("LoopCheck", refinement_loop)
    wf.add_node("Summary", final_summary)
    
    # 3. å®šä¹‰è¾¹ä¸æ¡ä»¶ (Topology)
    
    # å…¥å£ -> åˆ†æ
    wf.set_entry_point("Analyze")
    
    # åˆ†æ -> åˆ†æ”¯ (æ ¹æ®è¾“å…¥é•¿åº¦)
    wf.add_edge("Analyze", "Quick", lambda ctx: ctx.get_last_output() < 5)
    wf.add_edge("Analyze", "Deep", lambda ctx: ctx.get_last_output() >= 5)
    
    # åˆ†æ”¯æ±‡èš -> å¾ªç¯æ£€æŸ¥
    wf.add_edge("Quick", "LoopCheck")
    wf.add_edge("Deep", "LoopCheck")
    
    # å¾ªç¯æ£€æŸ¥ -> ç»“æŸ
    wf.add_edge("LoopCheck", "Summary")
    
    # 4. éªŒè¯ç»“æ„
    if not wf.validate():
        print("âŒ Workflow éªŒè¯å¤±è´¥")
        return

    # æ‰“å°ç»“æ„å›¾
    wf.print_structure()
    
    print("\n" + "="*40)
    print("Case 1: çŸ­è¾“å…¥ (èµ°å¿«é€Ÿåˆ†æ”¯)")
    print("="*40)
    res1 = await wf.execute("Hi")
    print(f"\n{res1}")
    
    print("\n" + "="*40)
    print("Case 2: é•¿è¾“å…¥ (èµ° Agent åˆ†æ”¯)")
    print("="*40)
    # æç¤ºï¼šç¡®ä¿ç¯å¢ƒå˜é‡ ZHIPU_API_KEY å·²è®¾ç½®
    # å¦‚æœæœªè®¾ç½® API Keyï¼ŒAgent ä¼šè¿”å› "Error: No API Key found"ï¼Œä»è€Œè§¦å‘ LoopCheck çš„é‡è¯•é€»è¾‘
    res2 = await wf.execute("æˆ‘ä»Šå¤©éå¸¸å¼€å¿ƒï¼Œæƒ³å†™ä»£ç ï¼")
    print(f"\n{res2}")


if __name__ == "__main__":
    # ä½¿ç”¨ uvloop (å¦‚æœå®‰è£…äº†) æˆ–æ ‡å‡†å¾ªç¯
    try:
        import uvloop
        uvloop.install()
    except ImportError:
        pass
        
    asyncio.run(main())
```

[28] examples/zhipu_agent.py
```python
import asyncio
from gecko.core.builder import AgentBuilder
from gecko.core.message import Message
from gecko.plugins.models.zhipu import glm_4_5_air  # ä¸€è¡Œå¯¼å…¥

async def main():
    agent = AgentBuilder()\
        .with_model(glm_4_5_air(temperature=0.8)).build()

    output = await agent.run([
        Message(role="user", content="ç”¨ä¸­æ–‡ä»‹ç»ä¸€ä¸‹ Gecko æ¡†æ¶çš„æ’ä»¶åŒ–è®¾è®¡ä¼˜åŠ¿")
    ])
    print(output)

if __name__ == "__main__":
    asyncio.run(main())
```

