# Gecko Plugins Models

`gecko.plugins.models` æ˜¯ Gecko æ¡†æ¶çš„ç»Ÿä¸€æ¨¡å‹æ¥å…¥å±‚ã€‚å®ƒåŸºäº [LiteLLM](https://github.com/BerriAI/litellm) æ„å»ºï¼Œä¸ºä¸Šå±‚ Agent å’Œ RAG æ¨¡å—æä¾›äº†ä¸€è‡´ã€å¥å£®ä¸”ç±»å‹å®‰å…¨çš„æ¨¡å‹è°ƒç”¨æ¥å£ã€‚

## ğŸŒŸ æ ¸å¿ƒç‰¹æ€§

*   **å…¨èƒ½æ¥å…¥**ï¼šç»Ÿä¸€æ”¯æŒ OpenAIã€Anthropicã€Gemini ç­‰ä¸»æµ SaaS æ¨¡å‹ï¼Œä»¥åŠ DeepSeekã€æ™ºè°± GLM ç­‰å›½äº§å¤§æ¨¡å‹ã€‚
*   **äº‘ç«¯ & æœ¬åœ°åŒè½¨**ï¼šæ— ç¼åˆ‡æ¢äº‘ç«¯ API å’Œæœ¬åœ°ç¦»çº¿æ¨¡å‹ï¼ˆOllama, vLLM, SGLangï¼‰ï¼Œä»…éœ€ä¿®æ”¹é…ç½®ã€‚
*   **èŒè´£åˆ†ç¦»**ï¼šä¸¥æ ¼åŒºåˆ† **Chat Model**ï¼ˆå¯¹è¯/å¤šæ¨¡æ€ï¼‰ä¸ **Embedder**ï¼ˆå‘é‡åŒ–ï¼‰ï¼Œé¿å…æ¥å£æ··ç”¨ã€‚
*   **åè®®é©±åŠ¨**ï¼šå®Œå…¨éµå¾ª Gecko Core çš„ `StreamableModelProtocol` å’Œ `EmbedderProtocol`ï¼Œæ”¯æŒæµå¼è¾“å‡ºå’Œ Function Callingã€‚
*   **å¤šæ¨¡æ€æ”¯æŒ**ï¼šé€šè¿‡èƒ½åŠ›æ ‡è¯†ï¼ˆCapability Flagsï¼‰æ”¯æŒè§†è§‰ï¼ˆVisionï¼‰ç­‰å¤šæ¨¡æ€è¾“å…¥ã€‚

## ğŸ“‚ ç›®å½•ç»“æ„

```text
gecko/plugins/models/
â”œâ”€â”€ __init__.py          # å¯¼å‡ºå¸¸ç”¨ç±»
â”œâ”€â”€ config.py            # ç»Ÿä¸€é…ç½®å¯¹è±¡ (ModelConfig)
â”œâ”€â”€ base.py              # æŠ½è±¡åŸºç±»å®šä¹‰ (BaseChatModel, BaseEmbedder)
â”œâ”€â”€ chat.py              # Chat æ¨¡å‹é€šç”¨å®ç° (LiteLLMChatModel)
â”œâ”€â”€ embedding.py         # Embedding æ¨¡å‹é€šç”¨å®ç° (LiteLLMEmbedder)
â””â”€â”€ presets/             # å‚å•†é¢„è®¾é…ç½®
    â”œâ”€â”€ openai.py
    â”œâ”€â”€ zhipu.py
    â”œâ”€â”€ ollama.py
    â””â”€â”€ ...
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ä½¿ç”¨ Chat æ¨¡å‹ (å¯¹è¯/æ¨ç†)

#### æ–¹å¼ Aï¼šä½¿ç”¨å‚å•†é¢„è®¾ (æ¨è)

Gecko é¢„ç½®äº†ä¸»æµå‚å•†çš„é…ç½®ç±»ï¼Œç®€åŒ–åˆå§‹åŒ–æµç¨‹ã€‚

```python
from gecko.plugins.models.presets.openai import OpenAIChat
from gecko.plugins.models.presets.zhipu import ZhipuChat

# 1. OpenAI
model_openai = OpenAIChat(
    api_key="sk-...", 
    model="gpt-4o"
)

# 2. æ™ºè°± AI (GLM-4)
model_zhipu = ZhipuChat(
    api_key="...", 
    model="glm-4-plus"
)

# è°ƒç”¨ (æ”¯æŒå¼‚æ­¥)
response = await model_zhipu.acompletion([{"role": "user", "content": "ä½ å¥½"}])
print(response.choices[0].message.content)
```

#### æ–¹å¼ Bï¼šè¿æ¥æœ¬åœ°æ¨¡å‹ (Ollama / vLLM)

æ”¯æŒå®Œå…¨ç¦»çº¿çš„æœ¬åœ°æ¨ç†ï¼Œé€‚åˆéšç§æ•æ„Ÿæˆ–æ— ç½‘ç¯å¢ƒã€‚

```python
from gecko.plugins.models.presets.ollama import OllamaChat

# è¿æ¥æœ¬åœ° Ollama æœåŠ¡
local_model = OllamaChat(
    model="llama3",                 # å¯¹åº” `ollama run llama3`
    base_url="http://localhost:11434",
    timeout=120.0                   # æœ¬åœ°æ¨ç†å¯èƒ½è¾ƒæ…¢ï¼Œå»ºè®®å¢åŠ è¶…æ—¶
)

# é›†æˆåˆ° Agent
agent = Agent(model=local_model, ...)
```

#### æ–¹å¼ Cï¼šé€šç”¨é…ç½® (è‡ªå®šä¹‰å‚å•†)

å¯¹äºæœªé¢„è®¾çš„å‚å•†ï¼ˆå¦‚ DeepSeekã€Moonshotï¼‰ï¼Œå¯ä½¿ç”¨é€šç”¨é€‚é…å™¨ã€‚

```python
from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.chat import LiteLLMChatModel

# è¿æ¥ DeepSeek (é€šè¿‡ OpenAI å…¼å®¹æ¥å£)
config = ModelConfig(
    model_name="deepseek-chat",
    api_key="sk-...",
    base_url="https://api.deepseek.com",
    max_retries=3
)

model = LiteLLMChatModel(config)
```

### 2. ä½¿ç”¨ Embedding æ¨¡å‹ (RAG)

Embedding æ¨¡å‹ç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡ï¼Œæ˜¯ RAG ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ã€‚

```python
from gecko.plugins.models.presets.openai import OpenAIEmbedder
from gecko.plugins.models.presets.ollama import OllamaEmbedder

# 1. OpenAI Embedding
embedder_cloud = OpenAIEmbedder(
    api_key="sk-...",
    model="text-embedding-3-small",
    dimension=1536
)

# 2. æœ¬åœ° Embedding (Ollama)
embedder_local = OllamaEmbedder(
    model="nomic-embed-text",
    base_url="http://localhost:11434",
    dimension=768  # éœ€æ‰‹åŠ¨æŒ‡å®šç»´åº¦ä»¥ä¾¿å‘é‡åº“åˆå§‹åŒ–
)

# ä½¿ç”¨
vectors = await embedder_local.embed_documents(["Gecko æ˜¯ä¸€ä¸ª AI æ¡†æ¶"])
```

## âš™ï¸ é…ç½®è¯¦è§£ (ModelConfig)

`ModelConfig` æ˜¯æ‰€æœ‰æ¨¡å‹åˆå§‹åŒ–çš„æ ¸å¿ƒé…ç½®å¯¹è±¡ï¼Œæ”¯æŒä»¥ä¸‹å‚æ•°ï¼š

| å‚æ•° | ç±»å‹ | æè¿° | é»˜è®¤å€¼ |
| :--- | :--- | :--- | :--- |
| `model_name` | str | æ¨¡å‹åç§° (å¦‚ `gpt-4o`, `ollama/llama3`) | å¿…å¡« |
| `api_key` | str | API å¯†é’¥ | None |
| `base_url` | str | API åŸºç¡€åœ°å€ (SaaS å¯ç©ºï¼Œæœ¬åœ°å¿…å¡«) | None |
| `timeout` | float | è¯·æ±‚è¶…æ—¶æ—¶é—´ (ç§’) | 60.0 |
| `max_retries` | int | å¤±è´¥é‡è¯•æ¬¡æ•° | 2 |
| `supports_vision` | bool | æ˜¯å¦æ”¯æŒè§†è§‰è¾“å…¥ | False |
| `supports_function_calling` | bool | æ˜¯å¦æ”¯æŒå·¥å…·è°ƒç”¨ | True |
| `extra_kwargs` | dict | é€ä¼ ç»™ LiteLLM çš„é¢å¤–å‚æ•° | {} |

## ğŸ”Œ é«˜çº§ç”¨æ³•

### å¤šæ¨¡æ€æ”¯æŒ (Vision)

Gecko çš„ `Message` å¯¹è±¡æ”¯æŒå¤šæ¨¡æ€å†…å®¹ã€‚è¦å¯ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·ç¡®ä¿æ¨¡å‹é…ç½®äº† `supports_vision=True`ã€‚

```python
from gecko.core.message import Message

# åˆå§‹åŒ–æ”¯æŒè§†è§‰çš„æ¨¡å‹
model = OpenAIChat(model="gpt-4o", api_key="...")

# å‘é€å›¾ç‰‡
msg = Message.user(
    text="è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ",
    images=["https://example.com/image.jpg"]
)

response = await model.acompletion([msg.to_openai_format()])
```

### æµå¼è¾“å‡º (Streaming)

æ‰€æœ‰ Chat æ¨¡å‹å‡å®ç°äº† `astream` æ¥å£ï¼Œè¿”å›æ ‡å‡†åŒ–çš„ `StreamChunk`ã€‚

```python
async for chunk in model.astream(messages):
    if chunk.content:
        print(chunk.content, end="", flush=True)
```

## ğŸ› ï¸ æ‰©å±•æŒ‡å—

å¦‚æœæ‚¨éœ€è¦æ”¯æŒæ–°çš„æ¨¡å‹å‚å•†ï¼Œåªéœ€ç»§æ‰¿ `LiteLLMChatModel` æˆ– `LiteLLMEmbedder` å¹¶é¢„è®¾é…ç½®å³å¯ï¼š

```python
# ç¤ºä¾‹ï¼šæ·»åŠ  Moonshot (Kimi) æ”¯æŒ
from gecko.plugins.models.chat import LiteLLMChatModel
from gecko.plugins.models.config import ModelConfig

class MoonshotChat(LiteLLMChatModel):
    def __init__(self, api_key: str, model: str = "moonshot-v1-8k", **kwargs):
        config = ModelConfig(
            model_name=model,
            api_key=api_key,
            base_url="https://api.moonshot.cn/v1",
            **kwargs
        )
        super().__init__(config)
```