[1] gecko/core/engine/base.py
```python
# gecko/core/engine/base.py
"""
认知引擎基类

定义 Agent 的推理和执行流程，所有引擎实现（ReAct、Chain、Tree 等）
都应继承此基类。

核心概念：
- CognitiveEngine: 抽象基类，定义引擎接口
- 支持普通推理和流式推理
- 支持结构化输出
- 提供 Hook 机制
- 统一的错误处理

优化点：
1. 强化类型注解（使用 ModelProtocol）
2. 完善抽象方法（step, step_stream, step_structured）
3. 添加 Hook 机制（before_step, after_step）
4. 提供工具方法（validate_input, log_execution）
5. 支持上下文管理器（资源管理）
"""
from __future__ import annotations

import asyncio
import time
import threading
from abc import ABC, abstractmethod
from typing import Any, AsyncIterator, Callable, Dict, List, Optional, Type, TypeVar

from pydantic import BaseModel, PrivateAttr

from gecko.core.events.bus import EventBus
from gecko.core.events.types import AgentStreamEvent
from gecko.core.exceptions import AgentError, ModelError
from gecko.core.logging import get_logger
from gecko.core.memory import TokenMemory
from gecko.core.message import Message
from gecko.core.output import AgentOutput
from gecko.core.protocols import ModelProtocol, supports_streaming, validate_model
from gecko.core.toolbox import ToolBox

logger = get_logger(__name__)

T = TypeVar("T", bound=BaseModel)

# [P3 增强] 模型定价配置（单位：USD per 1M tokens）
MODEL_PRICING = {
    # OpenAI pricing (as of 2024)
    "gpt-4": {"input": 30.0, "output": 60.0},          # $30, $60 per 1M tokens
    "gpt-4-turbo": {"input": 10.0, "output": 30.0},    # $10, $30 per 1M tokens
    "gpt-3.5-turbo": {"input": 0.5, "output": 1.5},    # $0.5, $1.5 per 1M tokens
    "claude-3-opus": {"input": 15.0, "output": 75.0},   # $15, $75 per 1M tokens
    "claude-3-sonnet": {"input": 3.0, "output": 15.0},  # $3, $15 per 1M tokens
    "claude-3-haiku": {"input": 0.25, "output": 1.25},  # $0.25, $1.25 per 1M tokens
}


# ====================== 执行统计 ======================

class ExecutionStats(BaseModel):
    """
    引擎执行统计
    
    用于性能监控和调试。支持 token 成本跟踪和模型定价。
    """
    total_steps: int = 0
    total_time: float = 0.0  # 总执行时间（秒）
    input_tokens: int = 0
    output_tokens: int = 0
    tool_calls: int = 0
    errors: int = 0
    
    # [P3 增强] 成本跟踪
    estimated_cost: float = 0.0  # 估算成本（单位：美元）
    _lock: threading.Lock = PrivateAttr(default_factory=threading.Lock)

    def add_step(self, duration: float, input_tokens: int = 0, output_tokens: int = 0, had_error: bool = False):
        """记录一次步骤执行"""
        with self._lock:
            self.total_steps += 1
            self.total_time += duration
            self.input_tokens += input_tokens
            self.output_tokens += output_tokens
            if had_error:
                self.errors += 1

    def add_tool_call(self):
        """记录一次工具调用"""
        with self._lock:
            self.tool_calls += 1

    def add_cost(self, cost: float):
        """累加成本估算（单位：美元）"""
        with self._lock:
            self.estimated_cost += cost

    def get_avg_step_time(self) -> float:
        """获取平均步骤时间（秒）"""
        return self.total_time / self.total_steps if self.total_steps > 0 else 0.0
    
    def get_total_tokens(self) -> int:
        """获取总 token 数"""
        return self.input_tokens + self.output_tokens
    
    def get_error_rate(self) -> float:
        """获取错误率（0.0 - 1.0）"""
        return self.errors / self.total_steps if self.total_steps > 0 else 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """转换为字典"""
        return {
            "total_steps": self.total_steps,
            "total_time": self.total_time,
            "avg_step_time": self.get_avg_step_time(),
            "input_tokens": self.input_tokens,
            "output_tokens": self.output_tokens,
            "total_tokens": self.get_total_tokens(),
            "tool_calls": self.tool_calls,
            "errors": self.errors,
            "error_rate": self.get_error_rate(),
            "estimated_cost": self.estimated_cost,
        }


# ====================== 认知引擎基类 ======================

class CognitiveEngine(ABC):
    """
    认知引擎抽象基类
    
    定义 Agent 的核心推理流程，所有具体引擎实现（ReAct、Chain、Tree 等）
    都应该继承此类。
    
    核心方法：
    - step(): 单次/多轮推理（必需）
    - step_stream(): 流式推理（可选）
    - step_structured(): 结构化输出（可选）
    
    Hook 方法：
    - before_step(): 步骤执行前
    - after_step(): 步骤执行后
    - on_error(): 错误处理
    
    生命周期：
    - initialize(): 初始化
    - cleanup(): 清理资源
    
    示例:
        ```python
        class MyEngine(CognitiveEngine):
            async def step(self, input_messages: List[Message]) -> AgentOutput:
                # 实现推理逻辑
                response = await self.model.acompletion(
                    messages=[m.to_openai_format() for m in input_messages]
                )
                return AgentOutput(content=response.choices[0].message["content"])
        
        # 使用
        engine = MyEngine(model=model, toolbox=toolbox, memory=memory)
        output = await engine.step([Message.user("Hello")])
        ```
    """
    
    def __init__(
        self,
        model: ModelProtocol,
        toolbox: ToolBox,
        memory: TokenMemory,
        event_bus: Optional[EventBus] = None,
        max_iterations: int = 10,
        enable_stats: bool = True,
        **kwargs
    ):
        """
        初始化认知引擎
        
        参数:
            model: 语言模型（必须实现 ModelProtocol）
            toolbox: 工具箱
            memory: 记忆管理器
            max_iterations: 最大迭代次数（防止死循环）
            enable_stats: 是否启用统计
            **kwargs: 子类的额外参数
        
        异常:
            TypeError: model 不符合 ModelProtocol
        """
        # 验证模型（鸭子类型检查）
        # 如果缺少必要方法，会由 validate_model 抛出带有 Missing methods 提示的 TypeError
        validate_model(model)
        self.model = model
        
        self.toolbox = toolbox
        self.event_bus = event_bus
        self.memory = memory
        self.max_iterations = max_iterations
        self.enable_stats = enable_stats
        
        # 统计信息
        self.stats = ExecutionStats() if enable_stats else None
        
        # Hook 函数（可由子类或外部设置）
        self.before_step_hook: Optional[Callable] = None
        self.after_step_hook: Optional[Callable] = None
        self.on_error_hook: Optional[Callable] = None
        
        # 存储额外的配置
        self._config = kwargs
        # hooks 出错时是否立即 fail-fast（默认 False，作为 P3 优化可开启）
        self.hooks_fail_fast: bool = bool(kwargs.get("hooks_fail_fast", False))
        
        logger.debug(
            "Engine initialized",
            engine=self.__class__.__name__,
            model=type(model).__name__,
            max_iterations=max_iterations
        )
    
    # ====================== 核心抽象方法 ======================
    
    @abstractmethod
    async def step(
        self, 
        input_messages: List[Message],
        **kwargs
    ) -> AgentOutput:
        """
        执行推理步骤（必需实现）
        
        这是引擎的核心方法，定义了如何处理输入并生成输出。
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数（如 temperature, max_tokens 等）
        
        返回:
            AgentOutput: 执行结果
        
        异常:
            AgentError: 执行失败
            ModelError: 模型调用失败
        
        实现指南:
            1. 验证输入
            2. 调用 before_step_hook（如果有）
            3. 执行推理逻辑
            4. 调用 after_step_hook（如果有）
            5. 返回结果
        
        示例:
            ```python
            async def step(self, input_messages: List[Message]) -> AgentOutput:
                # 转换为 OpenAI 格式
                messages = [m.to_openai_format() for m in input_messages]
                
                # 调用模型
                response = await self.model.acompletion(messages=messages)
                
                # 构建输出
                return AgentOutput(
                    content=response.choices[0].message["content"],
                    usage=response.usage
                )
            ```
        """
        pass
    
    # ====================== 可选方法 ======================
    
    async def step_stream(
        self, 
        input_messages: List[Message],
        **kwargs
    ) -> AsyncIterator[AgentStreamEvent]:
        """
        流式推理（可选实现）
        
        统一契约：step_stream 输出 AgentStreamEvent，而非纯文本 token。
        原因：
        - ReAct 需要输出 tool_input/tool_output/result/error 等结构化事件；
        - Agent.stream / Agent.stream_events 也以事件为核心；
        - 工业级系统（WebSocket/SSE/调试面板）更需要“事件流”而不是仅 token。

        若引擎不支持流式输出，抛 NotImplementedError。
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数
        
        返回:
            AsyncIterator[AgentStreamEvent]: 文本流
        
        异常:
            NotImplementedError: 引擎不支持流式输出
        
        示例:
            ```python
            async def step_stream(self, input_messages: List[Message]):
                if not supports_streaming(self.model):
                    raise NotImplementedError("Model does not support streaming")
                
                messages = [m.to_openai_format() for m in input_messages]
                
                async for chunk in self.model.astream(messages=messages):
                    if chunk.content:
                        yield chunk.content
            ```
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} does not support streaming events. "
            f"Override step_stream() to enable this feature."
        )
    
    async def step_text_stream(
        self,
        input_messages: List[Message],
        **kwargs
    ) -> AsyncIterator[str]:
        """
        纯文本 token 流（兼容层/简化层）

        默认实现：从 step_stream 事件流中过滤 token 事件并 yield。
        - 这样 Agent.stream 可以稳定依赖该方法；
        - Engine 作者若只想输出 token，可直接 override step_text_stream；
        - Engine 作者若输出完整事件，则只需实现 step_stream。
        """
        async for ev in self.step_stream(input_messages, **kwargs): # type: ignore
            if ev.type == "token" and ev.content is not None:
                yield str(ev.content)
    
    async def step_structured(
        self,
        input_messages: List[Message],
        response_model: Type[T],
        **kwargs
    ) -> T:
        """
        结构化输出推理（可选实现）
        
        执行推理并将输出解析为 Pydantic 模型。
        
        参数:
            input_messages: 输入消息列表
            response_model: 目标 Pydantic 模型类
            **kwargs: 额外参数
        
        返回:
            T: 解析后的模型实例
        
        异常:
            NotImplementedError: 引擎不支持结构化输出
        
        示例:
            ```python
            from pydantic import BaseModel
            
            class Answer(BaseModel):
                question: str
                answer: str
                confidence: float
            
            result = await engine.step_structured(
                input_messages=[Message.user("What is AI?")],
                response_model=Answer
            )
            print(result.answer)
            ```
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} does not support structured output. "
            f"Override step_structured() to enable this feature."
        )
    
    # ====================== Hook 方法 ======================
    
    async def before_step(
        self, 
        input_messages: List[Message],
        **kwargs
    ) -> None:
        """
        步骤执行前的 Hook
        
        在推理开始前调用，可用于：
        - 日志记录
        - 输入验证
        - 状态初始化
        - 发送事件
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数
        
        注意:
            此方法不应修改输入，如需修改请在子类中重写
        """
        if self.before_step_hook:
            try:
                if asyncio.iscoroutinefunction(self.before_step_hook):
                    await self.before_step_hook(input_messages, **kwargs)
                else:
                    self.before_step_hook(input_messages, **kwargs)
            except Exception as e:
                logger.warning("before_step_hook failed", error=str(e), exc_info=True)
                if self.hooks_fail_fast:
                    raise
    
    async def after_step(
        self,
        input_messages: List[Message],
        output: AgentOutput,
        **kwargs
    ) -> None:
        """
        步骤执行后的 Hook
        
        在推理完成后调用，可用于：
        - 日志记录
        - 结果验证
        - 统计更新
        - 发送事件
        
        参数:
            input_messages: 输入消息列表
            output: 执行结果
            **kwargs: 额外参数
        """
        if self.after_step_hook:
            try:
                if asyncio.iscoroutinefunction(self.after_step_hook):
                    await self.after_step_hook(input_messages, output, **kwargs)
                else:
                    self.after_step_hook(input_messages, output, **kwargs)
            except Exception as e:
                logger.warning("after_step_hook failed", error=str(e), exc_info=True)
                if self.hooks_fail_fast:
                    raise
    
    async def on_error(
        self,
        error: Exception,
        input_messages: List[Message],
        **kwargs
    ) -> None:
        """
        错误处理 Hook
        
        在推理过程中发生错误时调用，可用于：
        - 错误日志记录
        - 错误恢复
        - 降级处理
        - 发送告警
        
        参数:
            error: 异常对象
            input_messages: 输入消息列表
            **kwargs: 额外参数
        """
        if self.on_error_hook:
            try:
                if asyncio.iscoroutinefunction(self.on_error_hook):
                    await self.on_error_hook(error, input_messages, **kwargs)
                else:
                    self.on_error_hook(error, input_messages, **kwargs)
            except Exception as e:
                logger.error("on_error_hook failed", error=str(e), exc_info=True)
                if self.hooks_fail_fast:
                    raise

    # ===== 轻量统计/指标辅助 =====
    def record_step(self, duration: float, input_tokens: int = 0, output_tokens: int = 0, had_error: bool = False) -> None:
        """记录一次执行步骤的轻量统计（供子类在合适位置调用）。"""
        if self.stats is not None:
            try:
                self.stats.add_step(duration, input_tokens=input_tokens, output_tokens=output_tokens, had_error=had_error)
            except Exception:
                logger.debug("Failed to update stats")

    def record_tool_call(self) -> None:
        """记录一次工具调用次数"""
        if self.stats is not None:
            try:
                self.stats.add_tool_call()
            except Exception:
                logger.debug("Failed to increment tool call stat")

    # ===== 成本与定价辅助 =====
    def record_cost(self, input_tokens: int = 0, output_tokens: int = 0, model_name: str = "") -> None:
        """基于 token 数和模型名称记录估算成本。
        
        Args:
            input_tokens: 输入 token 数
            output_tokens: 输出 token 数  
            model_name: 模型名称（用于查询定价表）
        """
        if self.stats is None:
            return
        
        # 获取模型定价（默认使用 gpt-3.5-turbo）
        pricing = MODEL_PRICING.get(model_name)
        if not pricing:
            pricing = MODEL_PRICING.get("gpt-3.5-turbo")
        
        # 成本计算：从"每 1M tokens"转换为实际成本
        # cost = tokens * (price_per_million_tokens / 1_000_000)
        cost = (input_tokens * pricing["input"] / 1_000_000) + (output_tokens * pricing["output"] / 1_000_000) # type: ignore
        
        try:
            self.stats.add_cost(cost)
        except Exception:
            logger.debug("Failed to record cost", error_tokens=(input_tokens, output_tokens))
    
    def get_stats_summary(self) -> Dict[str, Any]:
        """获取执行统计摘要"""
        if self.stats is None:
            return {}
        return self.stats.to_dict()
    
    # ====================== 工具方法 ======================
    
    def validate_input(self, input_messages: List[Message]) -> None:
        """
        验证输入消息
        
        参数:
            input_messages: 输入消息列表
        
        异常:
            ValueError: 输入无效
        """
        if not input_messages:
            raise ValueError("input_messages 不能为空")
        
        if not all(isinstance(m, Message) for m in input_messages):
            raise TypeError("所有输入必须是 Message 实例")
        
        logger.debug("Input validated", message_count=len(input_messages))
    
    def supports_streaming(self) -> bool:
        """
        检查引擎是否支持流式输出
        
        返回:
            是否支持
        """
        # 检查模型能力
        model_supports = supports_streaming(self.model)
        
        # 检查引擎是否重写了 step_stream
        engine_supports = (
            self.__class__.step_stream != CognitiveEngine.step_stream
        )
        
        return model_supports and engine_supports
    
    def get_config(self, key: str, default: Any = None) -> Any:
        """
        获取配置项
        
        参数:
            key: 配置键
            default: 默认值
        
        返回:
            配置值
        """
        return self._config.get(key, default)
    
    def set_config(self, key: str, value: Any) -> None:
        """
        设置配置项
        
        参数:
            key: 配置键
            value: 配置值
        """
        self._config[key] = value
    
    def get_stats(self) -> Optional[Dict[str, Any]]:
        """
        获取执行统计
        
        返回:
            统计信息字典，如果未启用统计则返回 None
        """
        return self.stats.to_dict() if self.stats else None
    
    def reset_stats(self) -> None:
        """重置统计信息"""
        if self.stats:
            self.stats = ExecutionStats()
            logger.debug("Stats reset")
    
    # ====================== 生命周期管理 ======================
    
    async def initialize(self) -> None:
        """
        初始化引擎
        
        在首次使用前调用，可用于：
        - 加载资源
        - 预热模型
        - 初始化连接
        
        子类可以重写此方法以添加自定义初始化逻辑。
        """
        logger.debug("Engine initialized", engine=self.__class__.__name__)
    
    async def cleanup(self) -> None:
        """
        清理资源
        
        在引擎不再使用时调用，可用于：
        - 关闭连接
        - 释放资源
        - 保存状态
        
        子类可以重写此方法以添加自定义清理逻辑。
        """
        logger.debug("Engine cleanup", engine=self.__class__.__name__)
    
    # ====================== 上下文管理器 ======================
    
    async def __aenter__(self):
        """异步上下文管理器入口"""
        await self.initialize()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """异步上下文管理器出口"""
        await self.cleanup()
        return False
    
    # ====================== 辅助方法 ======================
    
    async def _safe_execute(
        self,
        func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        安全执行函数（带错误处理和统计）
        
        参数:
            func: 要执行的函数
            *args: 位置参数
            **kwargs: 关键字参数
        
        返回:
            函数执行结果
        
        异常:
            原始异常（已记录日志和统计）
        """
        start_time = time.time()
        had_error = False
        
        try:
            if asyncio.iscoroutinefunction(func):
                result = await func(*args, **kwargs)
            else:
                result = func(*args, **kwargs)
            
            return result
        
        except Exception as e:
            had_error = True
            
            # 记录统计
            if self.stats:
                self.stats.errors += 1
            
            # 调用错误 Hook
            await self.on_error(e, kwargs.get("input_messages", []))
            
            # 记录日志
            logger.exception(
                "Engine execution failed",
                engine=self.__class__.__name__,
                error=str(e)
            )
            
            raise
        
        finally:
            # 记录执行时间
            duration = time.time() - start_time
            if self.stats:
                self.stats.add_step(duration, had_error=had_error)
    
    def __repr__(self) -> str:
        """字符串表示"""
        return (
            f"{self.__class__.__name__}("
            f"model={type(self.model).__name__}, "
            f"max_iterations={self.max_iterations}"
            f")"
        )


# ====================== 工具函数 ======================

def create_engine(
    engine_class: Type[CognitiveEngine],
    model: ModelProtocol,
    toolbox: ToolBox,
    memory: TokenMemory,
    **kwargs
) -> CognitiveEngine:
    """
    创建引擎实例（工厂函数）
    
    参数:
        engine_class: 引擎类
        model: 模型
        toolbox: 工具箱
        memory: 记忆
        **kwargs: 额外参数
    
    返回:
        引擎实例
    
    示例:
        ```python
        engine = create_engine(
            ReActEngine,
            model=openai_model,
            toolbox=toolbox,
            memory=memory,
            max_iterations=5
        )
        ```
    """
    if not issubclass(engine_class, CognitiveEngine):
        raise TypeError(
            f"engine_class 必须是 CognitiveEngine 的子类，"
            f"收到: {engine_class.__name__}"
        )
    
    return engine_class(
        model=model,
        toolbox=toolbox,
        memory=memory,
        **kwargs
    )


# ====================== 导出 ======================

__all__ = [
    "CognitiveEngine",
    "ExecutionStats",
    "create_engine",
]
```

[2] gecko/core/engine/buffer.py
```python
# gecko/core/engine/buffer.py
"""
流式缓冲区模块

核心职责：
1. 解决 OpenAI 协议中流式 Tool Call 分片传输、乱序到达的问题。
2. 解决 LLM 输出不规范 JSON (如 Markdown 包裹) 导致的解析崩溃问题。
"""
from __future__ import annotations

import json
import re
from typing import Dict, List, Any, Optional
import threading

from gecko.core.message import Message
from gecko.core.protocols import StreamChunk
from gecko.core.logging import get_logger

logger = get_logger(__name__)

class StreamBuffer:
    """
    流式响应聚合缓冲区。
    
    使用场景：
    在 Engine 的 Thinking 阶段，随着 LLM 流式吐出 token，此类负责实时聚合，
    并在最后产出一个结构完整的 Message 对象。
    
    [P2-1 Fix] 增强的完整性检查和验证机制。
    """
    def __init__(self):
        self.content_parts: List[str] = []
        # tool_index -> tool_call_dict (处理并发或分片传输的工具调用)
        self.tool_calls_map: Dict[int, Dict[str, Any]] = {}
        self._max_tool_index: int = -1  # [P2-1 Fix] 跟踪最大工具索引
        self._max_content_chars: int = 200_000  # 防止工具返回超大内容
        # 使用可重入锁保护同步场景下的 add_chunk/构建操作
        self._lock: threading.RLock = threading.RLock()
        
    def add_chunk(self, chunk: StreamChunk) -> Optional[str]:
        """
        接收一个流式块 (StreamChunk)，更新内部状态。
        
        返回：
            Optional[str]: 本次 chunk 中新增的文本内容（用于流式回显）。
            如果是纯工具调用的 chunk，返回 None。
        
        [P2-1 Fix] 添加了索引验证和范围检查，防止稀疏索引导致的内存溢出。
        """
        # add_chunk 可能并发调用（来自不同协程/线程），使用锁保证原子性
        with self._lock:
            if not hasattr(chunk, 'delta') or chunk.delta is None:
                logger.warning("StreamChunk missing delta, skipping chunk")
                return None
            delta = chunk.delta

            # 1. 聚合文本内容
            content = delta.get("content")
            if content:
                # 防止累计过长的 content 导致内存峰值
                current_len = sum(len(p) for p in self.content_parts)
                if current_len + len(content) > self._max_content_chars:
                    logger.warning("Truncating incoming content to avoid memory blowup")
                    # 仅追加能容纳的部分
                    allowed = max(0, self._max_content_chars - current_len)
                    if allowed > 0:
                        self.content_parts.append(content[:allowed])
                else:
                    self.content_parts.append(content)

            # 2. 聚合工具调用 (处理 index 可能不连续或乱序的情况)
            if delta.get("tool_calls"):
                for tc in delta["tool_calls"]:
                    idx = tc.get("index")
                    if idx is None:
                        continue

                    # [P2-1 Fix] 改进的索引验证 - 允许更大的间隙但记录警告
                    if idx < 0:
                        logger.warning(f"Negative tool index received: {idx}, skipping")
                        continue

                    # 更现实的上限：允许 500+ 工具
                    MAX_TOOL_INDEX = 1000
                    if idx > MAX_TOOL_INDEX:
                        logger.warning(f"Excessive tool index: {idx}, likely malformed response")
                        continue

                    # 改进的稀疏索引检测：警告但继续处理而不是跳过
                    MAX_TOOL_INDEX_GAP = 500
                    if self._max_tool_index >= 0 and (idx - self._max_tool_index) > MAX_TOOL_INDEX_GAP:
                        logger.warning(
                            f"Unusually large tool index gap detected",
                            prev_max=self._max_tool_index,
                            new_idx=idx,
                            gap=idx - self._max_tool_index,
                            action="accepting_with_monitoring",
                            note="This may indicate model misconfiguration or concurrent requests"
                        )
                        # 继续处理而不是跳过！

                    # 跟踪最大索引
                    if idx > self._max_tool_index:
                        self._max_tool_index = idx

                    # 初始化该索引的结构
                    if idx not in self.tool_calls_map:
                        self.tool_calls_map[idx] = {
                            "id": "",
                            "type": "function",
                            "function": {"name": "", "arguments": ""}
                        }

                    target = self.tool_calls_map[idx]

                    # 增量拼接 ID
                    if tc.get("id"):
                        target["id"] += tc["id"]

                    # 增量合并函数名和参数
                    # 对于函数名使用“最新非空片段覆盖”策略，避免在流式分片时产生重复拼接
                    func = tc.get("function", {})
                    if func.get("name"):
                        # 使用最新的非空片段替换之前的 name（而不是拼接）
                        target["function"]["name"] = func.get("name")
                    if func.get("arguments"):
                        # 防止参数字符串过长
                        if len(target["function"]["arguments"]) + len(func.get("arguments", "")) > 100_000:
                            logger.warning("Truncating tool arguments due to excessive length")
                            # 只保留前 100k 字符
                            target["function"]["arguments"] = (
                                target["function"]["arguments"] + func.get("arguments", "")
                            )[:100_000]
                        else:
                            target["function"]["arguments"] += func["arguments"]
        
        return content

    def build_message(self) -> Message:
        """
        构建最终的 Message 对象。
        
        关键逻辑：
        在此阶段会对收集到的 JSON 参数字符串进行“清洗”和“修复”，
        防止因为 Markdown 符号或引号问题导致后续工具执行失败。
        """
        full_content = "".join(self.content_parts)
        tool_calls_list = []
        
        # 按索引排序，确保工具调用顺序一致
        for idx in sorted(self.tool_calls_map.keys()):
            raw_tc = self.tool_calls_map[idx]
            
            # [P2-1 Fix] 验证工具调用的完整性
            func_name = raw_tc["function"].get("name", "").strip()
            raw_args = raw_tc["function"].get("arguments", "").strip()
            
            if not func_name:
                logger.warning(f"Tool call at index {idx} has empty name, skipping")
                continue
            
            if not raw_args:
                # 如果 arguments 为空，默认设为 {}
                logger.warning(f"Tool call '{func_name}' at index {idx} has empty arguments, using default {{}}") 
                raw_args = "{}"
            
            # [生产级增强] 深度清洗参数 JSON
            cleaned_args = self._clean_arguments(raw_args)
            
            # 更新清洗后的参数
            raw_tc["function"]["arguments"] = cleaned_args
            
            tool_calls_list.append(raw_tc)
        
        if tool_calls_list:
            logger.debug(f"Built message with {len(tool_calls_list)} tool calls")
            
        return Message.assistant(
            content=full_content,
            # 只有当列表非空时才设值，符合 Pydantic 定义及 OpenAI 规范
            tool_calls=tool_calls_list if tool_calls_list else None
        )

    def _clean_arguments(self, raw_json: str) -> str:
        """
        [P1-4 Fix] 进阶 JSON 清洗：处理 LLM 的常见输出格式问题
        
        修复项：
        1. Markdown 代码块包裹 (```json ... ```)
        2. 首尾多余的误加引号 ('{...}')
        3. 尾部逗号 ({"key": "value",})
        4. 未转义的换行符
        """
        if not raw_json: 
            return "{}"
            
        # 1. 尝试直接解析（这是最快路径，如果模型输出规范）
        try:
            json.loads(raw_json)
            return raw_json
        except json.JSONDecodeError:
            pass
            
        cleaned = raw_json.strip()
        
        # 2. 去除 Markdown 代码块
        # 匹配 ```json {...} ``` 或 ``` {...} ```
        if cleaned.startswith("```"):
            match = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", cleaned)
            if match:
                cleaned = match.group(1).strip()
        
        # 3. [P1-4 Fix] 去除首尾多余的误加引号
        # 例如模型输出了: '{"arg": "val"}' (带单引号的字符串)
        if (cleaned.startswith("'") and cleaned.endswith("'")) or \
           (cleaned.startswith('"') and cleaned.endswith('"')):
            cleaned = cleaned[1:-1]
        
        # 4. [P1-4 Fix] 修复尾部逗号 ({"key": "value",})
        cleaned = re.sub(r',\s*}', '}', cleaned)  # {...,} -> {...}
        cleaned = re.sub(r',\s*\]', ']', cleaned)  # [...,] -> [...]

        # 5. 再次尝试解析验证 (Best Effort)
        try:
            json.loads(cleaned)
            return cleaned
        except json.JSONDecodeError as e:
            # [P1-4 Fix] 如果还无法解析，返回空对象而非脏数据
            # 这避免工具执行时因脏 JSON 而崩溃
            logger.warning(
                f"Failed to clean JSON arguments, returning empty dict",
                original={raw_json[:100] if raw_json else ""},
                parse_error=str(e)
            )
            # 返回空对象而非原始脏数据，这样工具调用会失败，
            # LLM 可以在错误消息中看到并修正
            return "{}"
```

[3] gecko/core/engine/react.py
```python
# gecko/core/engine/react.py
"""
ReAct 推理 引擎 (Production Grade / 生产级)

架构设计：
1. 无状态设计 (Stateless): 
   Engine 实例不持有任何单次请求的状态，所有状态封装在 ExecutionContext 中。
   这使得 Engine 单例可以在多线程/异步环境下安全地处理并发请求。

2. 生命周期分解 (Lifecycle Decomposition):
   将复杂的 ReAct while 循环拆解为 _phase_think (思考), _phase_act (行动), 
   _phase_observe (观察) 三个独立阶段。子类（如 ReflexionEngine）可以重写特定阶段
   而不必复制整个循环逻辑。

3. 事件驱动 (Event Driven):
   统一使用 AgentStreamEvent 协议，消除了 yield 返回类型不明确的问题。
   同步接口 step 仅仅是流式接口 step_stream 的消费者。

4. 鲁棒性 (Robustness):
   - 集成 StreamBuffer 处理流式碎片及修复不规范 JSON。
   - 内置死循环熔断机制。
   - 内置观察值截断机制，防止 Context Window 爆炸。
"""
from __future__ import annotations

import asyncio
import json
from datetime import datetime
import time
from typing import (
    Any,
    AsyncIterator,
    Callable,
    Dict,
    List,
    Optional,
    Type,
    TypeVar,
    Union,
    cast,
)

from pydantic import BaseModel

from gecko.core.engine.base import CognitiveEngine
from gecko.core.engine.buffer import StreamBuffer
from gecko.core.events.types import AgentStreamEvent
from gecko.core.protocols import StreamChunk 
from gecko.core.toolbox import ToolExecutionResult 
from gecko.core.exceptions import AgentError
from gecko.core.logging import get_logger
from gecko.core.memory import TokenMemory
from gecko.core.message import Message
from gecko.core.output import AgentOutput
from gecko.core.prompt import PromptTemplate
from gecko.core.structure import StructureEngine
from gecko.core.toolbox import ToolBox
from gecko.core.utils import ensure_awaitable
from gecko.config import get_settings

logger = get_logger(__name__)

T = TypeVar("T", bound=BaseModel)

# 默认的 ReAct 提示词模板
DEFAULT_REACT_TEMPLATE = """You are a helpful AI assistant.
Current Time: {{ current_time }}

Available Tools:
{% for tool in tools %}
- {{ tool['function']['name'] }}: {{ tool['function']['description'] }}
{% endfor %}

Answer the user's request. Use tools if necessary.
If you use a tool, just output the tool call format.
"""

class ExecutionContext:
    """
    执行上下文 (Runtime Context)
    
    承载单次 Agent.run/stream 的所有运行时状态。
    随请求创建，随请求销毁。
    """
    def __init__(self, messages: List[Message], max_history: int = 50):
        # 浅拷贝消息列表，防止污染传入的原始列表，但在处理过程中会追加新消息
        self.messages = messages.copy()
        self.max_history = max_history  # [P2-2 Fix] 消息历史上限
        self.turn = 0
        self.metadata: Dict[str, Any] = {}
        
        # --- 状态追踪：用于死循环检测与错误熔断 ---
        self.consecutive_errors: int = 0  # 连续工具错误次数
        self.last_tool_hash: Optional[int] = None # 上一次工具调用的指纹
        self.last_tool_hashes: List[int] = []  # [P2-3 Fix] 最近 3 轮的工具调用哈希

    def add_message(self, message: Message) -> None:
        self.messages.append(message)
        
        # 1. 基础条数限制
        if len(self.messages) > self.max_history:
            self._trim_context()
            return

        # 2. [新增] 字符级滑动窗口保护 (防止 Context Window 溢出)
        # 估算：1 token ≈ 4 chars。
        # 假设最大安全窗口为 100k chars (约 25k tokens)，留给 prompt template 和 reasoning
        # 这个阈值可以根据实际模型调整，或者作为参数传入
        SAFE_CHAR_LIMIT = 100000 
        
        current_chars = sum(len(m.get_text_content()) for m in self.messages)
        
        if current_chars > SAFE_CHAR_LIMIT:
            logger.warning(
                f"Context size ({current_chars} chars) exceeded limit during execution loop. Trimming history."
            )
            self._trim_context(target_chars=SAFE_CHAR_LIMIT)

    def _trim_context(self, target_chars: Optional[int] = None) -> None:
        """
        [新增] 智能裁剪逻辑
        策略：保留 System Message (index 0)，从最早的对话历史开始移除。
        """
        # 必须保留 system 消息
        system_msgs = [m for m in self.messages if m.role == "system"]
        # 对话消息
        conversation_msgs = [m for m in self.messages if m.role != "system"]
        
        # 如果没有对话消息可删，直接返回（避免删掉 system）
        if not conversation_msgs:
            return

        # 模式 A: 按条数裁剪
        if target_chars is None:
            # 计算保留数量
            keep_count = max(1, self.max_history - len(system_msgs))
            conversation_msgs = conversation_msgs[-keep_count:]
        
        # 模式 B: 按字符数裁剪
        else:
            current_len = sum(len(m.get_text_content()) for m in (system_msgs + conversation_msgs))
            while current_len > target_chars and len(conversation_msgs) > 1:
                # 移除最老的一条对话
                removed = conversation_msgs.pop(0)
                current_len -= len(removed.get_text_content())

        # 重组
        self.messages = system_msgs + conversation_msgs

    @property
    def last_message(self) -> Message:
        """获取历史中最后一条消息，用于检查 LLM 的最新输出"""
        if not self.messages:
            raise ValueError("Context is empty")
        return self.messages[-1]


class ReActEngine(CognitiveEngine):
    """
    生产级 ReAct 引擎实现。
    """

    def __init__(
        self,
        model: Any,
        toolbox: ToolBox,
        memory: TokenMemory,
        max_turns: int = 10,
        max_observation_length: int = 2000,
        system_prompt: Union[str, PromptTemplate, None] = None,
        # 生命周期钩子 Hooks
        on_turn_start: Optional[Callable[[ExecutionContext], Any]] = None,
        on_turn_end: Optional[Callable[[ExecutionContext], Any]] = None,
        **kwargs: Any,
    ):
        super().__init__(model, toolbox, memory, **kwargs)
        self.max_turns = max_turns
        self.max_observation_length = max_observation_length
        self.on_turn_start = on_turn_start
        self.on_turn_end = on_turn_end

        # 初始化 System Prompt 模板
        if system_prompt is None:
            self.prompt_template = PromptTemplate(template=DEFAULT_REACT_TEMPLATE)
        elif isinstance(system_prompt, str):
            self.prompt_template = PromptTemplate(template=system_prompt)
        else:
            self.prompt_template = system_prompt

        # 检测模型能力：是否支持原生 Function Calling
        self._supports_functions = getattr(self.model, "_supports_function_calling", True)

    # ================= 核心入口 (Public API) =================

    async def step( # type: ignore
        self,
        input_messages: List[Message],
        response_model: Optional[Type[T]] = None,
        max_retries: int = 0,
        **kwargs: Any,
    ) -> Union[AgentOutput, T]:
        """
        同步执行入口。
        
        逻辑流程：
        1. 作为一个消费者，迭代 `step_stream` 产生的事件流。
        2. 忽略中间的 Token 事件，只捕获 `result` 或处理 `error`。
        3. 如果指定了 `response_model`，则对结果进行结构化解析。
        4. 如果解析失败且配置了重试，则将错误反馈给模型并重新运行推理。
        
        参数:
            input_messages: 用户输入消息列表
            response_model: (可选) Pydantic 模型，用于强制结构化输出
            max_retries: 结构化解析失败时的最大重试次数
            **kwargs: 传递给 LLM 的额外参数
        """
        
        # 将 response_model 放入 kwargs 传递给 step_stream，以便底层构建 Tool Schema
        kwargs['response_model'] = response_model

        # [新增] 验证 response_model
        if response_model is not None:
            from inspect import isclass
            if not (isclass(response_model) and issubclass(response_model, BaseModel)):
                raise TypeError(
                    f"response_model must be a Pydantic BaseModel subclass, "
                    f"got {type(response_model).__name__}"
                )

        # 内部闭包：执行一次完整的推理流程，直到产生结果或报错
        async def _run_once(msgs: List[Message]) -> AgentOutput:
            """
            执行一次完整的 ReAct 生命周期，并从事件流中提取最终 AgentOutput。

            设计要点：
            - 只要看到 `result` 事件，就更新 final_res。
            - 遇到 `error` 事件时，根据错误内容做分类：
              * 已经是 "Infinite loop detected" -> 原样抛出；
              * 包含 "StopIteration" -> 视为模型流式异常导致的“死循环”场景，升级为 Infinite loop；
              * 其它错误 -> 原样抛出 AgentError。
            - 如果整个生命周期跑完 **没有任何 result 事件**（final_res 仍为 None），
              则视为 ReAct 未能收敛（例如 max_turns 用尽、逻辑走偏等），
              统一抛出 "Infinite loop detected: no result generated"。
            """
            final_res: Optional[AgentOutput] = None

            try:
                async for event in self.step_stream(msgs, **kwargs):
                    if event.type == "result" and event.data:
                        # 从事件载荷中提取 AgentOutput 对象
                        final_res = cast(AgentOutput, event.data.get("output"))
                    elif event.type == "error":
                        # 通过事件通道传上来的错误，按 AgentError 处理
                        msg = str(event.content)
                        logger.error("Engine step error event: %s", msg)

                        # 1) 已经是死循环检测
                        if "Infinite loop detected" in msg:
                            raise AgentError(msg)

                        # 2) 模型流式异常（StopIteration），视为一种“无法收敛”的死循环
                        if "StopIteration" in msg:
                            raise AgentError(
                                "Infinite loop detected: model streaming stopped unexpectedly"
                            )

                        # 3) 其他错误，原样抛出
                        raise AgentError(msg)

            except AgentError:
                # 保持 AgentError 原样向上抛出，避免包裹改变 message
                raise
            except Exception as e:
                # 其他异常统一兜底为 Step execution failed
                logger.exception("Stream processing failed in _run_once", error=str(e))
                raise

            # 整个 step_stream 跑完，没有任何 result 事件 -> 视为死循环/未收敛
            if final_res is None:
                err_msg = "Infinite loop detected: no result generated"
                logger.error(err_msg)
                raise AgentError(err_msg)

            return final_res


        # 1. 首次运行
        # [P1-1 Fix] 深拷贝消息列表，防止外部修改污染上下文
        try:
            current_messages = [
                Message(**m.dict()) if hasattr(m, 'dict') else m 
                for m in input_messages
            ]
        except Exception:
            # Fallback: 如果深拷贝失败，使用浅拷贝
            current_messages = list(input_messages)
        
        try:
            final_output = await _run_once(current_messages)
            
            if not final_output:
                # [P1-1 Fix] 补充计费信息，即使无输出也应记录 token 使用
                return AgentOutput(
                    content="[System Error] No output generated.",
                    usage={"input_tokens": 0, "output_tokens": 0} # type: ignore
                )
        except AgentError:
            # AgentError 直接重新抛出
            raise
        except Exception as e:
            logger.exception("step() execution failed", error=str(e))
            raise AgentError(f"Step execution failed: {e}") from e
            
        # 2. 结构化解析 + 自动重试循环
        # 如果不需要结构化输出，直接返回 AgentOutput
        if response_model:
            attempts = 0
            while True:
                try:
                    # 优先策略 A: 尝试从 Tool Calls 中解析 (OpenAI 模式)
                    # 如果 LLM 正确调用了我们注入的结构化工具，数据会在 tool_calls 中
                    if final_output.tool_calls:
                        return await StructureEngine.parse(
                            content="", 
                            model_class=response_model, 
                            raw_tool_calls=final_output.tool_calls
                        )
                    # 优先策略 B: 回退到 Content 解析 (JSON Mode / Text 提取)
                    return await StructureEngine.parse(final_output.content, response_model)
                
                except Exception as e:
                    # 解析失败，检查是否还有重试机会
                    if attempts >= max_retries:
                        raise AgentError(f"Structured parsing failed: {e}") from e
                    
                    attempts += 1
                    logger.warning(f"Structure parse failed, retrying ({attempts}/{max_retries})")
                    
                    # 构造反馈消息并追加到本次会话历史中
                    # 让模型看到自己上次的输出和对应的错误信息
                    current_messages.append(Message.assistant(
                        content=final_output.content,
                        tool_calls=final_output.tool_calls
                    ))
                    current_messages.append(Message.user(
                        f"Error parsing response: {e}. Please try again using the correct format."
                    ))
                    
                    # 再次运行 Engine (Retry)
                    final_output = await _run_once(current_messages)
                    if not final_output:
                         raise AgentError("Retry returned no output")

        return final_output


    async def step_stream( # type: ignore
        self, 
        input_messages: List[Message], 
        timeout: Optional[float] = None,
        **kwargs: Any
    ) -> AsyncIterator[AgentStreamEvent]:
        """
        流式执行入口。
        
        生成 `AgentStreamEvent` 流，包含：
        - `token`: 实时生成的文本片段
        - `tool_input`: 工具调用意图和参数
        - `tool_output`: 工具执行结果
        - `result`: 最终生成的回复
        - `error`: 执行过程中的非致命错误或异常
        
        参数:
            input_messages: 输入消息列表
            timeout: 本次执行的超时时间(秒)。为 None 时使用全局配置 settings.default_model_timeout。
            **kwargs: 传递给引擎的其他参数
        """
        # 统一超时配置：优先使用调用方传入，其次使用全局默认值
        if timeout is None:
            timeout = get_settings().default_model_timeout
        
        # 1. 基础校验与 Hook
        self.validate_input(input_messages)
        await self.before_step(input_messages, **kwargs)

        # 2. 构建执行上下文 (Context) - 这是线程安全的局部变量
        context = await self._build_execution_context(input_messages)
        start_time = time.time()
        
        try:
            # 3. 生命周期主循环，带超时保护和优雅关闭
            async for event in self._execute_lifecycle_with_timeout(context, timeout, **kwargs):
                yield event
        except AgentError as e:
            logger.error(f"step_stream caught AgentError: {e}")
            yield AgentStreamEvent(type="error", content=str(e))
            raise
        except asyncio.TimeoutError:
            elapsed = time.time() - start_time
            logger.error(
                f"step_stream timeout",
                elapsed_seconds=elapsed,
                max_timeout=timeout,
                current_turn=context.turn,
                message_count=len(context.messages)
            )
            
            # 尝试保存上下文（graceful shutdown）
            try:
                await self._save_context(context, force=True)
                logger.debug(f"Context saved before timeout shutdown")
            except Exception as save_error:
                logger.warning(f"Failed to save context on timeout: {save_error}")
            
            # 向客户端报告
            yield AgentStreamEvent(
                type="error",
                content=f"Execution timeout after {timeout}s at turn {context.turn}"
            )
            raise
            
        except asyncio.CancelledError:
            logger.warning(
                f"step_stream was cancelled",
                current_turn=context.turn
            )
            # 也要保存上下文
            try:
                await self._save_context(context, force=True)
            except Exception:
                pass
            raise
            
        except Exception as e:
            logger.exception("Lifecycle execution failed", error=str(e))
            await self.on_error(e, input_messages, **kwargs)
            # 将未捕获的异常转换为 Error 事件抛出给前端
            yield AgentStreamEvent(type="error", content=str(e))
            raise


    async def _execute_lifecycle_with_timeout(
        self, 
        context: ExecutionContext, 
        timeout: float, 
        **kwargs: Any
    ) -> AsyncIterator[AgentStreamEvent]:
        """生命周期执行，包含超时控制。"""
        import asyncio
        try:
            # Python 3.11+ 使用原生 asyncio.timeout
            if hasattr(asyncio, 'timeout'):
                async with asyncio.timeout(timeout):
                    async for event in self._execute_lifecycle(context, **kwargs):
                        yield event
            else:
                # Python 3.10 及以下兼容方案
                async for event in asyncio.wait_for(
                    self._execute_lifecycle(context, **kwargs), # type: ignore
                    timeout=timeout
                ):
                    yield event
        except asyncio.TimeoutError:
            # 重新抛出，让 step_stream 处理
            raise

    # ================= 生命周期主循环 (Lifecycle) =================

    async def _execute_lifecycle(
        self, 
        context: ExecutionContext, 
        **kwargs: Any
    ) -> AsyncIterator[AgentStreamEvent]:
        """
        ReAct 核心循环：Think -> Act -> Observe
        """
        
        # 预处理：检查是否在进行结构化输出
        # 如果是，我们需要获取那个“虚拟工具”的名称，以便拦截它
        response_model = kwargs.get('response_model')
        structure_tool_name = None
        if response_model and self._supports_functions:
            schema = StructureEngine.to_openai_tool(response_model)
            structure_tool_name = schema["function"]["name"]

        while context.turn < self.max_turns:
            context.turn += 1
            
            # Hook: 轮次开始
            if self.on_turn_start:
                await ensure_awaitable(self.on_turn_start, context)

            # ---------------- Phase 1: Think (Reasoning) ----------------
            buffer = StreamBuffer() # 创建新的流式缓冲区
            
            # 调用底层 LLM 生成流 (yield StreamChunk)
            async for chunk in self._phase_think(context, **kwargs):
                # 实时计算文本增量并加入缓冲区
                text_delta = buffer.add_chunk(chunk)
                if text_delta:
                    # 实时 yield 文本 Token 给前端
                    yield AgentStreamEvent(type="token", content=text_delta)
            
            # 从缓冲区构建完整的 Assistant 消息 (包含清洗过的 JSON 参数)
            assistant_msg = buffer.build_message()
            context.add_message(assistant_msg)

            # --- Safety Check: 死循环熔断 ---
            if self._detect_loop(context, assistant_msg):
                err_msg = "Infinite loop detected"
                yield AgentStreamEvent(type="error", content=err_msg)
                raise AgentError(err_msg)

            # --- Decision: 检查是否调用了结构化输出工具 ---
            # 如果 LLM 调用了我们指定的结构化工具，说明它完成了任务
            # 我们拦截这个调用，不传给 ToolBox，直接作为结果返回
            tool_calls = assistant_msg.safe_tool_calls
            
            if structure_tool_name and tool_calls:
                # 查找目标工具
                target_call = next((tc for tc in tool_calls if tc["function"]["name"] == structure_tool_name), None)
                if target_call:
                    # 这是一个结构化输出结果
                    final_output = AgentOutput(
                        content="", # 内容在 tool_calls 里
                        tool_calls=[target_call], 
                        metadata={"is_structured": True}
                    )
                    yield AgentStreamEvent(type="result", data={"output": final_output})
                    break # 任务结束

            # --- Decision: 正常结束 ---
            # 如果没有工具调用，说明 LLM 输出了纯文本回复，任务结束
            if not tool_calls:
                final_output = AgentOutput(
                    content=str(assistant_msg.content),
                    tool_calls=[],
                )
                yield AgentStreamEvent(type="result", data={"output": final_output})
                break

            # ---------------- Phase 2: Act (Tool Execution) ----------------
            # 通知前端：即将执行工具
            yield AgentStreamEvent(
                type="tool_input", 
                data={"tools": tool_calls}
            )
            
            # 执行工具 (并发)
            tool_results = await self._phase_act(tool_calls)
            
            for res in tool_results:
                # 截断过长的输出，防止 Context Window 爆炸
                content_to_save = self._truncate_observation(res.result, res.tool_name)
                
                # 将结果写入上下文
                context.add_message(
                    Message.tool_result(res.call_id, content_to_save, res.tool_name)
                )
                
                # 通知前端：工具执行完成
                yield AgentStreamEvent(
                    type="tool_output", 
                    content=res.result,
                    data={"tool_name": res.tool_name, "is_error": res.is_error}
                )

            # ---------------- Phase 3: Observe (Reflection Hook) ----------------
            # 这是一个关键的扩展点，子类可以重写此方法实现 Reflexion 或 Human-in-the-loop
            should_continue = await self._phase_observe(context, tool_results)
            
            # Hook: 轮次结束
            if self.on_turn_end:
                await ensure_awaitable(self.on_turn_end, context)
            
            # Checkpoint: 持久化上下文 (防止进程崩溃导致进度丢失)
            await self._save_context(context)

            if not should_continue:
                stop_output = AgentOutput(content="Task stopped by system monitor.")
                yield AgentStreamEvent(type="result", data={"output": stop_output})
                break

    # ================= 阶段实现 (Protected Methods) =================
    # 这些方法设计为可被子类 (如 ReflexionEngine) 重写

    async def _phase_think(
        self, 
        context: ExecutionContext, 
        **kwargs: Any
    ) -> AsyncIterator[StreamChunk]:
        """
        Thinking 阶段：构造 Prompt 并调用 LLM。
        
        负责：
        1. 构造 LLM 调用参数
        2. 流式接收模型响应
        3. 追踪 token 使用和成本
        """
        messages_payload = [m.to_openai_format() for m in context.messages]
        
        # 1. 构造参数
        llm_params = self._build_llm_params(kwargs.get('response_model'), "auto")
        
        # 2. 合并用户传入的 kwargs
        safe_kwargs = {k: v for k, v in kwargs.items() if k not in ['response_model']}
        llm_params.update(safe_kwargs)
        llm_params["stream"] = True
        
        # 3. Token 追踪变量
        input_tokens = 0
        output_tokens = 0
        model_name = self._get_model_name()
        
        try:
            stream_gen = self.model.astream(messages=messages_payload, **llm_params) # type: ignore
            async for chunk in stream_gen:
                # 验证块格式
                if not isinstance(chunk, StreamChunk):
                    logger.warning(f"Invalid StreamChunk type: {type(chunk)}")
                    continue
                
                # 提取 usage 信息（通常在最后的 chunk 中）
                if hasattr(chunk, 'usage') and chunk.usage: # type: ignore
                    # 使用 max 避免 usage 被覆盖
                    input_tokens = max(input_tokens, getattr(chunk.usage, 'prompt_tokens', 0)) # type: ignore
                    output_tokens = max(output_tokens, getattr(chunk.usage, 'completion_tokens', 0)) # type: ignore
                
                yield chunk
            
            # 记录 token 使用到统计（关键！）
            if input_tokens > 0 or output_tokens > 0:
                # 直接更新 stats 中的 token 数
                if self.stats:
                    self.stats.input_tokens += input_tokens
                    self.stats.output_tokens += output_tokens
                
                # 同时记录成本
                self.record_cost(input_tokens, output_tokens, model_name)
                
                logger.debug(
                    f"Tokens tracked in phase_think",
                    input=input_tokens,
                    output=output_tokens,
                    model=model_name
                )
        
        except Exception as e:
            logger.error(f"Model streaming failed: {e}", error_type=type(e).__name__)
            # 通知上游流处理已失败
            yield AgentStreamEvent(
                type="error",
                content=f"Model API error: {e}"
            ) # type: ignore
            raise
    
    def _get_model_name(self) -> str:
        """安全地获取模型名称，包含多个备选方案。"""
        # 优先级顺序：model_name > model > 默认值
        model_name = (
            getattr(self.model, 'model_name', None) or
            getattr(self.model, 'model', None) or
            'gpt-3.5-turbo'
        )
        
        if not isinstance(model_name, str):
            logger.warning(f"Model name is not a string: {type(model_name)}")
            model_name = 'gpt-3.5-turbo'
        
        return model_name

    async def _phase_act(
        self, 
        tool_calls: List[Dict[str, Any]]
    ) -> List[ToolExecutionResult]:
        """
        Acting 阶段：标准化工具调用并并发执行。
        """
        # _normalize_tool_call 会尝试解析 JSON 字符串为 Dict
        flat_calls = [self._normalize_tool_call(tc) for tc in tool_calls]
        return await self.toolbox.execute_many(flat_calls)

    async def _phase_observe(
        self, 
        context: ExecutionContext, 
        results: List[ToolExecutionResult]
    ) -> bool:
        """
        Observing 阶段：分析执行结果，决定是否继续。
        
        [P2-4 Fix] 改进的策略：
        - 统计连续错误次数。
        - 最多允许 2 轮自动重试，防止无限循环。
        - 使用系统反馈而非用户消息，避免 OpenAI API 兼容性问题。
        """
        error_count = sum(1 for r in results if r.is_error)
        
        if error_count > 0:
            context.consecutive_errors += 1
        else:
            context.consecutive_errors = 0
        
        # [P2-4 Fix] 最多允许 2 轮自动重试，防止无限循环
        max_auto_retries = 2
        
        if context.consecutive_errors >= 3:
            if context.turn < max_auto_retries:
                logger.warning(
                    f"Tool errors detected: {error_count}/{len(results)}, auto-retrying",
                    consecutive_errors=context.consecutive_errors,
                    turn=context.turn
                )
                
                # 构造错误摘要
                error_details = "\n".join(
                    f"- {r.tool_name}: {r.result}" for r in results if r.is_error
                )
                
                # [优化] 将错误信息放入 content，而非仅在 metadata
                # 提示词明确要求模型分析错误
                system_feedback = (
                    "System Notification: Multiple tool execution errors detected.\n"
                    "Error Details:\n"
                    f"{error_details}\n\n"
                    "Please analyze these errors, adjust your parameters or tool choice, and try again."
                )

                context.add_message(Message.assistant(
                    content=system_feedback,  # ✅ 填充实际内容
                    tool_calls=None,          # 明确为 None
                    metadata={"type": "system_reflection", "error_summary": error_details} # type: ignore
                ))
                
                context.consecutive_errors = 0
                return True
            else:
                # [P2-4 Fix] 超过自动重试上限，停止执行
                logger.error(
                    "Tool error threshold exceeded, stopping auto-retry",
                    consecutive_errors=context.consecutive_errors,
                    turn=context.turn,
                    max_retries=max_auto_retries
                )
                return False
            
        return True

    # ================= 辅助方法 (Helpers) =================

    def _detect_loop(self, context: ExecutionContext, msg: Message) -> bool:
        """改进的死循环检测算法。
        
        策略：
        1. 检测连续重复（A->A->A）
        2. 检测振荡模式（A->B->A）
        3. 避免合法的重复工具调用（带不同参数）
        """
        import hashlib
        
        if not msg.safe_tool_calls:
            return False
        
        try:
            # 计算当前工具调用的指纹
            calls_dump = json.dumps(
                [
                    {
                        "name": tc.get("function", {}).get("name"),
                        "args": tc.get("function", {}).get("arguments"),
                    }
                    for tc in msg.safe_tool_calls
                ],
                sort_keys=True,
            )
            current_hash = hashlib.sha256(calls_dump.encode()).hexdigest()
            
            # 策略 1: 检测严格的连续循环（同一工具调用重复 N 次）
            consecutive_count = sum(
                1 for h in reversed(context.last_tool_hashes) 
                if h == current_hash
            )
            
            if consecutive_count >= 3:  # 允许最多 3 次相同调用
                logger.warning(
                    f"Consecutive tool call loop detected ({consecutive_count} times)",
                    tool_hash=current_hash[:16],
                    advice="Consider adding early termination logic or changing tool parameters"
                )
                return True
            
            # 策略 2: 检测振荡模式（A->B->A->B）
            if len(context.last_tool_hashes) >= 2:
                # 检查当前 hash 是否与倒数第 2 个相同（且与倒数第 1 个不同）
                if (current_hash == context.last_tool_hashes[-2] and
                    current_hash != context.last_tool_hashes[-1]):
                    logger.warning(
                        "Oscillation pattern detected (A-B-A pattern)",
                        tool_hash=current_hash[:16],
                        advice="Model is alternating between two tool calls. Consider rephrasing instructions."
                    )
                    return True
            
            # 添加当前哈希到历史（保留最近 5 轮）
            context.last_tool_hashes.append(current_hash) # type: ignore
            context.last_tool_hashes = context.last_tool_hashes[-5:]
            context.last_tool_hash = current_hash  # type: ignore # 保持向后兼容
            
            return False
            
        except Exception as e:
            logger.warning(f"Loop detection failed: {e}", exc_info=True)
            # 失败时不触发熔断，继续执行（fail-open）
            return False

    def _truncate_observation(self, content: str, tool_name: str) -> str:
        """截断过长的工具输出，保留头部信息"""
        if len(content) > self.max_observation_length:
            logger.info(
                f"Truncating output for tool {tool_name}", 
                original_len=len(content)
            )
            return (
                content[: self.max_observation_length]
                + f"\n...(truncated, total {len(content)} chars)"
            )
        return content

    def _normalize_tool_call(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:
        """
        将 OpenAI 格式工具调用转换为 ToolBox 所需的扁平格式。
        在此处尝试解析 arguments JSON 字符串。
        """
        func_block = tool_call.get("function", {})
        name = func_block.get("name", "")
        raw_args = func_block.get("arguments", "{}")
        
        parsed_args: Dict[str, Any] = {}
        try:
            if isinstance(raw_args, str):
                parsed_args = json.loads(raw_args)
            elif isinstance(raw_args, dict):
                parsed_args = raw_args
        except json.JSONDecodeError as e:
            # 解析失败时，不抛出异常，而是传递特殊标记给 ToolBox
            # ToolBox 会识别此标记并返回友好的错误提示给 LLM
            parsed_args = {
                "__gecko_parse_error__": f"JSON format error: {str(e)}. Content: {raw_args}"
            }

        return {
            "id": tool_call.get("id", ""),
            "name": name,
            "arguments": parsed_args,
        }

    async def _build_execution_context(self, input_messages: List[Message]) -> ExecutionContext:
        """加载历史并构建 ExecutionContext"""
        history = await self._load_history()
        all_messages = history + input_messages
        
        # [P1-5 Fix] 更严格的系统提示管理
        # 检查是否已有 system 消息（包括 input_messages 中的）
        system_msg = next((m for m in all_messages if m.role == "system"), None)
        
        if system_msg is None:
            # 需要注入系统提示
            template_vars = {
                "tools": self.toolbox.to_openai_schema(),
                "current_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }
            try:
                system_content = self.prompt_template.format_safe(**template_vars)
                # [P1-5 Fix] 确保 system 消消息在最前面（OpenAI API 要求）
                all_messages.insert(0, Message.system(system_content))
            except Exception as e:
                logger.warning(f"System prompt formatting failed: {e}")
                # [P1-5 Fix] Fallback: 使用最小系统提示而非让其缺失
                all_messages.insert(0, Message.system("You are a helpful AI assistant."))
        elif any(m.role == "system" for m in input_messages):
            # [P1-5 Fix] 用户在 input_messages 中显式指定了 system，记录日志
            logger.info("User-specified system message will be used")
            
        return ExecutionContext(all_messages)

    async def _load_history(self) -> List[Message]:
        """从 Memory 加载历史消息"""
        if not self.memory.storage:
            return []
        try:
            data = await self.memory.storage.get(self.memory.session_id)
            if data and "messages" in data:
                return await self.memory.get_history(data["messages"])
        except Exception:
            return []
        return []

    async def _save_context(self, context: ExecutionContext, force: bool = False) -> None:
        """保存当前上下文到 Memory"""
        if not self.memory.storage:
            return
        try:
            messages_data = [m.to_openai_format() for m in context.messages]
            await self.memory.storage.set(
                self.memory.session_id, {"messages": messages_data}
            )
        except Exception as e:
            logger.warning("Failed to save context", error=str(e))

    def _build_llm_params(self, response_model: Any, strategy: str) -> Dict[str, Any]:
        """
        构建 LLM 调用参数 (Tools, Tool Choice)
        """
        params: Dict[str, Any] = {}
        tools_schema = self.toolbox.to_openai_schema()

        # 1. 结构化输出模式 (Structure Mode)
        if response_model and self._supports_functions:
            # 将 Response Model 转换为 Tool Schema
            structure_tool = StructureEngine.to_openai_tool(response_model)
            
            # 合并现有工具 (不能修改原列表)
            combined_tools = tools_schema + [structure_tool]
            params["tools"] = combined_tools
            
            # 强制调用该工具 (OpenAI `tool_choice` syntax)
            params["tool_choice"] = {
                "type": "function",
                "function": {"name": structure_tool["function"]["name"]}
            }
            
        # 2. 标准 ReAct 模式 (Standard Mode)
        elif tools_schema and self._supports_functions:
            params["tools"] = tools_schema
            params["tool_choice"] = "auto"
            
        return params
```

