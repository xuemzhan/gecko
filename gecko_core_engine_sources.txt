[1] gecko/core/engine/base.py
```python
# gecko/core/engine/base.py
"""
认知引擎基类模块

本模块定义了 Agent 推理引擎的抽象基类和相关工具类。
所有具体引擎实现（ReAct、Chain、Tree 等）都应继承 CognitiveEngine 基类。

核心概念：
- CognitiveEngine: 抽象基类，定义引擎统一接口
- ExecutionStats: 执行统计类，用于性能监控
- 支持普通推理、流式推理和结构化输出
- 提供 Hook 机制用于扩展
- 统一的错误处理和资源管理

修复记录：
- [P3] 将硬编码的模型定价配置改为可外部化加载
- [P2] 优化统计锁的使用方式
- [P1] 完善 Hook 机制的异常处理
"""
from __future__ import annotations

import asyncio
import json
import os
import time
import threading
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any, AsyncIterator, Callable, Dict, List, Optional, Type, TypeVar

from pydantic import BaseModel, PrivateAttr

from gecko.core.events.bus import EventBus
from gecko.core.events.types import AgentStreamEvent
from gecko.core.exceptions import AgentError, ModelError
from gecko.core.logging import get_logger
from gecko.core.memory import TokenMemory
from gecko.core.message import Message
from gecko.core.output import AgentOutput
from gecko.core.protocols import ModelProtocol, supports_streaming, validate_model
from gecko.core.toolbox import ToolBox

logger = get_logger(__name__)

# 泛型类型变量，用于结构化输出
T = TypeVar("T", bound=BaseModel)


# ====================== 模型定价配置 ======================

def _load_default_pricing() -> Dict[str, Dict[str, float]]:
    """
    加载默认的模型定价配置
    
    定价单位：USD per 1M tokens
    数据来源：各厂商官方定价（2024年）
    
    返回:
        Dict[str, Dict[str, float]]: 模型名称 -> {"input": 输入价格, "output": 输出价格}
    """
    return {
        # OpenAI 系列
        "gpt-4": {"input": 30.0, "output": 60.0},
        "gpt-4-turbo": {"input": 10.0, "output": 30.0},
        "gpt-4o": {"input": 5.0, "output": 15.0},
        "gpt-4o-mini": {"input": 0.15, "output": 0.6},
        "gpt-3.5-turbo": {"input": 0.5, "output": 1.5},
        # Anthropic 系列
        "claude-3-opus": {"input": 15.0, "output": 75.0},
        "claude-3-sonnet": {"input": 3.0, "output": 15.0},
        "claude-3-haiku": {"input": 0.25, "output": 1.25},
        "claude-3.5-sonnet": {"input": 3.0, "output": 15.0},
    }


def load_model_pricing() -> Dict[str, Dict[str, float]]:
    """
    加载模型定价配置，支持外部文件覆盖
    
    加载优先级：
    1. 环境变量 GECKO_PRICING_FILE 指定的文件
    2. 用户目录下的 ~/.gecko/pricing.json
    3. 内置默认配置
    
    外部配置文件格式示例：
    {
        "gpt-4": {"input": 30.0, "output": 60.0},
        "custom-model": {"input": 1.0, "output": 2.0}
    }
    
    返回:
        Dict[str, Dict[str, float]]: 合并后的定价配置
    """
    # 加载默认配置
    pricing = _load_default_pricing()
    
    # 尝试从环境变量指定的文件加载
    custom_path = os.environ.get("GECKO_PRICING_FILE")
    if custom_path:
        config_file = Path(custom_path)
        if config_file.exists():
            try:
                with open(config_file, "r", encoding="utf-8") as f:
                    custom_pricing = json.load(f)
                    pricing.update(custom_pricing)
                    logger.debug(f"已加载自定义定价配置: {config_file}")
            except (json.JSONDecodeError, IOError) as e:
                logger.warning(f"加载定价配置文件失败: {e}")
    
    # 尝试从用户目录加载
    user_config = Path.home() / ".gecko" / "pricing.json"
    if user_config.exists():
        try:
            with open(user_config, "r", encoding="utf-8") as f:
                custom_pricing = json.load(f)
                pricing.update(custom_pricing)
                logger.debug(f"已加载用户定价配置: {user_config}")
        except (json.JSONDecodeError, IOError) as e:
            logger.warning(f"加载用户定价配置失败: {e}")
    
    return pricing


# 全局定价配置（模块加载时初始化）
MODEL_PRICING = load_model_pricing()


# ====================== 执行统计类 ======================

class ExecutionStats(BaseModel):
    """
    引擎执行统计
    
    用于性能监控、调试和成本跟踪。支持线程安全的统计更新。
    
    属性:
        total_steps: 总执行步数
        total_time: 总执行时间（秒）
        input_tokens: 输入 token 总数
        output_tokens: 输出 token 总数
        tool_calls: 工具调用次数
        errors: 错误次数
        estimated_cost: 估算成本（美元）
    
    使用示例:
        ```python
        stats = ExecutionStats()
        stats.add_step(duration=1.5, input_tokens=100, output_tokens=50)
        print(stats.get_avg_step_time())  # 1.5
        print(stats.get_total_tokens())   # 150
        ```
    """
    
    # 统计字段
    total_steps: int = 0
    total_time: float = 0.0
    input_tokens: int = 0
    output_tokens: int = 0
    tool_calls: int = 0
    errors: int = 0
    estimated_cost: float = 0.0
    
    # 私有属性：可重入锁，保证线程安全
    _lock: threading.RLock = PrivateAttr(default_factory=threading.RLock)

    def add_step(
        self, 
        duration: float, 
        input_tokens: int = 0, 
        output_tokens: int = 0, 
        had_error: bool = False
    ) -> None:
        """
        记录一次步骤执行
        
        线程安全：使用非阻塞方式获取锁，如果无法立即获取则跳过本次统计，
        优先保证业务流程不被阻塞。
        
        参数:
            duration: 执行耗时（秒）
            input_tokens: 本次输入 token 数
            output_tokens: 本次输出 token 数
            had_error: 是否发生错误
        """
        # 尝试非阻塞获取锁
        acquired = self._lock.acquire(blocking=False)
        if not acquired:
            # 无法立即获取锁时跳过，避免阻塞业务
            logger.debug("统计锁竞争，跳过本次记录")
            return
        
        try:
            self.total_steps += 1
            self.total_time += duration
            self.input_tokens += input_tokens
            self.output_tokens += output_tokens
            if had_error:
                self.errors += 1
        finally:
            self._lock.release()

    def add_tool_call(self) -> None:
        """记录一次工具调用"""
        acquired = self._lock.acquire(blocking=False)
        if acquired:
            try:
                self.tool_calls += 1
            finally:
                self._lock.release()

    def add_cost(self, cost: float) -> None:
        """
        累加成本估算
        
        参数:
            cost: 本次成本（美元）
        """
        acquired = self._lock.acquire(blocking=False)
        if acquired:
            try:
                self.estimated_cost += cost
            finally:
                self._lock.release()

    def get_avg_step_time(self) -> float:
        """获取平均步骤执行时间（秒）"""
        return self.total_time / self.total_steps if self.total_steps > 0 else 0.0
    
    def get_total_tokens(self) -> int:
        """获取总 token 数"""
        return self.input_tokens + self.output_tokens
    
    def get_error_rate(self) -> float:
        """获取错误率（0.0 - 1.0）"""
        return self.errors / self.total_steps if self.total_steps > 0 else 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """
        转换为字典格式
        
        返回:
            包含所有统计信息的字典
        """
        return {
            "total_steps": self.total_steps,
            "total_time": round(self.total_time, 3),
            "avg_step_time": round(self.get_avg_step_time(), 3),
            "input_tokens": self.input_tokens,
            "output_tokens": self.output_tokens,
            "total_tokens": self.get_total_tokens(),
            "tool_calls": self.tool_calls,
            "errors": self.errors,
            "error_rate": round(self.get_error_rate(), 4),
            "estimated_cost": round(self.estimated_cost, 6),
        }


# ====================== 认知引擎基类 ======================

class CognitiveEngine(ABC):
    """
    认知引擎抽象基类
    
    定义 Agent 的核心推理流程。所有具体引擎实现（ReAct、Chain、Tree 等）
    都应该继承此类并实现抽象方法。
    
    核心方法（必须实现）:
        step(): 执行推理步骤，返回 AgentOutput
    
    可选方法（按需覆盖）:
        step_stream(): 流式推理，返回事件流
        step_structured(): 结构化输出推理
    
    Hook 方法（扩展点）:
        before_step(): 步骤执行前调用
        after_step(): 步骤执行后调用
        on_error(): 错误发生时调用
    
    生命周期方法:
        initialize(): 初始化引擎资源
        cleanup(): 清理引擎资源
    
    使用示例:
        ```python
        class MyEngine(CognitiveEngine):
            async def step(self, input_messages: List[Message]) -> AgentOutput:
                response = await self.model.acompletion(
                    messages=[m.to_openai_format() for m in input_messages]
                )
                return AgentOutput(content=response.choices[0].message["content"])
        
        # 使用上下文管理器确保资源清理
        async with MyEngine(model, toolbox, memory) as engine:
            output = await engine.step([Message.user("你好")])
        ```
    
    属性:
        model: 语言模型实例（必须实现 ModelProtocol）
        toolbox: 工具箱，包含可调用的工具
        memory: 记忆管理器
        event_bus: 事件总线（可选），用于发布引擎事件
        max_iterations: 最大迭代次数，防止死循环
        stats: 执行统计对象（如果启用）
    """
    
    def __init__(
        self,
        model: ModelProtocol,
        toolbox: ToolBox,
        memory: TokenMemory,
        event_bus: Optional[EventBus] = None,
        max_iterations: int = 10,
        enable_stats: bool = True,
        **kwargs: Any
    ):
        """
        初始化认知引擎
        
        参数:
            model: 语言模型实例，必须实现 ModelProtocol 协议
            toolbox: 工具箱实例
            memory: 记忆管理器实例
            event_bus: 事件总线（可选），用于发布引擎运行事件
            max_iterations: 最大迭代次数，默认 10，防止无限循环
            enable_stats: 是否启用执行统计，默认 True
            **kwargs: 额外配置参数，存储在 _config 中供子类使用
        
        异常:
            TypeError: 如果 model 不符合 ModelProtocol 协议
        
        配置项（通过 kwargs 传入）:
            hooks_fail_fast: bool - Hook 出错时是否立即终止，默认 False
        """
        # 验证模型是否符合协议（鸭子类型检查）
        validate_model(model)
        self.model = model
        
        self.toolbox = toolbox
        self.memory = memory
        self.event_bus = event_bus
        self.max_iterations = max_iterations
        self.enable_stats = enable_stats
        
        # 初始化统计对象
        self.stats = ExecutionStats() if enable_stats else None
        
        # Hook 函数槽位（可由子类或外部设置）
        self.before_step_hook: Optional[Callable] = None
        self.after_step_hook: Optional[Callable] = None
        self.on_error_hook: Optional[Callable] = None
        
        # 存储额外配置
        self._config: Dict[str, Any] = kwargs
        
        # Hook 失败时是否快速失败（默认 False，静默处理）
        self.hooks_fail_fast: bool = bool(kwargs.get("hooks_fail_fast", False))
        
        logger.debug(
            "引擎初始化完成",
            engine=self.__class__.__name__,
            model=type(model).__name__,
            max_iterations=max_iterations,
            event_bus_enabled=event_bus is not None
        )
    
    # ====================== 核心抽象方法 ======================
    
    @abstractmethod
    async def step(
        self, 
        input_messages: List[Message],
        **kwargs: Any
    ) -> AgentOutput:
        """
        执行推理步骤（必须实现）
        
        这是引擎的核心方法，定义了如何处理输入消息并生成输出。
        子类必须实现此方法。
        
        参数:
            input_messages: 输入消息列表，至少包含一条消息
            **kwargs: 额外参数，如 temperature, max_tokens 等
        
        返回:
            AgentOutput: 执行结果，包含内容、工具调用等信息
        
        异常:
            AgentError: 执行失败
            ModelError: 模型调用失败
            ValueError: 输入无效
        
        实现指南:
            1. 调用 validate_input() 验证输入
            2. 调用 before_step() Hook
            3. 执行推理逻辑（调用模型、处理工具等）
            4. 调用 after_step() Hook
            5. 返回 AgentOutput
        """
        pass
    
    # ====================== 可选方法（按需覆盖）======================
    
    async def step_stream(
        self, 
        input_messages: List[Message],
        **kwargs: Any
    ) -> AsyncIterator[AgentStreamEvent]:
        """
        流式推理（可选实现）
        
        统一契约：输出 AgentStreamEvent 事件流，而非纯文本 token。
        
        设计原因：
        - ReAct 等复杂引擎需要输出结构化事件（tool_input/tool_output/result/error）
        - 工业级系统（WebSocket/SSE/调试面板）需要事件流而非仅 token
        - 与 Agent.stream / Agent.stream_events 保持一致
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数
        
        返回:
            AsyncIterator[AgentStreamEvent]: 事件流
        
        异常:
            NotImplementedError: 引擎不支持流式输出时抛出
        
        事件类型:
            - token: 实时生成的文本片段
            - tool_input: 工具调用意图和参数
            - tool_output: 工具执行结果
            - result: 最终生成的回复
            - error: 执行过程中的错误
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} 不支持流式输出。"
            f"请重写 step_stream() 方法以启用此功能。"
        )
    
    async def step_text_stream(
        self,
        input_messages: List[Message],
        **kwargs: Any
    ) -> AsyncIterator[str]:
        """
        纯文本 token 流（兼容层）
        
        从 step_stream 事件流中过滤出 token 事件并 yield 文本内容。
        这提供了一个简化的接口，仅返回文本流。
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数
        
        返回:
            AsyncIterator[str]: 文本 token 流
        
        使用场景:
            - 简单的流式文本展示
            - 不需要工具调用事件的场景
        """
        async for event in self.step_stream(input_messages, **kwargs): # type: ignore
            if event.type == "token" and event.content is not None:
                yield str(event.content)
    
    async def step_structured(
        self,
        input_messages: List[Message],
        response_model: Type[T],
        **kwargs: Any
    ) -> T:
        """
        结构化输出推理（可选实现）
        
        执行推理并将输出解析为指定的 Pydantic 模型实例。
        
        参数:
            input_messages: 输入消息列表
            response_model: 目标 Pydantic 模型类
            **kwargs: 额外参数
        
        返回:
            T: 解析后的模型实例
        
        异常:
            NotImplementedError: 引擎不支持结构化输出时抛出
            AgentError: 解析失败
        
        使用示例:
            ```python
            from pydantic import BaseModel
            
            class Answer(BaseModel):
                question: str
                answer: str
                confidence: float
            
            result = await engine.step_structured(
                input_messages=[Message.user("什么是人工智能？")],
                response_model=Answer
            )
            print(result.answer)
            ```
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} 不支持结构化输出。"
            f"请重写 step_structured() 方法以启用此功能。"
        )
    
    # ====================== Hook 方法 ======================
    
    async def before_step(
        self, 
        input_messages: List[Message],
        **kwargs: Any
    ) -> None:
        """
        步骤执行前的 Hook
        
        在推理开始前调用，可用于：
        - 日志记录
        - 输入验证和预处理
        - 状态初始化
        - 发送开始事件
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数
        
        注意:
            - 此方法不应修改输入消息
            - Hook 异常默认被捕获并记录，不影响主流程
            - 设置 hooks_fail_fast=True 可让 Hook 异常终止执行
        """
        # 发布事件到 EventBus（如果配置了）
        await self._publish_event("step_started", {
            "message_count": len(input_messages),
            "engine": self.__class__.__name__
        })
        
        # 执行用户自定义 Hook
        if self.before_step_hook:
            try:
                if asyncio.iscoroutinefunction(self.before_step_hook):
                    await self.before_step_hook(input_messages, **kwargs)
                else:
                    self.before_step_hook(input_messages, **kwargs)
            except Exception as e:
                logger.warning("before_step_hook 执行失败", error=str(e), exc_info=True)
                if self.hooks_fail_fast:
                    raise
    
    async def after_step(
        self,
        input_messages: List[Message],
        output: AgentOutput,
        **kwargs: Any
    ) -> None:
        """
        步骤执行后的 Hook
        
        在推理完成后调用，可用于：
        - 日志记录
        - 结果验证和后处理
        - 统计更新
        - 发送完成事件
        
        参数:
            input_messages: 输入消息列表
            output: 执行结果
            **kwargs: 额外参数
        """
        # 发布事件到 EventBus
        await self._publish_event("step_completed", {
            "content_length": len(output.content) if output.content else 0,
            "has_tool_calls": bool(output.tool_calls),
            "engine": self.__class__.__name__
        })
        
        # 执行用户自定义 Hook
        if self.after_step_hook:
            try:
                if asyncio.iscoroutinefunction(self.after_step_hook):
                    await self.after_step_hook(input_messages, output, **kwargs)
                else:
                    self.after_step_hook(input_messages, output, **kwargs)
            except Exception as e:
                logger.warning("after_step_hook 执行失败", error=str(e), exc_info=True)
                if self.hooks_fail_fast:
                    raise
    
    async def on_error(
        self,
        error: Exception,
        input_messages: List[Message],
        **kwargs: Any
    ) -> None:
        """
        错误处理 Hook
        
        在推理过程中发生错误时调用，可用于：
        - 错误日志记录
        - 错误恢复尝试
        - 降级处理
        - 发送告警通知
        
        参数:
            error: 异常对象
            input_messages: 输入消息列表
            **kwargs: 额外参数
        """
        # 发布错误事件到 EventBus
        await self._publish_event("step_error", {
            "error_type": type(error).__name__,
            "error_message": str(error),
            "engine": self.__class__.__name__
        })
        
        # 执行用户自定义 Hook
        if self.on_error_hook:
            try:
                if asyncio.iscoroutinefunction(self.on_error_hook):
                    await self.on_error_hook(error, input_messages, **kwargs)
                else:
                    self.on_error_hook(error, input_messages, **kwargs)
            except Exception as e:
                logger.error("on_error_hook 执行失败", error=str(e), exc_info=True)
                if self.hooks_fail_fast:
                    raise

    # ====================== 事件发布 ======================

    async def _publish_event(self, event_type: str, data: Dict[str, Any]) -> None:
        """
        发布事件到 EventBus
        
        如果配置了 event_bus，则发布事件；否则静默忽略。
        事件发布失败不会影响主流程。
        
        参数:
            event_type: 事件类型（如 "step_started", "step_completed"）
            data: 事件数据
        """
        if self.event_bus is None:
            return
        
        try:
            # EventBus 可能是同步或异步的
            if hasattr(self.event_bus, 'publish'):
                publish_method = self.event_bus.publish
                if asyncio.iscoroutinefunction(publish_method):
                    await publish_method(event_type, data) # type: ignore
                else:
                    publish_method(event_type, data) # type: ignore
        except Exception as e:
            # 事件发布失败不应影响主流程
            logger.debug(f"事件发布失败: {event_type}", error=str(e))

    # ====================== 统计辅助方法 ======================
    
    def record_step(
        self, 
        duration: float, 
        input_tokens: int = 0, 
        output_tokens: int = 0, 
        had_error: bool = False
    ) -> None:
        """
        记录一次执行步骤的统计信息
        
        供子类在合适的位置调用，统一更新统计数据。
        
        参数:
            duration: 执行耗时（秒）
            input_tokens: 输入 token 数
            output_tokens: 输出 token 数
            had_error: 是否发生错误
        """
        if self.stats is not None:
            try:
                self.stats.add_step(
                    duration, 
                    input_tokens=input_tokens, 
                    output_tokens=output_tokens, 
                    had_error=had_error
                )
            except Exception:
                logger.debug("更新步骤统计失败")

    def record_tool_call(self) -> None:
        """记录一次工具调用"""
        if self.stats is not None:
            try:
                self.stats.add_tool_call()
            except Exception:
                logger.debug("更新工具调用统计失败")

    def record_cost(
        self, 
        input_tokens: int = 0, 
        output_tokens: int = 0, 
        model_name: str = ""
    ) -> None:
        """
        基于 token 数和模型名称记录估算成本
        
        参数:
            input_tokens: 输入 token 数
            output_tokens: 输出 token 数
            model_name: 模型名称，用于查询定价表
        """
        if self.stats is None:
            return
        
        # 获取模型定价（默认使用 gpt-3.5-turbo 的价格）
        pricing = MODEL_PRICING.get(model_name)
        if not pricing:
            # 尝试模糊匹配（如 "gpt-4-0125-preview" 匹配 "gpt-4"）
            for key in MODEL_PRICING:
                if model_name.startswith(key):
                    pricing = MODEL_PRICING[key]
                    break
            if not pricing:
                pricing = MODEL_PRICING.get("gpt-3.5-turbo", {"input": 0.5, "output": 1.5})
        
        # 计算成本：tokens * (price_per_million / 1_000_000)
        cost = (
            input_tokens * pricing["input"] / 1_000_000 +
            output_tokens * pricing["output"] / 1_000_000
        )
        
        try:
            self.stats.add_cost(cost)
        except Exception:
            logger.debug("记录成本失败", tokens=(input_tokens, output_tokens))
    
    def get_stats_summary(self) -> Dict[str, Any]:
        """
        获取执行统计摘要
        
        返回:
            统计信息字典，如果未启用统计则返回空字典
        """
        if self.stats is None:
            return {}
        return self.stats.to_dict()
    
    def get_stats(self) -> Optional[Dict[str, Any]]:
        """
        获取执行统计（兼容旧接口）
        
        返回:
            统计信息字典，如果未启用统计则返回 None
        """
        return self.stats.to_dict() if self.stats else None
    
    def reset_stats(self) -> None:
        """重置统计信息"""
        if self.stats:
            self.stats = ExecutionStats()
            logger.debug("统计信息已重置")
    
    # ====================== 工具方法 ======================
    
    def validate_input(self, input_messages: List[Message]) -> None:
        """
        验证输入消息
        
        参数:
            input_messages: 输入消息列表
        
        异常:
            ValueError: 输入为空
            TypeError: 输入类型错误
        """
        if not input_messages:
            raise ValueError("input_messages 不能为空")
        
        if not all(isinstance(m, Message) for m in input_messages):
            raise TypeError("所有输入必须是 Message 实例")
        
        logger.debug("输入验证通过", message_count=len(input_messages))
    
    def supports_streaming(self) -> bool:
        """
        检查引擎是否支持流式输出
        
        返回:
            bool: 是否支持流式输出
        """
        # 检查模型是否支持流式
        model_supports = supports_streaming(self.model)
        
        # 检查引擎是否重写了 step_stream 方法
        engine_supports = (
            self.__class__.step_stream != CognitiveEngine.step_stream
        )
        
        return model_supports and engine_supports
    
    def get_config(self, key: str, default: Any = None) -> Any:
        """
        获取配置项
        
        参数:
            key: 配置键
            default: 默认值
        
        返回:
            配置值
        """
        return self._config.get(key, default)
    
    def set_config(self, key: str, value: Any) -> None:
        """
        设置配置项
        
        参数:
            key: 配置键
            value: 配置值
        """
        self._config[key] = value
    
    # ====================== 生命周期管理 ======================
    
    async def initialize(self) -> None:
        """
        初始化引擎
        
        在首次使用前调用，可用于：
        - 加载外部资源
        - 预热模型连接
        - 初始化缓存
        
        子类可以重写此方法添加自定义初始化逻辑。
        """
        logger.debug("引擎初始化", engine=self.__class__.__name__)
    
    async def cleanup(self) -> None:
        """
        清理资源
        
        在引擎不再使用时调用，可用于：
        - 关闭连接
        - 释放资源
        - 保存状态
        
        子类可以重写此方法添加自定义清理逻辑。
        """
        logger.debug("引擎清理", engine=self.__class__.__name__)
    
    async def health_check(self) -> Dict[str, Any]:
        """
        健康检查接口
        
        用于监控系统检测引擎状态。
        
        返回:
            包含健康状态信息的字典
        """
        return {
            "engine": self.__class__.__name__,
            "model": type(self.model).__name__,
            "supports_streaming": self.supports_streaming(),
            "stats": self.get_stats_summary(),
            "status": "healthy"
        }
    
    # ====================== 上下文管理器 ======================
    
    async def __aenter__(self) -> "CognitiveEngine":
        """异步上下文管理器入口"""
        await self.initialize()
        return self
    
    async def __aexit__(
        self, 
        exc_type: Optional[type], 
        exc_val: Optional[BaseException], 
        exc_tb: Any
    ) -> bool:
        """异步上下文管理器出口"""
        await self.cleanup()
        return False  # 不抑制异常
    
    # ====================== 辅助方法 ======================
    
    async def _safe_execute(
        self,
        func: Callable,
        *args: Any,
        **kwargs: Any
    ) -> Any:
        """
        安全执行函数（带错误处理和统计）
        
        包装函数执行，自动处理：
        - 执行时间统计
        - 异常捕获和记录
        - Hook 调用
        
        参数:
            func: 要执行的函数
            *args: 位置参数
            **kwargs: 关键字参数
        
        返回:
            函数执行结果
        
        异常:
            原始异常（已记录日志和统计）
        """
        start_time = time.time()
        had_error = False
        
        try:
            if asyncio.iscoroutinefunction(func):
                result = await func(*args, **kwargs)
            else:
                result = func(*args, **kwargs)
            
            return result
        
        except Exception as e:
            had_error = True
            
            # 记录错误统计
            if self.stats:
                self.stats.errors += 1
            
            # 调用错误 Hook
            await self.on_error(e, kwargs.get("input_messages", []))
            
            # 记录日志
            logger.exception(
                "引擎执行失败",
                engine=self.__class__.__name__,
                error=str(e)
            )
            
            raise
        
        finally:
            # 记录执行时间
            duration = time.time() - start_time
            if self.stats:
                self.stats.add_step(duration, had_error=had_error)
    
    def __repr__(self) -> str:
        """字符串表示"""
        return (
            f"{self.__class__.__name__}("
            f"model={type(self.model).__name__}, "
            f"max_iterations={self.max_iterations}, "
            f"stats_enabled={self.stats is not None}"
            f")"
        )


# ====================== 工厂函数 ======================

def create_engine(
    engine_class: Type[CognitiveEngine],
    model: ModelProtocol,
    toolbox: ToolBox,
    memory: TokenMemory,
    **kwargs: Any
) -> CognitiveEngine:
    """
    创建引擎实例（工厂函数）
    
    提供统一的引擎创建接口，进行类型检查。
    
    参数:
        engine_class: 引擎类（必须是 CognitiveEngine 的子类）
        model: 模型实例
        toolbox: 工具箱实例
        memory: 记忆管理器实例
        **kwargs: 传递给引擎构造函数的额外参数
    
    返回:
        CognitiveEngine: 引擎实例
    
    异常:
        TypeError: engine_class 不是 CognitiveEngine 的子类
    
    使用示例:
        ```python
        engine = create_engine(
            ReActEngine,
            model=openai_model,
            toolbox=toolbox,
            memory=memory,
            max_iterations=5
        )
        ```
    """
    if not issubclass(engine_class, CognitiveEngine):
        raise TypeError(
            f"engine_class 必须是 CognitiveEngine 的子类，"
            f"收到: {engine_class.__name__}"
        )
    
    return engine_class(
        model=model,
        toolbox=toolbox,
        memory=memory,
        **kwargs
    )


# ====================== 模块导出 ======================

__all__ = [
    "CognitiveEngine",
    "ExecutionStats",
    "create_engine",
    "MODEL_PRICING",
    "load_model_pricing",
]
```

[2] gecko/core/engine/buffer.py
```python
# gecko/core/engine/buffer.py
"""
流式缓冲区模块

核心职责：
1. 解决 OpenAI 协议中流式 Tool Call 分片传输、乱序到达的问题
2. 解决 LLM 输出不规范 JSON（如 Markdown 包裹）导致的解析崩溃问题
3. 提供线程安全的流式数据聚合

设计原则：
- 防御性编程：假设 LLM 输出可能不规范
- 资源保护：限制缓冲区大小，防止内存溢出
- 容错处理：解析失败时优雅降级

修复记录：
- [P2] 增强完整性检查和验证机制
- [P1] 进阶 JSON 清洗，处理更多边缘情况
"""
from __future__ import annotations

import json
import re
import threading
from typing import Any, Dict, List, Optional

from gecko.core.message import Message
from gecko.core.protocols import StreamChunk
from gecko.core.logging import get_logger

logger = get_logger(__name__)


class StreamBuffer:
    """
    流式响应聚合缓冲区
    
    使用场景：
    在 Engine 的 Thinking 阶段，随着 LLM 流式吐出 token，此类负责实时聚合，
    并在最后产出一个结构完整的 Message 对象。
    
    主要功能：
    1. 聚合文本内容片段
    2. 处理分片传输的工具调用
    3. 修复不规范的 JSON 参数
    
    线程安全：
    使用可重入锁保护内部状态，支持并发调用。
    
    使用示例：
        ```python
        buffer = StreamBuffer()
        
        async for chunk in model.astream(messages):
            text_delta = buffer.add_chunk(chunk)
            if text_delta:
                print(text_delta, end="", flush=True)
        
        message = buffer.build_message()
        ```
    
    属性：
        max_content_chars: 最大内容字符数限制
        max_argument_chars: 最大参数字符数限制
        max_tool_index: 最大工具索引限制
    """
    
    def __init__(
        self,
        max_content_chars: int = 200_000,
        max_argument_chars: int = 100_000,
        max_tool_index: int = 1000,
    ):
        """
        初始化流式缓冲区
        
        参数：
            max_content_chars: 最大内容字符数（防止内存溢出），默认 200k
            max_argument_chars: 工具参数最大字符数，默认 100k
            max_tool_index: 最大工具索引值（防止稀疏数组攻击），默认 1000
        """
        # 文本内容片段列表
        self.content_parts: List[str] = []
        
        # 工具调用映射：index -> tool_call_dict
        # 用于处理并发或分片传输的工具调用
        self.tool_calls_map: Dict[int, Dict[str, Any]] = {}
        
        # 跟踪最大工具索引
        self._max_tool_index: int = -1
        
        # 可配置的限制参数
        self._max_content_chars: int = max_content_chars
        self._max_argument_chars: int = max_argument_chars
        self._max_tool_index_limit: int = max_tool_index
        
        # 可重入锁，保护并发访问
        self._lock: threading.RLock = threading.RLock()
        
    def add_chunk(self, chunk: StreamChunk) -> Optional[str]:
        """
        接收一个流式块，更新内部状态
        
        参数：
            chunk: StreamChunk 对象，包含 delta 字段
        
        返回：
            Optional[str]: 本次 chunk 中新增的文本内容（用于流式回显）。
                          如果是纯工具调用的 chunk，返回 None。
        
        注意：
            - 方法是线程安全的
            - 自动处理内容截断和工具索引验证
            - 无效的 chunk 会被静默跳过
        """
        with self._lock:
            # 验证 chunk 格式
            if not hasattr(chunk, 'delta') or chunk.delta is None:
                logger.debug("StreamChunk 缺少 delta 字段，跳过")
                return None
            
            delta = chunk.delta
            new_content: Optional[str] = None

            # 1. 聚合文本内容
            content = delta.get("content")
            if content:
                new_content = self._add_content(content)

            # 2. 聚合工具调用
            tool_calls = delta.get("tool_calls")
            if tool_calls:
                self._add_tool_calls(tool_calls)
        
        return new_content
    
    def _add_content(self, content: str) -> str:
        """
        添加文本内容到缓冲区
        
        参数：
            content: 新的文本片段
        
        返回：
            实际添加的文本（可能被截断）
        """
        # 计算当前累计长度
        current_len = sum(len(p) for p in self.content_parts)
        
        # 检查是否超出限制
        if current_len + len(content) > self._max_content_chars:
            logger.warning(
                "内容超出限制，将截断",
                current_len=current_len,
                incoming_len=len(content),
                limit=self._max_content_chars
            )
            # 只追加能容纳的部分
            allowed = max(0, self._max_content_chars - current_len)
            if allowed > 0:
                truncated = content[:allowed]
                self.content_parts.append(truncated)
                return truncated
            return ""
        
        self.content_parts.append(content)
        return content
    
    def _add_tool_calls(self, tool_calls: List[Dict[str, Any]]) -> None:
        """
        添加工具调用到缓冲区
        
        处理分片传输和乱序到达的工具调用。
        
        参数：
            tool_calls: 工具调用片段列表
        """
        for tc in tool_calls:
            idx = tc.get("index")
            if idx is None:
                continue

            # 验证索引有效性
            if idx < 0:
                logger.warning(f"收到负数工具索引: {idx}，跳过")
                continue

            if idx > self._max_tool_index_limit:
                logger.warning(
                    f"工具索引超出限制: {idx} > {self._max_tool_index_limit}，跳过"
                )
                continue

            # 检测异常大的索引间隙（可能是恶意数据或配置错误）
            if self._max_tool_index >= 0:
                gap = idx - self._max_tool_index
                if gap > 500:
                    logger.warning(
                        f"检测到异常大的工具索引间隙",
                        prev_max=self._max_tool_index,
                        new_idx=idx,
                        gap=gap,
                        action="继续处理但需监控"
                    )

            # 更新最大索引
            if idx > self._max_tool_index:
                self._max_tool_index = idx

            # 初始化该索引的工具调用结构
            if idx not in self.tool_calls_map:
                self.tool_calls_map[idx] = {
                    "id": "",
                    "type": "function",
                    "function": {"name": "", "arguments": ""}
                }

            target = self.tool_calls_map[idx]

            # 增量拼接 ID
            if tc.get("id"):
                target["id"] += tc["id"]

            # 增量合并函数信息
            func = tc.get("function", {})
            if func:
                self._merge_function(target["function"], func)
    
    def _merge_function(
        self, 
        target: Dict[str, str], 
        incoming: Dict[str, Any]
    ) -> None:
        """
        合并函数调用信息
        
        参数：
            target: 目标函数字典
            incoming: 新到达的函数片段
        """
        # 函数名使用"最新非空片段覆盖"策略
        # 避免在流式分片时产生重复拼接（如 "search" + "search" = "searchsearch"）
        if incoming.get("name"):
            target["name"] = incoming["name"]
        
        # 参数使用增量拼接
        if incoming.get("arguments"):
            new_args = incoming["arguments"]
            current_len = len(target["arguments"])
            
            # 检查参数长度限制
            if current_len + len(new_args) > self._max_argument_chars:
                logger.warning(
                    f"工具参数超出限制 ({current_len + len(new_args)} > {self._max_argument_chars})，截断"
                )
                allowed = max(0, self._max_argument_chars - current_len)
                if allowed > 0:
                    target["arguments"] += new_args[:allowed]
            else:
                target["arguments"] += new_args

    def build_message(self) -> Message:
        """
        构建最终的 Message 对象
        
        在此阶段会对收集到的 JSON 参数进行"清洗"和"修复"，
        防止因 Markdown 符号或引号问题导致后续工具执行失败。
        
        返回：
            Message: 完整的助手消息，包含聚合后的内容和工具调用
        
        注意：
            - 空名称的工具调用会被跳过
            - 无法解析的 JSON 参数会被替换为空对象 {}
            - 返回的消息符合 OpenAI API 规范
        """
        # 聚合所有文本内容
        full_content = "".join(self.content_parts)
        
        # 处理工具调用
        tool_calls_list: List[Dict[str, Any]] = []
        
        # 按索引排序，确保顺序一致性
        for idx in sorted(self.tool_calls_map.keys()):
            raw_tc = self.tool_calls_map[idx]
            
            # 验证工具调用完整性
            func_name = raw_tc["function"].get("name", "").strip()
            raw_args = raw_tc["function"].get("arguments", "").strip()
            
            # 跳过空名称的工具调用
            if not func_name:
                logger.warning(f"索引 {idx} 的工具调用缺少名称，跳过")
                continue
            
            # 处理空参数
            if not raw_args:
                logger.debug(f"工具 '{func_name}' (索引 {idx}) 参数为空，使用默认 {{}}")
                raw_args = "{}"
            
            # 深度清洗参数 JSON
            cleaned_args = self._clean_arguments(raw_args)
            
            # 更新清洗后的参数
            raw_tc["function"]["arguments"] = cleaned_args
            
            tool_calls_list.append(raw_tc)
        
        if tool_calls_list:
            logger.debug(f"构建消息完成，包含 {len(tool_calls_list)} 个工具调用")
            
        return Message.assistant(
            content=full_content,
            # 只有当列表非空时才设值，符合 OpenAI 规范
            tool_calls=tool_calls_list if tool_calls_list else None
        )

    def _clean_arguments(self, raw_json: str) -> str:
        """
        进阶 JSON 清洗：处理 LLM 的常见输出格式问题
        
        修复项：
        1. Markdown 代码块包裹 (```json ... ```)
        2. 首尾多余的误加引号 ('{...}')
        3. 尾部逗号 ({"key": "value",})
        4. 未转义的特殊字符
        5. 单引号替换为双引号
        
        参数：
            raw_json: 原始 JSON 字符串
        
        返回：
            清洗后的 JSON 字符串，如果无法修复则返回 "{}"
        """
        if not raw_json:
            return "{}"
        
        # 1. 快速路径：尝试直接解析（如果模型输出规范）
        try:
            json.loads(raw_json)
            return raw_json
        except json.JSONDecodeError:
            pass
        
        cleaned = raw_json.strip()
        
        # 2. 去除 Markdown 代码块
        # 匹配 ```json {...} ``` 或 ``` {...} ```
        if cleaned.startswith("```"):
            match = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", cleaned)
            if match:
                cleaned = match.group(1).strip()
        
        # 3. 去除首尾多余的误加引号
        # 例如模型输出了: '{"arg": "val"}' (带单引号的字符串)
        if (
            (cleaned.startswith("'") and cleaned.endswith("'")) or
            (cleaned.startswith('"') and cleaned.endswith('"'))
        ):
            # 确保不是合法的 JSON 字符串
            inner = cleaned[1:-1]
            if inner.startswith("{") or inner.startswith("["):
                cleaned = inner
        
        # 4. 修复尾部逗号
        # {"key": "value",} -> {"key": "value"}
        cleaned = re.sub(r',\s*}', '}', cleaned)
        # ["item",] -> ["item"]
        cleaned = re.sub(r',\s*\]', ']', cleaned)
        
        # 5. 尝试修复单引号（某些模型可能输出 Python dict 格式）
        # 只在确定不会破坏数据的情况下替换
        if "'" in cleaned and '"' not in cleaned:
            cleaned = cleaned.replace("'", '"')
        
        # 6. 再次尝试解析验证
        try:
            json.loads(cleaned)
            return cleaned
        except json.JSONDecodeError as e:
            # 如果还无法解析，记录警告并返回空对象
            logger.warning(
                "JSON 参数清洗失败，返回空对象",
                original=raw_json[:100] if len(raw_json) > 100 else raw_json,
                parse_error=str(e)
            )
            # 返回空对象而非脏数据，这样工具调用会因参数缺失而失败，
            # LLM 可以在错误消息中看到并尝试修正
            return "{}"
    
    def reset(self) -> None:
        """
        重置缓冲区状态
        
        清空所有累积的内容和工具调用，可用于复用缓冲区实例。
        """
        with self._lock:
            self.content_parts.clear()
            self.tool_calls_map.clear()
            self._max_tool_index = -1
    
    def get_current_content(self) -> str:
        """
        获取当前累积的文本内容（不构建完整消息）
        
        返回：
            当前累积的所有文本内容
        """
        with self._lock:
            return "".join(self.content_parts)
    
    def get_tool_call_count(self) -> int:
        """
        获取当前累积的工具调用数量
        
        返回：
            工具调用数量
        """
        with self._lock:
            return len(self.tool_calls_map)
    
    def __repr__(self) -> str:
        """字符串表示"""
        return (
            f"StreamBuffer("
            f"content_length={len(self.get_current_content())}, "
            f"tool_calls={self.get_tool_call_count()}"
            f")"
        )


# ====================== 模块导出 ======================

__all__ = ["StreamBuffer"]
```

[3] gecko/core/engine/react.py
```python
# gecko/core/engine/react.py
"""
ReAct 推理引擎 (Production Grade / 生产级)

ReAct (Reasoning + Acting) 是一种经典的 Agent 推理范式，
通过交替进行"思考"和"行动"来完成复杂任务。

架构设计原则：
==============

1. 无状态设计 (Stateless):
   Engine 实例不持有任何单次请求的状态，所有运行时状态封装在 ExecutionContext 中。
   这使得单个 Engine 实例可以在多线程/异步环境下安全地处理并发请求。

2. 生命周期分解 (Lifecycle Decomposition):
   将复杂的 ReAct while 循环拆解为三个独立阶段：
   - _phase_think (思考): 调用 LLM 进行推理
   - _phase_act (行动): 执行工具调用
   - _phase_observe (观察): 分析执行结果，决定是否继续
   
   子类（如 ReflexionEngine）可以重写特定阶段而不必复制整个循环逻辑。

3. 事件驱动 (Event Driven):
   统一使用 AgentStreamEvent 协议输出，消除了 yield 返回类型不明确的问题。
   同步接口 step() 仅仅是流式接口 step_stream() 的消费者。

4. 鲁棒性 (Robustness):
   - 集成 StreamBuffer 处理流式碎片及修复不规范 JSON
   - 内置死循环熔断机制（基于工具调用指纹检测）
   - 内置观察值截断机制，防止 Context Window 爆炸
   - 智能上下文裁剪，保证消息对的完整性

使用示例：
=========

    ```python
    from gecko.core.engine.react import ReActEngine
    from gecko.core.message import Message
    
    # 创建引擎
    engine = ReActEngine(
        model=openai_model,
        toolbox=toolbox,
        memory=memory,
        max_turns=10
    )
    
    # 同步执行
    result = await engine.step([Message.user("帮我查询天气")])
    print(result.content)
    
    # 流式执行
    async for event in engine.step_stream([Message.user("帮我查询天气")]):
        if event.type == "token":
            print(event.content, end="", flush=True)
        elif event.type == "tool_output":
            print(f"工具执行结果: {event.content}")
    ```
"""
from __future__ import annotations

import asyncio
import json
import hashlib
import time
from datetime import datetime
from typing import (
    Any,
    AsyncIterator,
    Callable,
    Dict,
    List,
    Optional,
    Set,
    Type,
    TypeVar,
    Union,
    cast,
)

from pydantic import BaseModel

from gecko.core.engine.base import CognitiveEngine
from gecko.core.engine.buffer import StreamBuffer
from gecko.core.events.types import AgentStreamEvent
from gecko.core.protocols import StreamChunk
from gecko.core.toolbox import ToolExecutionResult
from gecko.core.exceptions import AgentError
from gecko.core.logging import get_logger
from gecko.core.memory import TokenMemory
from gecko.core.message import Message
from gecko.core.output import AgentOutput
from gecko.core.prompt import PromptTemplate
from gecko.core.structure import StructureEngine
from gecko.core.toolbox import ToolBox
from gecko.core.utils import ensure_awaitable
from gecko.config import get_settings

logger = get_logger(__name__)

# 泛型类型变量，用于结构化输出
T = TypeVar("T", bound=BaseModel)

# 结构化输出工具名称前缀，用于避免与用户定义的工具冲突
STRUCTURE_TOOL_PREFIX = "__gecko_structured_output_"

# 默认的 ReAct 系统提示词模板
# 使用 Jinja2 语法，支持动态注入工具列表和当前时间
DEFAULT_REACT_TEMPLATE = """You are a helpful AI assistant.
Current Time: {{ current_time }}

Available Tools:
{% for tool in tools %}
- {{ tool['function']['name'] }}: {{ tool['function']['description'] }}
{% endfor %}

Answer the user's request. Use tools if necessary.
If you use a tool, just output the tool call format.
"""


# ====================== 执行上下文 ======================

class ExecutionContext:
    """
    执行上下文 (Runtime Context)
    
    承载单次 Agent.run/stream 请求的所有运行时状态。
    随请求创建，随请求销毁，确保线程安全。
    
    核心职责：
    1. 维护对话消息历史
    2. 跟踪执行轮次
    3. 检测死循环（工具调用指纹）
    4. 智能裁剪上下文（防止 Token 溢出）
    
    属性：
        messages: 当前会话的消息列表
        max_history: 最大消息条数限制
        turn: 当前执行轮次
        metadata: 扩展元数据
        consecutive_errors: 连续工具错误次数
        last_tool_hash: 上一次工具调用的哈希指纹
        last_tool_hashes: 最近 N 轮工具调用哈希（用于振荡检测）
    """
    
    # 安全字符数上限（约 25k tokens，留给 prompt 和 reasoning 空间）
    # 可根据实际使用的模型上下文窗口调整
    SAFE_CHAR_LIMIT: int = 100_000
    
    def __init__(self, messages: List[Message], max_history: int = 50):
        """
        初始化执行上下文
        
        参数：
            messages: 初始消息列表（会进行浅拷贝，防止污染原始数据）
            max_history: 最大保留消息条数（超出时触发裁剪）
        """
        # 浅拷贝消息列表，防止污染传入的原始列表
        self.messages: List[Message] = messages.copy()
        self.max_history: int = max_history
        self.turn: int = 0
        self.metadata: Dict[str, Any] = {}
        
        # === 状态追踪：用于死循环检测与错误熔断 ===
        self.consecutive_errors: int = 0
        # [修复] 类型声明改为 str（hexdigest 返回字符串）
        self.last_tool_hash: Optional[str] = None
        self.last_tool_hashes: List[str] = []  # 最近 5 轮的工具调用哈希

    def add_message(self, message: Message) -> None:
        """
        添加消息到上下文，并自动触发裁剪检查
        
        裁剪策略：
        1. 条数限制：超过 max_history 时裁剪
        2. 字符限制：超过 SAFE_CHAR_LIMIT 时裁剪（防止 Context Window 溢出）
        
        参数：
            message: 要添加的消息
        """
        self.messages.append(message)
        
        # 检查 1: 基础条数限制
        if len(self.messages) > self.max_history:
            self._trim_context()
            return

        # 检查 2: 字符级滑动窗口保护
        # 估算：1 token ≈ 4 chars（英文），中文约 1.5 chars/token
        current_chars = sum(len(m.get_text_content()) for m in self.messages)
        
        if current_chars > self.SAFE_CHAR_LIMIT:
            logger.warning(
                f"Context size ({current_chars} chars) exceeded limit, trimming",
                current_chars=current_chars,
                limit=self.SAFE_CHAR_LIMIT
            )
            self._trim_context(target_chars=self.SAFE_CHAR_LIMIT)

    def _trim_context(self, target_chars: Optional[int] = None) -> None:
        """
        智能裁剪上下文
        
        裁剪策略：
        1. 始终保留 System 消息（通常在首位）
        2. 按"对话轮次"成对删除，保证 tool_call 和 tool_result 的完整性
        3. 从最老的消息开始删除
        
        参数：
            target_chars: 目标字符数限制（None 表示只按条数裁剪）
        """
        # 分离 system 消息和对话消息
        system_msgs = [m for m in self.messages if m.role == "system"]
        conversation_msgs = [m for m in self.messages if m.role != "system"]
        
        # 如果没有对话消息可删，直接返回
        if not conversation_msgs:
            return

        # 模式 A: 按条数裁剪
        if target_chars is None:
            keep_count = max(1, self.max_history - len(system_msgs))
            conversation_msgs = conversation_msgs[-keep_count:]
        
        # 模式 B: 按字符数裁剪（智能保持 tool_call/tool_result 成对）
        else:
            current_len = sum(
                len(m.get_text_content()) 
                for m in (system_msgs + conversation_msgs)
            )
            
            i = 0
            while current_len > target_chars and i < len(conversation_msgs) - 1:
                msg = conversation_msgs[i]
                
                # 如果是带工具调用的 assistant 消息，需要同时删除对应的 tool_result
                if msg.role == "assistant" and msg.tool_calls:
                    # 收集该消息中所有工具调用的 ID
                    tool_ids: Set[str] = {
                        tc.get('id', '') 
                        for tc in msg.tool_calls 
                        if tc.get('id')
                    }
                    
                    # 找出所有相关的 tool_result 消息索引
                    indices_to_remove = [i]
                    for j in range(i + 1, len(conversation_msgs)):
                        check_msg = conversation_msgs[j]
                        if (check_msg.role == "tool" and 
                            getattr(check_msg, 'tool_call_id', None) in tool_ids):
                            indices_to_remove.append(j)
                        elif check_msg.role != "tool":
                            # 遇到非 tool 消息，停止搜索
                            break
                    
                    # 从后向前删除，避免索引偏移
                    for idx in reversed(indices_to_remove):
                        if idx < len(conversation_msgs):
                            removed = conversation_msgs.pop(idx)
                            current_len -= len(removed.get_text_content())
                else:
                    # 普通消息直接删除
                    removed = conversation_msgs.pop(i)
                    current_len -= len(removed.get_text_content())
                
                # 注意：删除后不增加 i，因为后续元素会前移

        # 重组消息列表
        self.messages = system_msgs + conversation_msgs
        
        logger.debug(
            f"Context trimmed",
            remaining_messages=len(self.messages),
            remaining_chars=sum(len(m.get_text_content()) for m in self.messages)
        )

    @property
    def last_message(self) -> Message:
        """
        获取历史中最后一条消息
        
        返回：
            最后一条消息
            
        异常：
            ValueError: 上下文为空时抛出
        """
        if not self.messages:
            raise ValueError("Context is empty, cannot get last message")
        return self.messages[-1]


# ====================== ReAct 引擎实现 ======================

class ReActEngine(CognitiveEngine):
    """
    生产级 ReAct 引擎实现
    
    ReAct (Reasoning + Acting) 引擎通过以下循环完成任务：
    
    ```
    while not done and turn < max_turns:
        1. Think: 调用 LLM 进行推理，生成回复或工具调用
        2. Act: 如果有工具调用，执行工具
        3. Observe: 分析工具结果，决定是否继续
    ```
    
    特性：
    - 流式输出支持（实时返回 token）
    - 结构化输出支持（通过 step_structured）
    - 死循环检测（基于工具调用指纹）
    - 自动重试机制（工具执行失败时）
    - 上下文智能裁剪（防止 Token 溢出）
    - 超时保护（防止无限等待）
    
    属性：
        max_turns: 最大执行轮次（防止死循环）
        max_observation_length: 工具输出最大长度（超出截断）
        prompt_template: 系统提示词模板
        on_turn_start: 轮次开始钩子
        on_turn_end: 轮次结束钩子
    """

    def __init__(
        self,
        model: Any,
        toolbox: ToolBox,
        memory: TokenMemory,
        max_turns: int = 10,
        max_observation_length: int = 2000,
        system_prompt: Union[str, PromptTemplate, None] = None,
        on_turn_start: Optional[Callable[[ExecutionContext], Any]] = None,
        on_turn_end: Optional[Callable[[ExecutionContext], Any]] = None,
        **kwargs: Any,
    ):
        """
        初始化 ReAct 引擎
        
        参数：
            model: 语言模型实例（需实现 ModelProtocol）
            toolbox: 工具箱实例
            memory: 记忆管理器实例
            max_turns: 最大执行轮次，默认 10（防止死循环）
            max_observation_length: 工具输出最大字符数，默认 2000
            system_prompt: 系统提示词（字符串或 PromptTemplate）
            on_turn_start: 轮次开始回调函数
            on_turn_end: 轮次结束回调函数
            **kwargs: 传递给基类的额外参数
        """
        super().__init__(model, toolbox, memory, **kwargs)
        
        self.max_turns = max_turns
        self.max_observation_length = max_observation_length
        self.on_turn_start = on_turn_start
        self.on_turn_end = on_turn_end

        # 初始化系统提示词模板
        if system_prompt is None:
            self.prompt_template = PromptTemplate(template=DEFAULT_REACT_TEMPLATE)
        elif isinstance(system_prompt, str):
            self.prompt_template = PromptTemplate(template=system_prompt)
        else:
            self.prompt_template = system_prompt

        # 检测模型能力：是否支持原生 Function Calling
        # 某些模型（如早期的开源模型）可能不支持
        self._supports_functions: bool = getattr(
            self.model, "_supports_function_calling", True
        )
        
        logger.debug(
            f"ReActEngine initialized",
            max_turns=max_turns,
            supports_functions=self._supports_functions
        )

    # ================= 核心公开接口 (Public API) =================

    async def step(  # type: ignore[override]
        self,
        input_messages: List[Message],
        response_model: Optional[Type[T]] = None,
        max_retries: int = 0,
        **kwargs: Any,
    ) -> Union[AgentOutput, T]:
        """
        同步执行入口
        
        执行完整的 ReAct 推理流程，返回最终结果。
        当指定 response_model 时，会自动进行结构化解析。
        
        参数：
            input_messages: 用户输入消息列表
            response_model: (可选) Pydantic 模型类，用于结构化输出
            max_retries: 结构化解析失败时的最大重试次数
            **kwargs: 传递给 LLM 的额外参数（如 temperature, max_tokens）
        
        返回：
            AgentOutput: 当 response_model 为 None 时
            T: 当指定 response_model 时，返回解析后的模型实例
        
        异常：
            AgentError: 执行过程中发生错误
            TypeError: response_model 不是 BaseModel 子类
        """
        # 如果指定了 response_model，验证并进行结构化处理
        if response_model is not None:
            from inspect import isclass
            if not (isclass(response_model) and issubclass(response_model, BaseModel)):
                raise TypeError(
                    f"response_model must be a subclass of Pydantic BaseModel, "
                    f"got: {type(response_model).__name__}"
                )
            
            # 执行推理
            output = await self._execute_step(
                input_messages,
                response_model=response_model,
                **kwargs
            )
            
            # 结构化解析 + 自动重试
            current_messages = list(input_messages)
            attempts = 0
            
            while True:
                try:
                    # 策略 A: 尝试从 Tool Calls 中解析
                    if output.tool_calls:
                        return await StructureEngine.parse(
                            content="",
                            model_class=response_model,
                            raw_tool_calls=output.tool_calls
                        )
                    
                    # 策略 B: 回退到 Content 解析
                    return await StructureEngine.parse(output.content, response_model)
                
                except Exception as e:
                    if attempts >= max_retries:
                        raise AgentError(f"Structured parsing failed: {e}") from e
                    
                    attempts += 1
                    logger.warning(
                        f"Structure parse failed, retrying ({attempts}/{max_retries})",
                        error=str(e)
                    )
                    
                    # 构造反馈消息，让模型看到错误并修正
                    current_messages.append(Message.assistant(
                        content=output.content,
                        tool_calls=output.tool_calls
                    ))
                    current_messages.append(Message.user(
                        f"Error parsing response: {e}. Please try again using the correct format."
                    ))
                    
                    # 重新执行
                    output = await self._execute_step(
                        current_messages,
                        response_model=response_model,
                        **kwargs
                    )
        
        # 无结构化输出，直接执行并返回 AgentOutput
        return await self._execute_step(input_messages, **kwargs)

    async def step_structured(
        self,
        input_messages: List[Message],
        response_model: Type[T],
        max_retries: int = 0,
        **kwargs: Any,
    ) -> T:
        """
        结构化输出执行入口
        
        这是 step() 方法的类型安全版本，强制要求 response_model。
        
        参数：
            input_messages: 用户输入消息列表
            response_model: 目标 Pydantic 模型类（必须）
            max_retries: 解析失败时的最大重试次数，默认 0
            **kwargs: 传递给 LLM 的额外参数
        
        返回：
            T: 解析后的 Pydantic 模型实例
        
        异常：
            TypeError: response_model 不是 BaseModel 子类
            AgentError: 解析失败且用尽重试次数
        """
        result = await self.step(
            input_messages,
            response_model=response_model,
            max_retries=max_retries,
            **kwargs
        )
        # step() 在指定 response_model 时已经返回 T 类型
        return cast(T, result)

    async def _execute_step(
        self,
        input_messages: List[Message],
        response_model: Optional[Type[T]] = None,
        **kwargs: Any,
    ) -> AgentOutput:
        """
        内部执行逻辑：消费事件流并提取最终结果
        
        参数：
            input_messages: 输入消息列表
            response_model: 可选的结构化输出模型
            **kwargs: 传递给 step_stream 的参数
        
        返回：
            AgentOutput: 最终执行结果
        
        异常：
            AgentError: 执行失败或未产生结果
        """
        # 深拷贝消息列表，只捕获预期的异常
        try:
            current_messages = [
                Message(**m.model_dump()) if hasattr(m, 'model_dump') else m
                for m in input_messages
            ]
        except (AttributeError, TypeError) as e:
            logger.warning(f"Message deep copy failed, using shallow copy: {e}")
            current_messages = list(input_messages)
        
        # 构建流式调用参数
        stream_kwargs: Dict[str, Any] = dict(kwargs)
        stream_kwargs['response_model'] = response_model
        
        final_result: Optional[AgentOutput] = None
        
        try:
            async for event in self.step_stream(current_messages, **stream_kwargs):
                if event.type == "result" and event.data:
                    # 从事件载荷中提取 AgentOutput
                    final_result = cast(AgentOutput, event.data.get("output"))
                
                elif event.type == "error":
                    error_msg = str(event.content)
                    logger.error(f"Received error event during execution: {error_msg}")
                    
                    # 死循环检测错误
                    if "Infinite loop detected" in error_msg:
                        raise AgentError(error_msg)
                    
                    # 模型流式异常
                    if "StopIteration" in error_msg:
                        raise AgentError(
                            "Infinite loop detected: model streaming stopped unexpectedly"
                        )
                    
                    raise AgentError(error_msg)
        
        except AgentError:
            raise
        except Exception as e:
            logger.exception("Event stream processing failed", error=str(e))
            raise AgentError(f"Step execution failed: {e}") from e
        
        # 验证结果
        if final_result is None:
            raise AgentError("Infinite loop detected: no result generated")
        
        return final_result

    async def step_stream( # type: ignore
        self,
        input_messages: List[Message],
        timeout: Optional[float] = None,
        **kwargs: Any
    ) -> AsyncIterator[AgentStreamEvent]:
        """
        流式执行入口
        
        生成 AgentStreamEvent 事件流，包含：
        - token: 实时生成的文本片段（用于前端流式显示）
        - tool_input: 工具调用意图和参数
        - tool_output: 工具执行结果
        - result: 最终生成的回复
        - error: 执行过程中的错误
        
        参数：
            input_messages: 输入消息列表
            timeout: 执行超时时间（秒），None 时使用全局配置
            **kwargs: 传递给引擎的其他参数
        
        返回：
            AsyncIterator[AgentStreamEvent]: 事件流
        
        异常：
            AgentError: 执行失败
            asyncio.TimeoutError: 执行超时
            asyncio.CancelledError: 执行被取消
        
        示例：
            ```python
            async for event in engine.step_stream([Message.user("你好")]):
                match event.type:
                    case "token":
                        print(event.content, end="", flush=True)
                    case "tool_input":
                        print(f"\\n调用工具: {event.data['tools']}")
                    case "tool_output":
                        print(f"工具结果: {event.content}")
                    case "result":
                        print(f"\\n完成: {event.data['output'].content}")
                    case "error":
                        print(f"错误: {event.content}")
            ```
        """
        # 确定超时时间
        if timeout is None:
            timeout = get_settings().default_model_timeout
        
        # 输入验证与前置钩子
        self.validate_input(input_messages)
        await self.before_step(input_messages, **kwargs)

        # 构建执行上下文
        context = await self._build_execution_context(input_messages)
        start_time = time.time()
        
        try:
            # [修复] 恢复 _execute_lifecycle_with_timeout 方法以兼容测试
            async for event in self._execute_lifecycle_with_timeout(context, timeout, **kwargs):
                yield event
        
        except AgentError as e:
            logger.error(f"step_stream caught AgentError: {e}")
            yield AgentStreamEvent(type="error", content=str(e))
            raise
        
        except asyncio.TimeoutError:
            elapsed = time.time() - start_time
            logger.error(
                f"step_stream timeout",
                elapsed_seconds=elapsed,
                max_timeout=timeout,
                current_turn=context.turn
            )
            
            # 优雅关闭：保存上下文
            await self._save_context(context, force=True)
            
            yield AgentStreamEvent(
                type="error",
                content=f"Execution timeout after {timeout}s at turn {context.turn}"
            )
            raise
        
        except asyncio.CancelledError:
            logger.warning(f"step_stream was cancelled", current_turn=context.turn)
            await self._save_context(context, force=True)
            raise
        
        except Exception as e:
            logger.exception("Lifecycle execution failed", error=str(e))
            await self.on_error(e, input_messages, **kwargs)
            yield AgentStreamEvent(type="error", content=str(e))
            raise
        
        finally:
            # [修复] 确保无论如何都尝试保存上下文
            try:
                await self._save_context(context)
            except Exception as save_error:
                logger.warning(f"Final context save failed: {save_error}")

    async def _execute_lifecycle_with_timeout(
        self,
        context: ExecutionContext,
        timeout: float,
        **kwargs: Any
    ) -> AsyncIterator[AgentStreamEvent]:
        """
        带超时控制的生命周期执行
        
        [修复] 恢复此方法以保持与测试的兼容性
        
        参数：
            context: 执行上下文
            timeout: 超时时间（秒）
            **kwargs: 额外参数
        
        返回：
            AsyncIterator[AgentStreamEvent]: 事件流
        """
        async with asyncio.timeout(timeout):
            async for event in self._execute_lifecycle(context, **kwargs):
                yield event

    # ================= 生命周期主循环 =================
    async def _execute_lifecycle(
        self,
        context: ExecutionContext,
        **kwargs: Any
    ) -> AsyncIterator[AgentStreamEvent]:
        """
        ReAct 核心循环：Think -> Act -> Observe
        
        ... 方法体保持不变，只修改最后的 max_turns 处理 ...
        """
        # 获取结构化输出工具名称（如果有）
        response_model = kwargs.get('response_model')
        structure_tool_name: Optional[str] = None
        
        if response_model and self._supports_functions:
            schema = StructureEngine.to_openai_tool(response_model)
            structure_tool_name = schema["function"]["name"]

        # === 主循环 ===
        while context.turn < self.max_turns:
            context.turn += 1
            
            # ... 循环体内容完全保持不变 ...
            
            logger.debug(f"Starting turn {context.turn}")
            
            if self.on_turn_start:
                await ensure_awaitable(self.on_turn_start, context)

            # Phase 1: Think
            buffer = StreamBuffer()
            async for chunk in self._phase_think(context, **kwargs):
                text_delta = buffer.add_chunk(chunk)
                if text_delta:
                    yield AgentStreamEvent(type="token", content=text_delta)
            
            assistant_msg = buffer.build_message()
            context.add_message(assistant_msg)

            if self._detect_loop(context, assistant_msg):
                error_msg = "Infinite loop detected"
                yield AgentStreamEvent(type="error", content=error_msg)
                raise AgentError(error_msg)

            tool_calls = assistant_msg.safe_tool_calls

            if structure_tool_name and tool_calls:
                target_call = next(
                    (tc for tc in tool_calls 
                    if tc.get("function", {}).get("name") == structure_tool_name),
                    None
                )
                if target_call:
                    final_output = AgentOutput(
                        content="",
                        tool_calls=[target_call],
                        metadata={"is_structured": True}
                    )
                    yield AgentStreamEvent(
                        type="result",
                        data={"output": final_output}
                    )
                    return

            if not tool_calls:
                final_output = AgentOutput(
                    content=str(assistant_msg.content or ""),
                    tool_calls=[],
                )
                yield AgentStreamEvent(
                    type="result",
                    data={"output": final_output}
                )
                return

            # Phase 2: Act
            yield AgentStreamEvent(
                type="tool_input",
                data={"tools": tool_calls}
            )
            
            for _ in tool_calls:
                self.record_tool_call()
            
            tool_results = await self._phase_act(tool_calls)
            
            for result in tool_results:
                truncated_content = self._truncate_observation(
                    result.result,
                    result.tool_name
                )
                
                context.add_message(
                    Message.tool_result(
                        result.call_id,
                        truncated_content,
                        result.tool_name
                    )
                )
                
                yield AgentStreamEvent(
                    type="tool_output",
                    content=result.result,
                    data={
                        "tool_name": result.tool_name,
                        "is_error": result.is_error
                    }
                )

            # Phase 3: Observe
            should_continue = await self._phase_observe(context, tool_results)
            
            if self.on_turn_end:
                await ensure_awaitable(self.on_turn_end, context)
            
            await self._save_context(context)

            if not should_continue:
                stop_output = AgentOutput(
                    content="Task stopped by system monitor."
                )
                yield AgentStreamEvent(
                    type="result",
                    data={"output": stop_output}
                )
                return
        
        # =====================================================
        # [修复] 达到 max_turns 时的处理
        # 只 yield 错误事件，不抛出异常
        # 由 _execute_step() 根据错误事件决定是否抛出
        # 这样 step_stream() 的直接调用者可以自行处理错误事件
        # =====================================================
        logger.warning(f"Reached max turns limit ({self.max_turns}), treating as infinite loop")
        error_msg = "Infinite loop detected"
        yield AgentStreamEvent(type="error", content=error_msg)
        # [关键修复] 使用 return 而不是 raise
        # step() -> _execute_step() 会捕获 error 事件并抛出异常
        # step_stream() 的直接调用者则可以选择如何处理
        return
    # ================= 阶段实现（可被子类重写） =================

    async def _phase_think(
        self,
        context: ExecutionContext,
        **kwargs: Any
    ) -> AsyncIterator[StreamChunk]:
        """
        思考阶段：构造 Prompt 并调用 LLM
        
        职责：
        1. 构造 LLM 调用参数（消息、工具、约束等）
        2. 流式接收模型响应
        3. 追踪 token 使用量和成本
        
        参数：
            context: 执行上下文
            **kwargs: 额外参数（如 response_model）
        
        返回：
            AsyncIterator[StreamChunk]: 模型响应流
        
        异常：
            直接向上抛出 LLM 调用异常
        """
        # 转换消息为 OpenAI 格式
        messages_payload = [m.to_openai_format() for m in context.messages]
        
        # 构造 LLM 参数
        llm_params = self._build_llm_params(
            kwargs.get('response_model'),
            "auto"
        )
        
        # 合并用户传入的 kwargs（排除内部参数）
        safe_kwargs = {
            k: v for k, v in kwargs.items()
            if k not in ['response_model']
        }
        llm_params.update(safe_kwargs)
        llm_params["stream"] = True
        
        # Token 追踪
        input_tokens = 0
        output_tokens = 0
        model_name = self._get_model_name()
        
        # [修复] 不在这里 yield 错误事件，直接让异常向上传播
        stream_gen = self.model.astream( # type: ignore
            messages=messages_payload,
            **llm_params
        )
        
        async for chunk in stream_gen:
            # 验证 chunk 类型
            if not isinstance(chunk, StreamChunk):
                logger.warning(
                    f"Received unexpected chunk type: {type(chunk).__name__}, skipping"
                )
                continue
            
            # 提取 usage 信息（通常在最后的 chunk 中）
            if hasattr(chunk, 'usage') and chunk.usage: # type: ignore
                input_tokens = max(
                    input_tokens,
                    getattr(chunk.usage, 'prompt_tokens', 0) # type: ignore
                )
                output_tokens = max(
                    output_tokens,
                    getattr(chunk.usage, 'completion_tokens', 0) # type: ignore
                )
            
            yield chunk
        
        # 记录 token 使用统计
        if input_tokens > 0 or output_tokens > 0:
            if self.stats:
                self.stats.input_tokens += input_tokens
                self.stats.output_tokens += output_tokens
            
            self.record_cost(input_tokens, output_tokens, model_name)
            
            logger.debug(
                f"Token stats",
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                model=model_name
            )

    async def _phase_act(
        self,
        tool_calls: List[Dict[str, Any]]
    ) -> List[ToolExecutionResult]:
        """
        行动阶段：标准化工具调用并并发执行
        
        参数：
            tool_calls: 工具调用列表（OpenAI 格式）
        
        返回：
            List[ToolExecutionResult]: 执行结果列表
        """
        # 标准化工具调用格式
        normalized_calls = [
            self._normalize_tool_call(tc)
            for tc in tool_calls
        ]
        
        # 并发执行
        return await self.toolbox.execute_many(normalized_calls)

    async def _phase_observe(
        self,
        context: ExecutionContext,
        results: List[ToolExecutionResult]
    ) -> bool:
        """
        观察阶段：分析执行结果，决定是否继续
        
        策略：
        1. 统计连续错误次数
        2. 最多允许 2 轮自动重试
        3. 超过阈值后停止执行，防止无限循环
        
        参数：
            context: 执行上下文
            results: 工具执行结果列表
        
        返回：
            bool: True 继续执行，False 停止
        """
        error_count = sum(1 for r in results if r.is_error)
        
        if error_count > 0:
            context.consecutive_errors += 1
            logger.warning(
                f"Tool execution errors",
                error_count=error_count,
                total_count=len(results),
                consecutive_errors=context.consecutive_errors
            )
        else:
            context.consecutive_errors = 0
        
        # 自动重试上限
        max_auto_retries = 2
        
        if context.consecutive_errors >= 3:
            if context.turn <= max_auto_retries:
                # 还有重试机会，构造错误反馈
                error_details = "\n".join(
                    f"- {r.tool_name}: {r.result}"
                    for r in results if r.is_error
                )
                
                system_feedback = (
                    "System Notification: Multiple tool execution errors detected.\n"
                    "Error Details:\n"
                    f"{error_details}\n\n"
                    "Please analyze these errors, adjust parameters or tool choice, and retry."
                )

                context.add_message(Message.assistant(
                    content=system_feedback,
                    tool_calls=None,
                    metadata={ # type: ignore
                        "type": "system_reflection",
                        "error_summary": error_details
                    }
                ))
                
                context.consecutive_errors = 0
                return True
            else:
                # 超过重试上限
                logger.error(
                    "Tool error threshold exceeded, stopping auto-retry",
                    consecutive_errors=context.consecutive_errors,
                    turn=context.turn,
                    max_retries=max_auto_retries
                )
                return False
        
        return True

    # ================= 辅助方法 =================

    def _detect_loop(self, context: ExecutionContext, msg: Message) -> bool:
        """
        死循环检测算法
        
        检测策略：
        1. 连续重复：同一工具调用连续出现 3 次以上
        2. 振荡模式：A->B->A 的交替调用模式
        
        参数：
            context: 执行上下文
            msg: 当前 Assistant 消息
        
        返回：
            bool: True 表示检测到死循环
        """
        if not msg.safe_tool_calls:
            return False
        
        try:
            # 计算当前工具调用的指纹
            calls_data = [
                {
                    "name": tc.get("function", {}).get("name"),
                    "args": tc.get("function", {}).get("arguments"),
                }
                for tc in msg.safe_tool_calls
            ]
            calls_dump = json.dumps(calls_data, sort_keys=True)
            current_hash = hashlib.sha256(calls_dump.encode()).hexdigest()
            
            # 策略 1: 连续重复检测
            consecutive_count = sum(
                1 for h in reversed(context.last_tool_hashes)
                if h == current_hash
            )
            
            if consecutive_count >= 3:
                logger.warning(
                    f"Consecutive tool call loop detected ({consecutive_count} times)",
                    tool_hash=current_hash[:16]
                )
                return True
            
            # 策略 2: 振荡模式检测 (A->B->A)
            if len(context.last_tool_hashes) >= 2:
                if (current_hash == context.last_tool_hashes[-2] and
                    current_hash != context.last_tool_hashes[-1]):
                    logger.warning(
                        "Oscillation pattern detected (A-B-A alternating calls)",
                        tool_hash=current_hash[:16]
                    )
                    return True
            
            # 更新历史（保留最近 5 轮）
            context.last_tool_hashes.append(current_hash)
            context.last_tool_hashes = context.last_tool_hashes[-5:]
            context.last_tool_hash = current_hash
            
            return False
        
        except Exception as e:
            logger.warning(f"Loop detection failed: {e}", exc_info=True)
            # 失败时不触发熔断（fail-open）
            return False

    def _truncate_observation(self, content: str, tool_name: str) -> str:
        """
        截断过长的工具输出
        
        保留头部信息，并添加截断说明。
        
        参数：
            content: 原始输出内容
            tool_name: 工具名称（用于日志）
        
        返回：
            截断后的内容
        """
        if len(content) > self.max_observation_length:
            logger.info(
                f"Truncating output for tool {tool_name}",
                original_length=len(content),
                max_length=self.max_observation_length
            )
            # [修复] 使用英文 "truncated" 以匹配测试
            return (
                content[:self.max_observation_length] +
                f"\n...(truncated, total {len(content)} chars)"
            )
        return content

    def _normalize_tool_call(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:
        """
        将 OpenAI 格式工具调用转换为 ToolBox 所需的扁平格式
        
        同时尝试解析 arguments JSON 字符串。
        
        参数：
            tool_call: OpenAI 格式的工具调用
        
        返回：
            ToolBox 格式的工具调用
        """
        func_block = tool_call.get("function", {})
        name = func_block.get("name", "")
        raw_args = func_block.get("arguments", "{}")
        
        parsed_args: Dict[str, Any] = {}
        
        try:
            if isinstance(raw_args, str):
                parsed_args = json.loads(raw_args)
            elif isinstance(raw_args, dict):
                parsed_args = raw_args
        except json.JSONDecodeError as e:
            # 解析失败时，传递特殊标记给 ToolBox
            # ToolBox 会识别此标记并返回友好的错误提示
            parsed_args = {
                "__gecko_parse_error__": (
                    f"JSON format error: {str(e)}. "
                    f"Content: {raw_args[:200] if len(raw_args) > 200 else raw_args}"
                )
            }
            logger.warning(
                f"Tool arguments JSON parse failed",
                tool_name=name,
                error=str(e)
            )

        return {
            "id": tool_call.get("id", ""),
            "name": name,
            "arguments": parsed_args,
        }

    def _get_model_name(self) -> str:
        """
        安全地获取模型名称
        
        返回：
            模型名称字符串
        """
        model_name = (
            getattr(self.model, 'model_name', None) or
            getattr(self.model, 'model', None) or
            'gpt-3.5-turbo'
        )
        
        if not isinstance(model_name, str):
            logger.warning(f"Model name type unexpected: {type(model_name)}")
            model_name = 'gpt-3.5-turbo'
        
        return model_name

    def _build_llm_params(
        self,
        response_model: Any,
        strategy: str
    ) -> Dict[str, Any]:
        """
        构建 LLM 调用参数
        
        参数：
            response_model: 结构化输出模型（可选）
            strategy: 工具选择策略（"auto", "required", "none"）
        
        返回：
            LLM 参数字典
        """
        params: Dict[str, Any] = {}
        tools_schema = self.toolbox.to_openai_schema()

        # 模式 1: 结构化输出
        if response_model and self._supports_functions:
            structure_tool = StructureEngine.to_openai_tool(response_model)
            
            # 合并工具（不修改原列表）
            combined_tools = tools_schema + [structure_tool]
            params["tools"] = combined_tools
            
            # 强制调用结构化输出工具
            params["tool_choice"] = {
                "type": "function",
                "function": {"name": structure_tool["function"]["name"]}
            }
        
        # 模式 2: 标准 ReAct
        elif tools_schema and self._supports_functions:
            params["tools"] = tools_schema
            params["tool_choice"] = "auto"
        
        return params

    async def _build_execution_context(
        self,
        input_messages: List[Message]
    ) -> ExecutionContext:
        """
        构建执行上下文
        
        流程：
        1. 加载历史消息
        2. 合并输入消息
        3. 注入系统提示（如果需要）
        
        参数：
            input_messages: 用户输入消息
        
        返回：
            ExecutionContext: 初始化完成的执行上下文
        """
        # 加载历史
        history = await self._load_history()
        all_messages = history + input_messages
        
        # 检查是否已有系统消息
        has_system_msg = any(m.role == "system" for m in all_messages)
        
        if not has_system_msg:
            # 需要注入系统提示
            template_vars = {
                "tools": self.toolbox.to_openai_schema(),
                "current_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }
            
            try:
                system_content = self.prompt_template.format_safe(**template_vars)
            except Exception as e:
                logger.warning(f"System prompt formatting failed: {e}")
                system_content = "You are a helpful AI assistant."
            
            # 确保 system 消息在最前面
            all_messages.insert(0, Message.system(system_content))
        else:
            logger.debug("Using user-specified system message")
        
        return ExecutionContext(all_messages)

    async def _load_history(self) -> List[Message]:
        """
        从 Memory 加载历史消息
        
        返回：
            历史消息列表（如果失败返回空列表）
        """
        if not self.memory.storage:
            return []
        
        try:
            data = await self.memory.storage.get(self.memory.session_id)
            if data and "messages" in data:
                return await self.memory.get_history(data["messages"])
        except Exception as e:
            logger.warning(f"Failed to load history: {e}")
        
        return []

    async def _save_context(
        self,
        context: ExecutionContext,
        force: bool = False,
        max_retries: int = 3
    ) -> None:
        """
        保存上下文到 Memory
        
        参数：
            context: 执行上下文
            force: 强制保存模式（失败时重试）
            max_retries: 强制模式下的最大重试次数
        """
        if not self.memory.storage:
            return
        
        messages_data = [m.to_openai_format() for m in context.messages]
        
        for attempt in range(max_retries if force else 1):
            try:
                await self.memory.storage.set(
                    self.memory.session_id,
                    {"messages": messages_data}
                )
                return
            except Exception as e:
                if not force or attempt >= max_retries - 1:
                    logger.warning(
                        f"Failed to save context",
                        error=str(e),
                        attempt=attempt + 1,
                        force=force
                    )
                    if force:
                        raise
                    return
                
                # 强制模式下重试
                await asyncio.sleep(0.1 * (attempt + 1))


# ====================== 导出 ======================

__all__ = [
    "ReActEngine",
    "ExecutionContext",
    "DEFAULT_REACT_TEMPLATE",
    "STRUCTURE_TOOL_PREFIX",
]
```

