[1] gecko/__init__.py
```python
# gecko/__init__.py
from __future__ import annotations

from gecko.core.agent import Agent
from gecko.core.builder import AgentBuilder
from gecko.core.message import Message

# 自动清理 LiteLLM 异步客户端，彻底消除 RuntimeWarning
import atexit
import asyncio
import litellm # type: ignore

def _cleanup_litellm():
    async def _close():
        try:
            if hasattr(litellm, "async_http_handler") and litellm.async_http_handler:
                await litellm.async_http_handler.client.close()
        except:
            pass
    try:
        asyncio.run(_close())
    except:
        pass

atexit.register(_cleanup_litellm)

__version__ = "0.1.0"
__all__ = ["Agent", "AgentBuilder", "Message"]
```

[2] gecko/compose/__init__.py
```python
# gecko/compose/__init__.py
"""
Gecko Compose 模块

提供多智能体编排能力：
- Workflow: DAG 工作流引擎
- Team: 并行多智能体执行
- step: 节点装饰器
- ensure_awaitable: 同步/异步统一调用工具
- Next: 控制流指令
"""
from gecko.compose.workflow import Workflow
from gecko.compose.team import Team
from gecko.compose.nodes import step, ensure_awaitable, Next

__all__ = ["Workflow", "Team", "step", "ensure_awaitable", "Next"]
```

[3] gecko/compose/nodes.py
```python
# gecko/compose/nodes.py
"""
Workflow 节点定义与辅助工具

核心功能：
1. Next: 控制流指令，用于动态跳转节点
2. step: 节点装饰器，用于标记和增强函数元数据

优化日志：
- [Fix] 使用 functools.wraps 保留被装饰函数的元数据 (签名、文档等)
- [Refactor] 移除本地 ensure_awaitable，引用 gecko.core.utils
- [Feat] step 装饰器自动将同步函数转为异步，统一调用行为
"""
from __future__ import annotations

import functools
from typing import Any, Callable, Optional

from pydantic import BaseModel, Field

from gecko.core.utils import ensure_awaitable


class Next(BaseModel):
    """
    控制流指令：用于 Workflow 节点返回值中，指示引擎跳转到特定节点。
    
    示例:
        ```python
        def check_score(score: int):
            if score > 60:
                return Next(node="Pass", input="Good job")
            return Next(node="Fail", input="Try again")
        ```
    """
    node: str = Field(..., description="下一个节点的名称")
    input: Optional[Any] = Field(
        default=None, 
        description="传递给下一个节点的输入数据。如果为 None，则保持上下文中的 last_output 不变。"
    )


def step(name: Optional[str] = None):
    """
    节点装饰器
    
    功能：
    1. 标记函数为 Workflow 节点
    2. 允许自定义节点名称 (metadata)
    3. 统一将同步函数包装为异步函数
    
    参数:
        name: 自定义节点名称（可选，默认使用函数名）
        
    示例:
        ```python
        @step(name="DataFetcher")
        def fetch_data(url: str):
            return requests.get(url).text
            
        # 在 Workflow 中使用
        workflow.add_node("fetch", fetch_data)
        ```
    """
    def decorator(func: Callable):
        # 1. 保留原始函数的元数据 (name, doc, signature)
        # 这对于 Workflow 的智能参数注入 (Smart Binding) 至关重要
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            # 2. 统一转为异步执行
            return await ensure_awaitable(func, *args, **kwargs)
        
        # 3. 附加标识位和名称
        setattr(wrapper, "_is_step", True)
        setattr(wrapper, "_step_name", name or func.__name__)
        
        return wrapper
    return decorator
```

[4] gecko/compose/team.py
```python
# gecko/compose/team.py
"""
Team 多智能体并行引擎

提供 Map-Reduce 模式的并行执行能力，将单一任务分发给多个 Agent/Function 执行，
并聚合结果。适用于 "专家评审团"、"多路赛马"、"并发搜索" 等场景。

核心功能：
1. 高效并行：基于 AnyIO TaskGroup 实现异步并发。
2. 流量整形：支持 max_concurrent 限制，防止触发 LLM Rate Limit。
3. 容错机制：单个成员失败不熔断整体任务 (Partial Success)。
4. 智能绑定：自动解析 WorkflowContext，支持数据流转。

优化日志：
- [Fix] 增加 max_concurrent 信号量控制，防止 API 速率超限
- [Fix] 完善异常捕获边界，确保 TaskGroup 稳定性
- [Refactor] 统一输入解析与结果标准化逻辑
"""
from __future__ import annotations

from typing import Any, Callable, List, Optional, Union, TYPE_CHECKING

import anyio
from pydantic import BaseModel

from gecko.core.agent import Agent
from gecko.core.logging import get_logger
from gecko.core.message import Message
from gecko.core.output import AgentOutput
from gecko.core.utils import ensure_awaitable

if TYPE_CHECKING:
    # 避免运行时循环导入，仅用于类型检查
    from gecko.compose.workflow import WorkflowContext

logger = get_logger(__name__)


class Team:
    """
    多智能体协作组 (Parallel Execution Engine)
    
    将同一个输入广播给多个成员并行执行，并收集结果。
    
    示例:
        ```python
        # 创建一个包含 3 个 Agent 的团队，最大并发数为 2
        team = Team(
            members=[researcher, reviewer, coder],
            max_concurrent=2
        )
        
        # 执行
        results = await team.run("Design a snake game")
        # results = ["Research doc...", "Review notes...", "Python code..."]
        ```
    """

    def __init__(
        self, 
        members: List[Union[Agent, Callable]],
        name: str = "Team",
        max_concurrent: int = 0,
        return_full_output: bool = False
    ):
        """
        初始化 Team
        
        参数:
            members: 成员列表 (Agent 实例或可调用的函数)
            name: Team 名称 (用于日志追踪)
            max_concurrent: 最大并发数 (0 表示不限制，默认不限制)
            return_full_output: 是否返回完整对象 (如 AgentOutput)。
                                False (默认): 仅提取文本内容 (content)
                                True: 返回原始执行结果对象
        """
        self.members = members
        self.name = name
        self.max_concurrent = max_concurrent
        self.return_full_output = return_full_output

    # ========================= 接口协议 =========================

    async def __call__(self, context_or_input: Any) -> List[Any]:
        """
        实现 Callable 协议，使 Team 实例可直接作为 Workflow 节点使用
        """
        return await self.run(context_or_input)

    async def run(self, context_or_input: Any) -> List[Any]:
        """
        执行 Team 逻辑
        
        参数:
            context_or_input: 输入数据，支持原始值或 WorkflowContext
            
        返回:
            成员执行结果列表 (顺序与 members 一致)
        """
        # 1. 解析输入 (Context -> Data)
        inp = self._resolve_input(context_or_input)
        
        member_count = len(self.members)
        logger.info(
            "Team execution started", 
            team=self.name, 
            member_count=member_count,
            max_concurrent=self.max_concurrent or "unlimited"
        )

        # 2. 初始化容器
        results: List[Any] = [None] * member_count
        # 用于统计
        errors: List[Optional[Exception]] = [None] * member_count

        # 3. 准备并发控制
        # 如果设置了 max_concurrent，创建信号量；否则为 None
        semaphore = anyio.Semaphore(self.max_concurrent) if self.max_concurrent > 0 else None

        # 4. 定义 Worker
        async def _worker(idx: int, member: Any):
            # 如果有信号量，先获取许可
            if semaphore:
                await semaphore.acquire()
            
            try:
                # 执行成员逻辑
                raw_result = await self._execute_member(member, inp)
                # 结果标准化
                results[idx] = self._process_result(raw_result)
            except Exception as e:
                # 捕获异常，保证其他成员继续执行
                logger.error(
                    "Team member execution failed",
                    team=self.name,
                    member_index=idx,
                    error=str(e)
                )
                errors[idx] = e
                # 优雅降级：返回错误字符串，而不是抛出异常中断流程
                results[idx] = f"Error: {str(e)}"
            finally:
                # 释放信号量
                if semaphore:
                    semaphore.release()

        # 5. 启动并发任务组
        async with anyio.create_task_group() as tg:
            for idx, member in enumerate(self.members):
                tg.start_soon(_worker, idx, member)

        # 6. 执行摘要
        fail_count = sum(1 for e in errors if e is not None)
        logger.info(
            "Team execution completed",
            team=self.name,
            success=member_count - fail_count,
            failed=fail_count
        )

        return results

    # ========================= 内部逻辑 =========================

    def _resolve_input(self, context_or_input: Any) -> Any:
        """
        智能输入解析
        
        从 WorkflowContext 中提取真正需要传递给 Agent 的 Prompt 数据，
        同时处理 Data Handover（上一步输出是复杂对象的情况）。
        """
        # 1. 检查是否为 WorkflowContext (Duck Typing)
        # 判断依据: 具有 input, history 属性，且 history 是字典
        if (
            hasattr(context_or_input, "history") 
            and hasattr(context_or_input, "input")
            and isinstance(getattr(context_or_input, "history", None), dict)
        ):
            ctx = context_or_input
            history = getattr(ctx, "history", {})
            state = getattr(ctx, "state", {})
            
            # 优先级: 
            # 1. 显式传递的 _next_input (Next 指令)
            # 2. 上一步输出 (last_output)
            # 3. 全局初始输入 (input)
            val = state.pop("_next_input", None) or history.get("last_output", getattr(ctx, "input"))
            
            # 2. Data Handover 清洗
            # 如果上一步输出是 AgentOutput 的字典形式 (含 content, role, tool_calls 等)
            # 我们通常只需要 content 传给下一个 Agent，避免 Prompt 污染
            if isinstance(val, dict) and "content" in val and "role" not in val:
                return val["content"]
            
            return val
            
        # 3. 普通输入直接返回
        return context_or_input

    async def _execute_member(self, member: Any, inp: Any) -> Any:
        """执行单个成员 (支持 Agent 和 Async/Sync Function)"""
        # Case A: Agent (具备 run 方法)
        if hasattr(member, "run"):
            return await member.run(inp)
            
        # Case B: Callable
        if callable(member):
            return await ensure_awaitable(member, inp)
            
        raise TypeError(f"Member {member} is not executable (must be Agent or Callable)")

    def _process_result(self, result: Any) -> Any:
        """结果标准化处理"""
        if self.return_full_output:
            # 返回完整对象 (Pydantic 序列化)
            if isinstance(result, (BaseModel, AgentOutput, Message)):
                return result.model_dump()
            return result
            
        # 默认模式：仅提取核心文本内容
        if isinstance(result, AgentOutput):
            return result.content
        if isinstance(result, Message):
            return result.content
        if isinstance(result, dict) and "content" in result:
            return result["content"]
            
        return result

    def __repr__(self) -> str:
        return f"Team(name='{self.name}', members={len(self.members)}, concurrency={self.max_concurrent})"
```

[5] gecko/compose/workflow.py
```python
# gecko/compose/workflow.py
"""
Workflow 引擎

提供基于 DAG（有向无环图）的任务编排能力，支持复杂的控制流和状态管理。

核心功能：
1. 节点编排：支持普通函数、Agent、Team 等多种节点类型混编
2. 状态管理：基于 Pydantic 的强类型上下文，支持完整的序列化与持久化
3. 控制流：支持条件分支、循环（通过 Next 指令）
4. 智能绑定：自动根据函数签名注入 Context 或 Input
5. 可观测性：详细的节点执行轨迹与统计

优化日志：
- [Fix] 修复参数注入逻辑，支持同时接收 Input 和 Context 的函数签名
- [Fix] 将 WorkflowContext 升级为 BaseModel，彻底解决持久化数据截断问题
- [Fix] 增加分支歧义检测，确保逻辑确定性
- [Fix] 优化 Agent 间的数据流转 (Data Handover)，避免 Prompt 污染
"""
from __future__ import annotations

import asyncio
import inspect
import time
import uuid
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple, Union, Set

from pydantic import BaseModel, Field, PrivateAttr

from gecko.compose.nodes import Next
from gecko.core.events import BaseEvent, EventBus
from gecko.core.exceptions import WorkflowCycleError, WorkflowError
from gecko.core.logging import get_logger
from gecko.core.message import Message
from gecko.core.utils import ensure_awaitable
from gecko.plugins.storage.interfaces import SessionInterface

logger = get_logger(__name__)


# ========================= 事件定义 =========================

class WorkflowEvent(BaseEvent):
    """Workflow 专用事件对象"""
    pass


# ========================= 状态模型 =========================

class NodeStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"


class NodeExecution(BaseModel):
    """
    节点执行记录（轨迹追踪）
    """
    node_name: str
    status: NodeStatus = NodeStatus.PENDING
    input_data: Any = None
    output_data: Any = None
    error: Optional[str] = None
    start_time: float = Field(default_factory=time.time)
    end_time: float = 0.0

    @property
    def duration(self) -> float:
        """计算执行耗时"""
        if self.end_time == 0.0:
            return 0.0
        return max(0.0, self.end_time - self.start_time)


class WorkflowContext(BaseModel):
    """
    工作流执行上下文
    
    使用 Pydantic 模型确保类型安全和原生序列化支持。
    """
    execution_id: str = Field(
        default_factory=lambda: uuid.uuid4().hex,
        description="单次运行的唯一 ID"
    )
    input: Any = Field(..., description="工作流初始输入")
    state: Dict[str, Any] = Field(
        default_factory=dict, 
        description="共享状态存储（用户自定义）"
    )
    history: Dict[str, Any] = Field(
        default_factory=dict, 
        description="节点历史输出记录"
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="元数据（如 session_id, trace_id）"
    )
    executions: List[NodeExecution] = Field(
        default_factory=list,
        description="完整执行轨迹"
    )

    def add_execution(self, execution: NodeExecution):
        """添加执行记录"""
        self.executions.append(execution)

    def get_last_output(self) -> Any:
        """获取上一个节点的输出（如果无历史则返回初始输入）"""
        return self.history.get("last_output", self.input)

    def get_summary(self) -> Dict[str, Any]:
        """获取执行摘要"""
        total_time = sum(e.duration for e in self.executions)
        is_failed = any(e.status == NodeStatus.FAILED for e in self.executions)
        return {
            "execution_id": self.execution_id,
            "total_nodes": len(self.executions),
            "total_time": total_time,
            "last_node": self.executions[-1].node_name if self.executions else None,
            "status": "failed" if is_failed else "completed"
        }


# ========================= 工作流引擎 =========================

class Workflow:
    """
    DAG 工作流引擎
    """

    def __init__(
        self,
        name: str = "Workflow",
        event_bus: Optional[EventBus] = None,
        storage: Optional[SessionInterface] = None,
        max_steps: int = 100,
        enable_retry: bool = False,
        max_retries: int = 3,
    ):
        self.name = name
        self.event_bus = event_bus or EventBus()
        self.storage = storage
        self.max_steps = max_steps
        self.enable_retry = enable_retry
        self.max_retries = max_retries

        # 内部存储
        self._nodes: Dict[str, Callable] = {}
        # edges: source -> [(target, condition_func), ...]
        self._edges: Dict[str, List[Tuple[str, Optional[Callable]]]] = {}
        self._entry_point: Optional[str] = None

        # 验证状态
        self._validated = False
        self._validation_errors: List[str] = []

    # ========================= 构建 API =========================

    def add_node(self, name: str, func: Callable) -> "Workflow":
        """
        添加节点
        
        参数:
            name: 节点唯一名称
            func: 可调用对象（函数、Agent、Team）
        """
        if name in self._nodes:
            raise ValueError(f"Node '{name}' already exists")
        self._nodes[name] = func
        self._validated = False
        logger.debug("Node added", workflow=self.name, node=name)
        return self

    def add_edge(
        self,
        source: str,
        target: str,
        condition: Optional[Callable[[WorkflowContext], bool]] = None,
    ) -> "Workflow":
        """
        添加边（支持条件分支）
        
        参数:
            source: 源节点名称
            target: 目标节点名称
            condition: 转移条件函数（接收 Context 返回 bool）
        """
        if source not in self._nodes:
            raise ValueError(f"Source node '{source}' not found")
        if target not in self._nodes:
            raise ValueError(f"Target node '{target}' not found")

        self._edges.setdefault(source, []).append((target, condition))
        self._validated = False
        logger.debug("Edge added", source=source, target=target, conditional=bool(condition))
        return self

    def set_entry_point(self, name: str) -> "Workflow":
        """设置入口节点"""
        if name not in self._nodes:
            raise ValueError(f"Node '{name}' not found")
        self._entry_point = name
        self._validated = False
        return self

    # ========================= 验证逻辑 =========================

    def validate(self) -> bool:
        """验证工作流结构的合法性"""
        if self._validated:
            return len(self._validation_errors) == 0

        self._validation_errors.clear()

        # 1. 检查入口
        if not self._entry_point:
            self._validation_errors.append("No entry point defined")
        elif self._entry_point not in self._nodes:
            self._validation_errors.append(f"Entry point '{self._entry_point}' not in nodes")

        # 2. 检查歧义分支 (同一节点存在多个无条件出边)
        for node, edges in self._edges.items():
            unconditional_edges = [t for t, c in edges if c is None]
            if len(unconditional_edges) > 1:
                self._validation_errors.append(
                    f"Node '{node}' has ambiguous edges: multiple unconditional targets {unconditional_edges}"
                )

        # 3. 检查死循环 (静态检测)
        try:
            self._detect_cycles()
        except WorkflowCycleError as e:
            self._validation_errors.append(str(e))

        # 4. 连通性警告
        self._check_connectivity()

        self._validated = True
        if self._validation_errors:
            logger.error("Workflow validation failed", errors=self._validation_errors)
            return False

        logger.info("Workflow validation passed", name=self.name)
        return True

    def _detect_cycles(self):
        """DFS 检测环"""
        visited = set()
        recursion_stack = set()

        def dfs(node: str, path: List[str]):
            visited.add(node)
            recursion_stack.add(node)
            path.append(node)

            for neighbor, _ in self._edges.get(node, []):
                if neighbor not in visited:
                    dfs(neighbor, path)
                elif neighbor in recursion_stack:
                    cycle_start = path.index(neighbor)
                    cycle = " -> ".join(path[cycle_start:] + [neighbor])
                    raise WorkflowCycleError(f"Cycle detected: {cycle}")

            recursion_stack.remove(node)
            path.pop()

        for node in self._nodes:
            if node not in visited:
                dfs(node, [])

    def _check_connectivity(self):
        """检查不可达节点（仅警告）"""
        if not self._entry_point:
            return

        reachable = set()
        queue = [self._entry_point]
        while queue:
            curr = queue.pop(0)
            if curr in reachable:
                continue
            reachable.add(curr)
            for target, _ in self._edges.get(curr, []):
                queue.append(target)
        
        unreachable = set(self._nodes.keys()) - reachable
        if unreachable:
            logger.warning("Unreachable nodes detected", nodes=list(unreachable))

    # ========================= 执行引擎 =========================

    async def execute(self, input_data: Any, session_id: Optional[str] = None) -> Any:
        """
        执行工作流
        
        参数:
            input_data: 初始输入数据
            session_id: 会话 ID（用于持久化和状态恢复）
            
        返回:
            最终输出（Context 中的 last_output）
        """
        if not self.validate():
            raise WorkflowError(f"Workflow validation failed:\n" + "\n".join(self._validation_errors))

        # 初始化上下文
        context = WorkflowContext(input=input_data)
        if session_id:
            context.metadata["session_id"] = session_id

        await self.event_bus.publish(
            WorkflowEvent(
                type="workflow_started", 
                data={"name": self.name, "execution_id": context.execution_id}
            )
        )

        try:
            await self._execute_loop(context, session_id)
            
            result = context.get_last_output()
            await self.event_bus.publish(
                WorkflowEvent(
                    type="workflow_completed",
                    data={"name": self.name, "summary": context.get_summary()},
                )
            )
            return result
            
        except Exception as e:
            logger.exception("Workflow execution failed")
            await self.event_bus.publish(
                WorkflowEvent(type="workflow_error", error=str(e), data={"name": self.name})
            )
            raise

    async def _execute_loop(self, context: WorkflowContext, session_id: Optional[str]):
        """核心执行循环"""
        current_node = self._entry_point
        steps = 0

        while current_node and steps < self.max_steps:
            steps += 1
            logger.debug("Executing step", step=steps, node=current_node)

            # 1. 执行节点（带异常捕获与重试）
            result = await self._execute_node_safe(current_node, context)

            # 2. 处理结果与控制流 (Next)
            if isinstance(result, Next):
                # 处理跳转
                current_node = result.node
                # 如果 Next 携带了新输入，更新 last_output
                if result.input is not None:
                    normalized_input = self._normalize_result(result.input)
                    context.history["last_output"] = normalized_input
                    context.state["_next_input"] = normalized_input
            else:
                # 普通 DAG 流转
                normalized = self._normalize_result(result)
                context.history[current_node] = normalized
                context.history["last_output"] = normalized
                
                # 寻找下一个节点
                current_node = await self._find_next_node(current_node, context)

            # 3. 状态持久化
            if self.storage and session_id:
                await self._persist_state(session_id, steps, current_node, context)

        if steps >= self.max_steps:
            raise WorkflowError(
                f"Workflow exceeded max steps: {self.max_steps}",
                context={"last_node": current_node}
            )

    async def _execute_node_safe(self, node_name: str, context: WorkflowContext) -> Any:
        """节点执行包装器：负责状态记录、重试和错误处理"""
        execution = NodeExecution(node_name=node_name, status=NodeStatus.RUNNING)
        await self.event_bus.publish(WorkflowEvent(type="node_started", data={"node": node_name}))

        try:
            node_func = self._nodes[node_name]
            
            # 执行逻辑（含重试）
            if self.enable_retry:
                result = await self._execute_with_retry(node_func, context)
            else:
                result = await self._run_any_node(node_func, context)
            
            # 如果是 Next 对象，不在这里进行序列化，直接返回给 Loop 处理
            if isinstance(result, Next):
                execution.output_data = f"Next(node={result.node})"
                execution.status = NodeStatus.SUCCESS
            else:
                # 规范化结果以便记录在 trace 中
                normalized = self._normalize_result(result)
                execution.output_data = normalized
                result = normalized

            execution.status = NodeStatus.SUCCESS
            
            await self.event_bus.publish(
                WorkflowEvent(
                    type="node_completed", 
                    data={"node": node_name, "duration": execution.duration}
                )
            )
            return result

        except Exception as e:
            execution.status = NodeStatus.FAILED
            execution.error = str(e)
            
            await self.event_bus.publish(
                WorkflowEvent(type="node_error", error=str(e), data={"node": node_name})
            )
            # 重新抛出异常以中断 loop (或者触发全局错误处理)
            raise WorkflowError(f"Node '{node_name}' failed: {e}") from e
            
        finally:
            execution.end_time = time.time()
            context.add_execution(execution)

    # ========================= 节点调度逻辑 (核心) =========================

    async def _run_any_node(self, node_callable: Callable, context: WorkflowContext) -> Any:
        """
        通用节点执行器 (Duck Typing & Smart Binding)
        
        支持:
        1. Agent/Team (具备 run 方法的对象)
        2. 普通函数 (自动注入 context/input/none)
        
        修复：智能处理参数绑定，支持同时需要 Input 和 Context 的场景
        """
        # 1. 智能体对象 (Agent or Team)
        if hasattr(node_callable, "run") and callable(node_callable.run):
            return await self._run_intelligent_object(node_callable, context)
        
        # 2. 普通可调用对象
        if callable(node_callable):
            sig = inspect.signature(node_callable)
            params = sig.parameters
            kwargs = {}
            args = []
            
            # 获取当前输入数据
            current_input = context.state.pop("_next_input", None) or context.get_last_output()
            
            # A. 注入 Context
            if "context" in params:
                kwargs["context"] = context
            elif "workflow_context" in params:
                kwargs["workflow_context"] = context
                
            # B. 注入 Input (Input Injection)
            # 找到除 context, self 之外的参数，通常是第一个位置参数或特定的参数名
            # 这里采取简单策略：如果有剩余参数位置，则将 Input 作为第一个位置参数传入
            
            # 过滤掉已处理的 context 参数
            remaining_params = [
                name for name, p in params.items() 
                if name not in ("context", "workflow_context", "self")
                and p.kind in (inspect.Parameter.POSITIONAL_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD)
            ]
            
            if remaining_params:
                # 如果有剩余参数，假设第一个是用来接收 input 的
                args.append(current_input)
            elif not kwargs:
                # 如果既不需要 Context 也没有其他参数（无参函数），直接调用
                pass
                
            # 执行
            return await ensure_awaitable(node_callable, *args, **kwargs)
            
        raise WorkflowError(f"Node '{node_callable}' is not callable")

    async def _run_intelligent_object(self, obj: Any, context: WorkflowContext) -> Any:
        """
        执行 Agent/Team 对象
        
        优化: 智能处理数据流转 (Data Handover)
        """
        # 1. 获取输入 (优先使用 Next 传递的，否则用上一步输出)
        raw_input = context.state.pop("_next_input", None) or context.get_last_output()
        
        # 2. 数据清洗 (Data Handover Fix)
        # 如果上一个节点返回的是 AgentOutput (dict)，且当前 Agent 需要文本输入，
        # 我们尝试提取 content，避免将整个 JSON 结构扔给 LLM。
        agent_input = raw_input
        if isinstance(raw_input, dict):
            # 如果有 content 且没有 role (说明不是 Message 对象，而是 Output 字典)
            if "content" in raw_input and "role" not in raw_input:
                agent_input = raw_input["content"]
        
        # 3. 执行
        output = await obj.run(agent_input)
        
        # 4. 结果处理
        if hasattr(output, "model_dump"):
            return output.model_dump()
        return output

    async def _execute_with_retry(self, func: Callable, context: WorkflowContext) -> Any:
        """重试逻辑"""
        last_error = None
        for attempt in range(self.max_retries):
            try:
                return await self._run_any_node(func, context)
            except Exception as e:
                last_error = e
                logger.warning(
                    "Node retry triggered",
                    attempt=attempt + 1,
                    error=str(e)
                )
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
        raise last_error

    # ========================= 辅助方法 =========================

    async def _find_next_node(self, current: str, context: WorkflowContext) -> Optional[str]:
        """
        寻找下一个节点
        
        包含歧义检测：不允许同时满足多个无条件路径
        """
        edges = self._edges.get(current, [])
        candidates = []
        
        for target, condition in edges:
            should_go = False
            if condition is None:
                should_go = True
            else:
                try:
                    if inspect.iscoroutinefunction(condition):
                        should_go = await condition(context)
                    else:
                        should_go = condition(context)
                except Exception as e:
                    logger.error("Condition evaluation failed", source=current, target=target, error=str(e))
            
            if should_go:
                candidates.append(target)
        
        if len(candidates) == 0:
            return None
        
        # 歧义检测
        if len(candidates) > 1:
            raise WorkflowError(
                f"Ambiguous branching from '{current}': multiple conditions met for targets {candidates}. "
                "Workflow logic must be deterministic."
            )
            
        return candidates[0]

    def _normalize_result(self, result: Any) -> Any:
        """
        标准化结果 (Pydantic Friendly)
        """
        if isinstance(result, BaseModel):
            return result.model_dump()
        if hasattr(result, "model_dump"):
            return result.model_dump()
        if isinstance(result, Message):
            return result.to_openai_format()
        return result

    async def _persist_state(
        self,
        session_id: str,
        steps: int,
        current_node: Optional[str],
        context: WorkflowContext,
    ):
        """
        状态持久化
        
        使用 Pydantic 的 mode='json' 确保完整序列化，不进行截断。
        """
        try:
            # mode='json' 会将 UUID, Enum 等转换为字符串
            context_data = context.model_dump(mode="json")
            
            await self.storage.set(
                f"workflow:{session_id}",
                {
                    "step": steps,
                    "last_node": current_node,
                    "context": context_data,
                    "updated_at": time.time(),
                },
            )
        except Exception as e:
            logger.warning("Failed to persist workflow state", session_id=session_id, error=str(e))

    # ========================= 可视化 =========================

    def to_mermaid(self) -> str:
        """生成 Mermaid 流程图代码"""
        lines = ["graph TD"]
        for node in self._nodes:
            # 入口节点使用双圆圈
            shape_start = "((" if node == self._entry_point else "("
            shape_end = "))" if node == self._entry_point else ")"
            lines.append(f"    {node}{shape_start}{node}{shape_end}")
            
        for source, targets in self._edges.items():
            for target, condition in targets:
                label = "|condition|" if condition else ""
                lines.append(f"    {source} --{label}--> {target}")
        return "\n".join(lines)

    def print_structure(self):
        """打印工作流结构"""
        print(f"\n=== Workflow: {self.name} ===")
        print(f"Entry Point: {self._entry_point}")
        print(f"\nNodes ({len(self._nodes)}):")
        for node in self._nodes:
            print(f"  - {node}")

        print(f"\nEdges ({sum(len(v) for v in self._edges.values())}):")
        for source, targets in self._edges.items():
            for target, condition in targets:
                cond_str = " [conditional]" if condition else ""
                print(f"  - {source} -> {target}{cond_str}")
        print()
```

[6] gecko/config.py
```python
# gecko/config.py  
"""  
配置系统（改进版）  
  
- 增加 configure_settings / reset_settings，便于测试重载  
- 使用 Lazy 初始化，防止导入 config 时立刻读取 .env  
- Docstring 更新，避免示例与 Agent API 不匹配  
"""  
  
from __future__ import annotations  
  
from typing import Optional  
  
from pydantic import Field, field_validator  
from pydantic_settings import BaseSettings, SettingsConfigDict  
  
  
class GeckoSettings(BaseSettings):  
    default_model: str = Field(default="gpt-3.5-turbo")  
    default_api_key: str = Field(default="")  
    default_base_url: Optional[str] = None  
    default_temperature: float = Field(default=0.7, ge=0.0, le=2.0)  
  
    max_turns: int = Field(default=5, ge=1, le=50)  
    max_context_tokens: int = Field(default=4000, ge=100)  
  
    default_storage_url: str = Field(default="sqlite://./gecko_data.db")  
    log_level: str = Field(default="INFO")  
    log_format: str = Field(default="text")  
  
    enable_cache: bool = True  
    tool_execution_timeout: float = Field(default=30.0, ge=1.0)  
  
    model_config = SettingsConfigDict(  
        env_prefix="GECKO_",  
        env_file=".env",  
        env_file_encoding="utf-8",  
        case_sensitive=False,  
        extra="ignore",  
    )  
  
    @field_validator("log_level")  
    @classmethod  
    def validate_log_level(cls, v: str) -> str:  
        valid = {"DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"}  
        if v.upper() not in valid:  
            raise ValueError(f"log_level must be one of {valid}")  
        return v.upper()  
  
    @field_validator("log_format")  
    @classmethod  
    def validate_log_format(cls, v: str) -> str:  
        if v not in {"text", "json"}:  
            raise ValueError("log_format must be 'text' or 'json'")  
        return v  
  
  
_default_settings: Optional[GeckoSettings] = None  
  
  
def get_settings(force_reload: bool = False) -> GeckoSettings:  
    global _default_settings  
    if _default_settings is None or force_reload:  
        _default_settings = GeckoSettings()  
    return _default_settings  
  
  
def configure_settings(**overrides) -> GeckoSettings:  
    """  
    允许测试/脚本传入覆盖参数，例如：  
        configure_settings(default_model="gpt-4")  
    """  
    global _default_settings  
    _default_settings = GeckoSettings(**overrides)  
    return _default_settings  
  
  
def reset_settings():  
    global _default_settings  
    _default_settings = None  
  
  
settings = get_settings()  
```

[7] gecko/core/__init__.py
```python
```

[8] gecko/core/agent.py
```python
# gecko/core/agent.py  
from __future__ import annotations  
  
from typing import Any, Iterable, List, Optional, Type, Union  
  
from pydantic import BaseModel  
  
from gecko.core.events import AgentRunEvent, EventBus  
from gecko.core.message import Message  
from gecko.core.output import AgentOutput  
from gecko.core.toolbox import ToolBox  
from gecko.core.memory import TokenMemory  
from gecko.core.engine.base import CognitiveEngine  
from gecko.core.engine.react import ReActEngine  
from gecko.core.logging import get_logger  
from gecko.core.exceptions import AgentError  
  
logger = get_logger(__name__)  
  
  
class Agent:  
    """  
    Agent 对象负责在模型、工具箱、记忆之间协调一次推理任务。  
    """  
  
    def __init__(  
        self,  
        model: Any,  
        toolbox: ToolBox,  
        memory: TokenMemory,  
        engine_cls: Type[CognitiveEngine] = ReActEngine,  
        event_bus: Optional[EventBus] = None,  
        **engine_kwargs: Any,  
    ):  
        self.event_bus = event_bus or EventBus()  
        self.toolbox = toolbox  
        self.memory = memory  
        self.engine = engine_cls(  
            model=model,  
            toolbox=toolbox,  
            memory=memory,  
            **engine_kwargs  
        )  
  
    async def run(  
        self,  
        messages: str | Message | List[Message] | List[dict] | dict,  
        response_model: Optional[Type[BaseModel]] = None  
    ) -> AgentOutput | BaseModel:  
        """  
        单次推理入口：对多种输入格式统一转换为 Message 列表  
        """  
        input_msgs = self._normalize_messages(messages)  
  
        await self.event_bus.publish(  
            AgentRunEvent(type="run_started", data={"input_count": len(input_msgs)})  
        )  
  
        try:  
            output = await self.engine.step(input_msgs, response_model=response_model)  
            payload = self._serialize_output(output)  
  
            await self.event_bus.publish(  
                AgentRunEvent(type="run_completed", data={"output": payload})  
            )  
            return output  
  
        except Exception as e:  
            logger.exception("Agent run failed")  
            await self.event_bus.publish(  
                AgentRunEvent(type="run_error", error=str(e))  
            )  
            raise  
  
    async def stream(self, messages: str | Message | List[Message] | List[dict] | dict):  
        """  
        流式推理：共用同一套输入标准化逻辑  
        """  
        input_msgs = self._normalize_messages(messages)  
  
        await self.event_bus.publish(AgentRunEvent(type="stream_started"))  
        try:  
            async for chunk in self.engine.step_stream(input_msgs):  
                yield chunk  
            await self.event_bus.publish(AgentRunEvent(type="stream_completed"))  
        except Exception as e:  
            logger.exception("Agent stream failed")  
            await self.event_bus.publish(AgentRunEvent(type="stream_error", error=str(e)))  
            raise  
  
    # ---------------- 辅助方法 ----------------  
    def _normalize_messages(  
        self,  
        messages: str | Message | List[Message] | List[dict] | dict  
    ) -> List[Message]:  
        """  
        支持以下输入：  
        1. 字符串 -> 单条 user 消息  
        2. Message -> [Message]  
        3. List[Message] -> 原样返回  
        4. dict -> 若包含 role/content 则构建 Message，否则视为 {"input": "..."}  
        5. List[dict] -> 每个 dict 转为 Message  
        """  
        if isinstance(messages, Message):  
            return [messages]  
  
        if isinstance(messages, str):  
            return [Message.user(messages)]  
  
        if isinstance(messages, dict):  
            if "role" in messages:  
                return [Message(**messages)]  
            text = messages.get("input") or str(messages)  
            return [Message.user(text)]  
  
        if isinstance(messages, list):  
            if not messages:  
                raise AgentError("消息列表为空")  
            if isinstance(messages[0], Message):  
                return messages  # 已经是标准 Message  
            normalized = []  
            for item in messages:  
                if isinstance(item, Message):  
                    normalized.append(item)  
                elif isinstance(item, dict):  
                    normalized.append(Message(**item))  
                else:  
                    raise AgentError(f"无法识别的消息元素类型: {type(item)}")  
            return normalized  
  
        raise AgentError(f"不支持的消息类型: {type(messages)}")  
  
    def _serialize_output(self, output: AgentOutput | BaseModel) -> dict:  
        if hasattr(output, "model_dump"):  
            return output.model_dump()  
        return {"content": str(output)}  
```

[9] gecko/core/builder.py
```python
# gecko/core/builder.py  
from __future__ import annotations  
  
from typing import Any, Sequence, Type  
  
from gecko.core.agent import Agent  
from gecko.core.memory import TokenMemory  
from gecko.core.toolbox import ToolBox  
from gecko.core.engine.base import CognitiveEngine  
from gecko.core.engine.react import ReActEngine  
from gecko.plugins.storage.interfaces import SessionInterface  
from gecko.plugins.tools.base import BaseTool  
from gecko.core.exceptions import ConfigurationError  
  
  
class AgentBuilder:  
    """  
    Agent 构建器（改进版）  
    关键改进：  
    1. system_prompt 等引擎参数统一通过 engine_kwargs 传递，避免与 Agent.__init__ 不匹配  
    2. 工具列表自动去重并校验是否继承 BaseTool  
    3. storage 必须实现 SessionInterface，否则在 TokenMemory 中使用会报错  
    4. 支持自定义 Engine 类 & 额外参数  
    """  
  
    def __init__(self):  
        self._model: Any | None = None  
        self._tools: list[BaseTool] = []  
        self._storage: SessionInterface | None = None  
        self._session_id: str = "default"  
        self._max_tokens: int = 4000  
        self._engine_cls: Type[CognitiveEngine] = ReActEngine  
        self._engine_kwargs: dict[str, Any] = {}  # 统一放置系统 Prompt、Hook 等  
        self._toolbox_config: dict[str, Any] = {}  
  
    # ---------------- 基础配置 ----------------  
    def with_model(self, model: Any) -> "AgentBuilder":  
        # 检查模型是否实现必要方法  
        missing = [m for m in ("acompletion",) if not hasattr(model, m)]  
        if missing:  
            raise ConfigurationError(  
                f"Model 缺少必要方法: {', '.join(missing)}",  
                context={"model": repr(model)}  
            )  
        self._model = model  
        return self  
  
    def with_tools(self, tools: Sequence[BaseTool]) -> "AgentBuilder":  
        for tool in tools:  
            if not isinstance(tool, BaseTool):  
                raise TypeError(f"Tool 必须继承 BaseTool，收到 {type(tool)}")  
            self._tools.append(tool)  
        return self  
  
    def with_storage(self, storage: SessionInterface | None) -> "AgentBuilder":  
        if storage and not isinstance(storage, SessionInterface):  
            raise TypeError(  
                "storage 必须实现 SessionInterface，用于 TokenMemory 持久化"  
            )  
        self._storage = storage  
        return self  
  
    def with_session_id(self, session_id: str) -> "AgentBuilder":  
        self._session_id = session_id  
        return self  
  
    def with_max_tokens(self, max_tokens: int) -> "AgentBuilder":  
        self._max_tokens = max_tokens  
        return self  
  
    def with_engine(  
        self,  
        engine_cls: Type[CognitiveEngine],  
        **engine_kwargs: Any  
    ) -> "AgentBuilder":  
        if not issubclass(engine_cls, CognitiveEngine):  
            raise TypeError("engine_cls 必须继承 CognitiveEngine")  
        self._engine_cls = engine_cls  
        self._engine_kwargs.update(engine_kwargs)  
        return self  
  
    def with_system_prompt(self, prompt: str) -> "AgentBuilder":  
        # 统一放入 engine_kwargs，确保 Engine 可接收  
        self._engine_kwargs["system_prompt"] = prompt  
        return self  
  
    def with_toolbox_config(self, **config: Any) -> "AgentBuilder":  
        """  
        允许调用者自定义 ToolBox 的并发/超时等参数  
        """  
        self._toolbox_config.update(config)  
        return self  
  
    # ---------------- 构建流程 ----------------  
    def build(self) -> Agent:  
        if not self._model:  
            raise ConfigurationError("构建 Agent 前必须调用 with_model 指定模型")  
  
        toolbox = self._build_toolbox()  
        memory = self._build_memory()  
  
        return Agent(  
            model=self._model,  
            toolbox=toolbox,  
            memory=memory,  
            engine_cls=self._engine_cls,  
            event_bus=self._engine_kwargs.pop("event_bus", None),  
            **self._engine_kwargs  # 其余参数直接传给 Engine  
        )  
  
    def _build_toolbox(self) -> ToolBox:  
        # 根据工具名称去重，后注册的同名工具会覆盖前者  
        deduped: dict[str, BaseTool] = {}  
        for tool in self._tools:  
            deduped[tool.name] = tool  
  
        return ToolBox(  
            tools=list(deduped.values()),  
            **self._toolbox_config  
        )  
  
    def _build_memory(self) -> TokenMemory:  
        return TokenMemory(  
            session_id=self._session_id,  
            storage=self._storage,  
            max_tokens=self._max_tokens  
        )  
```

[10] gecko/core/engine/base.py
```python
# gecko/core/engine/base.py
"""
认知引擎基类

定义 Agent 的推理和执行流程，所有引擎实现（ReAct、Chain、Tree 等）
都应继承此基类。

核心概念：
- CognitiveEngine: 抽象基类，定义引擎接口
- 支持普通推理和流式推理
- 支持结构化输出
- 提供 Hook 机制
- 统一的错误处理

优化点：
1. 强化类型注解（使用 ModelProtocol）
2. 完善抽象方法（step, step_stream, step_structured）
3. 添加 Hook 机制（before_step, after_step）
4. 提供工具方法（validate_input, log_execution）
5. 支持上下文管理器（资源管理）
"""
from __future__ import annotations

import asyncio
import time
from abc import ABC, abstractmethod
from typing import Any, AsyncIterator, Callable, Dict, List, Optional, Type, TypeVar

from pydantic import BaseModel

from gecko.core.exceptions import AgentError, ModelError
from gecko.core.logging import get_logger
from gecko.core.memory import TokenMemory
from gecko.core.message import Message
from gecko.core.output import AgentOutput
from gecko.core.protocols import ModelProtocol, supports_streaming
from gecko.core.toolbox import ToolBox

logger = get_logger(__name__)

T = TypeVar("T", bound=BaseModel)


# ====================== 执行统计 ======================

class ExecutionStats(BaseModel):
    """
    引擎执行统计
    
    用于性能监控和调试。
    """
    total_steps: int = 0
    total_time: float = 0.0
    total_tokens: int = 0
    tool_calls: int = 0
    errors: int = 0
    
    def add_step(self, duration: float, tokens: int = 0, had_error: bool = False):
        """记录一次步骤执行"""
        self.total_steps += 1
        self.total_time += duration
        self.total_tokens += tokens
        if had_error:
            self.errors += 1
    
    def add_tool_call(self):
        """记录一次工具调用"""
        self.tool_calls += 1
    
    def get_avg_step_time(self) -> float:
        """获取平均步骤时间"""
        return self.total_time / self.total_steps if self.total_steps > 0 else 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """转换为字典"""
        return {
            "total_steps": self.total_steps,
            "total_time": self.total_time,
            "avg_step_time": self.get_avg_step_time(),
            "total_tokens": self.total_tokens,
            "tool_calls": self.tool_calls,
            "errors": self.errors,
        }


# ====================== 认知引擎基类 ======================

class CognitiveEngine(ABC):
    """
    认知引擎抽象基类
    
    定义 Agent 的核心推理流程，所有具体引擎实现（ReAct、Chain、Tree 等）
    都应该继承此类。
    
    核心方法：
    - step(): 单次/多轮推理（必需）
    - step_stream(): 流式推理（可选）
    - step_structured(): 结构化输出（可选）
    
    Hook 方法：
    - before_step(): 步骤执行前
    - after_step(): 步骤执行后
    - on_error(): 错误处理
    
    生命周期：
    - initialize(): 初始化
    - cleanup(): 清理资源
    
    示例:
        ```python
        class MyEngine(CognitiveEngine):
            async def step(self, input_messages: List[Message]) -> AgentOutput:
                # 实现推理逻辑
                response = await self.model.acompletion(
                    messages=[m.to_openai_format() for m in input_messages]
                )
                return AgentOutput(content=response.choices[0].message["content"])
        
        # 使用
        engine = MyEngine(model=model, toolbox=toolbox, memory=memory)
        output = await engine.step([Message.user("Hello")])
        ```
    """
    
    def __init__(
        self,
        model: ModelProtocol,
        toolbox: ToolBox,
        memory: TokenMemory,
        max_iterations: int = 10,
        enable_stats: bool = True,
        **kwargs
    ):
        """
        初始化认知引擎
        
        参数:
            model: 语言模型（必须实现 ModelProtocol）
            toolbox: 工具箱
            memory: 记忆管理器
            max_iterations: 最大迭代次数（防止死循环）
            enable_stats: 是否启用统计
            **kwargs: 子类的额外参数
        
        异常:
            TypeError: model 不符合 ModelProtocol
        """
        # 验证模型
        if not isinstance(model, ModelProtocol):
            raise TypeError(
                f"model 必须实现 ModelProtocol，收到类型: {type(model).__name__}"
            )
        
        self.model = model
        self.toolbox = toolbox
        self.memory = memory
        self.max_iterations = max_iterations
        self.enable_stats = enable_stats
        
        # 统计信息
        self.stats = ExecutionStats() if enable_stats else None
        
        # Hook 函数（可由子类或外部设置）
        self.before_step_hook: Optional[Callable] = None
        self.after_step_hook: Optional[Callable] = None
        self.on_error_hook: Optional[Callable] = None
        
        # 存储额外的配置
        self._config = kwargs
        
        logger.debug(
            "Engine initialized",
            engine=self.__class__.__name__,
            model=type(model).__name__,
            max_iterations=max_iterations
        )
    
    # ====================== 核心抽象方法 ======================
    
    @abstractmethod
    async def step(
        self, 
        input_messages: List[Message],
        **kwargs
    ) -> AgentOutput:
        """
        执行推理步骤（必需实现）
        
        这是引擎的核心方法，定义了如何处理输入并生成输出。
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数（如 temperature, max_tokens 等）
        
        返回:
            AgentOutput: 执行结果
        
        异常:
            AgentError: 执行失败
            ModelError: 模型调用失败
        
        实现指南:
            1. 验证输入
            2. 调用 before_step_hook（如果有）
            3. 执行推理逻辑
            4. 调用 after_step_hook（如果有）
            5. 返回结果
        
        示例:
            ```python
            async def step(self, input_messages: List[Message]) -> AgentOutput:
                # 转换为 OpenAI 格式
                messages = [m.to_openai_format() for m in input_messages]
                
                # 调用模型
                response = await self.model.acompletion(messages=messages)
                
                # 构建输出
                return AgentOutput(
                    content=response.choices[0].message["content"],
                    usage=response.usage
                )
            ```
        """
        pass
    
    # ====================== 可选方法 ======================
    
    async def step_stream(
        self, 
        input_messages: List[Message],
        **kwargs
    ) -> AsyncIterator[str]:
        """
        流式推理（可选实现）
        
        如果引擎支持流式输出，应该重写此方法。
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数
        
        返回:
            AsyncIterator[str]: 文本流
        
        异常:
            NotImplementedError: 引擎不支持流式输出
        
        示例:
            ```python
            async def step_stream(self, input_messages: List[Message]):
                if not supports_streaming(self.model):
                    raise NotImplementedError("Model does not support streaming")
                
                messages = [m.to_openai_format() for m in input_messages]
                
                async for chunk in self.model.astream(messages=messages):
                    if chunk.content:
                        yield chunk.content
            ```
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} does not support streaming. "
            f"Override step_stream() to enable this feature."
        )
    
    async def step_structured(
        self,
        input_messages: List[Message],
        response_model: Type[T],
        **kwargs
    ) -> T:
        """
        结构化输出推理（可选实现）
        
        执行推理并将输出解析为 Pydantic 模型。
        
        参数:
            input_messages: 输入消息列表
            response_model: 目标 Pydantic 模型类
            **kwargs: 额外参数
        
        返回:
            T: 解析后的模型实例
        
        异常:
            NotImplementedError: 引擎不支持结构化输出
        
        示例:
            ```python
            from pydantic import BaseModel
            
            class Answer(BaseModel):
                question: str
                answer: str
                confidence: float
            
            result = await engine.step_structured(
                input_messages=[Message.user("What is AI?")],
                response_model=Answer
            )
            print(result.answer)
            ```
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} does not support structured output. "
            f"Override step_structured() to enable this feature."
        )
    
    # ====================== Hook 方法 ======================
    
    async def before_step(
        self, 
        input_messages: List[Message],
        **kwargs
    ) -> None:
        """
        步骤执行前的 Hook
        
        在推理开始前调用，可用于：
        - 日志记录
        - 输入验证
        - 状态初始化
        - 发送事件
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数
        
        注意:
            此方法不应修改输入，如需修改请在子类中重写
        """
        if self.before_step_hook:
            try:
                if asyncio.iscoroutinefunction(self.before_step_hook):
                    await self.before_step_hook(input_messages, **kwargs)
                else:
                    self.before_step_hook(input_messages, **kwargs)
            except Exception as e:
                logger.warning("before_step_hook failed", error=str(e))
    
    async def after_step(
        self,
        input_messages: List[Message],
        output: AgentOutput,
        **kwargs
    ) -> None:
        """
        步骤执行后的 Hook
        
        在推理完成后调用，可用于：
        - 日志记录
        - 结果验证
        - 统计更新
        - 发送事件
        
        参数:
            input_messages: 输入消息列表
            output: 执行结果
            **kwargs: 额外参数
        """
        if self.after_step_hook:
            try:
                if asyncio.iscoroutinefunction(self.after_step_hook):
                    await self.after_step_hook(input_messages, output, **kwargs)
                else:
                    self.after_step_hook(input_messages, output, **kwargs)
            except Exception as e:
                logger.warning("after_step_hook failed", error=str(e))
    
    async def on_error(
        self,
        error: Exception,
        input_messages: List[Message],
        **kwargs
    ) -> None:
        """
        错误处理 Hook
        
        在推理过程中发生错误时调用，可用于：
        - 错误日志记录
        - 错误恢复
        - 降级处理
        - 发送告警
        
        参数:
            error: 异常对象
            input_messages: 输入消息列表
            **kwargs: 额外参数
        """
        if self.on_error_hook:
            try:
                if asyncio.iscoroutinefunction(self.on_error_hook):
                    await self.on_error_hook(error, input_messages, **kwargs)
                else:
                    self.on_error_hook(error, input_messages, **kwargs)
            except Exception as e:
                logger.error("on_error_hook failed", error=str(e))
    
    # ====================== 工具方法 ======================
    
    def validate_input(self, input_messages: List[Message]) -> None:
        """
        验证输入消息
        
        参数:
            input_messages: 输入消息列表
        
        异常:
            ValueError: 输入无效
        """
        if not input_messages:
            raise ValueError("input_messages 不能为空")
        
        if not all(isinstance(m, Message) for m in input_messages):
            raise TypeError("所有输入必须是 Message 实例")
        
        logger.debug("Input validated", message_count=len(input_messages))
    
    def supports_streaming(self) -> bool:
        """
        检查引擎是否支持流式输出
        
        返回:
            是否支持
        """
        # 检查模型能力
        model_supports = supports_streaming(self.model)
        
        # 检查引擎是否重写了 step_stream
        engine_supports = (
            self.__class__.step_stream != CognitiveEngine.step_stream
        )
        
        return model_supports and engine_supports
    
    def get_config(self, key: str, default: Any = None) -> Any:
        """
        获取配置项
        
        参数:
            key: 配置键
            default: 默认值
        
        返回:
            配置值
        """
        return self._config.get(key, default)
    
    def set_config(self, key: str, value: Any) -> None:
        """
        设置配置项
        
        参数:
            key: 配置键
            value: 配置值
        """
        self._config[key] = value
    
    def get_stats(self) -> Optional[Dict[str, Any]]:
        """
        获取执行统计
        
        返回:
            统计信息字典，如果未启用统计则返回 None
        """
        return self.stats.to_dict() if self.stats else None
    
    def reset_stats(self) -> None:
        """重置统计信息"""
        if self.stats:
            self.stats = ExecutionStats()
            logger.debug("Stats reset")
    
    # ====================== 生命周期管理 ======================
    
    async def initialize(self) -> None:
        """
        初始化引擎
        
        在首次使用前调用，可用于：
        - 加载资源
        - 预热模型
        - 初始化连接
        
        子类可以重写此方法以添加自定义初始化逻辑。
        """
        logger.debug("Engine initialized", engine=self.__class__.__name__)
    
    async def cleanup(self) -> None:
        """
        清理资源
        
        在引擎不再使用时调用，可用于：
        - 关闭连接
        - 释放资源
        - 保存状态
        
        子类可以重写此方法以添加自定义清理逻辑。
        """
        logger.debug("Engine cleanup", engine=self.__class__.__name__)
    
    # ====================== 上下文管理器 ======================
    
    async def __aenter__(self):
        """异步上下文管理器入口"""
        await self.initialize()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """异步上下文管理器出口"""
        await self.cleanup()
        return False
    
    # ====================== 辅助方法 ======================
    
    async def _safe_execute(
        self,
        func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        安全执行函数（带错误处理和统计）
        
        参数:
            func: 要执行的函数
            *args: 位置参数
            **kwargs: 关键字参数
        
        返回:
            函数执行结果
        
        异常:
            原始异常（已记录日志和统计）
        """
        start_time = time.time()
        had_error = False
        
        try:
            if asyncio.iscoroutinefunction(func):
                result = await func(*args, **kwargs)
            else:
                result = func(*args, **kwargs)
            
            return result
        
        except Exception as e:
            had_error = True
            
            # 记录统计
            if self.stats:
                self.stats.errors += 1
            
            # 调用错误 Hook
            await self.on_error(e, kwargs.get("input_messages", []))
            
            # 记录日志
            logger.exception(
                "Engine execution failed",
                engine=self.__class__.__name__,
                error=str(e)
            )
            
            raise
        
        finally:
            # 记录执行时间
            duration = time.time() - start_time
            if self.stats:
                self.stats.add_step(duration, had_error=had_error)
    
    def __repr__(self) -> str:
        """字符串表示"""
        return (
            f"{self.__class__.__name__}("
            f"model={type(self.model).__name__}, "
            f"max_iterations={self.max_iterations}"
            f")"
        )


# ====================== 工具函数 ======================

def create_engine(
    engine_class: Type[CognitiveEngine],
    model: ModelProtocol,
    toolbox: ToolBox,
    memory: TokenMemory,
    **kwargs
) -> CognitiveEngine:
    """
    创建引擎实例（工厂函数）
    
    参数:
        engine_class: 引擎类
        model: 模型
        toolbox: 工具箱
        memory: 记忆
        **kwargs: 额外参数
    
    返回:
        引擎实例
    
    示例:
        ```python
        engine = create_engine(
            ReActEngine,
            model=openai_model,
            toolbox=toolbox,
            memory=memory,
            max_iterations=5
        )
        ```
    """
    if not issubclass(engine_class, CognitiveEngine):
        raise TypeError(
            f"engine_class 必须是 CognitiveEngine 的子类，"
            f"收到: {engine_class.__name__}"
        )
    
    return engine_class(
        model=model,
        toolbox=toolbox,
        memory=memory,
        **kwargs
    )


# ====================== 导出 ======================

__all__ = [
    "CognitiveEngine",
    "ExecutionStats",
    "create_engine",
]
```

[11] gecko/core/engine/react.py
```python
# gecko/core/engine/react.py
"""
ReAct 推理引擎

实现了 ReAct (Reason + Act) 认知架构，负责协调 LLM 与工具箱的交互循环。

核心功能：
1. 推理循环：基于 Thought-Action-Observation 模式的自动执行
2. 流式输出：支持低延迟的 Token 级流式响应，同时保持工具调用的完整性
3. 结构化输出：支持将推理结果解析为强类型的 Pydantic 对象
4. 稳健性设计：内置死循环检测、观测值截断、错误反馈与自动重试

优化日志：
- [Fix] 重构 step_stream: 移除 Peek 机制，显著降低首字延迟 (TTFT)
- [Fix] 修复 Jinja2 模板语法，正确处理字典属性访问
- [Fix] 完善生命周期钩子 (on_turn_start/end) 在流式模式下的覆盖
- [Feat] 增加工具调用死循环检测 (Hash-based loop detection)
- [Feat] 增加工具观测值 (Observation) 智能截断
"""
from __future__ import annotations

import json
import time
from datetime import datetime
from typing import (
    Any,
    AsyncIterator,
    Callable,
    Dict,
    List,
    Optional,
    Type,
    TypeVar,
    Union,
)

from pydantic import BaseModel

from gecko.core.engine.base import CognitiveEngine
from gecko.core.exceptions import AgentError, ModelError
from gecko.core.logging import get_logger
from gecko.core.memory import TokenMemory
from gecko.core.message import Message
from gecko.core.output import AgentOutput
from gecko.core.prompt import PromptTemplate
from gecko.core.structure import StructureEngine, StructureParseError
from gecko.core.toolbox import ToolBox
from gecko.core.utils import ensure_awaitable

logger = get_logger(__name__)

T = TypeVar("T", bound=BaseModel)

# 默认 ReAct 提示词模板
# 包含时间注入和工具列表渲染
DEFAULT_REACT_TEMPLATE = """You are a helpful AI assistant.
Current Time: {{ current_time }}

Available Tools:
{% for tool in tools %}
- {{ tool['function']['name'] }}: {{ tool['function']['description'] }}
{% endfor %}

Answer the user's request. Use tools if necessary.
If you use a tool, just output the tool call format.
"""


class ExecutionContext:
    """
    执行上下文
    
    封装每一轮 ReAct 循环的运行时状态，用于在 Engine 内部传递数据。
    """

    def __init__(self, messages: List[Message]):
        self.messages = messages.copy()  # 浅拷贝，避免污染原始列表
        self.turn = 0
        self.metadata: Dict[str, Any] = {}
        
        # 状态追踪：用于死循环检测
        self.last_tool_calls_hash: Optional[int] = None
        self.consecutive_tool_error_count: int = 0

    def add_message(self, message: Message):
        """追加消息到当前上下文"""
        self.messages.append(message)

    def get_last_message(self) -> Optional[Message]:
        """获取最后一条消息"""
        return self.messages[-1] if self.messages else None


class ReActEngine(CognitiveEngine):
    """
    ReAct (Reason + Act) 引擎实现
    """

    def __init__(
        self,
        model: Any,
        toolbox: ToolBox,
        memory: TokenMemory,
        max_turns: int = 5,
        max_observation_length: int = 2000,
        system_prompt: Union[str, PromptTemplate, None] = None,
        on_turn_start: Optional[Callable[[ExecutionContext], Any]] = None,
        on_turn_end: Optional[Callable[[ExecutionContext], Any]] = None,
        on_tool_execute: Optional[Callable[[str, Dict[str, Any]], Any]] = None,
        **kwargs,
    ):
        """
        初始化 ReAct 引擎
        
        参数:
            model: LLM 模型实例 (需实现 ModelProtocol)
            toolbox: 工具箱实例
            memory: 记忆管理器
            max_turns: 最大思考轮数 (防止无限循环)
            max_observation_length: 工具输出的最大字符数 (防止 Context 爆炸)
            system_prompt: 自定义系统提示词
            on_turn_start: 每一轮开始时的钩子
            on_turn_end: 每一轮结束时的钩子
            on_tool_execute: 工具执行前的钩子
        """
        super().__init__(model, toolbox, memory, **kwargs)
        self.max_turns = max_turns
        self.max_observation_length = max_observation_length
        
        # Hooks
        self.on_turn_start = on_turn_start
        self.on_turn_end = on_turn_end
        self.on_tool_execute = on_tool_execute

        # 初始化 Prompt 模板
        if system_prompt is None:
            self.prompt_template = PromptTemplate(template=DEFAULT_REACT_TEMPLATE)
        elif isinstance(system_prompt, str):
            self.prompt_template = PromptTemplate(template=system_prompt)
        else:
            self.prompt_template = system_prompt

        # 能力检测缓存
        self._supports_functions = self._check_function_calling_support()
        self._supports_stream = self._check_streaming_support()

    def _check_function_calling_support(self) -> bool:
        if hasattr(self.model, "_supports_function_calling"):
            return getattr(self.model, "_supports_function_calling")
        return True  # 默认假设支持，运行时报错再处理

    def _check_streaming_support(self) -> bool:
        return hasattr(self.model, "astream")

    # ===================== 核心接口实现 =====================

    async def step(
        self,
        input_messages: List[Message],
        response_model: Optional[Type[T]] = None,
        strategy: str = "auto",
        max_retries: int = 2,
        **kwargs,
    ) -> Union[AgentOutput, T]:
        """
        执行单次推理任务 (同步等待模式)
        
        流程:
        1. 验证输入 & Hook
        2. 构建上下文 (Context) & 参数
        3. 执行 ReAct 循环 (Reasoning Loop)
        4. 处理结构化输出 (如果需要)
        5. 保存记忆 & Hook
        """
        start_time = time.time()

        # 1. 验证与 Hook
        self.validate_input(input_messages)
        await self.before_step(input_messages, **kwargs)

        logger.info(
            "ReAct execution started",
            input_count=len(input_messages),
            has_structure=response_model is not None,
        )

        try:
            # 2. 预处理与上下文构建
            augmented_messages = self._augment_messages_for_structure(
                input_messages, response_model
            )
            context = await self._build_execution_context(augmented_messages)
            
            llm_params = self._build_llm_params(response_model, strategy)
            llm_params.update(kwargs)

            # 3. 运行推理循环
            final_output = await self._run_reasoning_loop(
                context, llm_params, response_model
            )

            # 4. 结构化输出处理
            result: Union[AgentOutput, T] = final_output
            if response_model:
                result = await self._handle_structured_output(
                    final_output, response_model, context, llm_params, max_retries
                )

            # 5. 保存与收尾
            await self._save_context(context)

            # 触发 after_step (需将 T 转换为 AgentOutput 以兼容 Hook 签名)
            hook_output = (
                result
                if isinstance(result, AgentOutput)
                else AgentOutput(content=str(result), raw=result)
            )
            await self.after_step(input_messages, hook_output, **kwargs)

            # 统计
            if self.stats:
                self.stats.add_step(time.time() - start_time)

            return result

        except Exception as e:
            if self.stats:
                self.stats.errors += 1
            logger.exception("ReAct execution failed")
            await self.on_error(e, input_messages, **kwargs)
            raise

    async def step_stream(
        self, input_messages: List[Message], **kwargs
    ) -> AsyncIterator[str]:
        """
        执行流式推理任务
        
        特点:
        - 立即返回首个 Token (Low Latency)
        - 遇到工具调用时暂停输出，执行工具后继续流式生成
        """
        if not self._supports_stream:
            raise AgentError("当前模型不支持流式输出")

        start_time = time.time()
        await self.before_step(input_messages, **kwargs)

        # 构建上下文
        context = await self._build_execution_context(input_messages)
        llm_params = self._build_llm_params(None, "auto")
        llm_params.update(kwargs)

        try:
            # 进入递归流式循环
            async for token in self._run_streaming_loop(context, llm_params):
                yield token

            # 保存记忆
            await self._save_context(context)

            if self.stats:
                self.stats.add_step(time.time() - start_time)

        except Exception as e:
            if self.stats:
                self.stats.errors += 1
            logger.exception("ReAct stream failed")
            await self.on_error(e, input_messages, **kwargs)
            raise

    # ===================== 推理循环逻辑 =====================

    async def _run_reasoning_loop(
        self,
        context: ExecutionContext,
        llm_params: Dict[str, Any],
        response_model: Optional[Type[T]],
    ) -> AgentOutput:
        """
        ReAct 主循环 (同步模式)
        
        负责：调用 LLM -> 解析响应 -> 检测死循环 -> 执行工具 -> 更新上下文
        """
        while context.turn < self.max_turns:
            context.turn += 1

            # Hook: Turn Start
            if self.on_turn_start:
                await ensure_awaitable(self.on_turn_start, context)

            # 1. LLM 推理
            response = await self._call_llm(context, llm_params)
            assistant_msg = self._parse_llm_response(response)

            # 2. 死循环检测
            if self._detect_infinite_loop(assistant_msg, context):
                logger.warning("Detected infinite tool loop, breaking.")
                break

            context.add_message(assistant_msg)
            context.metadata["last_response"] = response

            # 3. 终止条件检查
            # 条件 A: 是结构化提取 (Implicit Tool Call)
            if response_model and self._is_structure_extraction(
                assistant_msg, response_model
            ):
                await self._trigger_turn_end(context)
                break

            # 条件 B: 无工具调用 (纯文本回复)
            if not assistant_msg.tool_calls:
                await self._trigger_turn_end(context)
                break

            # 4. 执行工具
            if self.stats:
                self.stats.tool_calls += len(assistant_msg.tool_calls)

            await self._execute_tool_calls(assistant_msg.tool_calls, context)

            # Hook: Turn End
            await self._trigger_turn_end(context)

        # 循环结束，返回最后结果
        last_msg = context.get_last_message()
        if not last_msg:
            return AgentOutput(content="No response generated.")

        return AgentOutput(
            content=last_msg.content or "",
            raw=context.metadata.get("last_response"),
            tool_calls=last_msg.tool_calls or [],
        )

    async def _run_streaming_loop(
        self, context: ExecutionContext, llm_params: Dict[str, Any]
    ) -> AsyncIterator[str]:
        """
        ReAct 流式循环 (递归模式)
        
        负责：消费 Stream Chunks -> 累积工具调用 -> 执行工具 -> 递归调用自身
        """
        if context.turn >= self.max_turns:
            return

        context.turn += 1
        
        # Hook: Turn Start
        if self.on_turn_start:
            await ensure_awaitable(self.on_turn_start, context)

        messages_payload = [m.to_openai_format() for m in context.messages]
        
        # 状态累积器
        collected_content = []
        tool_calls_data: List[Dict[str, Any]] = []

        # 1. 消费流
        async for chunk in self.model.astream(messages=messages_payload, **llm_params):
            delta = self._extract_delta(chunk)

            # A. 文本内容：实时 Yield
            content = delta.get("content")
            if content:
                collected_content.append(content)
                yield content

            # B. 工具调用：后台累积
            if delta.get("tool_calls"):
                self._accumulate_tool_chunks(tool_calls_data, delta["tool_calls"])

        # 2. 组装完整消息
        final_text = "".join(collected_content)
        assistant_msg = Message.assistant(content=final_text)
        
        # 清洗无效的工具调用
        if tool_calls_data:
            valid_calls = [
                tc for tc in tool_calls_data 
                if tc["function"]["name"] or tc["function"]["arguments"]
            ]
            if valid_calls:
                assistant_msg.tool_calls = valid_calls

        # 3. 死循环检测
        if self._detect_infinite_loop(assistant_msg, context):
            logger.warning("Infinite loop detected in stream, stopping.")
            context.add_message(assistant_msg)
            return

        context.add_message(assistant_msg)

        # 4. 分支处理
        if assistant_msg.tool_calls:
            # Case A: 有工具调用 -> 执行并递归
            if self.stats:
                self.stats.tool_calls += len(assistant_msg.tool_calls)

            await self._execute_tool_calls(assistant_msg.tool_calls, context)
            await self._trigger_turn_end(context)
            
            # 递归：进入下一轮
            async for token in self._run_streaming_loop(context, llm_params):
                yield token
        else:
            # Case B: 结束
            await self._trigger_turn_end(context)

    # ===================== 辅助逻辑 =====================
    
    def _normalize_tool_call(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:
        """
        [New Helper] 规范化工具调用数据结构 (Adapter Pattern)
        
        目标：将 LLM 的原始输出统一转换为 ToolBox.execute_many 所需的扁平格式。
        兼容：
        1. OpenAI 嵌套格式: {"function": {"name": "...", "arguments": "..."}}
        2. 扁平格式: {"name": "...", "arguments": {...}}
        3. 参数类型: JSON String 或 Dict
        """
        # 1. 提取 Name 和 Arguments
        func_block = tool_call.get("function")
        
        # 优先尝试 OpenAI 嵌套结构
        if func_block and isinstance(func_block, dict):
            name = func_block.get("name")
            raw_args = func_block.get("arguments", "{}")
        else:
            # 降级尝试扁平结构
            name = tool_call.get("name")
            raw_args = tool_call.get("arguments", "{}")

        # 2. 安全解析参数 (JSON String -> Dict)
        if isinstance(raw_args, str):
            try:
                # 处理常见的 JSON 格式问题 (如包含换行符)
                parsed_args = json.loads(raw_args)
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse tool arguments for '{name}': {raw_args}")
                # 解析失败返回空字典，让 ToolBox 抛出参数校验错误，而不是在这里崩溃
                parsed_args = {} 
        else:
            parsed_args = raw_args if isinstance(raw_args, dict) else {}

        # 3. 返回标准扁平格式
        return {
            "id": tool_call.get("id", ""), # ID 丢失也允许执行
            "name": name,
            "arguments": parsed_args,
        }

    async def _execute_tool_calls(
        self, tool_calls: List[Dict[str, Any]], context: ExecutionContext
    ):
        """
        批量执行工具
        
        改进：使用 _normalize_tool_call 解耦数据清洗逻辑
        """
        # 1. 数据规范化 (Normalization)
        flat_tool_calls = [self._normalize_tool_call(tc) for tc in tool_calls]

        # 2. 批量执行 (Execution)
        results = await self.toolbox.execute_many(flat_tool_calls)

        # 3. 处理结果与副作用 (Side Effects)
        for res in results:
            if self.on_tool_execute:
                await ensure_awaitable(self.on_tool_execute, res.tool_name, {})

            content = res.result
            # 观测值截断
            if len(content) > self.max_observation_length:
                logger.warning(
                    "Truncating tool output",
                    tool=res.tool_name,
                    original_len=len(content),
                    limit=self.max_observation_length,
                )
                content = (
                    content[: self.max_observation_length]
                    + f"\n...(truncated, total {len(content)} chars)"
                )

            tool_msg = Message.tool_result(
                tool_call_id=res.call_id,
                content=content,
                tool_name=res.tool_name,
            )
            context.add_message(tool_msg)

            # 错误反馈策略
            if res.is_error:
                context.consecutive_tool_error_count += 1
                if context.consecutive_tool_error_count >= 3:
                    context.add_message(
                        Message.user(
                            "System: Too many tool errors. Please stop using this tool or change parameters."
                        )
                    )
            else:
                context.consecutive_tool_error_count = 0

    
    def _detect_infinite_loop(
        self, message: Message, context: ExecutionContext
    ) -> bool:
        """检测是否连续以相同参数调用同一工具"""
        if not message.tool_calls:
            return False

        try:
            # 计算工具调用的指纹 (Name + Args)
            calls_dump = json.dumps(
                [
                    {
                        "name": tc["function"]["name"],
                        "args": tc["function"]["arguments"],
                    }
                    for tc in message.tool_calls
                ],
                sort_keys=True,
            )
            current_hash = hash(calls_dump)

            if context.last_tool_calls_hash == current_hash:
                logger.warning("Infinite tool loop detected", calls=calls_dump)
                return True

            context.last_tool_calls_hash = current_hash
            return False
        except Exception:
            # 如果 JSON 解析失败或其他错误，保守放行
            return False

    async def _trigger_turn_end(self, context: ExecutionContext):
        """触发 Turn End Hook"""
        if self.on_turn_end:
            await ensure_awaitable(self.on_turn_end, context)

    def _accumulate_tool_chunks(self, target_list: List[Dict], chunks: List[Dict]):
        """
        合并流式工具调用片段 (OpenAI Protocol)
        """
        for tc_chunk in chunks:
            index = tc_chunk.get("index")
            
            # 确保列表长度足够
            if index is not None:
                while len(target_list) <= index:
                    target_list.append(
                        {
                            "id": "",
                            "type": "function",
                            "function": {"name": "", "arguments": ""},
                        }
                    )
            
            # 获取目标引用
            target = (
                target_list[index]
                if index is not None and index < len(target_list)
                else (target_list[-1] if target_list else None)
            )

            if target:
                if tc_chunk.get("id"):
                    target["id"] += tc_chunk["id"]
                
                func_chunk = tc_chunk.get("function", {})
                if func_chunk.get("name"):
                    target["function"]["name"] += func_chunk["name"]
                if func_chunk.get("arguments"):
                    target["function"]["arguments"] += func_chunk["arguments"]

    # ----------------- 上下文构建 -----------------

    async def _build_execution_context(
        self, input_messages: List[Message]
    ) -> ExecutionContext:
        """加载历史并构建包含 System Prompt 的上下文"""
        history = await self._load_history()

        system_msg = None
        # 检查是否已存在 System Prompt
        has_system = any(m.role == "system" for m in input_messages) or any(
            m.role == "system" for m in history
        )

        if not has_system:
            # 动态渲染 System Prompt
            template_vars = {
                "tools": self.toolbox.to_openai_schema(),
                "current_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }
            system_content = self.prompt_template.format_safe(**template_vars)
            system_msg = Message.system(system_content)

        all_messages = []
        if system_msg:
            all_messages.append(system_msg)
        all_messages.extend(history)
        all_messages.extend(input_messages)

        return ExecutionContext(all_messages)

    async def _load_history(self) -> List[Message]:
        """从 Memory 加载历史记录"""
        if not self.memory.storage:
            return []
        try:
            data = await self.memory.storage.get(self.memory.session_id)
            if data and "messages" in data:
                return await self.memory.get_history(data["messages"])
        except Exception:
            return []
        return []

    async def _save_context(self, context: ExecutionContext):
        """保存上下文到 Memory"""
        if not self.memory.storage:
            return
        try:
            messages_data = [m.to_openai_format() for m in context.messages]
            await self.memory.storage.set(
                self.memory.session_id, {"messages": messages_data}
            )
        except Exception as e:
            logger.warning("Failed to save context", error=str(e))

    # ----------------- LLM 交互 -----------------

    def _build_llm_params(
        self, response_model: Optional[Type[T]], strategy: str
    ) -> Dict[str, Any]:
        """构建传递给 LLM 的参数"""
        params: Dict[str, Any] = {}
        tools_schema = self.toolbox.to_openai_schema()

        if tools_schema and self._supports_functions:
            params["tools"] = tools_schema
            params["tool_choice"] = "auto"

        if response_model:
            if strategy in {"auto", "function_calling"} and self._supports_functions:
                structure_tool = StructureEngine.to_openai_tool(response_model)
                existing_names = {
                    t["function"]["name"] for t in params.get("tools", [])
                }
                if structure_tool["function"]["name"] not in existing_names:
                    params.setdefault("tools", []).append(structure_tool)
                params["tool_choice"] = "auto"
            else:
                params["response_format"] = {"type": "json_object"}

        return params

    def _augment_messages_for_structure(
        self, messages: List[Message], response_model: Optional[Type[T]]
    ) -> List[Message]:
        """注入结构化输出引导指令"""
        if not response_model or not self._supports_functions:
            return messages

        structure_tool_name = StructureEngine.to_openai_tool(response_model)[
            "function"
        ]["name"]
        instruction = (
            f"\nIMPORTANT: You MUST call the '{structure_tool_name}' function to provide your final answer. "
            "Do not reply with text only."
        )

        augmented = messages.copy()
        # 尝试追加到最后一条 User 消息
        for i in range(len(augmented) - 1, -1, -1):
            if augmented[i].role == "user":
                original = augmented[i]
                new_content = str(original.content) + instruction
                augmented[i] = Message(
                    role="user", content=new_content, name=original.name
                )
                return augmented

        # 否则新增一条 User 消息
        augmented.append(Message.user(instruction))
        return augmented

    async def _call_llm(self, context: ExecutionContext, params: Dict[str, Any]) -> Any:
        """调用 LLM API"""
        messages_payload = [m.to_openai_format() for m in context.messages]
        try:
            return await ensure_awaitable(
                self.model.acompletion, messages=messages_payload, **params
            )
        except Exception as e:
            raise ModelError(f"LLM API call failed: {e}") from e

    def _parse_llm_response(self, response: Any) -> Message:
        """解析 LLM 响应为 Message 对象"""
        if hasattr(response, "choices") and response.choices:
            choice = response.choices[0]
            message_data = choice.message
            if hasattr(message_data, "model_dump"):
                message_data = message_data.model_dump()
            elif hasattr(message_data, "to_dict"):
                message_data = message_data.to_dict()
            elif not isinstance(message_data, dict):
                message_data = {
                    "role": getattr(message_data, "role", "assistant"),
                    "content": getattr(message_data, "content", ""),
                    "tool_calls": getattr(message_data, "tool_calls", None),
                }

            if "tool_calls" in message_data and message_data["tool_calls"] is None:
                del message_data["tool_calls"]

            return Message(**message_data)

        raise ModelError("Invalid LLM response format")

    def _extract_delta(self, chunk: Any) -> Dict[str, Any]:
        """从 Stream Chunk 中提取 delta"""
        if hasattr(chunk, "choices") and chunk.choices:
            choice = chunk.choices[0]
            if isinstance(choice, dict):
                return choice.get("delta", {})
            return getattr(choice, "delta", {})
        return {}

    def _is_structure_extraction(self, message: Message, model_class: Type[T]) -> bool:
        """判断当前是否为结构化提取的工具调用"""
        if not message.tool_calls or not self._supports_functions:
            return False
        extraction_tool_name = StructureEngine.to_openai_tool(model_class)["function"][
            "name"
        ]
        return any(
            tc.get("function", {}).get("name") == extraction_tool_name
            for tc in message.tool_calls
        )

    async def _handle_structured_output(
        self,
        output: AgentOutput,
        response_model: Type[T],
        context: ExecutionContext,
        llm_params: Dict[str, Any],
        max_retries: int,
    ) -> T:
        """处理结构化输出解析与自动重试"""
        for attempt in range(max_retries + 1):
            try:
                return await StructureEngine.parse(
                    content=output.content,
                    model_class=response_model,
                    raw_tool_calls=output.tool_calls,
                    auto_fix=True,
                )
            except StructureParseError as e:
                if attempt >= max_retries:
                    raise AgentError(f"Failed to parse structured output: {e}")

                # 重试反馈
                feedback_msg = Message.user(
                    f"Error parsing response: {e}. Please ensure strict JSON format."
                )
                context.add_message(feedback_msg)
                response = await self._call_llm(context, llm_params)
                msg = self._parse_llm_response(response)
                context.add_message(msg)
                output = AgentOutput(
                    content=msg.content or "", tool_calls=msg.tool_calls or []
                )

        raise AgentError("Structured parsing failed")
```

[12] gecko/core/events.py
```python
# gecko/core/events.py
"""
事件总线

提供异步事件发布/订阅机制，支持中间件和后台任务管理。

核心功能：
1. 强类型事件（基于 Pydantic）
2. 异步/同步订阅者支持
3. 中间件拦截与处理
4. 健壮的后台任务管理（任务追踪与优雅关闭）

优化日志：
1. 增加后台任务追踪集合，防止任务被 GC
2. 增加 shutdown 方法等待后台任务完成
3. 增加上下文管理器支持
4. 优化中间件错误处理
5. 修复对异步可调用对象（非函数）的支持
"""

from __future__ import annotations

import asyncio
import inspect
import time
from typing import Any, Awaitable, Callable, Dict, List, Optional, Set, Type, Union

from pydantic import BaseModel, Field

from gecko.core.logging import get_logger

logger = get_logger(__name__)


# ===== 事件模型 =====

class BaseEvent(BaseModel):
    """事件基类"""
    type: str
    timestamp: float = Field(default_factory=time.time)
    data: Dict[str, Any] = Field(default_factory=dict)
    error: Optional[str] = None
    
    model_config = {"arbitrary_types_allowed": True}


# EventHandler 可以是返回 None 的同步函数，或者返回 Awaitable 的函数
EventHandler = Callable[[BaseEvent], Union[Awaitable[None], None, Any]]
Middleware = Callable[[BaseEvent], Awaitable[Optional[BaseEvent]]]


class EventBus:
    """
    异步事件总线
    """
    
    def __init__(self):
        self._subscribers: Dict[str, List[EventHandler]] = {}
        self._middlewares: List[Middleware] = []
        self._background_tasks: Set[asyncio.Task] = set()
        self._running = True

    # --- 订阅管理 ---
    
    def subscribe(self, event_type: str, handler: EventHandler) -> "EventBus":
        """
        订阅事件
        支持通配符 "*" 订阅所有事件
        """
        if not callable(handler):
            raise TypeError(f"Event handler must be callable, got {type(handler)}")
        
        self._subscribers.setdefault(event_type, []).append(handler)
        logger.debug("Handler subscribed", event_type=event_type, handler=handler)
        return self

    def unsubscribe(self, event_type: str, handler: EventHandler) -> "EventBus":
        """取消订阅"""
        handlers = self._subscribers.get(event_type, [])
        if handler in handlers:
            handlers.remove(handler)
            logger.debug("Handler unsubscribed", event_type=event_type, handler=handler)
        return self

    def add_middleware(self, middleware: Middleware) -> "EventBus":
        """
        添加中间件
        """
        self._middlewares.append(middleware)
        return self

    # --- 发布事件 ---
    async def publish(self, event: BaseEvent, wait: bool = False):
        """
        发布事件
        
        参数:
            event: 事件对象
            wait: 是否等待所有处理器执行完毕
        """
        if not self._running:
            logger.warning("EventBus is shutting down, event ignored", event_type=event.type)
            return

        # 保存原始事件类型用于日志（因为中间件可能返回 None）
        original_type = event.type

        # 1. 执行中间件
        try:
            for mw in self._middlewares:
                event = await mw(event)
                if event is None:
                    logger.debug("Event blocked by middleware", event_type=original_type)
                    return
        except Exception as e:
            logger.error("Middleware error", error=str(e), event_type=original_type)
            return

        # 2. 获取订阅者
        handlers = self._subscribers.get(event.type, []) + self._subscribers.get("*", [])
        if not handlers:
            return

        # 3. 执行处理（去重）
        unique_handlers = list(dict.fromkeys(handlers))
        
        # 创建执行协程
        tasks = [self._execute_handler(h, event) for h in unique_handlers]

        if wait:
            await asyncio.gather(*tasks, return_exceptions=True)
        else:
            for coro in tasks:
                task = asyncio.create_task(coro)
                self._background_tasks.add(task)
                task.add_done_callback(self._background_tasks.discard)
    
    async def _execute_handler(self, handler: EventHandler, event: BaseEvent):
        """
        执行单个处理器（包含错误捕获）
        
        采用统一的调用方式：先调用，再判断返回值是否为 Awaitable。
        这兼容了 async def, def, 以及 async __call__ 对象。
        """
        try:
            result = handler(event)
            if inspect.isawaitable(result):
                await result
        except Exception as e:
            logger.exception(
                "Event handler failed", 
                event_type=event.type, 
                handler=getattr(handler, "__name__", str(handler)),
                error=str(e)
            )

    # --- 生命周期 ---

    async def shutdown(self, wait: bool = True):
        """
        关闭事件总线
        """
        self._running = False
        
        if wait and self._background_tasks:
            count = len(self._background_tasks)
            if count > 0:
                logger.info("Waiting for background tasks to finish", count=count)
                await asyncio.gather(*self._background_tasks, return_exceptions=True)
        
        self._background_tasks.clear()
        logger.info("EventBus shutdown completed")

    async def __aenter__(self):
        self._running = True
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.shutdown()


# ==== 常用事件类型 ====

class AgentRunEvent(BaseEvent):
    """Agent 运行过程事件"""
    pass


class WorkflowEvent(BaseEvent):
    """Workflow 运行过程事件"""
    pass
```

[13] gecko/core/exceptions.py
```python
# gecko/core/exceptions.py
"""
Gecko 异常体系（改进版）

改进：移除装饰器，提倡显式错误处理
"""
from __future__ import annotations
from typing import Optional, Dict, Any

# ========== 异常基类 ==========

class GeckoError(Exception):
    """
    Gecko 统一异常基类
    
    设计原则：
    1. 包含结构化上下文
    2. 便于日志记录
    3. 支持异常链（from）
    """
    def __init__(
        self,
        message: str,
        *args,
        error_code: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        super().__init__(message, *args, **kwargs)
        self.message = message
        self.error_code = error_code or self.__class__.__name__
        self.context = context or {}
    
    def to_dict(self) -> Dict[str, Any]:
        """转换为字典（便于日志/API 返回）"""
        return {
            "error_type": self.__class__.__name__,
            "error_code": self.error_code,
            "message": self.message,
            "context": self.context,
        }
    
    def __str__(self) -> str:
        if self.context:
            ctx_str = ", ".join(f"{k}={v}" for k, v in self.context.items())
            return f"{self.message} [{ctx_str}]"
        return self.message

# ========== 领域异常 ==========

class AgentError(GeckoError):
    """Agent 执行异常"""
    pass

class ModelError(GeckoError):
    """模型调用异常"""
    pass

class ToolError(GeckoError):
    """工具执行异常"""
    pass

class ToolNotFoundError(ToolError):
    """工具未找到"""
    def __init__(self, tool_name: str):
        super().__init__(
            f"Tool '{tool_name}' not found in registry",
            error_code="TOOL_NOT_FOUND",
            context={"tool_name": tool_name}
        )

class ToolTimeoutError(ToolError):
    """工具超时"""
    def __init__(self, tool_name: str, timeout: float):
        super().__init__(
            f"Tool '{tool_name}' timed out after {timeout}s",
            error_code="TOOL_TIMEOUT",
            context={"tool_name": tool_name, "timeout": timeout}
        )

class WorkflowError(GeckoError):
    """工作流异常"""
    pass

class WorkflowCycleError(WorkflowError):
    """工作流循环依赖"""
    pass

class StorageError(GeckoError):
    """存储异常"""
    pass

class ConfigurationError(GeckoError):
    """配置错误"""
    pass

class ValidationError(GeckoError):
    """验证错误"""
    pass
```

[14] gecko/core/logging.py
```python
# gecko/core/logging.py
"""
Gecko 结构化日志系统（改进版）

改进：使用成熟的 structlog 库，代码减少 80%
"""
from __future__ import annotations
import logging
import sys
from typing import Any, Optional

try:
    import structlog
    STRUCTLOG_AVAILABLE = True
except ImportError:
    STRUCTLOG_AVAILABLE = False
    import warnings
    warnings.warn(
        "structlog not installed. Install with: pip install structlog\n"
        "Falling back to standard logging.",
        ImportWarning
    )

from gecko.config import settings

# ========== 日志初始化 ==========

_initialized = False

def setup_logging(
    level: Optional[str] = None,
    force: bool = False
):
    """
    初始化日志系统
    
    改进：
    1. 优先使用 structlog（如果可用）
    2. 降级到标准 logging（如果 structlog 未安装）
    """
    global _initialized
    
    if _initialized and not force:
        return
    
    level = level or settings.log_level
    log_level = getattr(logging, level.upper(), logging.INFO)
    
    if STRUCTLOG_AVAILABLE:
        _setup_structlog(log_level)
    else:
        _setup_standard_logging(log_level)
    
    # 降低第三方库日志级别
    for lib in ["httpx", "httpcore", "litellm", "openai"]:
        logging.getLogger(lib).setLevel(logging.WARNING)
    
    _initialized = True

def _setup_structlog(level: int):
    """配置 structlog"""
    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars, 
            structlog.processors.add_log_level,
            structlog.processors.StackInfoRenderer(),
            structlog.dev.set_exc_info,
            structlog.processors.TimeStamper(fmt="iso", utc=True),
            # 根据配置选择渲染器
            structlog.processors.JSONRenderer()
            if settings.log_format == "json"
            else structlog.dev.ConsoleRenderer(),
        ],
        wrapper_class=structlog.make_filtering_bound_logger(level),
        context_class=dict,
        logger_factory=structlog.PrintLoggerFactory(file=sys.stdout),
        cache_logger_on_first_use=True,
    )

def _setup_standard_logging(level: int):
    """配置标准 logging（降级方案）"""
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        stream=sys.stdout,
    )

# ========== 获取 Logger ==========

def get_logger(name: str) -> Any:
    """
    获取 Logger 实例
    
    返回：
    - structlog.BoundLogger（如果可用）
    - logging.Logger（降级方案）
    
    使用示例:
        logger = get_logger(__name__)
        logger.info("event happened", user_id=123, action="login")
    """
    if not _initialized:
        setup_logging()
    
    if STRUCTLOG_AVAILABLE:
        return structlog.get_logger(name)
    else:
        return logging.getLogger(name)

# ========== 自动初始化 ==========

setup_logging()
```

[15] gecko/core/memory.py
```python
# gecko/core/memory.py
"""
Token Memory - 上下文记忆管理器

负责管理对话历史的 Token 计数、裁剪与缓存。

核心功能：
1. 精确计数：基于 tiktoken 的模型特定 Token 计算
2. 智能裁剪：基于滑动窗口 (Sliding Window) 的历史记录加载
3. 性能缓存：LRU 缓存 Token 计算结果，减少重复计算开销
4. 多模态支持：估算图片/文件的 Token 占用
5. 完备的工具链：批量计算、统计打印、快速估算

优化日志：
- [Perf] get_history 采用 O(N) 算法 (append + reverse)
- [Perf] 缓存键生成使用 model_dump_json 加速
- [Fix] 补全所有原始方法 (print_cache_stats, batch optimizations)
- [Fix] 修正统计键名以通过单元测试
"""
from __future__ import annotations

import hashlib
import json
from collections import OrderedDict
from typing import Any, Callable, Dict, List, Optional

from gecko.core.logging import get_logger
from gecko.core.message import Message
from gecko.plugins.storage.interfaces import SessionInterface

logger = get_logger(__name__)


class TokenMemory:
    """
    Token 感知的记忆管理器
    
    负责在有限的 Context Window 内最大化保留有效对话历史。
    """

    def __init__(
        self,
        session_id: str,
        storage: Optional[SessionInterface] = None,
        max_tokens: int = 4000,
        model_name: str = "gpt-3.5-turbo",
        cache_size: int = 2000,
        max_message_length: int = 20000,
        enable_cache_for_batch: bool = True,
    ):
        """
        初始化 Memory
        
        参数:
            session_id: 会话唯一标识
            storage: 持久化存储后端
            max_tokens: 最大上下文 Token 限制
            model_name: 模型名称 (用于加载 tokenizer)
            cache_size: Token 计数缓存大小 (LRU)
            max_message_length: 单条消息最大字符数 (防御性截断)
            enable_cache_for_batch: 批量计算时是否启用缓存
        """
        if max_tokens <= 0:
            raise ValueError(f"max_tokens must be positive, got {max_tokens}")
        if cache_size <= 0:
            raise ValueError(f"cache_size must be positive, got {cache_size}")

        self.session_id = session_id
        self.storage = storage
        self.max_tokens = max_tokens
        self.model_name = model_name
        self.cache_size = cache_size  # 公开属性
        self.max_message_length = max_message_length
        self.enable_cache_for_batch = enable_cache_for_batch
        
        # LRU 缓存: Hash(Content) -> TokenCount
        self._token_cache: OrderedDict[str, int] = OrderedDict()
        
        # 缓存统计
        self._cache_hits = 0
        self._cache_misses = 0
        self._cache_evictions = 0
        
        # 延迟加载的 tokenizer
        self._encoding = None
        self._tokenizer_failed = False

    # ====================== Tokenizer ======================

    @property
    def tokenizer(self):
        """延迟加载 tiktoken encoder"""
        if self._encoding:
            return self._encoding
        
        if self._tokenizer_failed:
            return None

        try:
            import tiktoken
            try:
                self._encoding = tiktoken.encoding_for_model(self.model_name)
            except KeyError:
                logger.warning(f"Model {self.model_name} not found in tiktoken, using cl100k_base")
                self._encoding = tiktoken.get_encoding("cl100k_base")
        except ImportError:
            logger.warning("tiktoken not installed. Token counting will be estimated by char length.")
            self._tokenizer_failed = True
            return None
        except Exception as e:
            logger.error(f"Failed to load tokenizer: {e}")
            self._tokenizer_failed = True
            return None
            
        return self._encoding

    # ====================== 单条计数 ======================

    def count_message_tokens(self, message: Message) -> int:
        """
        计算单条消息的 Token 数（带缓存）
        """
        # 1. 生成缓存键
        cache_key = self._make_cache_key(message)
        
        # 2. 检查缓存
        if cache_key in self._token_cache:
            self._token_cache.move_to_end(cache_key)
            self._cache_hits += 1
            return self._token_cache[cache_key]
        
        # 3. 计算
        self._cache_misses += 1
        count = self._count_tokens_impl(message)
        
        # 4. 更新缓存 (LRU)
        self._token_cache[cache_key] = count
        self._token_cache.move_to_end(cache_key)
        
        if len(self._token_cache) > self.cache_size:
            self._token_cache.popitem(last=False)
            self._cache_evictions += 1
            
        return count

    def _make_cache_key(self, message: Message) -> str:
        """
        生成消息的缓存键
        
        优化: 使用 Pydantic model_dump_json (Rust) 加速序列化
        """
        # 快速路径: 普通文本消息直接哈希
        if isinstance(message.content, str) and not message.tool_calls:
            raw = f"{message.role}:{message.name}:{message.content}"
            return hashlib.md5(raw.encode("utf-8")).hexdigest()
            
        # 慢速路径: 多模态或工具调用
        # exclude_none=True 减少数据量，sort_keys=True (默认False) 在 dump_json 中不支持，
        # 但 Pydantic 字段顺序通常是固定的。为了绝对安全，可以用 json.dumps(model_dump)
        # 不过对于缓存键，model_dump_json 通常足够稳定且快。
        try:
            raw_json = message.model_dump_json(
                include={"role", "content", "tool_calls", "name"},
                exclude_none=True
            )
            return hashlib.md5(raw_json.encode("utf-8")).hexdigest()
        except Exception:
            # 降级方案
            data = message.model_dump(include={"role", "content", "tool_calls", "name"})
            return hashlib.md5(json.dumps(data, sort_keys=True, default=str).encode("utf-8")).hexdigest()

    # ====================== 批量计数 ======================

    def count_messages_batch(
        self,
        messages: List[Message],
        use_cache: Optional[bool] = None
    ) -> List[int]:
        """
        批量计算消息的 Token 数
        
        优化: 复用 encoder 对象，减少属性查找开销
        """
        if not messages:
            return []
        
        should_use_cache = (
            use_cache if use_cache is not None else self.enable_cache_for_batch
        )
        
        if should_use_cache:
            return [self.count_message_tokens(msg) for msg in messages]
        else:
            # 性能优化: 提取 encode 方法，避免在循环中重复查找 self.tokenizer
            encode_fn = self.tokenizer.encode if self.tokenizer else None
            return [self._count_tokens_impl(msg, encode=encode_fn) for msg in messages]

    def _count_tokens_impl(
        self,
        message: Message,
        encode: Optional[Callable[[str], List[int]]] = None
    ) -> int:
        """
        实际 Token 计算逻辑
        
        参数:
            message: 消息对象
            encode: 可选的编码函数（性能优化用）
        """
        if not encode:
            if self.tokenizer:
                encode = self.tokenizer.encode
            else:
                # 降级: 字符估算
                return len(message.get_text_content()) // 4 + 2

        num_tokens = 4  # Per-message overhead
        
        # 1. Content Tokens
        if isinstance(message.content, str):
            num_tokens += len(encode(message.content))
        elif isinstance(message.content, list):
            for block in message.content:
                if block.type == "text" and block.text:
                    num_tokens += len(encode(block.text))
                elif block.type == "image_url":
                    num_tokens += self._estimate_image_tokens(block.image_url)

        # 2. Tool Calls Overhead
        if message.tool_calls:
            try:
                # 使用快速序列化
                dump = self._fast_json_dumps(message.tool_calls)
                num_tokens += len(encode(dump))
            except Exception:
                num_tokens += 100

        # 3. Name Overhead
        if message.name:
            num_tokens += 1

        return num_tokens

    # ====================== 历史加载 ======================

    async def get_history(
        self,
        raw_messages: List[Dict[str, Any]],
        preserve_system: bool = True
    ) -> List[Message]:
        """
        加载并裁剪历史消息 (O(N) 复杂度优化版)
        """
        if not raw_messages:
            logger.debug("No history messages to load")
            return []

        # 1. 解析消息 (单次遍历)
        parsed_messages: List[Message] = []
        for i, raw in enumerate(raw_messages):
            try:
                msg = Message(**raw)
                self._truncate_message_safety(msg)
                parsed_messages.append(msg)
            except Exception as e:
                logger.warning("Skipping invalid message history", index=i, error=str(e))

        if not parsed_messages:
            return []

        # 2. 分离 System Message
        system_msg: Optional[Message] = None
        candidates = parsed_messages
        
        if preserve_system and parsed_messages[0].role == "system":
            system_msg = parsed_messages[0]
            candidates = parsed_messages[1:]

        # 3. 计算 System Token 开销
        current_tokens = 0
        if system_msg:
            sys_tokens = self.count_message_tokens(system_msg)
            if sys_tokens > self.max_tokens:
                logger.warning("System prompt exceeds max_tokens, force truncating")
                self._truncate_to_fit(system_msg, self.max_tokens)
                sys_tokens = self.count_message_tokens(system_msg)
            current_tokens += sys_tokens

        # 4. 反向回填 (Reverse Accumulation - O(N))
        selected_reverse: List[Message] = []
        
        for msg in reversed(candidates):
            tokens = self.count_message_tokens(msg)
            
            if current_tokens + tokens > self.max_tokens:
                logger.debug(
                    "Context limit reached", 
                    current=current_tokens, 
                    limit=self.max_tokens
                )
                break
            
            selected_reverse.append(msg)
            current_tokens += tokens

        # 5. 重组列表
        result = []
        if system_msg:
            result.append(system_msg)
        
        # 再次反转回时间正序
        result.extend(reversed(selected_reverse))
        
        return result

    # ====================== 辅助方法 ======================

    def clear_cache(self):
        """清空计数缓存"""
        cleared_size = len(self._token_cache)
        self._token_cache.clear()
        self._cache_hits = 0
        self._cache_misses = 0
        self._cache_evictions = 0
        logger.info("Token cache cleared", cleared_entries=cleared_size)

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        获取缓存统计信息
        
        [Fix] 键名修正为 'cache_size' 以符合测试预期
        """
        total_requests = self._cache_hits + self._cache_misses
        hit_rate = (self._cache_hits / total_requests) if total_requests > 0 else 0.0
        
        return {
            "cache_size": len(self._token_cache),
            "max_cache_size": self.cache_size,
            "cache_utilization": len(self._token_cache) / self.cache_size,
            "hits": self._cache_hits,
            "misses": self._cache_misses,
            "evictions": self._cache_evictions,
            "total_requests": total_requests,
            "hit_rate": hit_rate,
        }

    def print_cache_stats(self):
        """打印缓存统计信息（格式化输出）"""
        stats = self.get_cache_stats()
        
        print("\n" + "=" * 60)
        print("Token Cache Statistics".center(60))
        print("=" * 60)
        print(f"Cache Size:        {stats['cache_size']} / {stats['max_cache_size']}")
        print(f"Cache Utilization: {stats['cache_utilization']:.1%}")
        print(f"Total Requests:    {stats['total_requests']}")
        print(f"Cache Hits:        {stats['hits']}")
        print(f"Cache Misses:      {stats['misses']}")
        print(f"Cache Evictions:   {stats['evictions']}")
        print(f"Hit Rate:          {stats['hit_rate']:.1%}")
        print("=" * 60 + "\n")

    def estimate_tokens(self, text: str) -> int:
        """快速估算文本的 token 数（不使用缓存）"""
        if not text:
            return 0
        if self.tokenizer:
            return len(self.tokenizer.encode(text))
        return len(text) // 4

    # ====================== 内部工具 ======================

    def _fast_json_dumps(self, obj: Any) -> str:
        """快速 JSON 序列化"""
        try:
            import orjson
            return orjson.dumps(obj).decode("utf-8")
        except ImportError:
            import json
            return json.dumps(obj)

    def _estimate_image_tokens(self, image_resource: Any) -> int:
        """估算图片 Token"""
        if not image_resource:
            return 0
        detail = getattr(image_resource, "detail", "auto")
        if detail == "low":
            return 85
        return 1000 

    def _truncate_message_safety(self, message: Message):
        """防御性截断"""
        if isinstance(message.content, str):
            if len(message.content) > self.max_message_length:
                message.content = message.content[:self.max_message_length]

    def _truncate_to_fit(self, message: Message, limit_tokens: int):
        """强行截断文本"""
        if not isinstance(message.content, str):
            return
        char_limit = int(limit_tokens * 3.5)
        if len(message.content) > char_limit:
            message.content = message.content[:char_limit] + "...(truncated)"

    def __repr__(self) -> str:
        return (
            f"TokenMemory("
            f"session_id='{self.session_id}', "
            f"max_tokens={self.max_tokens}, "
            f"model='{self.model_name}', "
            f"cache_size={len(self._token_cache)}/{self.cache_size}"
            f")"
        )
```

[16] gecko/core/message.py
```python
# gecko/core/message.py
"""
Message 模型 - 对话消息的标准表示

核心功能：
1. 统一的消息格式（兼容 OpenAI API）
2. 多模态内容支持（文本 + 图片）
3. 工具调用支持
4. 类型安全的工厂方法

优化点：
1. 异步文件加载
2. 更好的文件大小检查
3. 消息验证
4. 边缘情况处理
5. 工具方法扩展
"""
from __future__ import annotations

import asyncio
import base64
import mimetypes
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional, Union

from pydantic import BaseModel, Field, field_serializer, field_validator, model_validator

from gecko.core.logging import get_logger

logger = get_logger(__name__)

# ===== 类型定义 =====

Role = Literal["system", "user", "assistant", "tool"]


# ===== 媒体资源 =====

class MediaResource(BaseModel):
    """
    媒体资源（主要用于图片）
    
    支持：
    - URL（http/https）
    - Base64 编码的数据
    - 本地文件路径（通过工厂方法）
    
    示例:
        ```python
        # 从 URL
        img = MediaResource(url="https://example.com/image.jpg")
        
        # 从本地文件（同步）
        img = MediaResource.from_file("./image.png")
        
        # 从本地文件（异步）
        img = await MediaResource.from_file_async("./large_image.png")
        
        # 从 base64
        img = MediaResource(
            base64_data="iVBORw0KG...",
            mime_type="image/png"
        )
        ```
    """
    url: Optional[str] = None
    base64_data: Optional[str] = None
    mime_type: Optional[str] = None
    detail: Literal["auto", "low", "high"] = "auto"

    @model_validator(mode="after")
    def validate_source(self):
        """验证至少提供了一个数据源"""
        if not self.url and not self.base64_data:
            raise ValueError("必须提供 url 或 base64_data")
        return self

    @classmethod
    def from_file(
        cls,
        path: str,
        mime_type: Optional[str] = None,
        max_size_mb: int = 5,
        detail: Literal["auto", "low", "high"] = "auto"
    ) -> MediaResource:
        """
        从本地文件加载（同步版本）
        
        参数:
            path: 文件路径
            mime_type: MIME 类型（None 则自动推断）
            max_size_mb: 最大文件大小（MB）
            detail: 图片质量（OpenAI API 参数）
        
        返回:
            MediaResource 实例
        
        异常:
            FileNotFoundError: 文件不存在
            ValueError: 文件过大
        
        注意:
            这是同步方法，会阻塞事件循环。
            对于大文件，建议使用 from_file_async()
        """
        p = Path(path)
        
        # 检查文件是否存在
        if not p.exists():
            raise FileNotFoundError(f"文件不存在: {path}")
        
        if not p.is_file():
            raise ValueError(f"路径不是文件: {path}")
        
        # ✅ 优化：先检查文件大小，再读取
        file_size = p.stat().st_size
        max_size_bytes = max_size_mb * 1024 * 1024
        
        if file_size > max_size_bytes:
            raise ValueError(
                f"文件过大: {file_size / 1024 / 1024:.2f} MB "
                f"(最大 {max_size_mb} MB)"
            )
        
        # 读取并编码
        try:
            with open(p, "rb") as f:
                encoded = base64.b64encode(f.read()).decode("utf-8")
        except Exception as e:
            raise IOError(f"文件读取失败: {e}") from e
        
        # 推断 MIME 类型
        mime = mime_type or mimetypes.guess_type(p.name)[0] or "application/octet-stream"
        
        logger.debug(
            "Media loaded from file",
            path=path,
            size_kb=file_size / 1024,
            mime_type=mime
        )
        
        return cls(
            base64_data=encoded,
            mime_type=mime,
            detail=detail
        )

    @classmethod
    async def from_file_async(
        cls,
        path: str,
        mime_type: Optional[str] = None,
        max_size_mb: int = 5,
        detail: Literal["auto", "low", "high"] = "auto"
    ) -> MediaResource:
        """
        从本地文件加载（异步版本）
        
        对于大文件，使用此方法避免阻塞事件循环
        
        参数:
            同 from_file()
        
        返回:
            MediaResource 实例
        """
        p = Path(path)
        
        # 检查文件
        if not p.exists():
            raise FileNotFoundError(f"文件不存在: {path}")
        
        if not p.is_file():
            raise ValueError(f"路径不是文件: {path}")
        
        # 检查大小
        file_size = p.stat().st_size
        max_size_bytes = max_size_mb * 1024 * 1024
        
        if file_size > max_size_bytes:
            raise ValueError(
                f"文件过大: {file_size / 1024 / 1024:.2f} MB "
                f"(最大 {max_size_mb} MB)"
            )
        
        # ✅ 异步读取文件（在线程池中执行）
        def _read_file():
            with open(p, "rb") as f:
                return base64.b64encode(f.read()).decode("utf-8")
        
        try:
            encoded = await asyncio.to_thread(_read_file)
        except Exception as e:
            raise IOError(f"文件读取失败: {e}") from e
        
        # 推断 MIME 类型
        mime = mime_type or mimetypes.guess_type(p.name)[0] or "application/octet-stream"
        
        logger.debug(
            "Media loaded from file (async)",
            path=path,
            size_kb=file_size / 1024,
            mime_type=mime
        )
        
        return cls(
            base64_data=encoded,
            mime_type=mime,
            detail=detail
        )

    def to_openai_image_url(self) -> Dict[str, Any]:
        """
        转换为 OpenAI API 所需的 image_url 格式
        
        返回:
            符合 OpenAI 规范的字典
        """
        # 构建 URL
        if self.url:
            url_value = self.url
        elif self.base64_data:
            mime = self.mime_type or "image/jpeg"
            url_value = f"data:{mime};base64,{self.base64_data}"
        else:
            raise ValueError("MediaResource 缺少 URL 或 base64_data")
        
        return {
            "url": url_value,
            "detail": self.detail
        }

    def get_size_estimate(self) -> int:
        """
        估算数据大小（字节）
        
        返回:
            估算的字节数
        """
        if self.base64_data:
            # Base64 编码后的大小约为原始大小的 4/3
            return int(len(self.base64_data) * 3 / 4)
        elif self.url:
            # URL 无法估算实际大小
            return 0
        return 0


# ===== 内容块 =====

class ContentBlock(BaseModel):
    """
    消息内容块（用于多模态消息）
    
    支持：
    - 文本块
    - 图片块
    
    示例:
        ```python
        # 文本块
        text = ContentBlock(type="text", text="Hello")
        
        # 图片块
        image = ContentBlock(
            type="image_url",
            image_url=MediaResource(url="https://...")
        )
        ```
    """
    type: Literal["text", "image_url"]
    text: Optional[str] = None
    image_url: Optional[MediaResource] = None

    @model_validator(mode="after")
    def ensure_valid(self):
        """验证块的完整性"""
        if self.type == "text":
            if self.text is None:
                raise ValueError("文本块缺少 text 字段")
        elif self.type == "image_url":
            if self.image_url is None:
                raise ValueError("图片块缺少 image_url 字段")
        return self

    def to_openai_format(self) -> Dict[str, Any]:
        """转换为 OpenAI API 格式"""
        if self.type == "text":
            return {"type": "text", "text": self.text}
        elif self.type == "image_url":
            return {
                "type": "image_url",
                "image_url": self.image_url.to_openai_image_url()
            }
        else:
            raise ValueError(f"未知的内容类型: {self.type}")

    def get_text_content(self) -> str:
        """
        提取文本内容（用于调试/日志）
        
        返回:
            文本内容或占位符
        """
        if self.type == "text":
            return self.text or ""
        elif self.type == "image_url":
            return "[image]"
        return ""


# ===== 消息 =====

class Message(BaseModel):
    """
    标准消息对象
    
    兼容 OpenAI Chat Completion API 格式
    
    示例:
        ```python
        # 简单文本消息
        msg = Message.user("Hello!")
        
        # 多模态消息
        msg = Message.user(
            text="What's in this image?",
            images=["./photo.jpg"]
        )
        
        # 助手消息
        msg = Message.assistant("I'm here to help!")
        
        # 工具返回消息
        msg = Message.tool_result(
            tool_call_id="call_123",
            content="Search results: ...",
            tool_name="search"
        )
        
        # 从 OpenAI 格式解析
        msg = Message.from_openai({
            "role": "user",
            "content": "Hello"
        })
        ```
    """
    role: Role
    content: Union[str, List[ContentBlock]] = Field(default="")
    name: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    tool_call_id: Optional[str] = None

    @field_validator("content", mode="before")
    @classmethod
    def validate_content(cls, v):
        """验证并规范化 content"""
        if v is None:
            return ""
        return v

    @field_serializer("content")
    def serialize_content(self, content: Union[str, List[ContentBlock]], _info):
        """序列化 content 为 OpenAI 格式"""
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            return [block.to_openai_format() for block in content]
        return str(content)

    # ===== 工厂方法 =====

    @classmethod
    def user(
        cls,
        text: str = "",
        images: Optional[List[str]] = None,
        name: Optional[str] = None
    ) -> Message:
        """
        创建用户消息
        
        参数:
            text: 文本内容
            images: 图片路径列表（URL 或本地文件）
            name: 用户名称（可选）
        
        返回:
            Message 实例
        
        示例:
            ```python
            # 纯文本
            msg = Message.user("Hello")
            
            # 文本 + 图片
            msg = Message.user(
                text="What's this?",
                images=["./photo.jpg", "https://example.com/img.png"]
            )
            ```
        """
        if not images:
            return cls(role="user", content=text, name=name)
        
        # 构建多模态内容
        blocks: List[ContentBlock] = []
        
        # 添加文本块
        if text:
            blocks.append(ContentBlock(type="text", text=text))
        
        # 添加图片块
        for img in images:
            try:
                # 判断是 URL 还是本地路径
                if img.startswith(("http://", "https://", "data:")):
                    resource = MediaResource(url=img)
                else:
                    resource = MediaResource.from_file(img)
                
                blocks.append(ContentBlock(type="image_url", image_url=resource))
            except Exception as e:
                logger.error("Failed to load image", path=img, error=str(e))
                # 继续处理其他图片
        
        return cls(role="user", content=blocks, name=name)

    @classmethod
    async def user_async(
        cls,
        text: str = "",
        images: Optional[List[str]] = None,
        name: Optional[str] = None
    ) -> Message:
        """
        创建用户消息（异步版本）
        
        对于大量或大文件图片，使用此方法避免阻塞
        """
        if not images:
            return cls(role="user", content=text, name=name)
        
        blocks: List[ContentBlock] = []
        
        if text:
            blocks.append(ContentBlock(type="text", text=text))
        
        # 异步加载所有图片
        async def _load_image(img: str) -> Optional[ContentBlock]:
            try:
                if img.startswith(("http://", "https://", "data:")):
                    resource = MediaResource(url=img)
                else:
                    resource = await MediaResource.from_file_async(img)
                return ContentBlock(type="image_url", image_url=resource)
            except Exception as e:
                logger.error("Failed to load image (async)", path=img, error=str(e))
                return None
        
        # 并发加载所有图片
        image_blocks = await asyncio.gather(*[_load_image(img) for img in images])
        blocks.extend([b for b in image_blocks if b is not None])
        
        return cls(role="user", content=blocks, name=name)

    @classmethod
    def assistant(cls, content: str, name: Optional[str] = None) -> Message:
        """
        创建助手消息
        
        参数:
            content: 回复内容
            name: 助手名称（可选）
        """
        return cls(role="assistant", content=content, name=name)

    @classmethod
    def system(cls, content: str) -> Message:
        """
        创建系统消息
        
        参数:
            content: 系统提示词
        """
        return cls(role="system", content=content)

    @classmethod
    def tool_result(
        cls,
        tool_call_id: str,
        content: Any,
        tool_name: str
    ) -> Message:
        """
        创建工具返回消息
        
        参数:
            tool_call_id: 工具调用 ID
            content: 工具返回结果（任意类型，会自动序列化）
            tool_name: 工具名称
        
        返回:
            Message 实例
        """
        # 序列化 content
        if isinstance(content, str):
            serialized = content
        elif isinstance(content, (dict, list)):
            import json
            serialized = json.dumps(content, ensure_ascii=False, indent=2)
        else:
            serialized = str(content)
        
        return cls(
            role="tool",
            content=serialized,
            tool_call_id=tool_call_id,
            name=tool_name
        )

    @classmethod
    def from_openai(cls, payload: Dict[str, Any]) -> Message:
        """
        从 OpenAI API 格式解析消息
        
        参数:
            payload: OpenAI 格式的消息字典
        
        返回:
            Message 实例
        
        示例:
            ```python
            openai_msg = {
                "role": "assistant",
                "content": "Hello!",
                "tool_calls": [...]
            }
            msg = Message.from_openai(openai_msg)
            ```
        """
        try:
            return cls(**payload)
        except Exception as e:
            logger.error("Failed to parse OpenAI message", error=str(e), payload=payload)
            raise ValueError(f"无效的 OpenAI 消息格式: {e}") from e

    # ===== 转换方法 =====

    def to_openai_format(self) -> Dict[str, Any]:
        """
        转换为 OpenAI API 格式
        
        返回:
            符合 OpenAI 规范的字典
        """
        # 使用 Pydantic 的序列化（会调用 field_serializer）
        data = self.model_dump(exclude_none=True, mode="json")
        
        # 确保必要字段存在
        if "role" not in data:
            raise ValueError("消息缺少 role 字段")
        
        return data

    # ===== 工具方法 =====

    def get_text_content(self) -> str:
        """
        提取文本内容（忽略多模态部分）
        
        返回:
            纯文本内容
        
        用途:
            - 日志记录
            - 文本搜索
            - Token 估算
        """
        if isinstance(self.content, str):
            return self.content
        elif isinstance(self.content, list):
            text_parts = []
            for block in self.content:
                text = block.get_text_content()
                if text:
                    text_parts.append(text)
            return " ".join(text_parts)
        return ""

    def is_empty(self) -> bool:
        """
        检查消息是否为空
        
        返回:
            是否为空消息
        """
        if isinstance(self.content, str):
            return not self.content.strip()
        elif isinstance(self.content, list):
            return len(self.content) == 0
        return True

    def has_images(self) -> bool:
        """
        检查消息是否包含图片
        
        返回:
            是否包含图片
        """
        if isinstance(self.content, list):
            return any(block.type == "image_url" for block in self.content)
        return False

    def get_image_count(self) -> int:
        """
        获取图片数量
        
        返回:
            图片数量
        """
        if isinstance(self.content, list):
            return sum(1 for block in self.content if block.type == "image_url")
        return 0

    def clone(self) -> Message:
        """
        创建消息的深拷贝
        
        返回:
            新的 Message 实例
        """
        return Message.model_validate(self.model_dump())

    def truncate_content(self, max_length: int) -> Message:
        """
        截断消息内容（返回新消息）
        
        参数:
            max_length: 最大字符长度
        
        返回:
            截断后的新消息
        
        注意:
            仅截断文本内容，图片保持不变
        """
        if isinstance(self.content, str):
            if len(self.content) > max_length:
                truncated = self.content[:max_length] + "..."
                return Message(
                    role=self.role,
                    content=truncated,
                    name=self.name,
                    tool_calls=self.tool_calls,
                    tool_call_id=self.tool_call_id
                )
        
        # 多模态消息：截断文本块
        elif isinstance(self.content, list):
            new_blocks = []
            for block in self.content:
                if block.type == "text" and block.text:
                    if len(block.text) > max_length:
                        new_blocks.append(ContentBlock(
                            type="text",
                            text=block.text[:max_length] + "..."
                        ))
                    else:
                        new_blocks.append(block)
                else:
                    new_blocks.append(block)
            
            return Message(
                role=self.role,
                content=new_blocks,
                name=self.name,
                tool_calls=self.tool_calls,
                tool_call_id=self.tool_call_id
            )
        
        return self

    def __str__(self) -> str:
        """字符串表示（用于调试）"""
        text = self.get_text_content()
        preview = text[:50] + "..." if len(text) > 50 else text
        
        extra = []
        if self.has_images():
            extra.append(f"{self.get_image_count()} images")
        if self.tool_calls:
            extra.append(f"{len(self.tool_calls)} tool_calls")
        
        extra_str = f" ({', '.join(extra)})" if extra else ""
        
        return f"Message(role={self.role}, content='{preview}'{extra_str})"

    def __repr__(self) -> str:
        """详细表示"""
        return (
            f"Message("
            f"role={self.role!r}, "
            f"content={self.get_text_content()[:30]!r}, "
            f"has_images={self.has_images()}, "
            f"tool_calls={'Yes' if self.tool_calls else 'No'}"
            f")"
        )
```

[17] gecko/core/output.py
```python
# gecko/core/output.py
"""
Agent 输出模型

定义 Agent 执行后的标准输出格式，包含：
- 最终回复内容
- 工具调用信息
- Token 使用统计
- 原始模型响应

优化点：
1. 结构化的 Usage 模型
2. 输出验证和后处理
3. 丰富的工具方法
4. 格式化输出
5. 统计信息提取
"""
from __future__ import annotations

from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, field_validator, model_validator

from gecko.core.logging import get_logger

logger = get_logger(__name__)


# ===== Token 使用统计 =====

class TokenUsage(BaseModel):
    """
    Token 使用统计
    
    符合 OpenAI API 的 usage 格式
    
    示例:
        ```python
        usage = TokenUsage(
            prompt_tokens=100,
            completion_tokens=50,
            total_tokens=150
        )
        ```
    """
    prompt_tokens: int = Field(
        default=0,
        ge=0,
        description="提示词（输入）消耗的 tokens"
    )
    completion_tokens: int = Field(
        default=0,
        ge=0,
        description="生成（输出）消耗的 tokens"
    )
    total_tokens: int = Field(
        default=0,
        ge=0,
        description="总消耗 tokens"
    )
    
    # 扩展字段（某些模型提供）
    prompt_tokens_details: Optional[Dict[str, int]] = Field(
        default=None,
        description="提示词 tokens 详细信息"
    )
    completion_tokens_details: Optional[Dict[str, int]] = Field(
        default=None,
        description="生成 tokens 详细信息"
    )

    @model_validator(mode="after")
    def validate_total(self):
        """验证总 tokens 是否正确"""
        calculated_total = self.prompt_tokens + self.completion_tokens
        
        # 如果 total_tokens 为 0，自动计算
        if self.total_tokens == 0:
            self.total_tokens = calculated_total
        
        # 如果不一致，记录警告
        elif self.total_tokens != calculated_total:
            logger.warning(
                "Token usage total mismatch",
                total=self.total_tokens,
                calculated=calculated_total
            )
        
        return self

    def get_cost_estimate(
        self,
        prompt_price_per_1k: float = 0.0,
        completion_price_per_1k: float = 0.0
    ) -> float:
        """
        估算成本（美元）
        
        参数:
            prompt_price_per_1k: 输入 token 每 1000 个的价格
            completion_price_per_1k: 输出 token 每 1000 个的价格
        
        返回:
            估算成本（美元）
        
        示例:
            ```python
            # GPT-4 价格（示例）
            cost = usage.get_cost_estimate(
                prompt_price_per_1k=0.03,      # $0.03/1K tokens
                completion_price_per_1k=0.06   # $0.06/1K tokens
            )
            print(f"Estimated cost: ${cost:.4f}")
            ```
        """
        prompt_cost = (self.prompt_tokens / 1000) * prompt_price_per_1k
        completion_cost = (self.completion_tokens / 1000) * completion_price_per_1k
        return prompt_cost + completion_cost

    def __str__(self) -> str:
        """简洁的字符串表示"""
        return (
            f"TokenUsage("
            f"prompt={self.prompt_tokens}, "
            f"completion={self.completion_tokens}, "
            f"total={self.total_tokens}"
            f")"
        )


# ===== Agent 输出 =====

class AgentOutput(BaseModel):
    """
    Agent 执行结果
    
    包含 Agent 执行后的完整输出信息。
    
    属性:
        content: 最终文本回复（可能为空，如果只有工具调用）
        tool_calls: 工具调用列表
        usage: Token 使用统计（可选）
        raw: 原始模型响应（用于调试）
        metadata: 附加元数据
    
    示例:
        ```python
        # 简单文本输出
        output = AgentOutput(content="Hello, how can I help?")
        
        # 带工具调用的输出
        output = AgentOutput(
            content="I'll search for that information.",
            tool_calls=[
                {
                    "id": "call_1",
                    "function": {
                        "name": "search",
                        "arguments": '{"query": "AI"}'
                    }
                }
            ],
            usage=TokenUsage(
                prompt_tokens=100,
                completion_tokens=50
            )
        )
        
        # 检查输出
        if output.has_tool_calls():
            print(f"需要执行 {output.tool_call_count()} 个工具")
        
        if output.has_content():
            print(f"回复: {output.content}")
        ```
    """
    content: str = Field(
        default="",
        description="最终文本回复"
    )
    tool_calls: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="工具调用列表（OpenAI 格式）"
    )
    usage: Optional[TokenUsage] = Field(
        default=None,
        description="Token 使用统计"
    )
    raw: Any = Field(
        default=None,
        description="原始模型响应（用于调试）"
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="附加元数据"
    )

    model_config = {"arbitrary_types_allowed": True}

    @field_validator("tool_calls", mode="before")
    @classmethod
    def ensure_tool_calls(cls, value):
        """确保 tool_calls 始终是列表"""
        if value is None:
            return []
        if not isinstance(value, list):
            logger.warning("tool_calls should be a list", type=type(value).__name__)
            return []
        return value

    @field_validator("content", mode="before")
    @classmethod
    def ensure_content(cls, value):
        """确保 content 是字符串"""
        if value is None:
            return ""
        if not isinstance(value, str):
            return str(value)
        return value

    # ===== 检查方法 =====

    def has_content(self) -> bool:
        """
        检查是否有文本内容
        
        返回:
            是否有非空文本
        """
        return bool(self.content and self.content.strip())

    def has_tool_calls(self) -> bool:
        """
        检查是否有工具调用
        
        返回:
            是否包含工具调用
        """
        return len(self.tool_calls) > 0

    def tool_call_count(self) -> int:
        """
        获取工具调用数量
        
        返回:
            工具调用的数量
        """
        return len(self.tool_calls)

    def is_empty(self) -> bool:
        """
        检查输出是否完全为空
        
        返回:
            是否既无内容也无工具调用
        """
        return not self.has_content() and not self.has_tool_calls()

    def has_usage(self) -> bool:
        """
        检查是否有 usage 信息
        
        返回:
            是否包含 token 使用统计
        """
        return self.usage is not None

    # ===== 提取方法 =====

    def get_tool_names(self) -> List[str]:
        """
        提取所有被调用的工具名称
        
        返回:
            工具名称列表
        
        示例:
            ```python
            output = AgentOutput(tool_calls=[...])
            tools = output.get_tool_names()
            print(f"调用的工具: {', '.join(tools)}")
            ```
        """
        names = []
        for call in self.tool_calls:
            func = call.get("function", {})
            name = func.get("name")
            if name:
                names.append(name)
        return names

    def get_tool_call_by_id(self, call_id: str) -> Optional[Dict[str, Any]]:
        """
        根据 ID 获取工具调用
        
        参数:
            call_id: 工具调用 ID
        
        返回:
            工具调用字典，如果不存在返回 None
        """
        for call in self.tool_calls:
            if call.get("id") == call_id:
                return call
        return None

    def get_text_preview(self, max_length: int = 100) -> str:
        """
        获取内容预览（用于日志/显示）
        
        参数:
            max_length: 最大长度
        
        返回:
            截断后的文本预览
        """
        if not self.content:
            return ""
        
        if len(self.content) <= max_length:
            return self.content
        
        return self.content[:max_length] + "..."

    # ===== 转换方法 =====

    def to_dict(self) -> Dict[str, Any]:
        """
        转换为字典（便于序列化）
        
        返回:
            包含所有字段的字典
        """
        data = {
            "content": self.content,
            "tool_calls": self.tool_calls,
            "metadata": self.metadata,
        }
        
        if self.usage:
            data["usage"] = self.usage.model_dump()
        
        # raw 字段可能无法序列化，仅在调试模式下包含
        if self.raw and self.metadata.get("include_raw"):
            try:
                data["raw"] = str(self.raw)
            except Exception:
                data["raw"] = "<non-serializable>"
        
        return data

    def to_message_dict(self) -> Dict[str, Any]:
        """
        转换为 OpenAI 消息格式（用于下一轮对话）
        
        返回:
            符合 OpenAI API 的消息字典
        
        示例:
            ```python
            output = AgentOutput(content="Hello", tool_calls=[...])
            msg_dict = output.to_message_dict()
            # 可以直接添加到对话历史
            ```
        """
        msg = {
            "role": "assistant",
            "content": self.content or None,  # OpenAI 允许 null
        }
        
        if self.tool_calls:
            msg["tool_calls"] = self.tool_calls
        
        return msg

    # ===== 格式化输出 =====

    def format(self, include_metadata: bool = False) -> str:
        """
        格式化输出为可读文本
        
        参数:
            include_metadata: 是否包含元数据
        
        返回:
            格式化后的字符串
        
        示例:
            ```python
            output = AgentOutput(...)
            print(output.format())
            ```
        """
        lines = []
        
        # 内容
        if self.has_content():
            lines.append("=== 回复内容 ===")
            lines.append(self.content)
            lines.append("")
        
        # 工具调用
        if self.has_tool_calls():
            lines.append("=== 工具调用 ===")
            for i, call in enumerate(self.tool_calls, 1):
                func = call.get("function", {})
                name = func.get("name", "unknown")
                args = func.get("arguments", "{}")
                lines.append(f"{i}. {name}")
                lines.append(f"   参数: {args}")
            lines.append("")
        
        # Token 使用
        if self.usage:
            lines.append("=== Token 使用 ===")
            lines.append(f"输入: {self.usage.prompt_tokens}")
            lines.append(f"输出: {self.usage.completion_tokens}")
            lines.append(f"总计: {self.usage.total_tokens}")
            lines.append("")
        
        # 元数据
        if include_metadata and self.metadata:
            lines.append("=== 元数据 ===")
            for key, value in self.metadata.items():
                lines.append(f"{key}: {value}")
            lines.append("")
        
        return "\n".join(lines)

    def summary(self) -> str:
        """
        生成简短摘要
        
        返回:
            一行摘要文本
        
        示例:
            ```python
            output = AgentOutput(...)
            print(output.summary())
            # 输出: "回复: Hello... (50 chars) | 工具调用: 2 | Tokens: 150"
            ```
        """
        parts = []
        
        if self.has_content():
            preview = self.get_text_preview(30)
            parts.append(f"回复: {preview}")
        
        if self.has_tool_calls():
            parts.append(f"工具调用: {self.tool_call_count()}")
        
        if self.usage:
            parts.append(f"Tokens: {self.usage.total_tokens}")
        
        if not parts:
            return "空输出"
        
        return " | ".join(parts)

    # ===== 统计方法 =====

    def get_stats(self) -> Dict[str, Any]:
        """
        获取输出统计信息
        
        返回:
            包含各种统计数据的字典
        """
        stats = {
            "content_length": len(self.content),
            "has_content": self.has_content(),
            "tool_call_count": self.tool_call_count(),
            "tool_names": self.get_tool_names(),
            "is_empty": self.is_empty(),
        }
        
        if self.usage:
            stats["usage"] = {
                "prompt_tokens": self.usage.prompt_tokens,
                "completion_tokens": self.usage.completion_tokens,
                "total_tokens": self.usage.total_tokens,
            }
        
        return stats

    # ===== 字符串表示 =====

    def __str__(self) -> str:
        """简洁的字符串表示"""
        return self.summary()

    def __repr__(self) -> str:
        """详细的字符串表示"""
        return (
            f"AgentOutput("
            f"content_length={len(self.content)}, "
            f"tool_calls={self.tool_call_count()}, "
            f"has_usage={self.has_usage()}"
            f")"
        )

    def __bool__(self) -> bool:
        """
        布尔值转换（是否有有效输出）
        
        返回:
            是否不为空
        """
        return not self.is_empty()


# ===== 工具函数 =====

def create_text_output(
    content: str,
    usage: Optional[TokenUsage] = None,
    **metadata
) -> AgentOutput:
    """
    快速创建纯文本输出
    
    参数:
        content: 文本内容
        usage: Token 使用统计（可选）
        **metadata: 附加元数据
    
    返回:
        AgentOutput 实例
    
    示例:
        ```python
        output = create_text_output(
            "Hello, world!",
            usage=TokenUsage(prompt_tokens=10, completion_tokens=5)
        )
        ```
    """
    return AgentOutput(
        content=content,
        usage=usage,
        metadata=metadata
    )


def create_tool_output(
    tool_calls: List[Dict[str, Any]],
    content: str = "",
    usage: Optional[TokenUsage] = None,
    **metadata
) -> AgentOutput:
    """
    快速创建工具调用输出
    
    参数:
        tool_calls: 工具调用列表
        content: 可选的文本内容
        usage: Token 使用统计（可选）
        **metadata: 附加元数据
    
    返回:
        AgentOutput 实例
    """
    return AgentOutput(
        content=content,
        tool_calls=tool_calls,
        usage=usage,
        metadata=metadata
    )


def merge_outputs(outputs: List[AgentOutput]) -> AgentOutput:
    """
    合并多个输出（用于多 Agent 场景）
    
    参数:
        outputs: AgentOutput 列表
    
    返回:
        合并后的 AgentOutput
    
    策略:
        - 内容：用换行符连接
        - 工具调用：合并所有
        - Usage：累加 tokens
        - 元数据：合并（后者覆盖前者）
    
    示例:
        ```python
        output1 = AgentOutput(content="Part 1")
        output2 = AgentOutput(content="Part 2")
        merged = merge_outputs([output1, output2])
        print(merged.content)  # "Part 1\nPart 2"
        ```
    """
    if not outputs:
        return AgentOutput()
    
    if len(outputs) == 1:
        return outputs[0]
    
    # 合并内容
    contents = [o.content for o in outputs if o.has_content()]
    merged_content = "\n".join(contents)
    
    # 合并工具调用
    merged_tool_calls = []
    for output in outputs:
        merged_tool_calls.extend(output.tool_calls)
    
    # 合并 usage
    merged_usage = None
    if any(o.has_usage() for o in outputs):
        total_prompt = sum(
            o.usage.prompt_tokens for o in outputs if o.usage
        )
        total_completion = sum(
            o.usage.completion_tokens for o in outputs if o.usage
        )
        merged_usage = TokenUsage(
            prompt_tokens=total_prompt,
            completion_tokens=total_completion,
            total_tokens=total_prompt + total_completion
        )
    
    # 合并元数据
    merged_metadata = {}
    for output in outputs:
        merged_metadata.update(output.metadata)
    
    return AgentOutput(
        content=merged_content,
        tool_calls=merged_tool_calls,
        usage=merged_usage,
        metadata=merged_metadata
    )
```

[18] gecko/core/prompt.py
```python
# gecko/core/prompt.py
"""
Prompt 模板系统

提供灵活的提示词模板管理，基于 Jinja2 实现。

核心功能：
1. 动态变量替换
2. 模板验证
3. 模板缓存
4. 常用模板库
5. 模板组合

优化点：
1. 更好的错误处理
2. 模板缓存提升性能
3. 预定义模板库
4. 模板组合和继承
5. 安全的沙箱环境
"""
from __future__ import annotations

import re
from typing import Any, Dict, List, Optional, Set

from pydantic import BaseModel, Field, field_validator

from gecko.core.logging import get_logger

logger = get_logger(__name__)

# ===== Jinja2 相关 =====

# 延迟导入 Jinja2（避免强依赖）
_jinja2_available = None
_jinja2_env = None


def _check_jinja2():
    """检查 Jinja2 是否可用"""
    global _jinja2_available
    if _jinja2_available is None:
        try:
            import jinja2
            _jinja2_available = True
        except ImportError:
            _jinja2_available = False
    return _jinja2_available


def _get_jinja2_env():
    """获取 Jinja2 环境（带缓存）"""
    global _jinja2_env
    
    if _jinja2_env is None:
        if not _check_jinja2():
            raise ImportError(
                "PromptTemplate 依赖 jinja2。\n"
                "请安装：pip install jinja2\n"
                "或：rye add jinja2"
            )
        
        from jinja2 import Environment, StrictUndefined
        
        # 创建安全的 Jinja2 环境
        _jinja2_env = Environment(
            # 严格模式：未定义变量会报错
            undefined=StrictUndefined,
            # ✅ 修复1：禁用自动转义（直接设置为 False）
            autoescape=False,
            # 保留换行符
            keep_trailing_newline=True,
            # 启用扩展
            extensions=[]
        )
        
        logger.debug("Jinja2 environment initialized")
    
    return _jinja2_env


# ===== Prompt 模板 =====

class PromptTemplate(BaseModel):
    """
    Prompt 模板
    
    使用 Jinja2 语法，支持动态变量替换、条件判断、循环等。
    
    示例:
        ```python
        # 基础模板
        template = PromptTemplate(
            template="Hello, {{ name }}! You are {{ age }} years old.",
            input_variables=["name", "age"]
        )
        result = template.format(name="Alice", age=25)
        
        # 带条件的模板
        template = PromptTemplate(
            template='''
            {% if tools %}
            You have access to these tools:
            {% for tool in tools %}
            - {{ tool.name }}: {{ tool.description }}
            {% endfor %}
            {% endif %}
            
            User: {{ question }}
            ''',
            input_variables=["tools", "question"]
        )
        
        # 从文件加载
        template = PromptTemplate.from_file("./prompts/system.txt")
        ```
    
    属性:
        template: 模板字符串（Jinja2 语法）
        input_variables: 必需的变量列表
        template_format: 模板格式（默认 'jinja2'）
        validate_template: 是否验证模板语法（默认 True）
    """
    template: str = Field(..., description="模板字符串")
    input_variables: List[str] = Field(
        default_factory=list,
        description="必需的输入变量列表"
    )
    template_format: str = Field(
        default="jinja2",
        description="模板格式（jinja2/f-string）"
    )
    validate_template: bool = Field(
        default=True,
        description="是否验证模板语法"
    )
    
    # 私有字段：缓存编译后的模板
    _compiled_template: Any = None

    @field_validator("template_format")
    @classmethod
    def validate_format(cls, v: str) -> str:
        """验证模板格式"""
        valid_formats = {"jinja2", "f-string"}
        if v not in valid_formats:
            raise ValueError(
                f"不支持的模板格式: {v}。支持的格式: {valid_formats}"
            )
        return v

    def model_post_init(self, __context):
        """初始化后验证"""
        if self.validate_template:
            self._validate_template_syntax()

    def _validate_template_syntax(self):
        """验证模板语法"""
        if self.template_format == "jinja2":
            try:
                env = _get_jinja2_env()
                # 尝试编译模板
                self._compiled_template = env.from_string(self.template)
                logger.debug("Template syntax validated")
            except Exception as e:
                error_msg = self._format_jinja2_error(str(e))
                raise ValueError(f"模板语法错误:\n{error_msg}") from e
        elif self.template_format == "f-string":
            # f-string 格式验证（基础检查）
            self._validate_fstring_syntax()

    def _validate_fstring_syntax(self):
        """验证 f-string 语法（基础）"""
        # 检查是否有未闭合的大括号
        open_count = self.template.count("{")
        close_count = self.template.count("}")
        
        if open_count != close_count:
            raise ValueError(
                f"f-string 语法错误: 大括号不匹配 "
                f"({{ {open_count} 个, }} {close_count} 个)"
            )

    def _format_jinja2_error(self, error: str) -> str:
        """格式化 Jinja2 错误信息"""
        # 提取关键信息
        lines = error.split("\n")
        formatted_lines = []
        
        for line in lines[:5]:  # 只取前 5 行
            if line.strip():
                formatted_lines.append(f"  {line}")
        
        # 添加模板片段
        template_preview = self.template[:100].replace("\n", "\\n")
        formatted_lines.append(f"\n模板片段: {template_preview}...")
        
        return "\n".join(formatted_lines)

    # ===== 格式化方法 =====

    def format(self, **kwargs: Any) -> str:
        """
        格式化模板（填充变量）
        
        参数:
            **kwargs: 模板变量
        
        返回:
            格式化后的字符串
        
        异常:
            ValueError: 缺少必需变量或渲染失败
        
        示例:
            ```python
            template = PromptTemplate(
                template="Hello, {{ name }}!",
                input_variables=["name"]
            )
            result = template.format(name="Alice")
            ```
        """
        # 检查必需变量
        missing = self._check_missing_variables(kwargs)
        if missing:
            raise ValueError(
                f"缺少必需的模板变量: {', '.join(missing)}\n"
                f"需要: {self.input_variables}\n"
                f"提供: {list(kwargs.keys())}"
            )
        
        # 根据格式渲染
        if self.template_format == "jinja2":
            return self._format_jinja2(**kwargs)
        elif self.template_format == "f-string":
            return self._format_fstring(**kwargs)
        else:
            raise ValueError(f"不支持的模板格式: {self.template_format}")

    def _check_missing_variables(self, kwargs: Dict[str, Any]) -> List[str]:
        """检查缺失的变量"""
        provided = set(kwargs.keys())
        required = set(self.input_variables)
        missing = required - provided
        return sorted(missing)

    def _format_jinja2(self, **kwargs: Any) -> str:
        """使用 Jinja2 渲染"""
        try:
            # 使用缓存的编译模板（如果有）
            if self._compiled_template is None:
                env = _get_jinja2_env()
                self._compiled_template = env.from_string(self.template)
            
            result = self._compiled_template.render(**kwargs)
            return result
            
        except Exception as e:
            # 提供更友好的错误信息
            error_msg = str(e)
            
            # 尝试识别具体错误
            if "is undefined" in error_msg:
                # 提取未定义的变量名
                match = re.search(r"'(\w+)' is undefined", error_msg)
                if match:
                    var_name = match.group(1)
                    raise ValueError(
                        f"模板变量 '{var_name}' 未定义。\n"
                        f"可用变量: {list(kwargs.keys())}"
                    ) from e
            
            # 通用错误
            raise ValueError(
                f"模板渲染失败: {error_msg}\n"
                f"模板: {self.template[:100]}..."
            ) from e

    def _format_fstring(self, **kwargs: Any) -> str:
        """使用 f-string 格式化"""
        try:
            return self.template.format(**kwargs)
        except KeyError as e:
            raise ValueError(
                f"缺少变量: {e}\n"
                f"可用变量: {list(kwargs.keys())}"
            ) from e
        except Exception as e:
            raise ValueError(f"f-string 格式化失败: {e}") from e

    def format_safe(self, **kwargs: Any) -> str:
        """
        安全格式化（缺少变量时使用默认值）
        
        缺少的变量会被替换为 "<MISSING: var_name>"
        
        返回:
            格式化后的字符串
        
        注意:
            此方法会自动提取模板中的所有变量，
            不仅仅是 input_variables 中声明的变量。
        """
        try:
            all_vars = self.get_variables_from_template()
        except Exception as e:
            logger.warning("Failed to extract variables", error=str(e))
            all_vars = set(self.input_variables)
        
        safe_kwargs = dict(kwargs)
        for var in all_vars:
            if var not in safe_kwargs:
                # 智能推测默认值
                if var in ('history', 'messages', 'items', 'tools', 'examples'):
                    safe_kwargs[var] = []
                elif var in ('system', 'context', 'prefix', 'suffix'):
                    safe_kwargs[var] = None
                else:
                    safe_kwargs[var] = f"<MISSING: {var}>"
        
        try:
            if self.template_format == "jinja2":
                return self._format_jinja2(**safe_kwargs)
            elif self.template_format == "f-string":
                return self._format_fstring(**safe_kwargs)
            else:
                return f"<TEMPLATE ERROR: 不支持的格式>"
        except Exception as e:
            logger.error("Safe format failed", error=str(e))
            return f"<TEMPLATE ERROR: {e}>"

    # ===== 变量提取 =====

    def get_variables_from_template(self) -> Set[str]:
        """
        从模板中提取所有变量
        
        返回:
            变量名集合
        
        示例:
            ```python
            template = PromptTemplate(template="Hello {{ name }}, you are {{ age }}")
            vars = template.get_variables_from_template()
            # {'name', 'age'}
            ```
        """
        if self.template_format == "jinja2":
            return self._extract_jinja2_variables()
        elif self.template_format == "f-string":
            return self._extract_fstring_variables()
        return set()

    def _extract_jinja2_variables(self) -> Set[str]:
        """从 Jinja2 模板中提取变量"""
        try:
            env = _get_jinja2_env()
            from jinja2 import meta
            
            ast = env.parse(self.template)
            variables = meta.find_undeclared_variables(ast)
            return variables
        except Exception as e:
            logger.warning("Failed to extract Jinja2 variables", error=str(e))
            return set()

    def _extract_fstring_variables(self) -> Set[str]:
        """从 f-string 模板中提取变量"""
        # 简单正则匹配 {var_name}
        pattern = r'\{(\w+)\}'
        matches = re.findall(pattern, self.template)
        return set(matches)

    # ===== 模板操作 =====

    def partial(self, **kwargs: Any) -> "PromptTemplate":
        """
        部分填充变量（返回新模板）
        
        参数:
            **kwargs: 要填充的变量
        
        返回:
            新的 PromptTemplate，已填充部分变量
        
        示例:
            ```python
            template = PromptTemplate(
                template="Hello {{ name }}, you are {{ age }}",
                input_variables=["name", "age"]
            )
            partial = template.partial(name="Alice")
            result = partial.format(age=25)
            ```
        """
        # 填充变量
        partial_result = self.format_safe(**kwargs)
        
        # 计算剩余变量
        remaining_vars = [v for v in self.input_variables if v not in kwargs]
        
        return PromptTemplate(
            template=partial_result,
            input_variables=remaining_vars,
            template_format=self.template_format,
            validate_template=False  # 已经验证过了
        )

    def clone(self) -> "PromptTemplate":
        """
        克隆模板
        
        返回:
            新的 PromptTemplate 实例
        """
        return PromptTemplate(
            template=self.template,
            input_variables=self.input_variables.copy(),
            template_format=self.template_format,
            validate_template=False
        )

    # ===== 工厂方法 =====

    @classmethod
    def from_file(
        cls,
        path: str,
        input_variables: Optional[List[str]] = None,
        encoding: str = "utf-8"
    ) -> "PromptTemplate":
        """
        从文件加载模板
        
        参数:
            path: 文件路径
            input_variables: 变量列表（None 则自动提取）
            encoding: 文件编码
        
        返回:
            PromptTemplate 实例
        
        示例:
            ```python
            template = PromptTemplate.from_file("./prompts/system.txt")
            ```
        """
        from pathlib import Path
        
        file_path = Path(path)
        if not file_path.exists():
            raise FileNotFoundError(f"模板文件不存在: {path}")
        
        try:
            with open(file_path, "r", encoding=encoding) as f:
                template_str = f.read()
        except Exception as e:
            raise IOError(f"读取模板文件失败: {e}") from e
        
        # 创建模板
        prompt = cls(
            template=template_str,
            input_variables=input_variables or []
        )
        
        # 如果未提供变量，自动提取
        if not input_variables:
            detected_vars = prompt.get_variables_from_template()
            prompt.input_variables = sorted(detected_vars)
            logger.info(
                "Auto-detected template variables",
                path=path,
                variables=prompt.input_variables
            )
        
        return prompt

    @classmethod
    def from_examples(
        cls,
        examples: List[Dict[str, str]],
        template: str = "{{ input }}\n{{ output }}\n",
        separator: str = "\n---\n"
    ) -> "PromptTemplate":
        """
        从示例列表创建 few-shot 模板
        
        参数:
            examples: 示例列表 [{"input": "...", "output": "..."}, ...]
            template: 单个示例的模板
            separator: 示例之间的分隔符
        
        返回:
            PromptTemplate 实例
        
        示例:
            ```python
            examples = [
                {"input": "2+2", "output": "4"},
                {"input": "3+5", "output": "8"},
            ]
            template = PromptTemplate.from_examples(examples)
            ```
        """
        # 渲染所有示例
        example_template = cls(template=template, input_variables=[])
        
        rendered_examples = []
        for ex in examples:
            rendered = example_template.format_safe(**ex)
            rendered_examples.append(rendered)
        
        # 合并为完整模板
        full_template = separator.join(rendered_examples)
        
        return cls(
            template=full_template,
            input_variables=[]
        )

    # ===== 字符串表示 =====

    def __str__(self) -> str:
        """简洁表示"""
        preview = self.template[:50].replace("\n", "\\n")
        if len(self.template) > 50:
            preview += "..."
        return f"PromptTemplate('{preview}', vars={self.input_variables})"

    def __repr__(self) -> str:
        """详细表示"""
        return (
            f"PromptTemplate("
            f"template_length={len(self.template)}, "
            f"input_variables={self.input_variables}, "
            f"format={self.template_format}"
            f")"
        )


# ===== 预定义模板库 =====

class PromptLibrary:
    """
    常用 Prompt 模板库
    
    提供预定义的常用模板。
    
    示例:
        ```python
        # 使用预定义模板
        template = PromptLibrary.get_react_prompt()
        prompt = template.format(
            tools=[...],
            question="What is AI?"
        )
        ```
    """
    
    @staticmethod
    def get_react_prompt() -> PromptTemplate:
        """
        获取 ReAct 推理模板
        
        返回:
            ReAct 格式的 PromptTemplate
        """
        template = """You are a helpful AI assistant with access to tools.

{% if tools %}
Available Tools:
{% for tool in tools %}
- {{ tool.name }}: {{ tool.description }}
{% endfor %}
{% endif %}

To use a tool, respond with a tool call in the following format:
Action: tool_name
Action Input: {"param": "value"}

Then wait for the observation before continuing.

Question: {{ question }}

Let's think step by step."""
        
        return PromptTemplate(
            template=template,
            input_variables=["tools", "question"]
        )
    
    @staticmethod
    def get_chat_prompt() -> PromptTemplate:
        """
        获取对话模板
        
        返回:
            对话格式的 PromptTemplate
        """
        template = """{% if system %}{{ system }}

{% endif %}{% for message in history %}{{ message.role }}: {{ message.content }}
{% endfor %}User: {{ user_input }}
Assistant:"""
        
        return PromptTemplate(
            template=template,
            input_variables=["user_input"],
            # system 和 history 是可选的
        )
    
    @staticmethod
    def get_summarization_prompt() -> PromptTemplate:
        """
        获取摘要模板
        
        返回:
            摘要格式的 PromptTemplate
        """
        template = """Please summarize the following text in {{ max_words }} words or less:

{{ text }}

Summary:"""
        
        return PromptTemplate(
            template=template,
            input_variables=["text", "max_words"]
        )
    
    @staticmethod
    def get_extraction_prompt() -> PromptTemplate:
        """
        获取信息提取模板
        
        返回:
            信息提取格式的 PromptTemplate
        """
        template = """Extract the following information from the text:

{% for field in fields %}
- {{ field }}
{% endfor %}

Text: {{ text }}

Respond in JSON format."""
        
        return PromptTemplate(
            template=template,
            input_variables=["fields", "text"]
        )
    
    @staticmethod
    def get_translation_prompt() -> PromptTemplate:
        """
        获取翻译模板
        
        返回:
            翻译格式的 PromptTemplate
        """
        template = """Translate the following text from {{ source_lang }} to {{ target_lang }}:

{{ text }}

Translation:"""
        
        return PromptTemplate(
            template=template,
            input_variables=["source_lang", "target_lang", "text"]
        )


# ===== 原有的默认模板（保持兼容性）=====

DEFAULT_REACT_PROMPT = PromptTemplate(
    template="""You are a helpful AI assistant.
Current time: {{ current_time }}

{% if tools %}
You have access to the following tools:
{% for tool in tools %}
- {{ tool.name }}: {{ tool.description }}
{% endfor %}
{% endif %}

Answer the user's question using the tools if necessary.
""",
    input_variables=["current_time", "tools"],
)
```

[19] gecko/core/protocols.py
```python
# gecko/core/protocols.py (完整修正版)
"""
Gecko 核心协议定义

定义框架中各组件的标准接口，使用 Protocol 实现鸭子类型。

核心设计原则：
1. Protocol 仅定义接口契约，不包含实现
2. 默认行为通过工具函数或基类提供
3. 支持运行时类型检查

核心协议：
- ModelProtocol: LLM 模型核心接口
- StreamableModelProtocol: 支持流式的模型接口
- StorageProtocol: 存储后端接口
- ToolProtocol: 工具接口
- EmbedderProtocol: 嵌入模型接口
- RunnableProtocol: 可运行对象接口
- VectorStoreProtocol: 向量存储接口
"""
from __future__ import annotations

from typing import (
    Any,
    AsyncIterator,
    Dict,
    List,
    Optional,
    Protocol,
    runtime_checkable,
)

from pydantic import BaseModel, Field


# ====================== 模型响应格式 ======================

class CompletionChoice(BaseModel):
    """单个补全选择"""
    index: int = 0
    message: Dict[str, Any]
    finish_reason: Optional[str] = None
    logprobs: Optional[Dict[str, Any]] = None


class CompletionUsage(BaseModel):
    """Token 使用统计"""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class CompletionResponse(BaseModel):
    """标准的模型补全响应格式"""
    id: str = Field(default="", description="响应 ID")
    object: str = Field(default="chat.completion", description="对象类型")
    created: int = Field(default=0, description="创建时间戳")
    model: str = Field(default="", description="模型名称")
    choices: List[CompletionChoice] = Field(default_factory=list)
    usage: Optional[CompletionUsage] = Field(default=None)
    system_fingerprint: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)


class StreamChunk(BaseModel):
    """流式响应的单个数据块"""
    id: str = ""
    object: str = "chat.completion.chunk"
    created: int = 0
    model: str = ""
    choices: List[Dict[str, Any]] = Field(default_factory=list)
    
    @property
    def delta(self) -> Dict[str, Any]:
        """获取增量内容"""
        if self.choices:
            return self.choices[0].get("delta", {})
        return {}
    
    @property
    def content(self) -> Optional[str]:
        """获取文本内容"""
        return self.delta.get("content")


# ====================== 模型协议 ======================

@runtime_checkable
class ModelProtocol(Protocol):
    """
    LLM 模型核心协议
    
    定义所有模型必须实现的接口。
    
    设计原则:
        - 仅定义必需的方法
        - 不包含任何默认实现
        - 支持运行时类型检查
    
    示例:
        ```python
        class MyModel:
            async def acompletion(
                self, 
                messages: List[Dict[str, Any]], 
                **kwargs
            ) -> CompletionResponse:
                # 调用 API
                response = await api_call(messages)
                return CompletionResponse(**response)
        
        # 验证
        assert isinstance(MyModel(), ModelProtocol)  # ✅ True
        ```
    """
    
    async def acompletion(
        self, 
        messages: List[Dict[str, Any]], 
        **kwargs
    ) -> CompletionResponse:
        """
        异步补全接口（必需）
        
        参数:
            messages: 消息列表（OpenAI 格式）
            **kwargs: 模型参数（temperature, max_tokens 等）
        
        返回:
            CompletionResponse 标准响应
        """
        ...


@runtime_checkable
class StreamableModelProtocol(ModelProtocol, Protocol):
    """
    支持流式输出的模型协议
    
    继承自 ModelProtocol，额外要求实现 astream 方法。
    
    示例:
        ```python
        class StreamingModel:
            async def acompletion(self, messages, **kwargs):
                ...
            
            async def astream(self, messages, **kwargs):
                async for chunk in api_stream(messages):
                    yield StreamChunk(**chunk)
        
        model = StreamingModel()
        assert isinstance(model, StreamableModelProtocol)  # ✅ True
        ```
    """
    
    async def astream(
        self, 
        messages: List[Dict[str, Any]], 
        **kwargs
    ) -> AsyncIterator[StreamChunk]:
        """
        异步流式补全接口
        
        参数:
            messages: 消息列表
            **kwargs: 模型参数
        
        返回:
            StreamChunk 异步生成器
        """
        ...
        # Protocol 要求有 yield，但不能真的执行
        yield  # type: ignore


# ====================== 存储协议 ======================

@runtime_checkable
class StorageProtocol(Protocol):
    """
    存储后端协议
    
    定义键值存储的标准接口。
    """
    
    async def get(self, key: str) -> Optional[Dict[str, Any]]:
        """获取数据"""
        ...
    
    async def set(
        self, 
        key: str, 
        value: Dict[str, Any], 
        ttl: Optional[int] = None
    ) -> None:
        """存储数据"""
        ...
    
    async def delete(self, key: str) -> bool:
        """删除数据"""
        ...


# ====================== 工具协议 ======================

@runtime_checkable
class ToolProtocol(Protocol):
    """
    工具协议
    
    定义 Agent 可使用的工具的标准接口。
    """
    
    name: str
    description: str
    parameters: Dict[str, Any]
    
    async def execute(self, arguments: Dict[str, Any]) -> str:
        """执行工具"""
        ...


# ====================== 嵌入模型协议 ======================

@runtime_checkable
class EmbedderProtocol(Protocol):
    """嵌入模型协议"""
    
    async def embed(self, texts: List[str]) -> List[List[float]]:
        """批量嵌入文本"""
        ...
    
    def get_dimension(self) -> int:
        """获取向量维度"""
        ...


# ====================== 可运行对象协议 ======================

@runtime_checkable
class RunnableProtocol(Protocol):
    """可运行对象协议"""
    
    async def run(self, input: Any) -> Any:
        """运行对象"""
        ...


@runtime_checkable
class StreamableRunnableProtocol(RunnableProtocol, Protocol):
    """支持流式输出的可运行对象"""
    
    async def stream(self, input: Any) -> AsyncIterator[str]:
        """流式运行"""
        ...
        yield  # type: ignore


# ====================== 向量存储协议 ======================

@runtime_checkable
class VectorStoreProtocol(Protocol):
    """向量数据库协议"""
    
    async def add(
        self,
        ids: List[str],
        vectors: List[List[float]],
        metadata: Optional[List[Dict[str, Any]]] = None,
    ) -> None:
        """添加向量"""
        ...
    
    async def search(
        self,
        query_vector: List[float],
        top_k: int = 5,
        filters: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        """检索相似向量"""
        ...
    
    async def delete(self, ids: List[str]) -> None:
        """删除向量"""
        ...


# ====================== 工具函数 ======================

def check_protocol(obj: Any, protocol: type) -> bool:
    """
    检查对象是否实现了指定协议
    
    参数:
        obj: 要检查的对象
        protocol: 协议类型
    
    返回:
        是否实现了协议
    """
    return isinstance(obj, protocol)


def supports_streaming(model: Any) -> bool:
    """
    检查模型是否支持流式输出
    
    参数:
        model: 模型对象
    
    返回:
        是否支持
    
    实现逻辑:
        检查是否实现了 StreamableModelProtocol
    """
    return isinstance(model, StreamableModelProtocol)


def supports_function_calling(model: Any) -> bool:
    """
    检查模型是否支持 Function Calling
    
    参数:
        model: 模型对象
    
    返回:
        是否支持
    
    实现逻辑:
        检查 _supports_function_calling 属性或 supports_function_calling 方法
    """
    # 优先检查属性
    if hasattr(model, "_supports_function_calling"):
        return model._supports_function_calling
    
    # 其次检查方法
    if hasattr(model, "supports_function_calling"):
        method = getattr(model, "supports_function_calling")
        if callable(method):
            return method()
    
    return False


def supports_vision(model: Any) -> bool:
    """
    检查模型是否支持视觉输入
    
    参数:
        model: 模型对象
    
    返回:
        是否支持
    """
    if hasattr(model, "_supports_vision"):
        return model._supports_vision
    
    if hasattr(model, "supports_vision"):
        method = getattr(model, "supports_vision")
        if callable(method):
            return method()
    
    return False


def get_model_name(model: Any) -> str:
    """
    获取模型名称
    
    参数:
        model: 模型对象
    
    返回:
        模型名称
    """
    # 优先使用 model_name 属性
    if hasattr(model, "model_name"):
        return model.model_name
    
    # 其次使用 name 属性
    if hasattr(model, "name"):
        return model.name
    
    # 最后使用类名
    return model.__class__.__name__


def get_missing_methods(obj: Any, protocol: type) -> List[str]:
    """
    获取对象未实现的协议方法
    
    参数:
        obj: 要检查的对象
        protocol: 协议类型
    
    返回:
        缺失的方法名列表
    """
    import inspect as insp
    
    missing = []
    
    # 获取协议的所有成员
    for name, value in insp.getmembers(protocol):
        # 跳过私有成员和特殊成员
        if name.startswith("_"):
            continue
        
        # 检查是否是方法或属性
        if insp.isfunction(value) or insp.ismethod(value):
            if not hasattr(obj, name):
                missing.append(name)
            elif not callable(getattr(obj, name)):
                missing.append(name)
        elif insp.isdatadescriptor(value):
            # 属性检查
            if not hasattr(obj, name):
                missing.append(name)
    
    return missing


def validate_model(model: Any) -> None:
    """
    验证模型是否符合 ModelProtocol
    
    参数:
        model: 模型对象
    
    异常:
        TypeError: 模型不符合协议
    """
    if not isinstance(model, ModelProtocol):
        missing = get_missing_methods(model, ModelProtocol)
        raise TypeError(
            f"Model does not implement ModelProtocol. "
            f"Missing: {', '.join(missing) if missing else 'unknown'}"
        )


def validate_storage(storage: Any) -> None:
    """验证存储后端是否符合 StorageProtocol"""
    if not isinstance(storage, StorageProtocol):
        missing = get_missing_methods(storage, StorageProtocol)
        raise TypeError(
            f"Storage does not implement StorageProtocol. "
            f"Missing: {', '.join(missing) if missing else 'unknown'}"
        )


def validate_tool(tool: Any) -> None:
    """
    验证工具是否符合 ToolProtocol
    
    验证步骤：
    1. name: 必需、非空、字符串、去除空格后非空
    2. description: 必需、非空、字符串、去除空格后非空
    3. parameters: 必需、字典类型
    4. execute: 必需、异步方法
    
    参数:
        tool: 工具对象
    
    异常:
        ValueError: 属性无效（缺失、为空、类型错误）
        TypeError: 方法未实现（缺少 execute）
    """
    # 1. 验证 name
    if not hasattr(tool, "name"):
        raise ValueError("Tool must have a 'name' attribute")
    
    if not isinstance(tool.name, str) or not tool.name.strip():
        raise ValueError("Tool must have a non-empty 'name' attribute")
    
    # 2. 验证 description
    if not hasattr(tool, "description"):
        raise ValueError("Tool must have a 'description' attribute")
    
    if not isinstance(tool.description, str) or not tool.description.strip():
        raise ValueError("Tool must have a non-empty 'description' attribute")
    
    # 3. 验证 parameters
    if not hasattr(tool, "parameters"):
        raise ValueError("Tool must have a 'parameters' attribute")
    
    if not isinstance(tool.parameters, dict):
        raise ValueError("Tool must have a 'parameters' dict attribute")
    
    # 4. 验证 execute 方法
    if not hasattr(tool, "execute"):
        raise TypeError("Tool must have an 'execute' method")
    
    if not callable(getattr(tool, "execute")):
        raise TypeError("Tool 'execute' must be callable")
    
    # 5. 完整协议检查（作为最后的保障）
    if not isinstance(tool, ToolProtocol):
        missing = get_missing_methods(tool, ToolProtocol)
        missing_methods = [m for m in missing if m not in ["name", "description", "parameters"]]
        
        if missing_methods:
            raise TypeError(
                f"Tool does not implement ToolProtocol. "
                f"Missing methods: {', '.join(missing_methods)}"
            )


# ====================== 类型别名 ======================

ModelResponse = CompletionResponse
MessageDict = Dict[str, Any]
MessageList = List[MessageDict]
ToolCall = Dict[str, Any]
ToolCallList = List[ToolCall]
StorageValue = Dict[str, Any]
Vector = List[float]
VectorList = List[Vector]


# ====================== 导出 ======================

__all__ = [
    # 模型相关
    "ModelProtocol",
    "StreamableModelProtocol",
    "CompletionResponse",
    "CompletionChoice",
    "CompletionUsage",
    "StreamChunk",
    # 存储相关
    "StorageProtocol",
    # 工具相关
    "ToolProtocol",
    # 嵌入相关
    "EmbedderProtocol",
    # 可运行对象
    "RunnableProtocol",
    "StreamableRunnableProtocol",
    # 向量存储
    "VectorStoreProtocol",
    # 工具函数
    "check_protocol",
    "supports_streaming",
    "supports_function_calling",
    "supports_vision",
    "get_model_name",
    "get_missing_methods",
    "validate_model",
    "validate_storage",
    "validate_tool",
    # 类型别名
    "ModelResponse",
    "MessageDict",
    "MessageList",
    "ToolCall",
    "ToolCallList",
    "StorageValue",
    "Vector",
    "VectorList",
]
```

[20] gecko/core/session.py
```python
# gecko/core/session.py
"""
会话管理系统

核心功能：
1. 会话状态管理（内存 + 持久化）
2. 会话元数据（创建时间、更新时间、访问次数等）
3. 会话生命周期（过期、自动清理）
4. 并发安全（异步锁）

优化日志：
1. 修复同步方法调用异步保存的并发问题
2. 引入延迟保存机制 (Debounce/Dirty Flag)
3. 增强 SessionManager 的清理逻辑
4. 完善类型注解和错误处理
5. 修复 SessionManager 缺失的方法
"""
from __future__ import annotations

import asyncio
import time
import uuid
from typing import Any, Dict, List, Optional, Set

from pydantic import BaseModel, Field

from gecko.core.events import BaseEvent, EventBus
from gecko.core.logging import get_logger
from gecko.plugins.storage.interfaces import SessionInterface

logger = get_logger(__name__)


# ===== 会话元数据 =====

class SessionMetadata(BaseModel):
    """会话元数据"""
    session_id: str = Field(..., description="会话 ID")
    created_at: float = Field(default_factory=time.time, description="创建时间")
    updated_at: float = Field(default_factory=time.time, description="更新时间")
    accessed_at: float = Field(default_factory=time.time, description="访问时间")
    access_count: int = Field(default=0, description="访问次数")
    ttl: Optional[int] = Field(default=None, description="生存时间（秒）")
    tags: Set[str] = Field(default_factory=set, description="标签")
    custom: Dict[str, Any] = Field(default_factory=dict, description="自定义数据")
    
    def is_expired(self) -> bool:
        """检查会话是否过期"""
        if self.ttl is None:
            return False
        age = time.time() - self.created_at
        return age > self.ttl
    
    def time_to_expire(self) -> Optional[float]:
        """获取距离过期的剩余时间（秒）"""
        if self.ttl is None:
            return None
        age = time.time() - self.created_at
        return max(0.0, self.ttl - age)
    
    def touch(self):
        """更新访问时间和计数"""
        self.accessed_at = time.time()
        self.access_count += 1


# ===== 会话事件 =====

class SessionEvent(BaseEvent):
    """会话相关事件"""
    pass


# ===== 会话对象 =====

class Session:
    """
    会话对象
    
    管理内存中的会话状态，并提供可选的持久化支持。
    """
    
    def __init__(
        self,
        session_id: Optional[str] = None,
        state: Optional[Dict[str, Any]] = None,
        storage: Optional[SessionInterface] = None,
        ttl: Optional[int] = None,
        event_bus: Optional[EventBus] = None,
        auto_save: bool = False, 
    ):
        self.session_id = session_id or self._generate_id()
        self.state: Dict[str, Any] = state or {}
        self.storage = storage
        self.event_bus = event_bus or EventBus()
        self.auto_save = auto_save
        
        # 元数据
        self.metadata = SessionMetadata(
            session_id=self.session_id,
            ttl=ttl
        )
        
        # 并发锁
        self._lock = asyncio.Lock()
        
        # 标记为已修改
        self._dirty = False
        
        logger.debug("Session created", session_id=self.session_id)
    
    @staticmethod
    def _generate_id() -> str:
        return f"session_{uuid.uuid4().hex[:16]}"
    
    # ===== 状态管理 (同步方法) =====
    
    def get(self, key: str, default: Any = None) -> Any:
        """
        获取状态值（不自动更新访问计数）
        """
        return self.state.get(key, default)
    
    def set(self, key: str, value: Any):
        """
        设置状态值（同步）
        """
        self.state[key] = value
        self.metadata.updated_at = time.time()
        self._dirty = True
        self._try_schedule_auto_save()
    
    def delete(self, key: str) -> bool:
        """删除状态值"""
        if key in self.state:
            del self.state[key]
            self.metadata.updated_at = time.time()
            self._dirty = True
            self._try_schedule_auto_save()
            return True
        return False
    
    def clear(self):
        """清空所有状态"""
        self.state.clear()
        self.metadata.updated_at = time.time()
        self._dirty = True
        self._try_schedule_auto_save()
    
    def update(self, data: Dict[str, Any]):
        """批量更新状态"""
        self.state.update(data)
        self.metadata.updated_at = time.time()
        self._dirty = True
        self._try_schedule_auto_save()
    
    def touch(self):
        """手动更新访问时间和计数"""
        self.metadata.touch()
    
    def _try_schedule_auto_save(self):
        """
        尝试调度自动保存任务
        """
        if not self.auto_save or not self.storage:
            return

        try:
            loop = asyncio.get_running_loop()
            loop.create_task(self._auto_save_task())
        except RuntimeError:
            pass

    async def _auto_save_task(self):
        """后台自动保存任务"""
        try:
            await self.save()
        except Exception as e:
            logger.error("Auto-save failed", session_id=self.session_id, error=str(e))

    # ===== 字典接口 =====
    
    def keys(self) -> List[str]:
        return list(self.state.keys())
    
    def values(self) -> List[Any]:
        return list(self.state.values())
    
    def items(self) -> List[tuple]:
        return list(self.state.items())
    
    def __contains__(self, key: str) -> bool:
        return key in self.state
    
    def __getitem__(self, key: str) -> Any:
        return self.state[key]
    
    def __setitem__(self, key: str, value: Any):
        self.set(key, value)
    
    # ===== 生命周期管理 =====
    
    def is_expired(self) -> bool:
        return self.metadata.is_expired()
    
    def extend_ttl(self, extra_seconds: int):
        if self.metadata.ttl is not None:
            self.metadata.ttl += extra_seconds
            self._dirty = True
    
    def renew(self):
        self.metadata.created_at = time.time()
        self._dirty = True
    
    # ===== 标签管理 =====
    
    def add_tag(self, tag: str):
        self.metadata.tags.add(tag)
        self._dirty = True
    
    def remove_tag(self, tag: str):
        self.metadata.tags.discard(tag)
        self._dirty = True
    
    def has_tag(self, tag: str) -> bool:
        return tag in self.metadata.tags
    
    # ===== 持久化 (异步方法) =====
    
    async def save(self, force: bool = False):
        """
        保存会话到存储
        """
        if not self.storage:
            return
        
        if not force and not self._dirty:
            return
        
        async with self._lock:
            try:
                data = self.to_dict()
                await self.storage.set(self.session_id, data)
                self._dirty = False
                
                logger.debug("Session saved", session_id=self.session_id)
                
                await self.event_bus.publish(SessionEvent(
                    type="session_saved",
                    data={"session_id": self.session_id}
                ))
            except Exception as e:
                logger.error(
                    "Failed to save session",
                    session_id=self.session_id,
                    error=str(e)
                )
                raise
    
    async def load(self) -> bool:
        """从存储加载会话"""
        if not self.storage:
            return False
        
        async with self._lock:
            try:
                data = await self.storage.get(self.session_id)
                if not data:
                    return False
                
                self.from_dict(data)
                self._dirty = False
                
                logger.debug("Session loaded", session_id=self.session_id)
                
                await self.event_bus.publish(SessionEvent(
                    type="session_loaded",
                    data={"session_id": self.session_id}
                ))
                
                return True
            except Exception as e:
                logger.error(
                    "Failed to load session",
                    session_id=self.session_id,
                    error=str(e)
                )
                return False
    
    async def destroy(self):
        """销毁会话（从存储中删除）"""
        if self.storage:
            async with self._lock:
                try:
                    await self.storage.delete(self.session_id)
                    logger.info("Session destroyed", session_id=self.session_id)
                    
                    await self.event_bus.publish(SessionEvent(
                        type="session_destroyed",
                        data={"session_id": self.session_id}
                    ))
                except Exception as e:
                    logger.error(
                        "Failed to destroy session",
                        session_id=self.session_id,
                        error=str(e)
                    )
        
        self.state.clear()
    
    # ===== 序列化 =====
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "state": self.state,
            "metadata": self.metadata.model_dump(),
        }
    
    def from_dict(self, data: Dict[str, Any]):
        self.state = data.get("state", {})
        
        metadata_data = data.get("metadata", {})
        if metadata_data:
            self.metadata = SessionMetadata(**metadata_data)
    
    def clone(self, new_id: Optional[str] = None) -> Session:
        cloned = Session(
            session_id=new_id,
            state=self.state.copy(),
            storage=self.storage,
            ttl=self.metadata.ttl,
            event_bus=self.event_bus,
            auto_save=False,
        )
        cloned.metadata.tags = self.metadata.tags.copy()
        cloned.metadata.custom = self.metadata.custom.copy()
        return cloned
    
    # ===== 上下文管理器支持 =====
    
    async def __aenter__(self):
        await self.load()
        self.touch()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self._dirty:
            await self.save()
    
    def __repr__(self) -> str:
        return (
            f"Session("
            f"id={self.session_id}, "
            f"keys={len(self.state)}, "
            f"dirty={self._dirty}"
            f")"
        )


# ===== 会话管理器 =====

class SessionManager:
    """会话管理器"""
    
    def __init__(
        self,
        storage: Optional[SessionInterface] = None,
        default_ttl: Optional[int] = None,
        auto_cleanup: bool = True,
        cleanup_interval: int = 300,
    ):
        self.storage = storage
        self.default_ttl = default_ttl
        self.auto_cleanup = auto_cleanup
        self.cleanup_interval = cleanup_interval
        
        self._sessions: Dict[str, Session] = {}
        self._lock = asyncio.Lock()
        
        self._cleanup_task: Optional[asyncio.Task] = None
        
        logger.info("SessionManager initialized", default_ttl=default_ttl)
    
    async def start(self):
        """启动管理器（主要是自动清理任务）"""
        if self.auto_cleanup and not self._cleanup_task:
            self._cleanup_task = asyncio.create_task(self._cleanup_loop())
            logger.debug("Cleanup task started", interval=self.cleanup_interval)

    async def _cleanup_loop(self):
        while True:
            try:
                await asyncio.sleep(self.cleanup_interval)
                await self.cleanup_expired()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error("Cleanup task error", error=str(e))
                await asyncio.sleep(60)

    async def create_session(
        self,
        session_id: Optional[str] = None,
        ttl: Optional[int] = None,
        **initial_state
    ) -> Session:
        """创建新会话"""
        async with self._lock:
            session = Session(
                session_id=session_id,
                state=initial_state,
                storage=self.storage,
                ttl=ttl or self.default_ttl,
                auto_save=True, 
            )
            
            self._sessions[session.session_id] = session
            
            if self.storage:
                await session.save()
            
            return session
    
    async def get_session(
        self,
        session_id: str,
        create_if_missing: bool = False
    ) -> Optional[Session]:
        """获取会话"""
        if session_id in self._sessions:
            session = self._sessions[session_id]
            if session.is_expired():
                await self.destroy_session(session_id)
                if create_if_missing:
                    return await self.create_session(session_id=session_id)
                return None
            session.touch()
            return session
        
        if self.storage:
            session = Session(
                session_id=session_id,
                storage=self.storage,
                auto_save=True,
            )
            if await session.load():
                if session.is_expired():
                    await session.destroy()
                    if create_if_missing:
                        return await self.create_session(session_id=session_id)
                    return None
                async with self._lock:
                    self._sessions[session_id] = session
                session.touch()
                return session
        
        if create_if_missing:
            return await self.create_session(session_id=session_id)
        
        return None
    
    async def destroy_session(self, session_id: str) -> bool:
        """销毁会话"""
        async with self._lock:
            session = self._sessions.pop(session_id, None)
            
            if session:
                await session.destroy()
                return True
            elif self.storage:
                try:
                    await self.storage.delete(session_id)
                    return True
                except Exception as e:
                    logger.error("Failed to destroy session", session_id=session_id, error=str(e))
        return False
    
    async def cleanup_expired(self) -> int:
        """清理所有过期会话"""
        expired_ids = []
        async with self._lock:
            for session_id, session in self._sessions.items():
                if session.is_expired():
                    expired_ids.append(session_id)
        
        for session_id in expired_ids:
            await self.destroy_session(session_id)
        
        if expired_ids:
            logger.info("Expired sessions cleaned", count=len(expired_ids))
        return len(expired_ids)
    
    def get_active_count(self) -> int:
        """获取活跃会话数量"""
        return len(self._sessions)
    
    def get_all_sessions(self) -> List[Session]:
        """获取所有活跃会话"""
        return list(self._sessions.values())
    
    async def shutdown(self):
        """关闭管理器"""
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        
        sessions = list(self._sessions.values())
        for session in sessions:
            await session.save(force=True)
        
        logger.info("SessionManager shutdown", sessions_saved=len(sessions))
```

[21] gecko/core/structure.py
```python
# gecko/core/structure.py
"""
结构化输出引擎

负责将 LLM 的文本/工具调用输出解析为 Pydantic 模型。

核心功能：
1. 多策略 JSON 提取（Tool Call、直接 JSON、Markdown、暴力截取）
2. Schema 验证与修复
3. 带反馈的重试机制
4. OpenAI Function Calling Schema 生成

优化点：
1. 改进错误信息收集和报告
2. 更智能的 JSON 提取算法
3. Schema 自动修复
4. 详细的调试日志
5. 可配置的解析策略
"""
from __future__ import annotations

import json
import re
from typing import Any, Dict, List, Optional, Type, TypeVar

from pydantic import BaseModel, ValidationError

from gecko.core.logging import get_logger

logger = get_logger(__name__)

T = TypeVar("T", bound=BaseModel)


# ===== 自定义异常 =====

class StructureParseError(ValueError):
    """
    结构化解析失败异常
    
    属性:
        message: 错误信息
        attempts: 所有尝试的解析策略及其错误
        raw_content: 原始内容
    """
    def __init__(
        self,
        message: str,
        attempts: Optional[List[Dict[str, str]]] = None,
        raw_content: Optional[str] = None
    ):
        super().__init__(message)
        self.attempts = attempts or []
        self.raw_content = raw_content
    
    def get_detailed_error(self) -> str:
        """获取详细错误信息"""
        lines = [f"结构化解析失败: {self.args[0]}"]
        
        if self.attempts:
            lines.append("\n尝试的解析策略:")
            for i, attempt in enumerate(self.attempts, 1):
                strategy = attempt.get("strategy", "unknown")
                error = attempt.get("error", "unknown error")
                lines.append(f"  {i}. {strategy}: {error}")
        
        if self.raw_content:
            preview = self.raw_content[:200].replace("\n", "\\n")
            lines.append(f"\n原始内容预览: {preview}...")
        
        return "\n".join(lines)


# ===== 结构化输出引擎 =====

class StructureEngine:
    """
    结构化输出引擎
    
    提供多种策略将文本解析为 Pydantic 模型。
    
    示例:
        ```python
        from pydantic import BaseModel
        
        class User(BaseModel):
            name: str
            age: int
        
        # 从工具调用解析
        result = await StructureEngine.parse(
            content="",
            model_class=User,
            raw_tool_calls=[{
                "function": {
                    "arguments": '{"name": "Alice", "age": 25}'
                }
            }]
        )
        
        # 从文本解析
        result = await StructureEngine.parse(
            content='{"name": "Bob", "age": 30}',
            model_class=User
        )
        ```
    """
    
    # ===== Schema 生成 =====
    
    @staticmethod
    def to_openai_tool(model: Type[BaseModel]) -> Dict[str, Any]:
        """
        将 Pydantic 模型转换为 OpenAI Function Calling 所需的 schema
        
        参数:
            model: Pydantic 模型类
        
        返回:
            OpenAI tool 定义
        
        示例:
            ```python
            from pydantic import BaseModel, Field
            
            class SearchQuery(BaseModel):
                query: str = Field(description="搜索关键词")
                max_results: int = Field(default=5, description="最大结果数")
            
            tool = StructureEngine.to_openai_tool(SearchQuery)
            # 可用于 OpenAI API 的 tools 参数
            ```
        """
        schema = model.model_json_schema()
        
        # 提取模型名称（移除特殊字符）
        name = re.sub(r"\W+", "_", schema.get("title", "extract_data")).lower()
        
        # 移除 Pydantic 内部字段
        if "title" in schema:
            del schema["title"]
        if "$defs" in schema:
            # 展开 definitions
            schema = StructureEngine._flatten_schema(schema)
        
        return {
            "type": "function",
            "function": {
                "name": name,
                "description": schema.get("description", f"Extract {name} data"),
                "parameters": schema,
            },
        }
    
    @staticmethod
    def _flatten_schema(schema: Dict[str, Any]) -> Dict[str, Any]:
        """
        展开 schema 中的 $ref 引用
        
        简化版实现，处理常见情况
        """
        defs = schema.pop("$defs", {})
        if not defs:
            return schema
            
        def resolve_ref(obj):
            if isinstance(obj, dict):
                if "$ref" in obj:
                    ref_key = obj["$ref"].split("/")[-1]
                    if ref_key in defs:
                        # 递归解析引用
                        return resolve_ref(defs[ref_key])
                return {k: resolve_ref(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [resolve_ref(item) for item in obj]
            return obj

        return resolve_ref(schema)
    
    # ===== 解析方法 =====
    
    @classmethod
    async def parse(
        cls,
        content: str,
        model_class: Type[T],
        raw_tool_calls: Optional[List[Dict[str, Any]]] = None,
        strict: bool = True,
        auto_fix: bool = True,
    ) -> T:
        """
        解析文本为 Pydantic 模型
        
        参数:
            content: 文本内容
            model_class: 目标 Pydantic 模型类
            raw_tool_calls: 原始工具调用列表（优先使用）
            strict: 是否严格模式（False 时会尝试修复）
            auto_fix: 是否自动修复常见问题
        
        返回:
            模型实例
        
        异常:
            StructureParseError: 解析失败
        """
        attempts = []
        
        # 策略 1: 从 Tool Calls 提取
        if raw_tool_calls:
            for idx, call in enumerate(raw_tool_calls):
                try:
                    result = cls._parse_from_tool_call(call, model_class)
                    logger.info(
                        "Parsed from tool call",
                        model=model_class.__name__,
                        tool_call_index=idx
                    )
                    return result
                except Exception as e:
                    attempts.append({
                        "strategy": f"tool_call_{idx}",
                        "error": str(e)
                    })
                    logger.debug("Tool call parse failed", index=idx, error=str(e))
        
        # 策略 2-5: 从文本提取
        try:
            return cls._extract_json(content, model_class, strict=strict, auto_fix=auto_fix)
        except Exception as e:
            # 收集所有尝试的错误
            # [修复] 增加 and e.attempts 判断
            # 如果子异常有 attempts 且不为空，说明是深层策略失败，合并历史
            # 否则（如快速失败），将其视为当前步骤的一个错误记录下来
            if hasattr(e, 'attempts') and e.attempts:
                attempts.extend(e.attempts)
            else:
                attempts.append({
                    "strategy": "text_extraction",
                    "error": str(e)
                })
            
            # 构建详细错误信息
            error_details = "\n".join(
                f"  - {a['strategy']}: {a['error'][:100]}"
                for a in attempts
            )
            
            raise StructureParseError(
                f"无法解析为 {model_class.__name__}。尝试了 {len(attempts)} 种策略:\n{error_details}",
                attempts=attempts,
                raw_content=content
            ) from e
    
    # ===== 内部解析方法 =====
    
    @classmethod
    def _parse_from_tool_call(
        cls,
        call: Dict[str, Any],
        model_class: Type[T]
    ) -> T:
        """从单个工具调用中解析"""
        func = call.get("function", {})
        args = func.get("arguments", "")
        
        # 解析参数
        if isinstance(args, str):
            data = json.loads(args)
        elif isinstance(args, dict):
            data = args
        else:
            raise ValueError(f"Invalid arguments type: {type(args)}")
        
        # 验证并创建模型实例
        return model_class(**data)
    
    @classmethod
    def _extract_json(
        cls,
        text: str,
        model_class: Type[T],
        strict: bool = True,
        auto_fix: bool = True,
    ) -> T:
        """
        从文本中提取 JSON 并解析为模型
        
        尝试多种策略：
        1. 直接解析整个文本
        2. 提取 Markdown 代码块
        3. 暴力括号匹配
        4. 清理并重试
        """
        text = text.strip()
        attempts = []
        
        # 0. 快速失败检查 (Fail-Fast)
        if not text or ('{' not in text and '[' not in text):
            raise StructureParseError(
                "Content does not contain JSON-like structure (missing '{' or '[')",
                raw_content=text
            )

        # 策略 A: 直接解析 (最快)
        try:
            data = json.loads(text)
            return cls.validate(data, model_class)
        except Exception as e:
            attempts.append({"strategy": "direct_json", "error": str(e)})
        
        # 策略 B: Markdown 代码块
        markdown_pattern = r"```(?:\w+)?\s*([\s\S]*?)```"
        for match in re.finditer(markdown_pattern, text):
            candidate = match.group(1).strip()
            try:
                data = json.loads(candidate)
                return cls.validate(data, model_class)
            except Exception as e:
                attempts.append({"strategy": "markdown", "error": str(e)})
        
        # 策略 C: 暴力括号匹配 (限制尝试次数)
        json_candidates = cls._extract_braced_json(text)
        for idx, candidate in enumerate(json_candidates):
            try:
                data = json.loads(candidate)
                return cls.validate(data, model_class)
            except Exception as e:
                if idx < 3: # 仅记录前3次
                    attempts.append({"strategy": f"braced_{idx}", "error": str(e)})
        
        # 策略 D: 清理并重试
        if auto_fix:
            cleaned = cls._clean_json_string(text)
            if cleaned != text:
                try:
                    data = json.loads(cleaned)
                    logger.info("Parsed after cleaning", model=model_class.__name__)
                    return cls.validate(data, model_class)
                except Exception as e:
                    attempts.append({"strategy": "cleaned_json", "error": str(e)})
        
        # 构建错误详情
        error_details = "\n".join(f"  - {a['strategy']}: {a['error'][:100]}" for a in attempts)
        raise StructureParseError(
            f"无法解析为 {model_class.__name__}。尝试了 {len(attempts)} 种策略:\n{error_details}",
            attempts=attempts,
            raw_content=text
        )
    
    @staticmethod
    def _extract_braced_json(text: str) -> List[str]:
        """
        使用栈提取所有 {...} 块
        
        改进：
        - 返回所有可能的 JSON 对象，不仅仅是第一个
        - 按长度排序，优先尝试最长的
        """
        candidates = []
        stack = []
        start = None
        
        # 简单优化：只在看起来像 JSON 的区域搜索
        search_start = text.find('{')
        if search_start == -1:
            return []
            
        for idx, ch in enumerate(text[search_start:], start=search_start):
            if ch == "{":
                if not stack:
                    start = idx
                stack.append(ch)
            elif ch == "}" and stack:
                stack.pop()
                if not stack and start is not None:
                    candidates.append(text[start:idx + 1])
                    start = None
                    # 限制最大候选数量，防止DoS
                    if len(candidates) >= 5:
                        break
        
        # 按长度降序排序
        candidates.sort(key=len, reverse=True)
        return candidates
    
    @staticmethod
    def _clean_json_string(text: str) -> str:
        """
        清理常见的 JSON 格式问题
        
        处理：
        - 单引号 -> 双引号
        - 尾部逗号
        - 注释
        - 控制字符
        """
        # 移除注释
        text = re.sub(r'//.*?\n', '\n', text)
        text = re.sub(r'/\*.*?\*/', '', text, flags=re.DOTALL)
        
        # 尝试修复单引号（简单情况）
        # 注意：这可能会误伤字符串内容，需谨慎
        # text = text.replace("'", '"')
        
        # 移除尾部逗号
        text = re.sub(r',(\s*[}\]])', r'\1', text)
        
        # 移除控制字符
        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
        
        return text.strip()
    
    # ===== 辅助方法 =====
    
    @classmethod
    def validate(cls, data: Dict[str, Any], model_class: Type[T]) -> T:
        """
        验证数据并创建模型实例
        
        参数:
            data: 数据字典
            model_class: 模型类
        
        返回:
            模型实例
        
        异常:
            ValidationError: 验证失败
        """
        try:
            return model_class(**data)
        except ValidationError as e:
            logger.error(
                "Model validation failed",
                model=model_class.__name__,
                errors=e.errors()
            )
            raise
    
    @classmethod
    def get_schema_diff(
        cls,
        data: Dict[str, Any],
        model_class: Type[BaseModel]
    ) -> Dict[str, Any]:
        """
        比较数据与 Schema 的差异
        
        参数:
            data: 实际数据
            model_class: 期望的模型
        
        返回:
            差异信息
        """
        schema = model_class.model_json_schema()
        required = set(schema.get("required", []))
        properties = schema.get("properties", {})
        
        data_keys = set(data.keys())
        schema_keys = set(properties.keys())
        
        return {
            "missing_required": list(required - data_keys),
            "extra_fields": list(data_keys - schema_keys),
            "type_mismatches": cls._check_type_mismatches(data, properties),
        }
    
    @staticmethod
    def _check_type_mismatches(
        data: Dict[str, Any],
        properties: Dict[str, Any]
    ) -> List[Dict[str, str]]:
        """检查类型不匹配"""
        mismatches = []
        
        for key, value in data.items():
            if key not in properties:
                continue
            
            expected_type = properties[key].get("type")
            actual_type = type(value).__name__
            
            # 简化的类型检查
            type_map = {
                "string": str,
                "integer": int,
                "number": (int, float),
                "boolean": bool,
                "array": list,
                "object": dict,
            }
            
            expected_python_type = type_map.get(expected_type)
            if expected_python_type and not isinstance(value, expected_python_type):
                mismatches.append({
                    "field": key,
                    "expected": expected_type,
                    "actual": actual_type,
                })
        
        return mismatches


# ===== 工具函数 =====

def parse_structured_output(
    content: str,
    model_class: Type[T],
    tool_calls: Optional[List[Dict[str, Any]]] = None,
) -> T:
    """
    同步版本的结构化输出解析（便捷函数）
    
    参数:
        content: 文本内容
        model_class: 目标模型类
        tool_calls: 工具调用列表
    
    返回:
        模型实例
    """
    import asyncio
    
    # 创建事件循环执行异步解析
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    return loop.run_until_complete(
        StructureEngine.parse(content, model_class, tool_calls)
    )


def extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
    """
    从文本中提取第一个有效的 JSON 对象
    
    参数:
        text: 文本内容
    
    返回:
        JSON 字典，如果未找到返回 None
    """
    # 尝试直接解析
    try:
        return json.loads(text.strip())
    except json.JSONDecodeError:
        pass
    
    # 尝试 Markdown
    pattern = r"```(?:json)?\s*([\s\S]*?)```"
    for match in re.finditer(pattern, text):
        try:
            return json.loads(match.group(1).strip())
        except json.JSONDecodeError:
            continue
    
    # 尝试括号匹配
    candidates = StructureEngine._extract_braced_json(text)
    for candidate in candidates:
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            continue
    
    return None
```

[22] gecko/core/toolbox.py
```python
# gecko/core/toolbox.py
"""
ToolBox - Agent 工具箱

核心功能：
1. 工具注册与管理（支持实例注入与注册表加载）
2. 单个/批量工具执行
3. 并发控制与超时管理
4. 执行统计与监控
5. OpenAI Function Calling Schema 生成

优化日志：
- [Refactor] 集成 ToolRegistry，支持通过字符串名称加载工具
- [Refactor] 适配新版 BaseTool 接口
- [Fix] 修复并发控制的信号量使用方式
- [Feat] 线程安全的统计数据
- [Fix] 补全 get_summary, reset_stats 及魔术方法
"""
from __future__ import annotations

import asyncio
import threading
import time
from collections import defaultdict
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union

from anyio import create_task_group, fail_after, Semaphore
from anyio import get_cancelled_exc_class

from gecko.config import settings
from gecko.core.exceptions import ToolError, ToolNotFoundError
from gecko.core.logging import get_logger
from gecko.plugins.tools.base import BaseTool, ToolResult
from gecko.plugins.tools.registry import ToolRegistry

logger = get_logger(__name__)


# ===== 返回值模型 =====

@dataclass
class ToolExecutionResult:
    """
    工具执行结果（ToolBox 层面的封装）
    
    包含工具本身的返回内容，以及 ToolBox 记录的执行元数据（耗时、ID等）。
    
    属性:
        tool_name: 工具名称
        call_id: 调用 ID（用于关联请求）
        result: 执行结果（成功时为字符串，失败时为错误信息）
        is_error: 是否执行失败
        duration: 执行耗时（秒）
        metadata: 附加信息
    """
    tool_name: str
    call_id: str
    result: str
    is_error: bool
    duration: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """转换为字典格式"""
        return {
            "tool_name": self.tool_name,
            "call_id": self.call_id,
            "result": self.result,
            "is_error": self.is_error,
            "duration": self.duration,
            "metadata": self.metadata,
        }


# ===== 工具箱主类 =====

class ToolBox:
    """
    Agent 工具箱
    
    负责工具的注册、执行、并发控制和统计。
    既支持直接传入 BaseTool 实例，也支持通过名称从 ToolRegistry 加载。
    
    示例:
        ```python
        # 混合加载工具
        toolbox = ToolBox(
            tools=["calculator", MyCustomTool()],
            max_concurrent=5
        )
        
        # 执行
        result = await toolbox.execute("calculator", {"expression": "1+1"})
        
        # 获取统计
        summary = toolbox.get_summary()
        print(f"Success Rate: {summary['overall_success_rate']:.2%}")
        ```
    """

    def __init__(
        self,
        tools: Optional[List[Union[BaseTool, str]]] = None,
        max_concurrent: int = 5,
        default_timeout: Optional[float] = None,
        enable_retry: bool = False,
        max_retries: int = 2,
    ):
        """
        初始化工具箱
        
        参数:
            tools: 初始工具列表（支持 BaseTool 实例或注册表中的字符串名称）
            max_concurrent: 最大并发执行数
            default_timeout: 默认超时时间（秒）
            enable_retry: 是否启用重试
            max_retries: 最大重试次数
        """
        # 工具存储
        self._tools: Dict[str, BaseTool] = {}
        
        # 配置
        self.max_concurrent = max_concurrent
        self.default_timeout = default_timeout or settings.tool_execution_timeout
        self.enable_retry = enable_retry
        self.max_retries = max_retries
        
        # 统计数据（线程安全）
        self._stats_lock = threading.Lock()
        self._execution_count: Dict[str, int] = defaultdict(int)
        self._error_count: Dict[str, int] = defaultdict(int)
        self._total_time: Dict[str, float] = defaultdict(float)
        
        # 注册初始工具
        if tools:
            for item in tools:
                self.add_tool(item)
    
    # ====================== 工具管理 ======================
    
    def add_tool(self, item: Union[BaseTool, str], **kwargs) -> "ToolBox":
        """
        添加工具（高层接口）
        
        支持：
        1. 字符串：从 ToolRegistry 加载
        2. 实例：直接注册
        
        参数:
            item: 工具名称或实例
            **kwargs: 如果是字符串加载，kwargs 将传递给工具构造函数
        """
        tool_instance: Optional[BaseTool] = None
        
        if isinstance(item, str):
            try:
                tool_instance = ToolRegistry.load_tool(item, **kwargs)
            except Exception as e:
                logger.error(f"Failed to load tool '{item}' from registry: {e}")
                # 此时可以选择抛出异常，或者仅记录错误跳过，这里选择抛出以便尽早发现配置错误
                raise ToolNotFoundError(f"Registry load failed for '{item}': {e}") from e
        elif isinstance(item, BaseTool):
            tool_instance = item
        else:
            raise TypeError(f"Tool must be BaseTool or str, got {type(item)}")
            
        if tool_instance:
            self.register(tool_instance)
            
        return self

    def register(self, tool: BaseTool, replace: bool = True) -> "ToolBox":
        """
        注册工具实例（底层接口）
        
        参数:
            tool: 工具实例
            replace: 是否替换同名工具
        """
        if not isinstance(tool, BaseTool):
            raise TypeError(f"Tool must inherit from BaseTool, got {type(tool)}")
        
        if tool.name in self._tools and not replace:
            raise ValueError(f"Tool '{tool.name}' already registered.")
        
        self._tools[tool.name] = tool
        
        # 初始化统计
        with self._stats_lock:
            if tool.name not in self._execution_count:
                self._execution_count[tool.name] = 0
                self._error_count[tool.name] = 0
                self._total_time[tool.name] = 0.0
        
        logger.debug("Tool registered", tool_name=tool.name)
        return self
    
    def unregister(self, tool_name: str) -> "ToolBox":
        """注销工具"""
        if tool_name in self._tools:
            del self._tools[tool_name]
            logger.info("Tool unregistered", tool_name=tool_name)
        return self
    
    def get(self, name: str) -> Optional[BaseTool]:
        """获取工具实例"""
        return self._tools.get(name)
    
    def list_tools(self) -> List[BaseTool]:
        """获取所有已注册工具"""
        return list(self._tools.values())
    
    def has_tool(self, name: str) -> bool:
        """检查工具是否存在"""
        return name in self._tools
    
    # ====================== Schema 生成 ======================
    
    def to_openai_schema(self) -> List[Dict[str, Any]]:
        """
        生成 OpenAI Function Calling Schema
        
        直接调用 BaseTool.openai_schema 属性
        """
        return [t.openai_schema for t in self._tools.values()]
    
    # ====================== 执行逻辑 ======================
    
    async def execute(
        self,
        name: str,
        arguments: Dict[str, Any],
        timeout: Optional[float] = None,
        call_id: str = "",
    ) -> str:
        """
        执行单个工具（简易版）
        
        返回:
            结果字符串
        """
        result = await self.execute_with_result(name, arguments, timeout, call_id)
        if result.is_error:
            raise ToolError(
                f"Tool execution failed: {result.result}",
                context={"tool": name, "args": arguments}
            )
        return result.result
    
    async def execute_with_result(
        self,
        name: str,
        arguments: Dict[str, Any],
        timeout: Optional[float] = None,
        call_id: str = "",
    ) -> ToolExecutionResult:
        """
        执行单个工具（完整版）
        
        包含：超时控制、重试逻辑、统计记录
        """
        tool = self.get(name)
        if not tool:
            self._update_stats(name, 0, is_error=True)
            raise ToolNotFoundError(name)
        
        actual_timeout = timeout or self.default_timeout
        start_time = time.time()
        
        # 选择执行策略
        if self.enable_retry:
            result_str, is_error = await self._execute_with_retry(
                tool, arguments, actual_timeout
            )
        else:
            result_str, is_error = await self._execute_once(
                tool, arguments, actual_timeout
            )
            
        duration = time.time() - start_time
        self._update_stats(name, duration, is_error)
        
        return ToolExecutionResult(
            tool_name=name,
            call_id=call_id,
            result=result_str,
            is_error=is_error,
            duration=duration
        )

    async def _execute_once(
        self,
        tool: BaseTool,
        arguments: Dict[str, Any],
        timeout: float
    ) -> tuple[str, bool]:
        """
        单次执行封装
        
        处理超时和异常，适配 BaseTool 的 ToolResult 返回值
        """
        try:
            with fail_after(timeout):
                # BaseTool.execute 已经处理了参数校验和内部异常，返回 ToolResult
                res: ToolResult = await tool.execute(arguments)
                return res.content, res.is_error
                
        except TimeoutError:
            return f"Execution timed out after {timeout}s", True
        except get_cancelled_exc_class():
            return "Execution cancelled", True
        except Exception as e:
            logger.exception("Unexpected tool execution error", tool=tool.name)
            return f"System error: {str(e)}", True

    async def _execute_with_retry(
        self,
        tool: BaseTool,
        arguments: Dict[str, Any],
        timeout: float
    ) -> tuple[str, bool]:
        """带重试的执行逻辑"""
        last_result = ""
        
        for attempt in range(self.max_retries + 1):
            content, is_error = await self._execute_once(tool, arguments, timeout)
            
            if not is_error:
                return content, False
            
            last_result = content
            
            # 如果是超时或系统错误，尝试重试
            # 如果是 BaseTool 返回的业务逻辑错误（如参数不对），通常重试无用，但在通用层我们还是给机会
            if attempt < self.max_retries:
                wait_time = 2 ** attempt
                logger.warning(
                    "Tool failed, retrying",
                    tool=tool.name,
                    attempt=attempt + 1,
                    error=content[:100]
                )
                await asyncio.sleep(wait_time)
        
        return last_result, True

    # ====================== 批量执行 ======================
    
    async def execute_many(
        self,
        tool_calls: List[Dict[str, Any]],
        timeout: Optional[float] = None
    ) -> List[ToolExecutionResult]:
        """
        并发批量执行
        
        参数:
            tool_calls: [{"name": "...", "arguments": {...}, "id": "..."}, ...]
            
        返回:
            结果列表（顺序与输入一致）
        """
        if not tool_calls:
            return []
            
        results: List[Optional[ToolExecutionResult]] = [None] * len(tool_calls)
        
        # 使用 anyio 信号量控制并发
        semaphore = Semaphore(self.max_concurrent)
        
        async def _worker(idx: int, call: Dict[str, Any]):
            async with semaphore:
                name = call.get("name", "")
                args = call.get("arguments", {})
                cid = call.get("id", "")
                
                if not name:
                    results[idx] = ToolExecutionResult(
                        tool_name="unknown", call_id=cid, result="Missing tool name", is_error=True
                    )
                    return

                try:
                    # 复用 execute_with_result 以获得完整的统计和重试支持
                    results[idx] = await self.execute_with_result(name, args, timeout, cid)
                except Exception as e:
                    results[idx] = ToolExecutionResult(
                        tool_name=name, call_id=cid, result=f"Batch error: {e}", is_error=True
                    )

        async with create_task_group() as tg:
            for i, call in enumerate(tool_calls):
                tg.start_soon(_worker, i, call)
                
        # 过滤 None (理论上不应该存在)
        return [r for r in results if r is not None]

    # ====================== 统计与辅助 ======================
    
    def _update_stats(self, name: str, duration: float, is_error: bool):
        """更新统计数据（线程安全）"""
        with self._stats_lock:
            self._execution_count[name] += 1
            self._total_time[name] += duration
            if is_error:
                self._error_count[name] += 1
    
    def get_stats(self) -> Dict[str, Dict[str, Any]]:
        """获取详细统计快照"""
        with self._stats_lock:
            stats = {}
            for name in self._tools:
                cnt = self._execution_count[name]
                err = self._error_count[name]
                total = self._total_time[name]
                stats[name] = {
                    "calls": cnt,
                    "errors": err,
                    "avg_time": (total / cnt) if cnt > 0 else 0.0,
                    "success_rate": ((cnt - err) / cnt) if cnt > 0 else 1.0
                }
            return stats
            
    def print_stats(self):
        """打印格式化统计信息"""
        stats = self.get_stats()
        print("\n=== ToolBox Statistics ===")
        if not stats:
            print("No tools executed.")
        for name, data in stats.items():
            print(f"🔧 {name:<15} Calls: {data['calls']:<5} Errors: {data['errors']:<5} "
                  f"Avg: {data['avg_time']:.3f}s Rate: {data['success_rate']:.1%}")
        print("=" * 30 + "\n")
        
    def reset_stats(self):
        """
        重置所有统计数据
        """
        with self._stats_lock:
            self._execution_count.clear()
            self._error_count.clear()
            self._total_time.clear()
        logger.info("Statistics reset")

    def get_summary(self) -> Dict[str, Any]:
        """
        获取工具箱的全局摘要
        """
        stats = self.get_stats()
        
        total_executions = sum(s["calls"] for s in stats.values())
        total_errors = sum(s["errors"] for s in stats.values())
        total_time_sum = sum(
            self._total_time.get(name, 0.0) for name in stats.keys()
        )
        
        return {
            "tool_count": len(self._tools),
            "total_executions": total_executions,
            "total_errors": total_errors,
            "total_time": total_time_sum,
            "overall_success_rate": (
                (total_executions - total_errors) / total_executions
                if total_executions > 0 else 1.0
            ),
            "avg_time_per_call": (
                total_time_sum / total_executions
                if total_executions > 0 else 0.0
            ),
        }

    # ====================== 魔术方法 ======================

    def __repr__(self) -> str:
        return f"ToolBox(tools={len(self._tools)}, concurrent={self.max_concurrent})"
    
    def __len__(self) -> int:
        """返回已注册工具的数量"""
        return len(self._tools)
    
    def __contains__(self, tool_name: str) -> bool:
        """支持 'tool_name' in toolbox 语法"""
        return tool_name in self._tools
```

[23] gecko/core/utils.py
```python
# gecko/core/utils.py
"""
通用工具函数库

提供框架常用的工具函数，包括：
- 异步/同步统一处理
- 重试机制
- 超时控制
- 数据转换
- 字符串处理
- 装饰器

优化点：
1. 扩展工具函数集合
2. 添加超时和重试支持
3. 提供数据转换工具
4. 添加常用装饰器
"""
from __future__ import annotations

import asyncio
import functools
import hashlib
import inspect
import json
import time
from typing import Any, Awaitable, Callable, Dict, List, Optional, TypeVar, Union

from gecko.core.logging import get_logger

logger = get_logger(__name__)

T = TypeVar("T")
R = TypeVar("R")


# ===== 异步/同步统一处理 =====

async def ensure_awaitable(
    func: Callable[..., T | Awaitable[T]],
    *args,
    timeout: Optional[float] = None,
    **kwargs
) -> T:
    """
    统一处理同步/异步函数调用
    
    参数:
        func: 可调用对象（同步或异步）
        *args: 位置参数
        timeout: 超时时间（秒），None 表示无限制
        **kwargs: 关键字参数
    
    返回:
        函数执行结果
    
    异常:
        asyncio.TimeoutError: 超时
        Exception: 函数执行异常
    
    示例:
        ```python
        # 同步函数
        result = await ensure_awaitable(sync_func, arg1, arg2)
        
        # 异步函数
        result = await ensure_awaitable(async_func, arg1, arg2)
        
        # 带超时
        result = await ensure_awaitable(func, timeout=5.0)
        ```
    """
    # 确定是否为协程函数
    if asyncio.iscoroutinefunction(func):
        coro = func(*args, **kwargs)
    else:
        result = func(*args, **kwargs)
        if asyncio.iscoroutine(result):
            coro = result
        else:
            # 同步函数，直接返回结果
            return result
    
    # 执行异步函数（带可选超时）
    if timeout:
        try:
            return await asyncio.wait_for(coro, timeout=timeout)
        except asyncio.TimeoutError:
            logger.warning(
                "Function execution timeout",
                func=getattr(func, "__name__", str(func)),
                timeout=timeout
            )
            raise
    else:
        return await coro


def run_sync(coro: Awaitable[T]) -> T:
    """
    在同步上下文中运行异步函数
    
    参数:
        coro: 协程对象
    
    返回:
        执行结果
    
    示例:
        ```python
        async def async_func():
            return "result"
        
        # 在同步代码中调用
        result = run_sync(async_func())
        ```
    """
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        # 没有运行中的 loop，直接 run
        return asyncio.run(coro)
    
    if loop.is_running():
        # ⚠️ 检测到嵌套循环
        logger.debug(
            "Running async code in sync context with nested loop. "
            "Trying to use nest_asyncio."
        )
        try:
            import nest_asyncio
            nest_asyncio.apply()
            return loop.run_until_complete(coro)
        except ImportError:
            raise RuntimeError(
                "Detected nested event loop. "
                "Please install 'nest_asyncio' to allow nested usage: pip install nest_asyncio"
            )
    else:
        return loop.run_until_complete(coro)


# ===== 重试机制 =====

async def retry_async(
    func: Callable[..., Awaitable[T]],
    *args,
    max_attempts: int = 3,
    delay: float = 1.0,
    backoff: float = 2.0,
    exceptions: tuple = (Exception,),
    on_retry: Optional[Callable[[int, Exception], None]] = None,
    **kwargs
) -> T:
    """
    异步函数重试装饰器
    
    参数:
        func: 异步函数
        *args: 位置参数
        max_attempts: 最大尝试次数
        delay: 初始延迟（秒）
        backoff: 退避倍数
        exceptions: 需要重试的异常类型
        on_retry: 重试时的回调函数
        **kwargs: 关键字参数
    
    返回:
        函数执行结果
    
    异常:
        最后一次尝试的异常
    
    示例:
        ```python
        async def unstable_api_call():
            # 可能失败的 API 调用
            ...
        
        result = await retry_async(
            unstable_api_call,
            max_attempts=5,
            delay=2.0,
            backoff=2.0
        )
        ```
    """
    last_exception = None
    current_delay = delay
    
    func_name = getattr(func, "__name__", str(func))
    
    for attempt in range(1, max_attempts + 1):
        try:
            return await func(*args, **kwargs)
        except exceptions as e:
            last_exception = e
            
            if attempt >= max_attempts:
                logger.error(
                    "All retry attempts failed",
                    func=func_name,
                    attempts=max_attempts,
                    error=str(e)
                )
                raise
            
            logger.warning(
                "Function failed, retrying",
                func=func_name,
                attempt=attempt,
                max_attempts=max_attempts,
                delay=current_delay,
                error=str(e)
            )
            
            if on_retry:
                try:
                    # 新增：检测回调是否为异步函数
                    if inspect.iscoroutinefunction(on_retry):
                        await on_retry(attempt, e)
                    else:
                        on_retry(attempt, e)
                except Exception as cb_err:
                    logger.warning("Retry callback failed", error=str(cb_err))
            
            await asyncio.sleep(current_delay)
            current_delay *= backoff
    
    raise last_exception


def retry(
    max_attempts: int = 3,
    delay: float = 1.0,
    backoff: float = 2.0,
    exceptions: tuple = (Exception,)
):
    """
    重试装饰器（支持同步和异步）
    
    参数:
        max_attempts: 最大尝试次数
        delay: 初始延迟（秒）
        backoff: 退避倍数
        exceptions: 需要重试的异常类型
    
    示例:
        ```python
        @retry(max_attempts=5, delay=2.0)
        async def unstable_function():
            # 可能失败的操作
            ...
        ```
    """
    def decorator(func: Callable) -> Callable:
        if asyncio.iscoroutinefunction(func):
            @functools.wraps(func)
            async def async_wrapper(*args, **kwargs):
                return await retry_async(
                    func,
                    *args,
                    max_attempts=max_attempts,
                    delay=delay,
                    backoff=backoff,
                    exceptions=exceptions,
                    **kwargs
                )
            return async_wrapper
        else:
            @functools.wraps(func)
            def sync_wrapper(*args, **kwargs):
                last_exception = None
                current_delay = delay
                
                for attempt in range(1, max_attempts + 1):
                    try:
                        return func(*args, **kwargs)
                    except exceptions as e:
                        last_exception = e
                        if attempt >= max_attempts:
                            raise
                        time.sleep(current_delay)
                        current_delay *= backoff
                
                raise last_exception
            
            return sync_wrapper
    
    return decorator


# ===== 数据转换 =====

def safe_dict(obj: Any, max_depth: int = 3, _current_depth: int = 0) -> Any:
    """
    安全地将对象转换为字典（递归处理）
    
    参数:
        obj: 要转换的对象
        max_depth: 最大递归深度
        _current_depth: 当前深度（内部使用）
    
    返回:
        字典或可序列化的值
    
    示例:
        ```python
        class MyClass:
            def __init__(self):
                self.name = "test"
                self.value = 123
        
        obj = MyClass()
        data = safe_dict(obj)
        # {"name": "test", "value": 123}
        ```
    """
    if _current_depth >= max_depth:
        return str(obj)[:100]
    
    # Pydantic 模型
    if hasattr(obj, "model_dump"):
        try:
            return obj.model_dump()
        except Exception:
            pass
    
    # 字典
    if isinstance(obj, dict):
        return {
            str(k): safe_dict(v, max_depth, _current_depth + 1)
            for k, v in obj.items()
        }
    
    # 列表/元组
    if isinstance(obj, (list, tuple)):
        return [safe_dict(item, max_depth, _current_depth + 1) for item in obj]
    
    # 基本类型
    if isinstance(obj, (str, int, float, bool, type(None))):
        return obj
    
    # 对象属性
    if hasattr(obj, "__dict__"):
        return {
            k: safe_dict(v, max_depth, _current_depth + 1)
            for k, v in obj.__dict__.items()
            if not k.startswith("_")
        }
    
    # 其他情况：转字符串
    return str(obj)[:200]


def merge_dicts(*dicts: Dict, deep: bool = False) -> Dict:
    """
    合并多个字典
    
    参数:
        *dicts: 要合并的字典
        deep: 是否深度合并
    
    返回:
        合并后的字典
    
    示例:
        ```python
        d1 = {"a": 1, "b": {"x": 1}}
        d2 = {"b": {"y": 2}, "c": 3}
        
        # 浅合并
        result = merge_dicts(d1, d2)
        # {"a": 1, "b": {"y": 2}, "c": 3}
        
        # 深合并
        result = merge_dicts(d1, d2, deep=True)
        # {"a": 1, "b": {"x": 1, "y": 2}, "c": 3}
        ```
    """
    if not dicts:
        return {}
    
    result = {}
    
    for d in dicts:
        if not isinstance(d, dict):
            continue
        
        for key, value in d.items():
            if deep and key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = merge_dicts(result[key], value, deep=True)
            else:
                result[key] = value
    
    return result


# ===== 字符串处理 =====

def truncate(
    text: str,
    max_length: int = 100,
    suffix: str = "..."
) -> str:
    """
    截断文本
    
    参数:
        text: 文本
        max_length: 最大长度
        suffix: 后缀
    
    返回:
        截断后的文本
    """
    if len(text) <= max_length:
        return text
    
    return text[:max_length - len(suffix)] + suffix


def format_size(size_bytes: int) -> str:
    """
    格式化字节大小
    
    参数:
        size_bytes: 字节数
    
    返回:
        可读的大小字符串
    
    示例:
        ```python
        format_size(1024)       # "1.00 KB"
        format_size(1048576)    # "1.00 MB"
        format_size(1073741824) # "1.00 GB"
        ```
    """
    for unit in ["B", "KB", "MB", "GB", "TB"]:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} PB"


def format_duration(seconds: float) -> str:
    """
    格式化时长
    
    参数:
        seconds: 秒数
    
    返回:
        可读的时长字符串
    
    示例:
        ```python
        format_duration(65)    # "1m 5s"
        format_duration(3665)  # "1h 1m 5s"
        ```
    """
    if seconds < 60:
        return f"{seconds:.1f}s"
    
    minutes = int(seconds // 60)
    secs = int(seconds % 60)
    
    if minutes < 60:
        return f"{minutes}m {secs}s"
    
    hours = minutes // 60
    minutes = minutes % 60
    
    return f"{hours}h {minutes}m {secs}s"


def compute_hash(text: str, algorithm: str = "md5") -> str:
    """
    计算文本的哈希值
    
    参数:
        text: 文本
        algorithm: 算法（md5/sha1/sha256）
    
    返回:
        哈希值（十六进制字符串）
    
    示例:
        ```python
        hash_value = compute_hash("Hello, World!")
        ```
    """
    if algorithm == "md5":
        hasher = hashlib.md5()
    elif algorithm == "sha1":
        hasher = hashlib.sha1()
    elif algorithm == "sha256":
        hasher = hashlib.sha256()
    else:
        raise ValueError(f"不支持的哈希算法: {algorithm}")
    
    hasher.update(text.encode("utf-8"))
    return hasher.hexdigest()


# ===== 性能监控 =====

class Timer:
    """
    简单的计时器（上下文管理器）
    
    示例:
        ```python
        with Timer("操作名称") as t:
            # 执行耗时操作
            do_something()
        
        print(f"耗时: {t.elapsed:.2f}s")
        ```
    """
    
    def __init__(self, name: str = "Timer", log: bool = True):
        self.name = name
        self.log = log
        self.start_time = None
        self.end_time = None
        self.elapsed = 0.0
    
    def __enter__(self):
        self.start_time = time.time()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.time()
        self.elapsed = self.end_time - self.start_time
        
        if self.log:
            logger.info(
                "Timer completed",
                name=self.name,
                elapsed=f"{self.elapsed:.3f}s"
            )
        
        return False

def safe_json_loads(text: str, default: Any = None) -> Any:
    """
    安全地解析 JSON
    """
    try:
        return json.loads(text)
    except (json.JSONDecodeError, TypeError):
        return default

def timing(func: Callable) -> Callable:
    """
    计时装饰器（支持同步和异步）
    
    示例:
        ```python
        @timing
        async def slow_function():
            await asyncio.sleep(1)
        ```
    """
    if asyncio.iscoroutinefunction(func):
        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            start = time.time()
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                elapsed = time.time() - start
                logger.info(
                    "Function executed",
                    func=func.__name__,
                    elapsed=f"{elapsed:.3f}s"
                )
        return async_wrapper
    else:
        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            start = time.time()
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                elapsed = time.time() - start
                logger.info(
                    "Function executed",
                    func=func.__name__,
                    elapsed=f"{elapsed:.3f}s"
                )
        return sync_wrapper


# ===== 函数签名工具 =====

def get_function_args(func: Callable) -> List[str]:
    """
    获取函数的参数名列表
    
    参数:
        func: 函数对象
    
    返回:
        参数名列表
    """
    sig = inspect.signature(func)
    return [
        name for name, param in sig.parameters.items()
        if param.kind in (
            inspect.Parameter.POSITIONAL_OR_KEYWORD,
            inspect.Parameter.KEYWORD_ONLY,
        )
    ]


def has_argument(func: Callable, arg_name: str) -> bool:
    """
    检查函数是否有指定参数
    
    参数:
        func: 函数对象
        arg_name: 参数名
    
    返回:
        是否存在该参数
    """
    return arg_name in get_function_args(func)


# ===== 其他工具 =====

def chunk_list(lst: List[T], chunk_size: int) -> List[List[T]]:
    """
    将列表分块
    
    参数:
        lst: 列表
        chunk_size: 每块大小
    
    返回:
        分块后的列表
    
    示例:
        ```python
        chunks = chunk_list([1, 2, 3, 4, 5], chunk_size=2)
        # [[1, 2], [3, 4], [5]]
        ```
    """
    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]


def flatten_list(nested: List[List[T]]) -> List[T]:
    """
    展平嵌套列表
    
    参数:
        nested: 嵌套列表
    
    返回:
        展平后的列表
    
    示例:
        ```python
        flat = flatten_list([[1, 2], [3, 4], [5]])
        # [1, 2, 3, 4, 5]
        ```
    """
    return [item for sublist in nested for item in sublist]


def deduplicate(
    items: List[T],
    key: Optional[Callable[[T], Any]] = None
) -> List[T]:
    """
    列表去重（保持顺序）
    
    参数:
        items: 列表
        key: 可选的键函数
    
    返回:
        去重后的列表
    
    示例:
        ```python
        # 简单去重
        unique = deduplicate([1, 2, 2, 3, 1])
        # [1, 2, 3]
        
        # 按属性去重
        users = [{"id": 1, "name": "A"}, {"id": 1, "name": "B"}]
        unique = deduplicate(users, key=lambda u: u["id"])
        ```
    """
    seen = set()
    result = []
    
    for item in items:
        k = key(item) if key else item
        
        # 处理不可哈希的情况
        try:
            if k not in seen:
                seen.add(k)
                result.append(item)
        except TypeError:
            # 不可哈希，使用 == 比较
            if not any(k == s for s in seen):
                seen.add(k)
                result.append(item)
    
    return result


# ===== 向后兼容导出 =====

__all__ = [
    # 异步工具
    "ensure_awaitable",
    "run_sync",
    # 重试
    "retry",
    "retry_async",
    # 数据转换
    "safe_dict",
    "merge_dicts",
    # 字符串
    "truncate",
    "format_size",
    "format_duration",
    "compute_hash",
    # 性能
    "Timer",
    "timing",
    # 函数工具
    "get_function_args",
    "has_argument",
    # 列表工具
    "chunk_list",
    "flatten_list",
    "deduplicate",
    "safe_json_loads",
]
```

[24] gecko/plugins/__init__.py
```python
```

[25] gecko/plugins/base.py
```python
# gecko/plugins/base.py  
  
"""  
占位：未来可在此定义所有插件的抽象基类或通用接口。  
目前实际内容请参考各子目录（tools/ storage/ knowledge 等）。  
"""  
```

[26] gecko/plugins/guardrails/__init__.py
```python
```

[27] gecko/plugins/guardrails/pii.py
```python
```

[28] gecko/plugins/knowledge/__init__.py
```python
# gecko/plugins/knowledge/__init__.py
from gecko.plugins.knowledge.document import Document
from gecko.plugins.knowledge.interfaces import EmbedderProtocol
from gecko.plugins.knowledge.embedders import OpenAIEmbedder, OllamaEmbedder
from gecko.plugins.knowledge.splitters import RecursiveCharacterTextSplitter
from gecko.plugins.knowledge.pipeline import IngestionPipeline
from gecko.plugins.knowledge.tool import RetrievalTool

__all__ = [
    "Document", 
    "EmbedderProtocol", 
    "OpenAIEmbedder", 
    "OllamaEmbedder",
    "RecursiveCharacterTextSplitter",
    "IngestionPipeline",
    "RetrievalTool"
]
```

[29] gecko/plugins/knowledge/base.py
```python
```

[30] gecko/plugins/knowledge/default.py
```python
```

[31] gecko/plugins/knowledge/document.py
```python
# gecko/plugins/knowledge/document.py
from __future__ import annotations
from uuid import uuid4
from typing import Dict, Any, Optional, List
from pydantic import BaseModel, Field

class Document(BaseModel):
    """
    Gecko 标准文档对象
    在 Pipeline 中流转的核心数据结构
    """
    id: str = Field(default_factory=lambda: str(uuid4()))
    text: str
    metadata: Dict[str, Any] = Field(default_factory=dict)
    embedding: Optional[List[float]] = None

    def to_dict(self) -> Dict[str, Any]:
        """转换为存储层所需的字典格式"""
        return {
            "id": self.id,
            "text": self.text,
            "metadata": self.metadata,
            "embedding": self.embedding
        }
```

[32] gecko/plugins/knowledge/embedders.py
```python
# gecko/plugins/knowledge/embedders.py
from __future__ import annotations
import os
from typing import List
import litellm
from gecko.plugins.knowledge.interfaces import EmbedderProtocol

class OpenAIEmbedder(EmbedderProtocol):
    """
    基于 LiteLLM 的通用 Embedder
    支持 OpenAI, Azure, Ollama 等所有 LiteLLM 支持的 embedding 模型
    """
    def __init__(self, model: str = "text-embedding-3-small", dimension: int = 1536, **kwargs):
        self.model = model
        self._dimension = dimension
        self.kwargs = kwargs

    @property
    def dimension(self) -> int:
        return self._dimension

    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        # 替换换行符以提升某些模型的表现
        texts = [t.replace("\n", " ") for t in texts]
        response = await litellm.aembedding(
            model=self.model,
            input=texts,
            **self.kwargs
        )
        return [r["embedding"] for r in response.data]

    async def embed_query(self, text: str) -> List[float]:
        text = text.replace("\n", " ")
        response = await litellm.aembedding(
            model=self.model,
            input=[text],
            **self.kwargs
        )
        return response.data[0]["embedding"]

# 预设 Ollama 配置
class OllamaEmbedder(OpenAIEmbedder):
    """
    Ollama 本地嵌入模型适配器
    """
    def __init__(self, model: str = "ollama/nomic-embed-text", base_url: str = "http://localhost:11434", dimension: int = 768):
        super().__init__(
            model=model, 
            dimension=dimension, 
            api_base=base_url
        )
```

[33] gecko/plugins/knowledge/interfaces.py
```python
# gecko/plugins/knowledge/interfaces.py
from __future__ import annotations
from typing import List, Protocol, runtime_checkable

@runtime_checkable
class EmbedderProtocol(Protocol):
    """
    嵌入模型协议
    负责将文本转换为向量
    """
    @property
    def dimension(self) -> int:
        """返回向量维度 (例如 1536)"""
        ...

    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """批量嵌入文档列表"""
        ...

    async def embed_query(self, text: str) -> List[float]:
        """嵌入单个查询语句"""
        ...

@runtime_checkable
class ReaderProtocol(Protocol):
    """
    文件读取协议
    """
    def load(self, file_path: str) -> str:
        """读取文件内容为字符串"""
        ...
```

[34] gecko/plugins/knowledge/pipeline.py
```python
# gecko/plugins/knowledge/pipeline.py
from typing import List, Optional
from gecko.plugins.knowledge.interfaces import EmbedderProtocol
from gecko.plugins.storage.interfaces import VectorInterface
from gecko.plugins.knowledge.splitters import RecursiveCharacterTextSplitter
from gecko.plugins.knowledge.readers import AutoReader
from gecko.core.utils import ensure_awaitable

class IngestionPipeline:
    """
    RAG 数据入库流水线
    Load -> Split -> Embed -> Store
    """
    def __init__(
        self,
        vector_store: VectorInterface,
        embedder: EmbedderProtocol,
        splitter = None
    ):
        self.vector_store = vector_store
        self.embedder = embedder
        self.splitter = splitter or RecursiveCharacterTextSplitter()

    async def run(self, file_paths: List[str], batch_size: int = 100):
        """
        执行入库流程
        :param file_paths: 文件路径列表
        :param batch_size: 向量库写入批次大小
        """
        print(f"🚀 开始处理 {len(file_paths)} 个文件...")
        
        # 1. Load
        raw_docs = []
        for path in file_paths:
            try:
                docs = AutoReader.read(path)
                raw_docs.extend(docs)
            except Exception as e:
                print(f"⚠️ 读取失败 {path}: {e}")

        # 2. Split
        chunks = self.splitter.split_documents(raw_docs)
        print(f"✂️ 切分为 {len(chunks)} 个片段")

        # 3. Embed & Store (Batch Processing)
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i : i + batch_size]
            texts = [doc.text for doc in batch]
            
            # 生成向量
            embeddings = await ensure_awaitable(self.embedder.embed_documents, texts)
            
            # 注入向量到文档对象
            docs_to_upsert = []
            for doc, emb in zip(batch, embeddings):
                doc.embedding = emb
                docs_to_upsert.append(doc.to_dict())
            
            # 写入数据库
            await self.vector_store.upsert(docs_to_upsert)
            print(f"💾 已存储批次 {i} - {i+len(batch)}")
            
        print("✅ 入库完成")
```

[35] gecko/plugins/knowledge/readers.py
```python
# gecko/plugins/knowledge/readers.py
import os
from pathlib import Path
from typing import List
from gecko.plugins.knowledge.document import Document
from gecko.plugins.knowledge.interfaces import ReaderProtocol

class TextReader(ReaderProtocol):
    """简单文本读取器 (.txt, .md, .py, etc)"""
    def load(self, file_path: str) -> str:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()

class PDFReader(ReaderProtocol):
    """PDF 读取器 (依赖 pypdf)"""
    def load(self, file_path: str) -> str:
        try:
            import pypdf
        except ImportError:
            raise ImportError("请安装 pypdf 以支持 PDF 读取: pip install pypdf")
            
        text = ""
        with open(file_path, "rb") as f:
            reader = pypdf.PdfReader(f)
            for page in reader.pages:
                text += page.extract_text() + "\n"
        return text

class AutoReader:
    """自动分发读取器"""
    _READERS = {
        ".txt": TextReader,
        ".md": TextReader,
        ".py": TextReader,
        ".json": TextReader,
        ".pdf": PDFReader
    }

    @classmethod
    def read(cls, file_path: str) -> List[Document]:
        path = Path(file_path)
        ext = path.suffix.lower()
        
        reader_cls = cls._READERS.get(ext)
        if not reader_cls:
            raise ValueError(f"不支持的文件类型: {ext}")
        
        content = reader_cls().load(str(path))
        return [Document(text=content, metadata={"source": str(path), "filename": path.name})]
```

[36] gecko/plugins/knowledge/splitters.py
```python
# gecko/plugins/knowledge/splitters.py
from typing import List
from gecko.plugins.knowledge.document import Document

class RecursiveCharacterTextSplitter:
    """
    递归字符切分器 (参考 LangChain 逻辑)
    尝试按顺序使用分隔符切分文本，直到块大小符合要求。
    """
    def __init__(
        self, 
        chunk_size: int = 1000, 
        chunk_overlap: int = 200,
        separators: List[str] | None = None
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or ["\n\n", "\n", " ", ""]

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """切分文档列表"""
        final_docs = []
        for doc in documents:
            chunks = self.split_text(doc.text)
            for i, chunk in enumerate(chunks):
                # 继承元数据，并增加切片信息
                new_meta = doc.metadata.copy()
                new_meta.update({"chunk_index": i, "source_id": doc.id})
                final_docs.append(Document(text=chunk, metadata=new_meta))
        return final_docs

    def split_text(self, text: str) -> List[str]:
        """切分单文本核心逻辑"""
        final_chunks = []
        if self._length(text) <= self.chunk_size:
            return [text]
            
        # 找到最优分隔符
        separator = self.separators[-1]
        for sep in self.separators:
            if sep == "":
                separator = ""
                break
            if sep in text:
                separator = sep
                break
                
        # 切分
        splits = text.split(separator) if separator else list(text)
        
        # 合并碎片
        good_splits = []
        current_chunk = ""
        
        for s in splits:
            if self._length(current_chunk) + self._length(s) < self.chunk_size:
                current_chunk += (separator if current_chunk else "") + s
            else:
                if current_chunk:
                    good_splits.append(current_chunk)
                current_chunk = s
        
        if current_chunk:
            good_splits.append(current_chunk)
            
        return good_splits

    def _length(self, text: str) -> int:
        return len(text)
```

[37] gecko/plugins/knowledge/tool.py
```python
# gecko/plugins/knowledge/tool.py
from typing import Type
from pydantic import BaseModel, Field
from gecko.plugins.tools.base import BaseTool
from gecko.plugins.storage.interfaces import VectorInterface
from gecko.plugins.knowledge.interfaces import EmbedderProtocol
from gecko.core.utils import ensure_awaitable

class RetrievalTool(BaseTool):
    name: str = "knowledge_search"
    description: str = "搜索内部知识库以获取相关信息。当问题涉及特定文档、报告或私有数据时使用。"
    parameters: dict = {
        "type": "object",
        "properties": {
            "query": {
                "type": "string", 
                "description": "用于在知识库中检索的查询语句"
            }
        },
        "required": ["query"]
    }

    def __init__(self, vector_store: VectorInterface, embedder: EmbedderProtocol, top_k: int = 3):
        super().__init__()
        # Private attributes are not Pydantic fields
        object.__setattr__(self, "_vector_store", vector_store)
        object.__setattr__(self, "_embedder", embedder)
        object.__setattr__(self, "_top_k", top_k)

    async def execute(self, arguments: dict) -> str:
        query = arguments.get("query")
        if not query:
            return "错误：查询语句为空"

        # 1. Embed Query
        query_vec = await ensure_awaitable(self._embedder.embed_query, query)
        
        # 2. Vector Search
        results = await self._vector_store.search(query_vec, top_k=self._top_k)
        
        if not results:
            return "未在知识库中找到相关内容。"
            
        # 3. Format Results
        context = "找到以下相关内容：\n\n"
        for i, res in enumerate(results, 1):
            source = res['metadata'].get('filename', 'unknown')
            score = f"{res['score']:.2f}" if 'score' in res else 'N/A'
            context += f"--- 文档 {i} (来源: {source}, 相关度: {score}) ---\n{res['text']}\n\n"
            
        return context
```

[38] gecko/plugins/models/__init__.py
```python
# gecko/plugins/models/__init__.py
"""
Gecko Models Plugin

提供统一的模型接入层。
架构：配置 (Config) -> 工厂 (Factory) -> 注册表 (Registry) -> 驱动 (Driver)
"""
import litellm

# ================= Global Configuration =================
# 关闭 LiteLLM 的遥测和冗余打印，保持控制台整洁
litellm.suppress_debug_info = True
litellm.telemetry = False
# ========================================================

from gecko.plugins.models.base import AbstractModel, BaseChatModel, BaseEmbedder
from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.drivers.litellm_driver import LiteLLMDriver
from gecko.plugins.models.embedding import LiteLLMEmbedder
from gecko.plugins.models.factory import create_model
from gecko.plugins.models.presets.ollama import OllamaChat, OllamaEmbedder
from gecko.plugins.models.presets.openai import OpenAIChat, OpenAIEmbedder
from gecko.plugins.models.presets.zhipu import ZhipuChat

# 确保默认驱动被注册
import gecko.plugins.models.drivers.litellm_driver  # noqa: F401

__all__ = [
    "ModelConfig",
    "AbstractModel",
    "BaseChatModel",
    "BaseEmbedder",
    "LiteLLMDriver",
    "LiteLLMEmbedder",
    "create_model",
    "OpenAIChat",
    "OpenAIEmbedder",
    "OllamaChat",
    "OllamaEmbedder",
    "ZhipuChat",
]
```

[39] gecko/plugins/models/adapter.py
```python
# gecko/plugins/models/adapter.py
from __future__ import annotations

from typing import Any, List, Optional

from gecko.core.protocols import (
    CompletionChoice,
    CompletionResponse,
    CompletionUsage,
    StreamChunk,
)


def safe_access(obj: Any, key: str, default: Any = None) -> Any:
    """
    [工具] 通用属性/字典获取 (增强版)
    
    修复了 Pydantic getattr 抛错和 Mock 对象误判的问题。
    """
    if obj is None:
        return default

    # 1. 优先尝试字典访问 (最安全)
    try:
        return obj[key]
    except (TypeError, KeyError, IndexError, AttributeError):
        pass

    # 2. 尝试属性访问 (需捕获 AttributeError)
    try:
        # 注意：不要使用 hasattr，因为它在某些动态代理对象(如 Mock, Pydantic Lazy)上可能误判
        val = getattr(obj, key)
        return val if val is not None else default
    except (AttributeError, TypeError):
        pass

    return default


class LiteLLMAdapter:
    """
    LiteLLM 响应适配器 (Anti-Corruption Layer)
    
    职责：
    将 LiteLLM 返回的异构对象手动映射为 Gecko 的标准协议对象。
    避免调用 model_dump() 从而消除 Pydantic 序列化警告。
    """

    @staticmethod
    def to_gecko_response(resp: Any) -> CompletionResponse:
        """将 LiteLLM 响应转换为 Gecko CompletionResponse"""
        
        # 1. 提取 Choices
        choices: List[CompletionChoice] = []
        raw_choices = safe_access(resp, "choices", [])
        
        if isinstance(raw_choices, list):
            for c in raw_choices:
                # 提取 Message
                raw_msg = safe_access(c, "message", {})
                message_dict = {
                    "role": safe_access(raw_msg, "role", "assistant"),
                    "content": safe_access(raw_msg, "content", None),
                }
                
                # 提取 Tool Calls
                raw_tool_calls = safe_access(raw_msg, "tool_calls", None)
                if raw_tool_calls:
                    sanitized_tool_calls = []
                    for tc in raw_tool_calls:
                        sanitized_tool_calls.append({
                            "id": safe_access(tc, "id", ""),
                            "type": safe_access(tc, "type", "function"),
                            "function": {
                                "name": safe_access(safe_access(tc, "function"), "name", ""),
                                "arguments": safe_access(safe_access(tc, "function"), "arguments", "")
                            }
                        })
                    message_dict["tool_calls"] = sanitized_tool_calls

                choices.append(CompletionChoice(
                    index=safe_access(c, "index", 0),
                    finish_reason=safe_access(c, "finish_reason", None),
                    message=message_dict,
                    logprobs=safe_access(c, "logprobs", None)
                ))

        # 2. 提取 Usage
        usage = None
        raw_usage = safe_access(resp, "usage", None)
        if raw_usage:
            usage = CompletionUsage(
                prompt_tokens=safe_access(raw_usage, "prompt_tokens", 0),
                completion_tokens=safe_access(raw_usage, "completion_tokens", 0),
                total_tokens=safe_access(raw_usage, "total_tokens", 0)
            )

        # 3. 构建最终响应
        return CompletionResponse(
            id=safe_access(resp, "id", ""),
            object=safe_access(resp, "object", "chat.completion"),
            created=safe_access(resp, "created", 0),
            model=safe_access(resp, "model", ""),
            choices=choices,
            usage=usage,
            system_fingerprint=safe_access(resp, "system_fingerprint", None),
            metadata=safe_access(resp, "_hidden_params", {})
        )

    @staticmethod
    def to_gecko_chunk(chunk: Any) -> Optional[StreamChunk]:
        """将 LiteLLM 流式块转换为 StreamChunk"""
        raw_choices = safe_access(chunk, "choices", [])
        
        # 过滤 Keep-Alive 空包
        if not raw_choices and not safe_access(chunk, "id"):
            return None

        mapped_choices = []
        if isinstance(raw_choices, list):
            for c in raw_choices:
                delta = safe_access(c, "delta", {})
                mapped_choices.append({
                    "index": safe_access(c, "index", 0),
                    "delta": {
                        "role": safe_access(delta, "role", None),
                        "content": safe_access(delta, "content", None),
                        "tool_calls": safe_access(delta, "tool_calls", None)
                    },
                    "finish_reason": safe_access(c, "finish_reason", None)
                })

        return StreamChunk(
            id=safe_access(chunk, "id", ""),
            created=safe_access(chunk, "created", 0),
            model=safe_access(chunk, "model", ""),
            choices=mapped_choices
        )
```

[40] gecko/plugins/models/base.py
```python
# gecko/plugins/models/base.py
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any, AsyncIterator, Dict, List

from gecko.core.protocols import (
    CompletionResponse,
    EmbedderProtocol,
    StreamChunk,
    StreamableModelProtocol,
)
from gecko.plugins.models.config import ModelConfig


class AbstractModel(ABC):
    """所有模型的根基类"""

    def __init__(self, config: ModelConfig):
        self.config = config


class BaseChatModel(AbstractModel, StreamableModelProtocol):
    """
    Chat 模型驱动基类
    
    所有具体的驱动器 (Driver) 都必须继承此类并实现 `acompletion` 和 `astream`。
    """

    @abstractmethod
    async def acompletion(self, messages: List[Dict[str, Any]], **kwargs: Any) -> CompletionResponse:
        """单次生成"""
        ...

    @abstractmethod
    async def astream(self, messages: List[Dict[str, Any]], **kwargs: Any) -> AsyncIterator[StreamChunk]: # type: ignore
        """流式生成"""
        ...


class BaseEmbedder(AbstractModel, EmbedderProtocol):
    """
    Embedding 模型基类
    """
    
    def __init__(self, config: ModelConfig, dimension: int):
        super().__init__(config)
        self._dimension = dimension

    @property
    def dimension(self) -> int:
        """返回向量维度"""
        return self._dimension

    @abstractmethod
    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """批量嵌入"""
        ...

    @abstractmethod
    async def embed_query(self, text: str) -> List[float]:
        """单条查询嵌入"""
        ...
```

[41] gecko/plugins/models/chat.py
```python
# gecko/plugins/models/chat.py
from __future__ import annotations

import json
from typing import Any, AsyncIterator, Dict, List

import litellm
from pydantic import ValidationError

from gecko.core.exceptions import ModelError
from gecko.core.logging import get_logger
from gecko.core.protocols import CompletionResponse, StreamChunk
from gecko.plugins.models.base import BaseChatModel

logger = get_logger(__name__)

class LiteLLMChatModel(BaseChatModel):
    """
    基于 LiteLLM 的通用 Chat 模型实现
    """

    def _get_params(self, messages: List[Dict[str, Any]], stream: bool, **kwargs: Any) -> Dict[str, Any]:
        """构造 LiteLLM 调用参数，合并配置与运行时参数"""
        params = {
            "model": self.config.model_name,
            "messages": messages,
            "timeout": self.config.timeout,
            "stream": stream,
            **self.config.extra_kwargs,
            **kwargs,
        }

        if self.config.api_key:
            params["api_key"] = self.config.api_key
        if self.config.base_url:
            params["api_base"] = self.config.base_url

        return params

    def _sanitize_response(self, resp: Any) -> Dict[str, Any]:
        """
        [核心修复] 深度清洗 LiteLLM 响应对象
        
        目标：将不可靠的 Pydantic 对象转换为纯 Python 字典，
        避免因 litellm 内部 Schema 校验失败导致 crash。
        """
        # 策略 1: 尝试通过 JSON 序列化 (通常最稳健，因为会忽略 Pydantic 类型检查)
        try:
            if hasattr(resp, "model_dump_json"):
                # Pydantic v2 JSON 序列化
                return json.loads(resp.model_dump_json())
            if hasattr(resp, "json") and callable(resp.json):
                # Pydantic v1 / 传统 JSON 方法
                return json.loads(resp.json()) # type: ignore
        except Exception:
            pass

        # 策略 2: 尝试标准 model_dump (如果 JSON 失败)
        try:
            if hasattr(resp, "model_dump"):
                return resp.model_dump(mode='json') # mode='json' 强制转换类型
            if hasattr(resp, "dict"):
                return resp.dict()
        except Exception:
            pass

        # 策略 3: 手动暴力提取 (Ultimate Fallback)
        # 当 litellm 对象损坏无法 dump 时，手动提取关键字段
        try:
            data = {
                "id": getattr(resp, "id", ""),
                "object": getattr(resp, "object", "chat.completion"),
                "created": getattr(resp, "created", 0),
                "model": getattr(resp, "model", ""),
                "choices": [],
                "usage": None
            }
            
            # 提取 Choices
            raw_choices = getattr(resp, "choices", [])
            if isinstance(raw_choices, list):
                for c in raw_choices:
                    c_dict = {
                        "index": getattr(c, "index", 0),
                        "finish_reason": getattr(c, "finish_reason", "stop"),
                        "message": {}
                    }
                    # 提取 Message
                    msg = getattr(c, "message", None)
                    if msg:
                        c_dict["message"] = {
                            "role": getattr(msg, "role", "assistant"),
                            "content": getattr(msg, "content", None),
                            "tool_calls": getattr(msg, "tool_calls", None)
                        }
                    data["choices"].append(c_dict)
            
            # 提取 Usage
            usage = getattr(resp, "usage", None)
            if usage:
                data["usage"] = {
                    "prompt_tokens": getattr(usage, "prompt_tokens", 0),
                    "completion_tokens": getattr(usage, "completion_tokens", 0),
                    "total_tokens": getattr(usage, "total_tokens", 0)
                }
            
            return data
            
        except Exception as e:
            logger.error("Manual response extraction failed", error=str(e))
            # 如果到了这一步，说明对象完全不可读，返回空结构防止 crash
            return {"choices": [], "model": "unknown-error"}

    async def acompletion(self, messages: List[Dict[str, Any]], **kwargs: Any) -> CompletionResponse:
        try:
            params = self._get_params(messages, stream=False, **kwargs)
            resp = await litellm.acompletion(**params)
            
            # 使用新的清洗逻辑
            data = self._sanitize_response(resp)
            
            return CompletionResponse(**data)

        except (ValidationError, TypeError) as e:
            logger.error("Failed to parse model response", error=str(e))
            raise ModelError(f"Response parsing failed: {e}") from e
        except Exception as e:
            self._handle_litellm_error(e)
            raise

    async def astream(self, messages: List[Dict[str, Any]], **kwargs: Any) -> AsyncIterator[StreamChunk]: # type: ignore
        try:
            params = self._get_params(messages, stream=True, **kwargs)
            response_iterator = await litellm.acompletion(**params)
            
            async for chunk in response_iterator: # type: ignore
                # 流式 Chunk 同样需要清洗
                data = self._sanitize_response(chunk)
                # 确保 data 不为空且有 choices (防止空包)
                if data and data.get("choices") or data.get("id"):
                    yield StreamChunk(**data)

        except Exception as e:
            self._handle_litellm_error(e)

    def _handle_litellm_error(self, e: Exception) -> None:
        """统一异常映射"""
        msg = str(e)
        error_type = type(e).__name__
        
        if "AuthenticationError" in error_type:
            raise ModelError(f"Authentication failed: {msg}", error_code="AUTH_ERROR") from e
        if "RateLimitError" in error_type:
            raise ModelError(f"Rate limit exceeded: {msg}", error_code="RATE_LIMIT") from e
        if "ContextWindowExceededError" in error_type:
             raise ModelError(f"Context window exceeded: {msg}", error_code="CONTEXT_LIMIT") from e
             
        raise ModelError(f"Model execution failed ({error_type}): {msg}") from e
```

[42] gecko/plugins/models/config.py
```python
# gecko/plugins/models/config.py
from __future__ import annotations

from typing import Any, Dict, Optional

from pydantic import BaseModel, Field


class ModelConfig(BaseModel):
    """
    模型通用配置对象
    """
    model_name: str = Field(..., description="模型名称，如 'gpt-4o', 'ollama/llama3'")
    
    # 驱动类型 (不再是 Enum，支持字符串扩展)
    driver_type: str = Field(default="litellm", description="驱动类型: litellm, openai_native, etc.")
    
    # 连接配置
    api_key: Optional[str] = Field(default=None, description="API Key")
    base_url: Optional[str] = Field(default=None, description="API Base URL (本地模型必填)")
    timeout: float = Field(default=60.0, description="请求超时时间(秒)")
    max_retries: int = Field(default=2, description="最大重试次数")
    
    # 运行时参数透传 (Provider Specific)
    extra_kwargs: Dict[str, Any] = Field(default_factory=dict, description="透传给底层驱动的额外参数")
    
    # 能力标识 (Capability Flags)
    supports_vision: bool = Field(default=False, description="是否支持视觉输入")
    supports_audio: bool = Field(default=False, description="是否支持音频输入")
    supports_function_calling: bool = Field(default=True, description="是否支持工具调用")
```

[43] gecko/plugins/models/drivers/litellm_driver.py
```python
# gecko/plugins/models/drivers/litellm_driver.py
from __future__ import annotations

from typing import Any, AsyncIterator, Dict, List

import litellm

from gecko.core.exceptions import ModelError
from gecko.core.logging import get_logger
from gecko.core.protocols import CompletionResponse, StreamChunk
from gecko.plugins.models.adapter import LiteLLMAdapter
from gecko.plugins.models.base import BaseChatModel
from gecko.plugins.models.registry import register_driver

logger = get_logger(__name__)


@register_driver("litellm")
class LiteLLMDriver(BaseChatModel):
    """
    LiteLLM 通用驱动
    
    特点：
    1. 兼容性最强（支持 OpenAI, Zhipu, Ollama 等）。
    2. 使用 LiteLLMAdapter 进行响应清洗，解决 Pydantic 兼容性问题。
    """

    def _get_params(self, messages: List[Dict[str, Any]], stream: bool, **kwargs: Any) -> Dict[str, Any]:
        """构造调用参数"""
        params = {
            "model": self.config.model_name,
            "messages": messages,
            "timeout": self.config.timeout,
            "stream": stream,
            **self.config.extra_kwargs,
            **kwargs,
        }
        # 显式传递参数，确保并发安全
        if self.config.api_key:
            params["api_key"] = self.config.api_key
        if self.config.base_url:
            params["api_base"] = self.config.base_url
        return params

    async def acompletion(self, messages: List[Dict[str, Any]], **kwargs: Any) -> CompletionResponse:
        try:
            params = self._get_params(messages, stream=False, **kwargs)
            resp = await litellm.acompletion(**params)
            # 使用适配器清洗
            return LiteLLMAdapter.to_gecko_response(resp)
        except Exception as e:
            self._handle_error(e)
            raise

    async def astream(self, messages: List[Dict[str, Any]], **kwargs: Any) -> AsyncIterator[StreamChunk]: # type: ignore
        try:
            params = self._get_params(messages, stream=True, **kwargs)
            iterator = await litellm.acompletion(**params)
            
            async for chunk in iterator: # type: ignore
                # 使用适配器清洗
                gecko_chunk = LiteLLMAdapter.to_gecko_chunk(chunk)
                if gecko_chunk:
                    yield gecko_chunk
        except Exception as e:
            self._handle_error(e)

    def _handle_error(self, e: Exception) -> None:
        msg = str(e)
        err_name = type(e).__name__
        
        if "AuthenticationError" in err_name:
            raise ModelError(f"Auth failed: {msg}", error_code="AUTH_ERROR") from e
        if "RateLimitError" in err_name:
            raise ModelError(f"Rate limit: {msg}", error_code="RATE_LIMIT") from e
        if "ContextWindowExceededError" in err_name:
             raise ModelError(f"Context limit: {msg}", error_code="CONTEXT_LIMIT") from e
             
        raise ModelError(f"LiteLLM execution failed ({err_name}): {msg}") from e
```

[44] gecko/plugins/models/embedding.py
```python
# gecko/plugins/models/embedding.py
from __future__ import annotations

from typing import List

import litellm

from gecko.core.exceptions import ModelError
from gecko.plugins.models.adapter import safe_access
from gecko.plugins.models.base import BaseEmbedder


class LiteLLMEmbedder(BaseEmbedder):
    """
    基于 LiteLLM 的通用 Embedding 模型实现
    """

    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        try:
            # 预处理：移除换行符
            clean_texts = [t.replace("\n", " ") for t in texts]
            
            params = {
                "model": self.config.model_name,
                "input": clean_texts,
                "timeout": self.config.timeout,
                **self.config.extra_kwargs
            }
            
            if self.config.api_key:
                params["api_key"] = self.config.api_key
            if self.config.base_url:
                params["api_base"] = self.config.base_url

            resp = await litellm.aembedding(**params)
            
            # [修复] 使用 safe_access 进行健壮提取
            embeddings: List[List[float]] = []
            data_items = safe_access(resp, "data", [])
            
            if isinstance(data_items, list):
                 for item in data_items:
                     emb = safe_access(item, "embedding")
                     if emb:
                         embeddings.append(emb)
            
            return embeddings

        except Exception as e:
            raise ModelError(f"Embedding failed: {str(e)}") from e

    async def embed_query(self, text: str) -> List[float]:
        res = await self.embed_documents([text])
        if not res:
            raise ModelError("Embedding returned empty result")
        return res[0]
```

[45] gecko/plugins/models/factory.py
```python
# gecko/plugins/models/factory.py
from gecko.core.exceptions import ConfigurationError
from gecko.plugins.models.base import BaseChatModel
from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.registry import get_driver_class

# 导入默认驱动以触发注册
import gecko.plugins.models.drivers.litellm_driver  # noqa: F401


def create_model(config: ModelConfig) -> BaseChatModel:
    """
    根据配置创建模型实例 (工厂方法)
    """
    driver_cls = get_driver_class(config.driver_type)
    
    if not driver_cls:
        raise ConfigurationError(
            f"Driver '{config.driver_type}' not found. "
            f"Available drivers: {list(gecko.plugins.models.registry._DRIVER_REGISTRY.keys())}. " # type: ignore
            f"Have you registered it?"
        )
    
    return driver_cls(config)
```

[46] gecko/plugins/models/presets/ollama.py
```python
# gecko/plugins/models/presets/ollama.py
from __future__ import annotations

from typing import Any

from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.drivers.litellm_driver import LiteLLMDriver
from gecko.plugins.models.embedding import LiteLLMEmbedder


class OllamaChat(LiteLLMDriver):
    """Ollama 本地 Chat 模型预设"""
    
    def __init__(self, model: str = "llama3", base_url: str = "http://localhost:11434", **kwargs: Any):
        full_model_name = f"ollama/{model}" if not model.startswith("ollama/") else model
        
        config = ModelConfig(
            model_name=full_model_name,
            driver_type="litellm",
            base_url=base_url,
            api_key="ollama",
            timeout=kwargs.pop("timeout", 120.0),
            supports_function_calling=kwargs.pop("supports_function_calling", False),
            **kwargs
        )
        super().__init__(config)


class OllamaEmbedder(LiteLLMEmbedder):
    """Ollama 本地 Embedding 模型预设"""
    
    def __init__(
        self, 
        model: str = "nomic-embed-text", 
        base_url: str = "http://localhost:11434", 
        dimension: int = 768, 
        **kwargs: Any
    ):
        full_model_name = f"ollama/{model}" if not model.startswith("ollama/") else model
        
        config = ModelConfig(
            model_name=full_model_name,
            base_url=base_url,
            api_key="ollama",
            **kwargs
        )
        super().__init__(config, dimension=dimension)
```

[47] gecko/plugins/models/presets/openai.py
```python
# gecko/plugins/models/presets/openai.py
from __future__ import annotations

from typing import Any

from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.drivers.litellm_driver import LiteLLMDriver
from gecko.plugins.models.embedding import LiteLLMEmbedder


class OpenAIChat(LiteLLMDriver):
    """OpenAI Chat 模型预设"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o", **kwargs: Any):
        config = ModelConfig(
            model_name=model,
            driver_type="litellm",
            api_key=api_key,
            supports_vision=True,
            supports_function_calling=True,
            **kwargs
        )
        super().__init__(config)


class OpenAIEmbedder(LiteLLMEmbedder):
    """OpenAI Embedding 模型预设"""
    
    def __init__(self, api_key: str, model: str = "text-embedding-3-small", dimension: int = 1536, **kwargs: Any):
        config = ModelConfig(
            model_name=model,
            api_key=api_key,
            **kwargs
        )
        super().__init__(config, dimension=dimension)
```

[48] gecko/plugins/models/presets/zhipu.py
```python
# gecko/plugins/models/presets/zhipu.py
from __future__ import annotations

from typing import Any, AsyncIterator, Dict, List

from gecko.core.protocols import CompletionResponse, StreamChunk
from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.drivers.litellm_driver import LiteLLMDriver


class ZhipuChat(LiteLLMDriver):
    """
    智谱 AI (GLM) 预设
    
    继承自 LiteLLMDriver，复用其稳定的清洗逻辑和 OpenAI 协议。
    """
    
    def __init__(self, api_key: str, model: str = "glm-4-plus", **kwargs: Any):
        # 自动判断视觉支持
        is_vision = "v" in model.lower() or "vision" in model.lower()
        
        config = ModelConfig(
            model_name=model,
            # 显式指定使用 litellm 驱动
            driver_type="litellm",
            api_key=api_key,
            # 智谱官方 OpenAI 兼容接口
            base_url="https://open.bigmodel.cn/api/paas/v4/",
            # 强制 LiteLLM 使用 openai 协议处理
            extra_kwargs={"custom_llm_provider": "openai"},
            supports_vision=is_vision,
            supports_function_calling=True,
            **kwargs
        )
        super().__init__(config)
```

[49] gecko/plugins/models/registry.py
```python
# gecko/plugins/models/registry.py
from __future__ import annotations

from typing import Callable, Dict, Optional, Type, TypeVar

from gecko.core.logging import get_logger
from gecko.plugins.models.base import BaseChatModel

logger = get_logger(__name__)

_DRIVER_REGISTRY: Dict[str, Type[BaseChatModel]] = {}

# [修复] 定义一个泛型变量，限定范围是 BaseChatModel 的子类
T = TypeVar("T", bound=Type[BaseChatModel])

def register_driver(name: str) -> Callable[[T], T]:
    """
    装饰器：注册模型驱动
    
    使用泛型 T 确保类型推断能够透传：
    输入是具体的 Driver 类，返回的也是具体的 Driver 类（保留了具体实现信息）。
    """
    def decorator(cls: T) -> T:
        if name in _DRIVER_REGISTRY:
            logger.warning(f"Driver '{name}' already registered, overwriting with {cls.__name__}")
        _DRIVER_REGISTRY[name] = cls
        logger.debug(f"Registered model driver: {name}")
        return cls
    return decorator


def get_driver_class(name: str) -> Optional[Type[BaseChatModel]]:
    """获取驱动类"""
    return _DRIVER_REGISTRY.get(name)
```

[50] gecko/plugins/registry.py
```python
# gecko/plugins/registry.py  
  
"""  
占位：预留统一插件注册机制的位置。  
  
TODO:  
    - 整合 tools/storage 等注册器  
    - 支持入口点加载第三方插件  
"""  
```

[51] gecko/plugins/storage/__init__.py
```python
# gecko/plugins/storage/__init__.py
"""
Gecko Storage 插件系统

提供统一的接口用于访问 Session (KV) 和 Vector (RAG) 存储。
"""
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.factory import create_storage
from gecko.plugins.storage.interfaces import SessionInterface, VectorInterface
from gecko.plugins.storage.registry import register_storage

__all__ = [
    "AbstractStorage",
    "create_storage",
    "SessionInterface",
    "VectorInterface",
    "register_storage",
]
```

[52] gecko/plugins/storage/abc.py
```python
# gecko/plugins/storage/abc.py
"""
存储抽象基类

定义所有存储后端必须实现的生命周期方法。
确保所有插件都有统一的初始化和关闭流程。
"""
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any, Dict


class AbstractStorage(ABC):
    """
    存储后端抽象基类
    
    所有具体的存储实现（SQLite, Redis, Chroma 等）都必须继承此类，
    并实现异步的生命周期管理方法。
    """
    
    def __init__(self, url: str, **kwargs: Any):
        """
        初始化存储后端配置
        
        参数:
            url: 连接字符串 (例如: sqlite:///./data.db)
            **kwargs: 额外的配置参数
        """
        self.url = url
        self.config = kwargs
        self._is_initialized = False

    @abstractmethod
    async def initialize(self) -> None:
        """
        异步初始化
        
        用于建立数据库连接、创建表结构、检查索引等耗时 IO 操作。
        必须确保此方法是幂等的（多次调用不会出错）。
        """
        pass

    @abstractmethod
    async def shutdown(self) -> None:
        """
        异步关闭
        
        用于释放连接池、关闭文件句柄、清理临时资源。
        """
        pass
    
    @property
    def is_initialized(self) -> bool:
        """检查是否已初始化"""
        return self._is_initialized

    async def __aenter__(self):
        """支持上下文管理器"""
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """退出上下文时自动关闭"""
        await self.shutdown()
```

[53] gecko/plugins/storage/backends/__init__.py
```python
```

[54] gecko/plugins/storage/backends/chroma.py
```python
# gecko/plugins/storage/backends/chroma.py
"""
ChromaDB 存储后端

ChromaDB 是一个开源的嵌入式向量数据库。
本实现通过 ThreadOffloadMixin 将其同步 I/O 操作卸载到线程池，以避免阻塞 Event Loop。

核心特性：
1. **非阻塞架构**：所有数据库操作均在 Worker 线程执行。
2. **性能优化**：默认禁用 SentenceTransformer 模型加载，大幅降低内存占用和启动时间。
3. **双重接口**：同时支持 VectorInterface (RAG) 和 SessionInterface (KV)。

注意：
Session 数据将被序列化为 JSON 字符串存储在 Document 字段中，以绕过 Chroma Metadata 的类型限制。
"""
from __future__ import annotations

from typing import Any, Dict, List, Optional, TYPE_CHECKING

# 仅用于类型检查的导入
if TYPE_CHECKING:
    from chromadb import ClientAPI, Collection # type: ignore

from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.interfaces import SessionInterface, VectorInterface
from gecko.plugins.storage.mixins import (
    JSONSerializerMixin,
    ThreadOffloadMixin,
)
from gecko.plugins.storage.registry import register_storage
from gecko.plugins.storage.utils import parse_storage_url

logger = get_logger(__name__)


class IdentityEmbeddingFunction:
    """
    空实现的 Embedding Function。
    
    作用：
    1. 告诉 Chroma 不要加载默认的 SentenceTransformer 模型。
    2. 提供一个最小的合法向量 (dim=1) 以满足 Chroma 的非空检查。
    """
    
    def __call__(self, input: Any) -> Any:
        # [修复] 返回维度为 1 的哑向量，防止 Chroma 报错
        # 这里的向量值不重要，因为 Session 存储不依赖相似度搜索
        return [[0.0] for _ in input]

    def name(self) -> str:
        return "gecko_identity"


@register_storage("chroma")
class ChromaStorage(
    AbstractStorage,
    VectorInterface,
    SessionInterface,
    ThreadOffloadMixin,
    JSONSerializerMixin
):
    """
    基于 ChromaDB 的统一存储后端
    
    URL 示例:
        chroma://./chroma_db?collection=my_app
    """

    def __init__(self, url: str, **kwargs):
        super().__init__(url, **kwargs)
        scheme, path, params = parse_storage_url(url)
        
        self.persist_path = path
        self.collection_name = params.get("collection", "gecko_default")
        
        # 运行时对象 (初始化后可用)
        self.client: Optional[ClientAPI] = None
        self.vector_col: Optional[Collection] = None
        self.session_col: Optional[Collection] = None

    async def initialize(self) -> None:
        """
        异步初始化
        
        建立连接并确保集合存在。显式指定 embedding_function 以禁用 Chroma 内置模型。
        """
        if self.is_initialized:
            return

        def _init_sync():
            # 懒加载以加快启动速度
            import chromadb
            from chromadb.config import Settings

            logger.info("Initializing ChromaDB", path=self.persist_path)
            
            client = chromadb.PersistentClient(
                path=self.persist_path,
                settings=Settings(anonymized_telemetry=False)
            )
            
            ef : Any = IdentityEmbeddingFunction()
            
            # 1. 向量集合
            v_col = client.get_or_create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"},  # 强制使用余弦相似度
                embedding_function=ef
            )
            
            # 2. 会话集合 (KV 模式)
            s_col = client.get_or_create_collection(
                name=f"{self.collection_name}_sessions",
                embedding_function=ef
            )
            
            return client, v_col, s_col

        # 卸载到线程池执行
        self.client, self.vector_col, self.session_col = await self._run_sync(_init_sync)
        self._is_initialized = True

    async def shutdown(self) -> None:
        """关闭连接（清理引用）"""
        self.client = None
        self.vector_col = None
        self.session_col = None
        self._is_initialized = False

    # ==================== VectorInterface 实现 ====================

    async def upsert(self, documents: List[Dict[str, Any]]) -> None:
        """
        插入或更新向量
        
        documents结构:
        [
            {"id": "...", "embedding": [...], "text": "...", "metadata": {...}}
        ]
        """
        if not documents or not self.vector_col:
            return

        def _sync_upsert():
            # 这里的 self.vector_col 在初始化后一定存在，但静态检查不知道
            # 实际运行中有 is_initialized 保护
            if self.vector_col: 
                self.vector_col.upsert(
                    ids=[d["id"] for d in documents],
                    embeddings=[d["embedding"] for d in documents],
                    metadatas=[d.get("metadata", {}) for d in documents],
                    documents=[d.get("text", "") for d in documents]
                )

        await self._run_sync(_sync_upsert)

    async def search(
        self, 
        query_embedding: List[float], 
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        向量搜索
        
        返回结果包含 score (相似度，范围 0-1)
        """
        if not self.vector_col:
            return []

        def _sync_search():
            if not self.vector_col:
                return []
                
            results = self.vector_col.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                include=["metadatas", "documents", "distances"]
            )
            
            parsed_results = []
            if not results["ids"]:
                return []

            # Chroma 返回的是 list of lists
            count = len(results["ids"][0])
            for i in range(count):
                dist = results["distances"][0][i] # type: ignore
                # Cosine Distance 范围 [0, 2], 0 表示完全相同
                # 转换为相似度: 1.0 - distance
                score = max(0.0, 1.0 - dist)
                
                parsed_results.append({
                    "id": results["ids"][0][i],
                    "text": results["documents"][0][i], # type: ignore
                    "metadata": results["metadatas"][0][i], # type: ignore
                    "score": score
                })
            
            return parsed_results

        return await self._run_sync(_sync_search)

    # ==================== SessionInterface 实现 ====================

    async def get(self, session_id: str) -> Optional[Dict[str, Any]]:
        """
        获取会话状态
        
        原理：读取 document 字段中的 JSON 字符串并反序列化
        """
        if not self.session_col:
            return None

        def _sync_get():
            if not self.session_col:
                return None
            result = self.session_col.get(
                ids=[session_id],
                include=["documents"] # type: ignore
            )
            if result["ids"] and result["documents"]:
                return result["documents"][0]
            return None

        json_str = await self._run_sync(_sync_get)
        return self._deserialize(json_str)

    async def set(self, session_id: str, state: Dict[str, Any]) -> None:
        """
        保存会话状态
        
        原理：将 state 序列化为 JSON 存入 document 字段
        """
        if not self.session_col:
            return

        json_str = self._serialize(state)

        def _sync_set():
            if not self.session_col:
                return
            # 使用 upsert 覆盖旧值
            self.session_col.upsert(
                ids=[session_id],
                documents=[json_str],
                metadatas=[{"type": "session_state"}] # 必须有 metadata 或 embedding
            )

        await self._run_sync(_sync_set)

    async def delete(self, session_id: str) -> None:
        """删除会话"""
        if not self.session_col:
            return

        def _sync_delete():
            if not self.session_col:
                return
            self.session_col.delete(ids=[session_id])
        
        await self._run_sync(_sync_delete)
```

[55] gecko/plugins/storage/backends/lancedb.py
```python
# gecko/plugins/storage/backends/lancedb.py
"""
LanceDB 存储后端

LanceDB 是一个无服务器、基于 Arrow 的高性能向量数据库。
本实现通过 ThreadOffloadMixin 处理文件 I/O。

核心特性：
1. **自动建表**：首次 Upsert 时根据数据结构自动创建表。
2. **线程安全**：I/O 操作隔离。
"""
from __future__ import annotations

from typing import Any, Dict, List, Optional

from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.interfaces import VectorInterface
from gecko.plugins.storage.mixins import ThreadOffloadMixin
from gecko.plugins.storage.registry import register_storage
from gecko.plugins.storage.utils import parse_storage_url

logger = get_logger(__name__)


@register_storage("lancedb")
class LanceDBStorage(
    AbstractStorage,
    VectorInterface,
    ThreadOffloadMixin
):
    """
    LanceDB 向量存储
    
    URL 示例:
        lancedb://./data/lancedb?table=vectors
    """

    def __init__(self, url: str, **kwargs):
        super().__init__(url, **kwargs)
        scheme, path, params = parse_storage_url(url)
        
        self.db_path = path
        self.table_name = params.get("table", "gecko_vectors")
        self.embedding_dim = int(params.get("dim", 1536))
        
        self.db: Any = None
        self.table: Any = None

    async def initialize(self) -> None:
        """异步初始化：建立连接并打开表（如果存在）"""
        if self.is_initialized:
            return

        def _init_sync():
            import lancedb
            logger.info("Connecting to LanceDB", path=self.db_path)
            
            self.db = lancedb.connect(self.db_path)
            
            # 检查表是否存在
            if self.table_name in self.db.table_names():
                self.table = self.db.open_table(self.table_name)
                logger.debug(f"Opened existing table: {self.table_name}")
            else:
                self.table = None # 首次写入时创建
                logger.debug(f"Table {self.table_name} will be created on first upsert")

        await self._run_sync(_init_sync)
        self._is_initialized = True

    async def shutdown(self) -> None:
        """资源清理"""
        self.db = None
        self.table = None
        self._is_initialized = False

    async def upsert(self, documents: List[Dict[str, Any]]) -> None:
        """
        插入数据 (首次自动建表)
        
        如果表不存在，根据第一批数据推断 Schema 并创建表。
        """
        if not documents:
            return

        def _sync_upsert():
            # 构造数据
            # LanceDB 期望数据格式：[{"vector": ..., "id": ..., ...}]
            data = []
            for doc in documents:
                item = {
                    "id": doc["id"],
                    "vector": doc["embedding"],
                    "text": doc.get("text", ""),
                    "metadata": doc.get("metadata", {})
                }
                data.append(item)

            if self.table is None:
                # 首次创建表
                try:
                    self.table = self.db.create_table(self.table_name, data=data)
                    logger.info(f"Created LanceDB table: {self.table_name}")
                except Exception as e:
                    # 处理并发创建冲突：如果创建失败，再次尝试打开
                    if self.table_name in self.db.table_names():
                        self.table = self.db.open_table(self.table_name)
                        self.table.add(data)
                    else:
                        raise e
            else:
                # 追加数据
                self.table.add(data)

        await self._run_sync(_sync_upsert)

    async def search(
        self, 
        query_embedding: List[float], 
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """向量搜索"""
        if self.table is None:
            return []

        def _sync_search():
            if self.table is None:
                return []
                
            # LanceDB search API
            results = self.table.search(query_embedding).limit(top_k).to_list()
            
            parsed_results = []
            for r in results:
                # 获取距离，默认为 L2
                distance = r.get("_distance", 0.0)
                
                parsed_results.append({
                    "id": r["id"],
                    "text": r["text"],
                    "metadata": r["metadata"],
                    # 粗略转换，具体视 metric 而定。Gecko 约定 score 越高越相关。
                    "score": 1.0 / (1.0 + distance) 
                })
            return parsed_results

        return await self._run_sync(_sync_search)
```

[56] gecko/plugins/storage/backends/redis.py
```python
# gecko/plugins/storage/backends/redis.py
"""
Redis 存储后端

基于 redis-py (asyncio) 实现的高性能会话存储。
由于 Redis 客户端原生支持 asyncio，因此不需要使用 ThreadOffloadMixin。

核心特性：
1. **原生异步**：直接利用 asyncio Event Loop，性能极高。
2. **TTL 管理**：支持会话自动过期。
"""
from __future__ import annotations

from typing import Any, Dict, Optional

try:
    import redis.asyncio as redis
except ImportError:
    redis = None  # type: ignore

from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.interfaces import SessionInterface
from gecko.plugins.storage.mixins import JSONSerializerMixin
from gecko.plugins.storage.registry import register_storage
from gecko.plugins.storage.utils import parse_storage_url

logger = get_logger(__name__)


@register_storage("redis")
class RedisStorage(
    AbstractStorage,
    SessionInterface,
    JSONSerializerMixin
):
    """
    Redis 会话存储
    
    URL 示例: 
        redis://localhost:6379/0?ttl=3600
    """
    
    def __init__(self, url: str, **kwargs):
        super().__init__(url, **kwargs)
        if redis is None:
            raise ImportError(
                "Redis client not installed. Please install: pip install redis"
            )
        
        scheme, path, params = parse_storage_url(url)
        
        # 解析 TTL 参数 (默认 7 天)
        try:
            self.ttl = int(params.get("ttl", 3600 * 24 * 7))
        except ValueError:
            self.ttl = 3600 * 24 * 7
            
        self.prefix = kwargs.get("prefix", "gecko:session:")
        
        # 客户端引用 (延迟初始化)
        self.client: Optional[redis.Redis] = None # type: ignore

    async def initialize(self) -> None:
        """异步初始化：建立连接并测试连通性"""
        if self.is_initialized:
            return

        logger.info("Connecting to Redis", url=self.url)
        
        # redis-py 的 from_url 会复用连接池
        # decode_responses=True 让 Redis 直接返回 str 而不是 bytes
        self.client = redis.from_url( # type: ignore
            self.url,
            decode_responses=True,
            encoding="utf-8"
        )
        
        try:
            # Ping 测试确保连接可用
            if self.client:
                await self.client.ping()
            self._is_initialized = True
            logger.debug("Redis connected successfully")
        except Exception as e:
            logger.error("Failed to connect to Redis", error=str(e))
            # 连接失败时确保清理资源
            await self.shutdown()
            raise

    async def shutdown(self) -> None:
        """关闭连接"""
        if self.client:
            await self.client.aclose()
            self.client = None
        self._is_initialized = False

    # ==================== SessionInterface 实现 ====================

    async def get(self, session_id: str) -> Optional[Dict[str, Any]]:
        if not self.client:
            raise RuntimeError("RedisStorage not initialized")
            
        key = f"{self.prefix}{session_id}"
        try:
            data = await self.client.get(key)
            return self._deserialize(data)
        except Exception as e:
            logger.error("Redis get failed", session_id=session_id, error=str(e))
            raise

    async def set(self, session_id: str, state: Dict[str, Any]) -> None:
        if not self.client:
            raise RuntimeError("RedisStorage not initialized")
            
        key = f"{self.prefix}{session_id}"
        json_str = self._serialize(state)
        
        try:
            # setex: 设置值并指定过期时间 (原子操作)
            await self.client.setex(key, self.ttl, json_str)
        except Exception as e:
            logger.error("Redis set failed", session_id=session_id, error=str(e))
            raise

    async def delete(self, session_id: str) -> None:
        if not self.client:
            raise RuntimeError("RedisStorage not initialized")
            
        key = f"{self.prefix}{session_id}"
        try:
            await self.client.delete(key)
        except Exception as e:
            logger.error("Redis delete failed", session_id=session_id, error=str(e))
            raise
```

[57] gecko/plugins/storage/backends/sqlite.py
```python
# gecko/plugins/storage/backends/sqlite.py
"""
SQLite 存储后端 (高并发优化版)

优化策略：
1. Mixin 组合：继承 ThreadOffloadMixin 防止阻塞 Loop，继承 AtomicWriteMixin 防止写冲突。
2. WAL 模式：开启 Write-Ahead Logging，大幅提升读写并发性能。
3. 线程安全：配置 check_same_thread=False 以支持在线程池中使用连接。
"""
from __future__ import annotations

import os
from pathlib import Path
from typing import Any, Dict, Optional

from sqlalchemy import text
from sqlmodel import Field, Session, SQLModel, create_engine, select

from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.interfaces import SessionInterface
from gecko.plugins.storage.mixins import (
    AtomicWriteMixin,
    JSONSerializerMixin,
    ThreadOffloadMixin,
)
from gecko.plugins.storage.utils import parse_storage_url

logger = get_logger(__name__)


class SessionModel(SQLModel, table=True):
    """SQLModel 表定义"""
    __tablename__ = "gecko_sessions" # type: ignore
    session_id: str = Field(primary_key=True)
    state_json: str = Field(default="{}")


class SQLiteStorage(
    AbstractStorage,
    SessionInterface,
    ThreadOffloadMixin,
    AtomicWriteMixin,
    JSONSerializerMixin
):
    def __init__(self, url: str, **kwargs):
        super().__init__(url, **kwargs)
        
        scheme, path, params = parse_storage_url(url)
        
        # 处理文件路径
        self.db_path = path
        self.is_memory = path == ":memory:"
        
        if not self.is_memory:
            # 确保父目录存在
            db_file = Path(path)
            if not db_file.parent.exists():
                db_file.parent.mkdir(parents=True, exist_ok=True)
        
        # 构造 SQLAlchemy URL
        # 注意：sqlite://<path> (3 slashes for relative, 4 for absolute)
        # 这里简化处理，假设 utils 解析出的 path 已经是文件系统路径
        if self.is_memory:
            sqlalchemy_url = "sqlite:///:memory:"
        else:
            sqlalchemy_url = f"sqlite:///{self.db_path}"

        # 创建引擎
        # check_same_thread=False: 允许在不同线程使用连接（必须，因为用了 ThreadOffload）
        # timeout=30: 增加 SQLite 忙等待时间
        self.engine = create_engine(
            sqlalchemy_url,
            connect_args={"check_same_thread": False, "timeout": 30},
            # echo=True # 调试时开启
        )

    async def initialize(self) -> None:
        """异步初始化：建表 + 开启 WAL"""
        if self.is_initialized:
            return

        # 1. 建表 (在线程中执行)
        await self._run_sync(SQLModel.metadata.create_all, self.engine)
        
        # 2. 开启 WAL 模式 (提升并发)
        # 内存数据库不需要 WAL
        if not self.is_memory:
            def _enable_wal():
                with self.engine.connect() as conn:
                    conn.execute(text("PRAGMA journal_mode=WAL;"))
                    conn.execute(text("PRAGMA synchronous=NORMAL;"))
            
            await self._run_sync(_enable_wal)
            logger.debug("SQLite WAL mode enabled", path=self.db_path)
            
        self._is_initialized = True
        logger.info("SQLite storage initialized", url=self.url)

    async def shutdown(self) -> None:
        """释放资源"""
        self.engine.dispose()
        self._is_initialized = False

    async def get(self, session_id: str) -> Optional[Dict[str, Any]]:
        """获取 Session"""
        def _sync_get():
            with Session(self.engine) as session:
                statement = select(SessionModel).where(
                    SessionModel.session_id == session_id
                )
                result = session.exec(statement).first()
                return result.state_json if result else None

        # 卸载到线程池读取
        json_str = await self._run_sync(_sync_get)
        return self._deserialize(json_str)

    async def set(self, session_id: str, state: Dict[str, Any]) -> None:
        """保存 Session"""
        json_str = self._serialize(state)

        def _sync_set():
            with Session(self.engine) as session:
                # 查询是否存在
                statement = select(SessionModel).where(
                    SessionModel.session_id == session_id
                )
                existing = session.exec(statement).first()
                
                if existing:
                    existing.state_json = json_str
                    session.add(existing)
                else:
                    new_rec = SessionModel(
                        session_id=session_id, 
                        state_json=json_str
                    )
                    session.add(new_rec)
                
                session.commit()

        # 写锁 + 线程卸载
        async with self.write_guard():
            await self._run_sync(_sync_set)

    async def delete(self, session_id: str) -> None:
        """删除 Session"""
        def _sync_delete():
            with Session(self.engine) as session:
                statement = select(SessionModel).where(
                    SessionModel.session_id == session_id
                )
                result = session.exec(statement).first()
                if result:
                    session.delete(result)
                    session.commit()
        
        # 写锁 + 线程卸载
        async with self.write_guard():
            await self._run_sync(_sync_delete)
```

[58] gecko/plugins/storage/factory.py
```python
# gecko/plugins/storage/factory.py
"""
存储工厂

负责解析 URL，自动加载对应的后端模块，实例化并初始化存储对象。
实现了懒加载机制、注册表刷新和回退扫描机制，确保在各种环境下都能正确加载后端。
"""
from __future__ import annotations

import importlib
import inspect
import sys
from typing import Any, Optional, Type

from gecko.core.exceptions import ConfigurationError
from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
# [重要] 导入模块本身，防止闭包引用陈旧的 _STORAGE_REGISTRY 字典
import gecko.plugins.storage.registry as storage_registry
from gecko.plugins.storage.utils import parse_storage_url

logger = get_logger(__name__)

# 模块映射表：Scheme -> Module Path
# 用于懒加载，防止 Import Hell
_BACKEND_MODULES = {
    "sqlite": "gecko.plugins.storage.backends.sqlite",
    "redis": "gecko.plugins.storage.backends.redis",
    "chroma": "gecko.plugins.storage.backends.chroma",
    "lancedb": "gecko.plugins.storage.backends.lancedb",
    "postgres": "gecko.plugins.storage.backends.postgres",
    "qdrant": "gecko.plugins.storage.backends.qdrant",
    "milvus": "gecko.plugins.storage.backends.milvus",
}


async def create_storage(url: str, **kwargs: Any) -> AbstractStorage:
    """
    创建并初始化存储后端
    
    流程：
    1. 解析 URL 获取协议 (scheme)。
    2. 检查注册表是否已有对应类。
    3. 如果没有，尝试动态导入对应的模块。
    4. 如果导入后仍未注册（常见于测试 Mock 环境），手动扫描模块寻找类。
    5. 实例化并异步初始化。
    
    参数:
        url: 存储 URL (e.g., "sqlite:///data.db")
        **kwargs: 传递给存储后端的额外参数
    
    返回:
        已初始化的存储对象
        
    异常:
        ConfigurationError: 无法加载或初始化存储后端
    """
    try:
        scheme, _, _ = parse_storage_url(url)
        # 处理特殊变体 (如 postgres+pgvector -> postgres)
        clean_scheme = scheme.split("+")[0]
    except Exception as e:
        raise ConfigurationError(f"Invalid storage URL: {e}") from e

    # 1. 尝试从注册表获取
    cls: Optional[Type[AbstractStorage]] = storage_registry.get_storage_class(scheme)

    # 2. 如果未注册，尝试动态导入
    if not cls:
        module_path = _BACKEND_MODULES.get(clean_scheme)
        if not module_path:
            raise ConfigurationError(
                f"Unknown storage scheme: '{scheme}'. "
                f"Supported: {list(_BACKEND_MODULES.keys())}"
            )
        
        try:
            logger.debug("Lazy loading storage backend", module=module_path)
            
            # 如果模块已在 sys.modules 中（可能是之前的测试加载过），强制重载以触发注册副作用
            if module_path in sys.modules:
                module = importlib.reload(sys.modules[module_path])
            else:
                module = importlib.import_module(module_path)
                
        except ImportError as e:
            raise ConfigurationError(
                f"Failed to load storage backend for '{scheme}'.\n"
                f"Missing dependency? Try installing: pip install gecko-ai[{clean_scheme}]\n"
                f"Error: {e}"
            ) from e
        
        # 3. 再次尝试从注册表获取 (正常流程)
        cls = storage_registry.get_storage_class(scheme)
        
        # 4. 回退机制：如果装饰器注册失败（常见于测试环境 patch 导致注册表不一致），
        # 手动扫描模块寻找 AbstractStorage 的子类
        if not cls:
            logger.warning(
                f"Registry lookup failed for {scheme}, scanning module {module_path}..."
            )
            for name, obj in inspect.getmembers(module):
                if (inspect.isclass(obj) 
                    and issubclass(obj, AbstractStorage) 
                    and obj is not AbstractStorage):
                    
                    cls = obj
                    # 手动补注册，避免下次扫描
                    storage_registry._STORAGE_REGISTRY[scheme] = cls
                    logger.info(f"Manually registered {name} for {scheme}")
                    break
            
        if not cls:
            # 调试信息
            current_keys = list(storage_registry._STORAGE_REGISTRY.keys())
            raise ConfigurationError(
                f"Module '{module_path}' imported but no storage registered for '{scheme}'.\n"
                f"Current registry keys: {current_keys}"
            )

    # 5. 实例化与初始化
    # 预先定义 instance 防止 unbound variable error
    instance: Optional[AbstractStorage] = None
    
    try:
        instance = cls(url, **kwargs)
        await instance.initialize()
        return instance
    except Exception as e:
        # 如果初始化失败，尝试清理资源
        if instance:
            await instance.shutdown()
        raise ConfigurationError(f"Failed to initialize {cls.__name__}: {e}") from e
```

[59] gecko/plugins/storage/interfaces.py
```python
# gecko/plugins/storage/interfaces.py
"""
业务接口定义

定义 Session（会话存储）和 Vector（向量存储）的标准行为。
"""
from __future__ import annotations

from abc import abstractmethod
from typing import Any, Dict, List, Optional, Protocol, runtime_checkable


@runtime_checkable
class SessionInterface(Protocol):
    """
    Session 存储接口协议
    
    负责 Agent 的短期记忆（Conversation History）和状态（State）的持久化。
    """
    
    @abstractmethod
    async def get(self, session_id: str) -> Optional[Dict[str, Any]]:
        """
        获取会话状态
        
        参数:
            session_id: 会话唯一标识
            
        返回:
            状态字典，如果不存在返回 None
        """
        ...

    @abstractmethod
    async def set(self, session_id: str, state: Dict[str, Any]) -> None:
        """
        设置/更新会话状态
        
        参数:
            session_id: 会话唯一标识
            state: 要保存的状态字典
        """
        ...

    @abstractmethod
    async def delete(self, session_id: str) -> None:
        """
        删除会话
        
        参数:
            session_id: 会话唯一标识
        """
        ...


@runtime_checkable
class VectorInterface(Protocol):
    """
    Vector 存储接口协议 (RAG 用)
    
    负责文档的向量存储与检索。
    """
    
    @abstractmethod
    async def upsert(self, documents: List[Dict[str, Any]]) -> None:
        """
        插入或更新向量文档
        
        参数:
            documents: 文档列表，每项需包含 id, embedding, text, metadata
        """
        ...

    @abstractmethod
    async def search(self, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]:
        """
        向量相似度搜索
        
        参数:
            query_embedding: 查询向量
            top_k: 返回结果数量
            
        返回:
            包含 text, metadata, score 的结果列表
        """
        ...
```

[60] gecko/plugins/storage/mixins.py
```python
# gecko/plugins/storage/mixins.py
"""
存储功能混入类 (Mixins)

包含解决异步阻塞、并发冲突和数据序列化的通用逻辑。
这是本次重构的核心，用于修复 Event Loop 阻塞问题。
"""
from __future__ import annotations

import asyncio
import json
from contextlib import asynccontextmanager
from functools import partial
from typing import Any, Callable, TypeVar, Optional

from anyio import to_thread

from gecko.core.logging import get_logger

logger = get_logger(__name__)

T = TypeVar("T")


class ThreadOffloadMixin:
    """
    [核心] 线程卸载混入类
    
    将同步的 IO 操作（如 sqlite3, chromadb, pandas 操作）卸载到
    独立的线程池中执行，防止阻塞主线程的 Event Loop。
    
    原理: 使用 anyio.to_thread.run_sync
    """
    
    async def _run_sync(self, func: Callable[..., T], *args: Any, **kwargs: Any) -> T:
        """
        在线程池中执行同步函数
        
        参数:
            func: 同步函数
            *args, **kwargs: 传递给函数的参数
            
        返回:
            函数执行结果
        """
        if kwargs:
            func = partial(func, **kwargs)
        
        return await to_thread.run_sync(func, *args)


class AtomicWriteMixin:
    """
    [核心] 原子写混入类
    
    提供应用层的异步写锁，防止文件型数据库（如 SQLite, LanceDB）
    在并发写入时发生 'database is locked' 错误。
    
    修复：使用 Lazy Loading 替代 __init__，防止多重继承时初始化链断裂。
    """
    
    _write_lock: Optional[asyncio.Lock] = None

    @property
    def write_lock(self) -> asyncio.Lock:
        """懒加载获取锁"""
        # 注意：这里需要处理 _write_lock 可能不存在的情况（虽然类属性已定义）
        if getattr(self, "_write_lock", None) is None:
            self._write_lock = asyncio.Lock()
        return self._write_lock # type: ignore

    @asynccontextmanager
    async def write_guard(self):
        """
        写操作保护上下文
        
        示例:
            async with self.write_guard():
                await self._run_sync(sync_write_func)
        """
        # 使用 property 获取锁
        async with self.write_lock:
            yield


class JSONSerializerMixin:
    """
    JSON 序列化混入类
    
    提供标准化的数据序列化/反序列化方法。
    """
    
    def _serialize(self, data: Any) -> str:
        """序列化为 JSON 字符串"""
        try:
            # ensure_ascii=False 减少体积并保持中文可读性
            return json.dumps(data, ensure_ascii=False)
        except (TypeError, ValueError) as e:
            logger.error("Serialization failed", error=str(e))
            raise

    def _deserialize(self, data: str | bytes | None) -> Any:
        """从 JSON 字符串反序列化"""
        if not data:
            return None
        try:
            return json.loads(data)
        except json.JSONDecodeError as e:
            logger.error("Deserialization failed", error=str(e))
            return None
```

[61] gecko/plugins/storage/registry.py
```python
# gecko/plugins/storage/registry.py
"""
存储插件注册器

负责管理 URL Scheme 到存储后端类的映射。
采用装饰器模式进行注册。
"""
from __future__ import annotations

from typing import Callable, Dict, Type

from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage

logger = get_logger(__name__)

# 存储后端类注册表
# Key: URL scheme (e.g., "sqlite", "redis")
# Value: Storage Class
_STORAGE_REGISTRY: Dict[str, Type[AbstractStorage]] = {}


def register_storage(scheme: str) -> Callable[[Type[AbstractStorage]], Type[AbstractStorage]]:
    """
    装饰器：注册存储后端实现
    
    参数:
        scheme: URL 协议前缀 (如 'sqlite', 'redis')
    
    示例:
        @register_storage("redis")
        class RedisStorage(AbstractStorage):
            ...
    """
    def decorator(cls: Type[AbstractStorage]) -> Type[AbstractStorage]:
        if scheme in _STORAGE_REGISTRY:
            logger.warning(
                "Storage scheme already registered, overwriting",
                scheme=scheme,
                existing=_STORAGE_REGISTRY[scheme].__name__,
                new=cls.__name__
            )
        
        _STORAGE_REGISTRY[scheme] = cls
        logger.debug("Registered storage backend", scheme=scheme, cls=cls.__name__)
        return cls
    
    return decorator


def get_storage_class(scheme: str) -> Type[AbstractStorage] | None:
    """获取已注册的存储类"""
    return _STORAGE_REGISTRY.get(scheme)
```

[62] gecko/plugins/storage/utils.py
```python
# gecko/plugins/storage/utils.py
"""
Storage 插件工具函数

提供统一的 URL 解析和验证逻辑。
"""
from __future__ import annotations

from urllib.parse import parse_qs, urlparse
from typing import Dict, Tuple, Optional


def parse_storage_url(url: str) -> Tuple[str, str, Dict[str, str]]:
    """
    解析存储 URL
    
    格式：scheme://path?param1=value1&param2=value2
    
    返回：(scheme, path, params)
    
    示例:
        "sqlite:///./data.db" -> ("sqlite", "./data.db", {})
        "sqlite://:memory:" -> ("sqlite", ":memory:", {})
    """
    if "://" not in url:
        raise ValueError(f"Invalid storage URL: '{url}'. Must include scheme.")
    
    parsed = urlparse(url)
    scheme = parsed.scheme
    
    # 解析路径
    if scheme == "sqlite":
        # 特殊处理 sqlite
        # sqlite:///foo.db -> path = /foo.db (urlparse behavior)
        # 我们需要去掉开头的 / 变成相对路径，或者保留绝对路径
        if parsed.netloc:
            # sqlite://:memory: -> netloc=':memory:'
            path = parsed.netloc
        else:
            # sqlite:///./data.db -> path='/./data.db'
            path = parsed.path
            if path.startswith("/") and len(path) > 1 and path[1] == ".":
                # 修正相对路径 /./data.db -> ./data.db
                path = path[1:]
            elif path.startswith("/"):
                 # 绝对路径保持不变，或者根据 OS 调整
                 # 这里简化处理，对于 Windows 可能需要更复杂的逻辑
                 pass
    else:
        path = f"{parsed.netloc}{parsed.path}"

    # 解析参数
    params: Dict[str, str] = {}
    if parsed.query:
        query_dict = parse_qs(parsed.query)
        params = {k: v[0] for k, v in query_dict.items()}
    
    return scheme, path, params
```

[63] gecko/plugins/tools/__init__.py
```python
# gecko/plugins/tools/__init__.py
"""
Gecko 工具插件系统

提供工具的定义、注册和发现机制。

核心组件：
- BaseTool: 所有工具的基类
- ToolResult: 标准化执行结果
- register_tool: 工具注册装饰器
- load_tool: 工具加载工厂
- ToolRegistry: 注册表管理类
"""
from gecko.plugins.tools.base import BaseTool, ToolResult
from gecko.plugins.tools.registry import ToolRegistry, load_tool, register_tool

__all__ = [
    "BaseTool",
    "ToolResult",
    "ToolRegistry",
    "register_tool",
    "load_tool",
]
```

[64] gecko/plugins/tools/base.py
```python
# gecko/plugins/tools/base.py
"""
工具基类定义

核心功能：
1. 定义统一的工具接口 BaseTool
2. 强制参数校验 (Pydantic)
3. 自动生成 OpenAI Function Schema
4. 统一的错误处理与结果封装
"""
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any, Dict, Type, Union

from pydantic import BaseModel, Field, ValidationError

from gecko.core.utils import ensure_awaitable


class ToolResult(BaseModel):
    """工具执行结果封装"""
    content: str = Field(..., description="工具执行的文本输出")
    is_error: bool = Field(default=False, description="是否执行出错")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="额外的元数据")


class BaseTool(BaseModel, ABC):
    """
    工具抽象基类
    
    所有自定义工具必须继承此类，并提供 args_schema 用于参数校验。
    
    示例:
        ```python
        class MyArgs(BaseModel):
            query: str
            
        class MyTool(BaseTool):
            name = "my_tool"
            description = "My awesome tool"
            args_schema = MyArgs
            
            async def _run(self, args: MyArgs) -> ToolResult:
                return ToolResult(content=f"Echo: {args.query}")
        ```
    """
    name: str = Field(..., description="工具唯一标识名称 (e.g., 'calculator')")
    description: str = Field(..., description="工具功能描述，用于 LLM 决策")
    
    # 排除 args_schema 不参与 BaseTool 本身的序列化，仅用于元编程
    args_schema: Type[BaseModel] = Field(..., exclude=True, description="参数定义的 Pydantic 模型类")

    @property
    def openai_schema(self) -> Dict[str, Any]:
        """
        自动生成 OpenAI Function Calling Schema
        """
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": self.args_schema.model_json_schema(),
            }
        }
    
    # 兼容旧代码的属性
    @property
    def parameters(self) -> Dict[str, Any]:
        return self.args_schema.model_json_schema()

    async def execute(self, arguments: Dict[str, Any]) -> ToolResult:
        """
        执行工具（模板方法）
        
        负责：
        1. 参数校验
        2. 异常捕获
        3. 调用具体的 _run 实现
        """
        try:
            # 1. Pydantic 校验
            validated_args = self.args_schema(**arguments)
        except ValidationError as e:
            return ToolResult(
                content=f"参数校验错误: {str(e)}",
                is_error=True
            )
        except Exception as e:
            return ToolResult(
                content=f"参数解析错误: {str(e)}",
                is_error=True
            )

        try:
            # 2. 执行业务逻辑 (支持同步/异步)
            result = await ensure_awaitable(self._run, validated_args)
            
            # 3. 结果标准化
            if isinstance(result, ToolResult):
                return result
            return ToolResult(content=str(result))
            
        except Exception as e:
            return ToolResult(
                content=f"工具执行内部错误: {str(e)}",
                is_error=True
            )

    @abstractmethod
    async def _run(self, args: BaseModel) -> Union[ToolResult, str]:
        """
        工具的具体实现逻辑
        
        参数:
            args: 已校验的 Pydantic 对象
        """
        pass
```

[65] gecko/plugins/tools/registry.py
```python
# gecko/plugins/tools/registry.py
"""
工具注册表

提供工具的自动发现、注册与工厂化创建能力。
"""
from __future__ import annotations

from typing import Dict, Type, List, Any, Optional

from gecko.core.logging import get_logger
from gecko.plugins.tools.base import BaseTool

logger = get_logger(__name__)


class ToolRegistry:
    """全局工具注册中心"""
    
    _registry: Dict[str, Type[BaseTool]] = {}

    @classmethod
    def register(cls, name: str):
        """
        装饰器：注册工具类
        
        示例:
            @register_tool("my_tool")
            class MyTool(BaseTool): ...
        """
        def decorator(tool_cls: Type[BaseTool]):
            if not issubclass(tool_cls, BaseTool):
                raise TypeError(f"Registered class {tool_cls.__name__} must inherit from BaseTool")
            
            if name in cls._registry:
                logger.warning(f"Tool '{name}' is being overwritten in registry.")
            
            cls._registry[name] = tool_cls
            logger.debug(f"Tool registered: {name} -> {tool_cls.__name__}")
            return tool_cls
        return decorator

    @classmethod
    def load_tool(cls, name: str, **kwargs: Any) -> BaseTool:
        """
        工厂方法：根据名称加载并实例化工具
        
        参数:
            name: 工具名称
            **kwargs: 传递给工具构造函数的参数
            
        异常:
            ValueError: 工具未找到
        """
        if name not in cls._registry:
            raise ValueError(f"Tool '{name}' not found in registry. Available: {list(cls._registry.keys())}")
        
        tool_cls = cls._registry[name]
        try:
            # 实例化工具
            # 注意：BaseTool 是 Pydantic 模型，kwargs 会被 validate
            return tool_cls(**kwargs)
        except Exception as e:
            raise ValueError(f"Failed to instantiate tool '{name}': {e}") from e

    @classmethod
    def list_tools(cls) -> List[str]:
        """列出所有已注册工具"""
        return list(cls._registry.keys())


# 便捷导出
register_tool = ToolRegistry.register
load_tool = ToolRegistry.load_tool
```

[66] gecko/plugins/tools/standard/__init__.py
```python
# gecko/plugins/tools/standard/__init__.py
"""
Gecko 标准工具库

包含一组经过安全审查和优化的内置工具。
导入此模块时，会自动将工具注册到 ToolRegistry。
"""
from gecko.plugins.tools.standard.calculator import CalculatorTool
from gecko.plugins.tools.standard.duckduckgo import DuckDuckGoSearchTool

# 可以在此定义 lazy_load 逻辑，目前为了简单直接导入以触发注册
__all__ = [
    "CalculatorTool",
    "DuckDuckGoSearchTool",
]
```

[67] gecko/plugins/tools/standard/calculator.py
```python
# gecko/plugins/tools/standard/calculator.py
"""
安全计算器工具

基于 AST 解析的安全数学表达式计算，防止代码注入。
"""
from __future__ import annotations

import ast
import math
import operator
from typing import Type, Union, Dict

from pydantic import BaseModel, Field

from gecko.plugins.tools.base import BaseTool, ToolResult
from gecko.plugins.tools.registry import register_tool


class CalculatorArgs(BaseModel):
    expression: str = Field(
        ..., 
        description="数学表达式，支持加减乘除、幂运算及常用数学函数 (sqrt, log, sin, etc.)。例如: '2 + 2 * sqrt(4)'"
    )


@register_tool("calculator")
class CalculatorTool(BaseTool):
    name: str = "calculator"
    description: str = "用于执行精确的数学计算。"
    args_schema: Type[BaseModel] = CalculatorArgs

    async def _run(self, args: CalculatorArgs) -> ToolResult: # type: ignore
        expr = args.expression.strip()
        
        # 长度限制防止 DoS
        if len(expr) > 500:
             return ToolResult(content="错误：表达式过长", is_error=True)

        try:
            result = self._safe_eval(expr)
            return ToolResult(content=str(result))
        except Exception as e:
            return ToolResult(content=f"计算错误: {str(e)}", is_error=True)

    def _safe_eval(self, expr: str) -> Union[int, float]:
        """
        基于 AST 的安全求值
        仅允许特定的节点类型和函数。
        """
        # 支持的操作符
        operators = {
            ast.Add: operator.add,
            ast.Sub: operator.sub,
            ast.Mult: operator.mul,
            ast.Div: operator.truediv,
            ast.Pow: operator.pow,
            ast.BitXor: operator.xor,
            ast.USub: operator.neg,
        }

        # 支持的函数
        functions = {
            "sqrt": math.sqrt,
            "log": math.log,
            "ln": math.log,
            "sin": math.sin,
            "cos": math.cos,
            "tan": math.tan,
            "abs": abs,
            "round": round,
            "ceil": math.ceil,
            "floor": math.floor,
            "pi": math.pi,
            "e": math.e,
        }

        def _eval(node):
            # 1. 数字
            if isinstance(node, ast.Constant): 
                if isinstance(node.value, (int, float)):
                    return node.value
                raise ValueError(f"不支持的常量类型: {type(node.value)}")

            # 2. 二元运算 (a + b)
            elif isinstance(node, ast.BinOp):
                op_type = type(node.op)
                if op_type in operators:
                    left = _eval(node.left)
                    right = _eval(node.right)
                    return operators[op_type](left, right)
                raise ValueError(f"不支持的操作符: {op_type}")

            # 3. 一元运算 (-a)
            elif isinstance(node, ast.UnaryOp):
                op_type = type(node.op)
                if op_type in operators:
                    return operators[op_type](_eval(node.operand))
                raise ValueError(f"不支持的一元操作符: {op_type}")

            # 4. 函数调用 (sqrt(4))
            elif isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name):
                    func_name = node.func.id
                    if func_name in functions:
                        # 递归计算参数
                        args = [_eval(arg) for arg in node.args]
                        return functions[func_name](*args)
                    raise ValueError(f"禁止调用的函数: {func_name}")
                raise ValueError("不支持的复杂函数调用")

            # 5. 变量名 (pi, e)
            elif isinstance(node, ast.Name):
                if node.id in functions:
                    val = functions[node.id]
                    if isinstance(val, (int, float)):
                        return val
                raise ValueError(f"未知变量: {node.id}")
            
            # [优化建议] 显式拦截属性访问和下标访问，给出更明确的提示
            elif isinstance(node, (ast.Attribute, ast.Subscript, ast.List, ast.Dict, ast.Tuple)):
                raise ValueError("出于安全考虑，禁止使用属性访问、下标或复杂数据结构")

            raise ValueError(f"非法表达式结构: {type(node)}")

        # 解析并求值
        tree = ast.parse(expr, mode='eval')
        return _eval(tree.body)
```

[68] gecko/plugins/tools/standard/duckduckgo.py
```python
# gecko/plugins/tools/standard/duckduckgo.py
"""
DuckDuckGo 搜索工具 (标准版)

基于 duckduckgo-search 库实现。
重构改进：
1. 继承新的 BaseTool，使用 Pydantic V2 定义参数。
2. 使用 run_sync 将同步的网络 I/O 卸载到线程池，防止阻塞 Event Loop。
3. 增强错误处理和结果格式化。
"""
from __future__ import annotations

from typing import Any, Dict, List, Type

from pydantic import BaseModel, Field

from gecko.core.logging import get_logger
from gecko.core.utils import run_sync
from gecko.plugins.tools.base import BaseTool, ToolResult
from gecko.plugins.tools.registry import register_tool

logger = get_logger(__name__)


class DuckDuckGoArgs(BaseModel):
    query: str = Field(
        ..., 
        description="搜索关键词",
        min_length=1,
        max_length=200
    )
    max_results: int = Field(
        default=5, 
        description="返回的最大结果数量 (1-10)", 
        ge=1, 
        le=10
    )


@register_tool("duckduckgo_search")
class DuckDuckGoSearchTool(BaseTool):
    name: str = "duckduckgo_search"
    description: str = (
        "使用 DuckDuckGo 搜索引擎搜索互联网信息。"
        "当需要获取实时新闻、具体事实或不知道的信息时使用。"
        "无需 API Key。"
    )
    args_schema: Type[BaseModel] = DuckDuckGoArgs

    async def _run(self, args: DuckDuckGoArgs) -> ToolResult: # type: ignore
        """
        执行搜索
        注意：DDGS 库主要是同步 IO，必须通过 run_sync 卸载到线程池
        """
        # 检查依赖
        try:
            from duckduckgo_search import DDGS
        except ImportError:
            return ToolResult(
                content=(
                    "错误：未安装 duckduckgo_search 库。\n"
                    "请运行: pip install duckduckgo-search"
                ),
                is_error=True
            )

        query = args.query.strip()
        
        # 定义同步执行函数
        def _search_sync() -> List[Dict[str, str]]:
            results = []
            try:
                # 使用上下文管理器确保 session 关闭
                with DDGS() as ddgs:
                    # text() 方法是生成器，需要转 list
                    # backend="api" 通常更稳定
                    raw_results = ddgs.text(
                        keywords=query,
                        max_results=args.max_results,
                        backend="api" 
                    )
                    # 立即消费生成器以捕获可能的网络异常
                    results = list(raw_results)
            except Exception as e:
                logger.error("DuckDuckGo search failed", error=str(e))
                raise e
            return results

        try:
            # 异步非阻塞执行
            # 这里的 run_sync 引用自 gecko.core.utils，底层使用 to_thread
            raw_data = await run_sync(_search_sync()) # type: ignore

            if not raw_data:
                return ToolResult(content=f"未找到关于 '{query}' 的相关结果。")

            return self._format_results(raw_data)

        except Exception as e:
            return ToolResult(
                content=f"搜索请求失败: {str(e)}",
                is_error=True
            )

    def _format_results(self, results: List[Dict[str, Any]]) -> ToolResult:
        """格式化搜索结果为易读文本"""
        lines = []
        for i, r in enumerate(results, 1):
            title = r.get('title', 'No Title')
            link = r.get('href', '#')
            body = r.get('body', '')
            
            lines.append(f"[{i}] {title}")
            lines.append(f"    Link: {link}")
            lines.append(f"    Snippet: {body}\n")
        
        formatted_text = "DuckDuckGo 搜索结果:\n" + "\n".join(lines)
        
        return ToolResult(
            content=formatted_text,
            metadata={"source": "duckduckgo", "count": len(results)}
        )
```

[69] gecko/utils/cleanup.py
```python
# gecko/utils/cleanup.py
import atexit
import asyncio

def register_litellm_cleanup():
    """在进程退出时优雅关闭 LiteLLM 异步客户端，避免 RuntimeWarning"""
    def _cleanup():
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # 如果循环还在运行，调度清理任务
                loop.create_task(_close_clients())
            else:
                # 循环已关闭，直接新开一个临时循环执行清理
                asyncio.run(_close_clients())
        except Exception:
            pass  # 防止清理本身抛错

    async def _close_clients():
        try:
            import litellm
            # LiteLLM 官方提供的异步关闭方法（v1.40+ 支持）
            if hasattr(litellm, "async_http_handler"):
                if litellm.async_http_handler:
                    await litellm.async_http_handler.client.close()
            # 兼容旧版本
            if hasattr(litellm, "http_client"):
                if litellm.http_client:
                    await litellm.http_client.close()
        except Exception:
            pass

    atexit.register(_cleanup)

# 自动注册（模块导入即生效）
register_litellm_cleanup()
```

