import asyncio
import time
from typing import Any, AsyncIterator, List, Type, Dict

from pydantic import BaseModel, Field

# Gecko æ ¸å¿ƒç»„ä»¶å¯¼å…¥
from gecko.core.engine.base import CognitiveEngine, AgentOutput
from gecko.core.message import Message
from gecko.core.memory import TokenMemory
from gecko.core.toolbox import ToolBox
from gecko.core.protocols import (
    ModelProtocol, 
    CompletionResponse, 
    CompletionChoice, 
    StreamChunk
)

# ==========================================
# 1. æ¨¡æ‹Ÿç»„ä»¶ (Mock Components)
# ==========================================

class MockModel(ModelProtocol):
    """
    ä¸€ä¸ªç®€å•çš„ Mock æ¨¡å‹ï¼Œå®ç°äº† ModelProtocolã€‚
    å®ƒåªæ˜¯å›æ˜¾ç”¨æˆ·çš„è¾“å…¥ï¼Œæˆ–è€…ç”Ÿæˆé¢„å®šä¹‰çš„æµå¼æ•°æ®ã€‚
    """
    async def acompletion(self, messages: List[Dict[str, Any]], **kwargs) -> CompletionResponse:
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await asyncio.sleep(0.1)
        
        last_content = messages[-1]["content"]
        response_text = f"Mock Response to: {last_content}"
        
        return CompletionResponse(
            choices=[
                CompletionChoice(message={"role": "assistant", "content": response_text})
            ],
            usage={"prompt_tokens": 10, "completion_tokens": 5, "total_tokens": 15} # type: ignore
        )

    async def astream(self, messages: List[Dict[str, Any]], **kwargs) -> AsyncIterator[StreamChunk]:
        # æ¨¡æ‹Ÿæµå¼è¾“å‡º
        full_text = "This is a streaming response from the mock model."
        for word in full_text.split():
            await asyncio.sleep(0.05)
            yield StreamChunk(
                choices=[{"delta": {"content": word + " "}}]
            )

    def count_tokens(self, text_or_messages) -> int:
        # ç®€å•æ¨¡æ‹Ÿï¼šæŒ‰å­—ç¬¦æ•°ä¼°ç®—ï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹
        if isinstance(text_or_messages, list):
            text = "".join(str(m.get("content", "")) for m in text_or_messages)
        else:
            text = str(text_or_messages)
        return len(text) // 4

# ==========================================
# 2. è‡ªå®šä¹‰å¼•æ“å®ç° (Custom Engine)
# ==========================================

class DemoEngine(CognitiveEngine):
    """
    ç»§æ‰¿ CognitiveEngine çš„æ¼”ç¤ºå¼•æ“ã€‚
    å¿…é¡»å®ç° step() æ–¹æ³•ã€‚
    """
    
    async def step(self, input_messages: List[Message], **kwargs) -> AgentOutput:
        """
        å®ç°æ ¸å¿ƒæ¨ç†é€»è¾‘ï¼ˆå¸¦ç»Ÿè®¡ä¿®å¤ï¼‰
        """
        # â±ï¸ 1. å¼€å§‹è®¡æ—¶
        start_time = time.time()
        
        # 2. éªŒè¯è¾“å…¥
        self.validate_input(input_messages)
        
        # 3. è§¦å‘ before_step hook
        await self.before_step(input_messages, **kwargs)

        # 4. å‡†å¤‡æ•°æ®
        formatted_msgs = [m.to_openai_format() for m in input_messages]
        
        try:
            # 5. è°ƒç”¨æ¨¡å‹
            response = await self.model.acompletion(formatted_msgs)
            content = response.choices[0].message["content"]
            
            # è·å– token ä½¿ç”¨é‡ (MockModel è¿”å›äº† usage)
            # å¦‚æœ response.usage æ˜¯å¯¹è±¡åˆ™å–å±æ€§ï¼Œå¦‚æœæ˜¯å­—å…¸åˆ™å–é”®å€¼
            usage_info = response.usage
            total_tokens = 0
            if isinstance(usage_info, dict):
                total_tokens = usage_info.get("total_tokens", 0)
            elif hasattr(usage_info, "total_tokens"):
                total_tokens = usage_info.total_tokens # type: ignore

            # 6. æ„å»ºè¾“å‡º
            output = AgentOutput(
                content=content,
                metadata={"finish_reason": "stop"}
            )
            
            # 7. è§¦å‘ after_step hook
            await self.after_step(input_messages, output, **kwargs)
            
            # âœ… 8. [ä¿®å¤ç‚¹] æ‰‹åŠ¨æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            duration = time.time() - start_time
            if self.stats:
                self.stats.add_step(duration, tokens=total_tokens)
            
            return output
            
        except Exception as e:
            # âœ… 9. [ä¿®å¤ç‚¹] è®°å½•é”™è¯¯ç»Ÿè®¡
            if self.stats:
                self.stats.errors += 1
                
            # é”™è¯¯å¤„ç† hook
            await self.on_error(e, input_messages)
            raise

    async def step_stream(self, input_messages: List[Message], **kwargs) -> AsyncIterator[str]: # type: ignore
        """
        è¦†ç›–æµå¼æ¨ç†æ–¹æ³•
        """
        formatted_msgs = [m.to_openai_format() for m in input_messages]
        
        async for chunk in self.model.astream(formatted_msgs): # type: ignore
            content = chunk.content
            if content:
                yield content

    async def step_structured(
        self, 
        input_messages: List[Message], 
        response_model: Type[BaseModel], 
        **kwargs
    ) -> BaseModel:
        """
        è¦†ç›–ç»“æ„åŒ–è¾“å‡ºæ–¹æ³• (æ¨¡æ‹Ÿå®ç°)
        """
        # æ¨¡æ‹Ÿï¼šç›´æ¥è¿”å›ä¸€ä¸ªä¼ªé€ çš„ç»“æ„åŒ–å¯¹è±¡
        # å®é™…åœºæ™¯ä¸­è¿™é‡Œä¼šè°ƒç”¨ StructureEngine
        print(f"   [Engine] Parsing structured output for {response_model.__name__}...")
        await asyncio.sleep(0.1)
        
        return response_model(
            reasoning="Simulated reasoning",
            score=95,
            tags=["demo", "mock"]
        )

# ==========================================
# 3. è¾…åŠ©æ•°æ®ç»“æ„
# ==========================================

class AnalysisResult(BaseModel):
    """ç”¨äºæµ‹è¯•ç»“æ„åŒ–è¾“å‡ºçš„æ¨¡å‹"""
    reasoning: str = Field(description="æ€è€ƒè¿‡ç¨‹")
    score: int = Field(description="è¯„åˆ†")
    tags: List[str] = Field(description="æ ‡ç­¾")

# ==========================================
# 4. ä¸»æ¼”ç¤ºæµç¨‹
# ==========================================

async def main():
    print("ğŸš€ Starting Engine Base Demo...\n")

    # --- åˆå§‹åŒ–ä¾èµ– ---
    model = MockModel()
    toolbox = ToolBox() # ç©ºå·¥å…·ç®±
    # æ³¨æ„ï¼šè¿™é‡Œç®€å• mock memoryï¼Œå®é™…åº”ä¼ å…¥ SessionInterface å®ç°
    memory = TokenMemory(session_id="demo_session", max_tokens=1000)

    # --- ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆå§‹åŒ–å¼•æ“ ---
    print("1ï¸âƒ£  Testing Context Manager & Basic Step")
    async with DemoEngine(model, toolbox, memory) as engine:
        
        # --- è®¾ç½® Hooks ---
        async def my_before_hook(messages, **kwargs):
            print(f"   [Hook] Before step: Processing {len(messages)} messages")

        async def my_after_hook(messages, output, **kwargs):
            print(f"   [Hook] After step: Generated {len(output.content)} chars")

        engine.before_step_hook = my_before_hook
        engine.after_step_hook = my_after_hook

        # --- æµ‹è¯•æ™®é€šæ¨ç† ---
        user_msg = Message.user("Hello Gecko!")
        print(f"   User: {user_msg.content}")
        
        output = await engine.step([user_msg])
        print(f"   Agent: {output.content}\n")

        # --- æµ‹è¯•æµå¼æ¨ç† ---
        print("2ï¸âƒ£  Testing Streaming")
        print("   Agent (Stream): ", end="", flush=True)
        async for token in engine.step_stream([Message.user("Stream me!")]):
            print(token, end="", flush=True)
        print("\n")

        # --- æµ‹è¯•ç»“æ„åŒ–è¾“å‡º ---
        print("3ï¸âƒ£  Testing Structured Output")
        result = await engine.step_structured(
            [Message.user("Analyze this")], 
            response_model=AnalysisResult
        )
        print(f"   Result: {result.model_dump_json()}\n")

        # --- æŸ¥çœ‹ç»Ÿè®¡ ---
        print("4ï¸âƒ£  Execution Stats")
        stats = engine.get_stats()
        print(f"   Total Steps: {stats['total_steps']}") # type: ignore
        print(f"   Total Time:  {stats['total_time']:.4f}s") # type: ignore
        print(f"   Avg Time:    {stats['avg_step_time']:.4f}s") # type: ignore
        print(f"   Errors:      {stats['errors']}") # type: ignore

if __name__ == "__main__":
    asyncio.run(main())