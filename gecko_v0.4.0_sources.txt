[1] gecko/__init__.py
```python
# gecko/__init__.py
from __future__ import annotations

"""
Gecko 顶层包入口

职责：
1. 暴露 v1.0 规划中的核心稳定 API（L1）
2. 提供统一的版本号 (__version__)
3. 保持 import gecko 即可访问常用类，而无需记复杂子模块路径
"""

from gecko.version import __version__  # ✅ 从单一版本源导入版本号

# ======= 核心对象导出（L1 稳定 API） =======

from gecko.core.agent import Agent
from gecko.core.builder import AgentBuilder
from gecko.core.message import Message, Role
from gecko.core.output import AgentOutput, TokenUsage
from gecko.core.memory import TokenMemory, SummaryTokenMemory
from gecko.core.structure import StructureEngine

from gecko.compose.workflow import Workflow
from gecko.compose.nodes import step, Next
from gecko.compose.team import Team

# 导入侧效：在 gecko.utils.cleanup 中注册 LiteLLM 的 atexit 清理
import gecko.utils.cleanup  # noqa: F401

__all__ = [
    # 版本
    "__version__",
    # Agent & Builder
    "Agent",
    "AgentBuilder",
    # 消息与角色
    "Message",
    "Role",
    # 输出与 Token 统计
    "AgentOutput",
    "TokenUsage",
    # 记忆模块
    "TokenMemory",
    "SummaryTokenMemory",
    # 结构化输出
    "StructureEngine",
    # 工作流与多智能体
    "Workflow",
    "step",
    "Next",
    "Team",
]
```

[2] gecko/cli/__init__.py
```python
# gecko/cli/__init__.py
"""
Gecko CLI 工具

提供命令行交互能力，支持：
- 快速测试 Agent
- 工作流调试
- 配置验证
"""
from __future__ import annotations

import asyncio
import sys
from typing import Optional

# 尝试导入 click
try:
    import click
    CLICK_AVAILABLE = True
except ImportError:
    CLICK_AVAILABLE = False


def main():
    """CLI 入口点"""
    if not CLICK_AVAILABLE:
        print("CLI requires 'click' package. Install with: pip install click")
        sys.exit(1)
    
    cli()

from gecko.version import __version__

if CLICK_AVAILABLE:
    @click.group() # type: ignore
    @click.version_option(version=__version__) # type: ignore
    def cli():
        """Gecko AI Agent Framework CLI"""
        pass
    
    @cli.command()
    @click.option("--model", "-m", default="gpt-3.5-turbo", help="Model name") # type: ignore
    @click.option("--api-key", "-k", envvar="OPENAI_API_KEY", help="API Key") # type: ignore
    @click.option("--system", "-s", default=None, help="System prompt") # type: ignore
    def chat(model: str, api_key: Optional[str], system: Optional[str]):
        """Interactive chat with an agent"""
        if not api_key:
            click.echo("Error: API key required. Use --api-key or set OPENAI_API_KEY") # type: ignore
            return
        
        asyncio.run(_run_chat(model, api_key, system))
    
    @cli.command()
    @click.argument("workflow_file") # type: ignore
    @click.option("--input", "-i", default=None, help="Input data (JSON)") # type: ignore
    @click.option("--visualize", "-v", is_flag=True, help="Print workflow graph") # type: ignore
    def run(workflow_file: str, input: Optional[str], visualize: bool):
        """Execute a workflow from file"""
        click.echo(f"Running workflow: {workflow_file}") # type: ignore
        # TODO: 实现工作流加载和执行
    
    @cli.command()
    def config():
        """Show current configuration"""
        from gecko.config import get_settings
        
        settings = get_settings()
        click.echo("Current Gecko Configuration:") # type: ignore
        click.echo(f"  Default Model: {settings.default_model}") # type: ignore
        click.echo(f"  Max Turns: {settings.max_turns}") # type: ignore
        click.echo(f"  Log Level: {settings.log_level}") # type: ignore
        click.echo(f"  Storage URL: {settings.default_storage_url}") # type: ignore
    
    @cli.command()
    def tools():
        """List available tools"""
        from gecko.plugins.tools.registry import ToolRegistry
        
        # 导入标准工具以触发注册
        try:
            import gecko.plugins.tools.standard  # noqa
        except ImportError:
            pass
        
        tool_list = ToolRegistry.list_tools()
        
        if not tool_list:
            click.echo("No tools registered") # type: ignore
            return
        
        click.echo("Available Tools:") # type: ignore
        for name in tool_list:
            click.echo(f"  - {name}") # type: ignore


async def _run_chat(model: str, api_key: str, system: Optional[str]):
    """运行交互式聊天"""
    from gecko import AgentBuilder
    from gecko.plugins.models import OpenAIChat
    
    click.echo(f"Starting chat with {model}...") # type: ignore
    click.echo("Type 'exit' or 'quit' to end the conversation.\n") # type: ignore
    
    # 创建模型和 Agent
    llm = OpenAIChat(api_key=api_key, model=model)
    
    builder = AgentBuilder().with_model(llm)
    if system:
        builder = builder.with_system_prompt(system)
    
    agent = builder.build()
    
    # 交互循环
    while True:
        try:
            user_input = click.prompt("You", type=str) # type: ignore
            
            if user_input.lower() in ("exit", "quit"): # type: ignore
                click.echo("Goodbye!") # type: ignore
                break
            
            # 执行推理
            result = await agent.run(user_input)
            click.echo(f"\nAssistant: {result.content}\n") # type: ignore
            
        except KeyboardInterrupt:
            click.echo("\nGoodbye!") # type: ignore
            break
        except Exception as e:
            click.echo(f"\nError: {e}\n") # type: ignore


# ==================== 导出 ====================

__all__ = ["main", "cli"]
```

[3] gecko/compose/__init__.py
```python
# gecko/compose/__init__.py
"""
Gecko Compose 模块

提供多智能体编排能力：
- Workflow: DAG 工作流引擎
- Team: 并行多智能体执行
- step: 节点装饰器
- ensure_awaitable: 同步/异步统一调用工具
- Next: 控制流指令
"""
from gecko.compose.workflow import Workflow
from gecko.compose.team import Team
from gecko.compose.nodes import step, ensure_awaitable, Next

__all__ = ["Workflow", "Team", "step", "ensure_awaitable", "Next"]
```

[4] gecko/compose/nodes.py
```python
# gecko/compose/nodes.py
"""
Workflow 节点定义与辅助工具

核心功能：
1. Next: 控制流指令，用于动态跳转节点
2. step: 节点装饰器，用于标记和增强函数元数据

优化日志：
- [Fix] 使用 functools.wraps 保留被装饰函数的元数据 (签名、文档等)
- [Refactor] 移除本地 ensure_awaitable，引用 gecko.core.utils
- [Feat] step 装饰器自动将同步函数转为异步，统一调用行为
"""
from __future__ import annotations

import functools
from typing import Any, Callable, Dict, Optional

from pydantic import BaseModel, Field

from gecko.core.utils import ensure_awaitable


class Next(BaseModel):
    """
    控制流指令：用于 Workflow 节点返回值中，指示引擎跳转到特定节点。
    
    示例:
        ```python
        def check_score(score: int):
            if score > 60:
                return Next(node="Pass", input="Good job")
            return Next(node="Fail", input="Try again")
        ```
    """
    node: str = Field(..., description="下一个节点的名称")
    input: Optional[Any] = Field(default=None, 
        description="传递给下一个节点的输入数据。如果为 None，则保持上下文中的 last_output 不变。"
    ) # type: ignore
    # [New] 允许在跳转时更新 Context.state # type: ignore
    update_state: Dict[str, Any] = Field(
        default_factory=dict,
        description="需要合并到 WorkflowContext.state 的字典"
    )


def step(name: Optional[str] = None):
    """
    节点装饰器
    
    功能：
    1. 标记函数为 Workflow 节点
    2. 允许自定义节点名称 (metadata)
    3. 统一将同步函数包装为异步函数
    
    参数:
        name: 自定义节点名称（可选，默认使用函数名）
        
    示例:
        ```python
        @step(name="DataFetcher")
        def fetch_data(url: str):
            return requests.get(url).text
            
        # 在 Workflow 中使用
        workflow.add_node("fetch", fetch_data)
        ```
    """
    def decorator(func: Callable):
        # 1. 保留原始函数的元数据 (name, doc, signature)
        # 这对于 Workflow 的智能参数注入 (Smart Binding) 至关重要
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            # 2. 统一转为异步执行
            return await ensure_awaitable(func, *args, **kwargs)
        
        # 3. 附加标识位和名称
        setattr(wrapper, "_is_step", True)
        setattr(wrapper, "_step_name", name or func.__name__)
        
        return wrapper
    return decorator
```

[5] gecko/compose/team.py
```python
# gecko/compose/team.py
"""
Team 多智能体并行引擎 (Enhanced)

优化日志：
- [Feat] 增加 ExecutionStrategy 枚举 (ALL, RACE)
- [Feat] 增加 input_mapper 支持输入分片 (Sharding)
- [Perf] 实现 Race 模式下的快速返回与任务取消
- [Refactor] 保持 MemberResult 标准化结构
"""
from __future__ import annotations

from enum import Enum
from typing import Any, Callable, Generic, List, Optional, TypeVar, Union, TYPE_CHECKING

import anyio
from pydantic import BaseModel, Field

from gecko.core.agent import Agent
from gecko.core.logging import get_logger
from gecko.core.message import Message
from gecko.core.output import AgentOutput
from gecko.core.utils import ensure_awaitable

if TYPE_CHECKING:
    from gecko.compose.workflow.models import WorkflowContext

logger = get_logger(__name__)

R = TypeVar("R")


class ExecutionStrategy(str, Enum):
    """
    执行策略枚举
    """
    ALL = "all"       # 等待所有成员执行完毕 (Map-Reduce 默认)
    RACE = "race"     # 赛马模式：取第一个成功的结果，取消其他任务 (降低延迟)
    # Future: MAJORITY (投票), BEST_OF_N (采样)


class MemberResult(BaseModel, Generic[R]):
    """标准化成员执行结果"""
    result: Optional[R] = Field(default=None, description="执行成功时的返回值")
    error: Optional[str] = Field(default=None, description="执行失败时的错误信息")
    member_index: int = Field(..., description="成员在 Team 中的索引")
    is_success: bool = Field(default=True, description="是否执行成功")

    @property
    def value(self) -> R:
        if not self.is_success:
            raise RuntimeError(f"Member execution failed: {self.error}")
        return self.result  # type: ignore


class Team:
    """
    多智能体协作组 (Parallel Execution Engine)
    """

    def __init__(
        self,
        members: List[Union[Agent, Callable]],
        name: str = "Team",
        max_concurrent: int = 0,
        return_full_output: bool = False,
        strategy: ExecutionStrategy = ExecutionStrategy.ALL,  # [新增] 策略
        input_mapper: Optional[Callable[[Any, int], Any]] = None, # [新增] 输入分片
    ):
        self.members = members
        self.name = name
        self.max_concurrent = max_concurrent
        self.return_full_output = return_full_output
        self.strategy = strategy
        self.input_mapper = input_mapper

    # ========================= 接口协议 =========================

    async def __call__(self, context_or_input: Any) -> List[MemberResult]:
        return await self.run(context_or_input)

    async def run(self, context_or_input: Any) -> List[MemberResult]:
        """执行 Team 逻辑"""
        # 1. 解析原始输入
        raw_input = self._resolve_input(context_or_input)
        member_count = len(self.members)

        logger.info(
            "Team execution started",
            team=self.name,
            members=member_count,
            strategy=self.strategy
        )

        # 2. 准备分片输入 (Sharding Logic)
        inputs = []
        for i in range(member_count):
            if self.input_mapper:
                try:
                    # input_mapper(raw_input, member_index)
                    # 允许根据索引切分数据
                    val = self.input_mapper(raw_input, i)
                    inputs.append(val)
                except Exception as e:
                    logger.error(f"Input mapping failed for member {i}", error=str(e))
                    inputs.append(None) # 映射失败视为 None 或抛出，这里选择防御性 None
            else:
                inputs.append(raw_input)

        # 3. 根据策略分发
        if self.strategy == ExecutionStrategy.RACE:
            return await self._execute_race(inputs)
        else:
            return await self._execute_all(inputs)

    # ========================= 核心执行模式 =========================

    async def _execute_all(self, inputs: List[Any]) -> List[MemberResult]:
        """
        [默认模式] 等待所有成员完成
        """
        results: List[Optional[MemberResult]] = [None] * len(self.members)
        semaphore = anyio.Semaphore(self.max_concurrent) if self.max_concurrent > 0 else None

        async def _worker(idx: int, member: Any, inp: Any):
            if semaphore:
                await semaphore.acquire()
            try:
                res = await self._safe_execute_member(idx, member, inp)
                results[idx] = res
            finally:
                if semaphore:
                    semaphore.release()

        async with anyio.create_task_group() as tg:
            for i, member in enumerate(self.members):
                tg.start_soon(_worker, i, member, inputs[i])

        # 整理结果
        final_results = [r for r in results if r is not None]
        self._log_completion(final_results)
        return final_results

    async def _execute_race(self, inputs: List[Any]) -> List[MemberResult]:
        """
        [赛马模式] 返回最快成功的那个，取消其他
        """
        # 用于存储获胜者的容器 (list 是可变的，闭包可写)
        winner: List[MemberResult] = []
        
        # 使用 CancelScope 实现“一人成功，全员取消”
        # 注意：这里需要在外部包裹 try-except 处理取消异常
        try:
            async with anyio.create_task_group() as tg:
                for i, member in enumerate(self.members):
                    
                    async def _racer(idx: int, mem: Any, inp: Any):
                        # 执行成员逻辑
                        res = await self._safe_execute_member(idx, mem, inp)
                        
                        # 只有成功的才算赢
                        if res.is_success:
                            if not winner: # 双重检查避免覆盖
                                winner.append(res)
                                # 核心：取消整个 TaskGroup 的 Scope
                                tg.cancel_scope.cancel()
                        
                    tg.start_soon(_racer, i, member, inputs[i])
                    
        except anyio.get_cancelled_exc_class():
            # 捕获取消异常是预期行为（因为我们主动 cancel 了）
            pass
        except Exception as e:
            logger.error("Race execution crashed", error=str(e))

        if winner:
            logger.info(f"Team {self.name} Race won by member {winner[0].member_index}")
            return winner
        
        # 如果所有人都失败了，或者没有任何人成功，返回空列表或全失败记录
        # 这里简化处理，返回空列表表示无 winner
        logger.warning(f"Team {self.name} Race failed: no winner")
        return []

    # ========================= 内部逻辑 =========================

    async def _safe_execute_member(self, idx: int, member: Any, inp: Any) -> MemberResult:
        """单成员执行封装（含异常处理与结果标准化）"""
        try:
            raw = await self._execute_member(member, inp)
            processed = self._process_result(raw)
            return MemberResult(member_index=idx, result=processed, is_success=True)
        except Exception as e:
            logger.error(f"Member {idx} failed", error=str(e))
            return MemberResult(member_index=idx, error=str(e), is_success=False)

    def _resolve_input(self, context_or_input: Any) -> Any:
        """从 Context 提取输入 (保留 WorkflowContext 的 Duck Typing)"""
        if (
            hasattr(context_or_input, "history")
            and hasattr(context_or_input, "input")
            and isinstance(getattr(context_or_input, "history", None), dict)
        ):
            ctx = context_or_input
            history = getattr(ctx, "history", {})
            state = getattr(ctx, "state", {})

            if "_next_input" in state:
                return state.pop("_next_input")
            elif "last_output" in history:
                return history["last_output"]
            else:
                return getattr(ctx, "input")
        return context_or_input

    async def _execute_member(self, member: Any, inp: Any) -> Any:
        if hasattr(member, "run"):
            return await member.run(inp)
        if callable(member):
            return await ensure_awaitable(member, inp)
        raise TypeError(f"Member {member} is not executable")

    def _process_result(self, result: Any) -> Any:
        if self.return_full_output:
            if isinstance(result, (BaseModel, AgentOutput, Message)):
                return result.model_dump()
            return result
        # 默认提取内容
        if isinstance(result, AgentOutput):
            return result.content
        if isinstance(result, Message):
            return result.content
        return result

    def _log_completion(self, results: List[MemberResult]):
        success = sum(1 for r in results if r.is_success)
        logger.info(
            "Team execution completed",
            team=self.name,
            success=success,
            total=len(results)
        )
```

[6] gecko/compose/workflow/__init__.py
```python
# gecko/compose/workflow/__init__.py
"""
Workflow 包入口

通过在此处重新导出子模块的类，保持对旧版单文件 workflow.py 的 API 兼容性。
外部调用者无需感知内部拆分。
"""

# 1. 核心引擎
from gecko.compose.workflow.engine import Workflow

# 2. 数据模型 (Context, Enums, Trace)
from gecko.compose.workflow.models import (
    WorkflowContext,
    CheckpointStrategy,
    NodeStatus,
    NodeExecution
)

# 3. 异常 (如果之前是在 workflow.py 定义的，现在建议引用 core.exceptions)
# 但如果为了兼容性保留了别名，也可以在这里处理
from gecko.core.exceptions import WorkflowError, WorkflowCycleError

__all__ = [
    "Workflow",
    "WorkflowContext",
    "CheckpointStrategy",
    "NodeStatus",
    "NodeExecution",
    "WorkflowError",
    "WorkflowCycleError"
]
```

[7] gecko/compose/workflow/engine.py
```python
# gecko/compose/workflow/engine.py
"""
Workflow 引擎主入口 (v0.4 Stable / 2025-12-03 Optimized)

核心职责：
1. **并行执行调度 (Phase 2 Core)**: 将 DAG 解析为执行层级 (Layers)，利用 TaskGroup 并行执行。
2. **状态管理 (State Management)**:
   - 隔离: 使用 Copy-On-Write (COW) 机制防止并行节点污染主上下文。
   - 合并: 基于 Diff 的状态合并策略 (Last Write Wins)。
3. **动态流控制 (Dynamic Flow)**: 支持 Next 指令动态跳转，可中断静态 DAG 计划。
4. **断点恢复 (Resume Support)**: 支持从存储加载状态并从指定节点继续执行。
5. **持久化 (Persistence)**: 细粒度的 Step 级状态保存 (Pre-Commit / Post-Commit)。
"""
from __future__ import annotations

import asyncio
import inspect
from collections import deque
from typing import Any, Callable, Dict, Optional, Set, Union, List

import anyio

from gecko.config import get_settings
from gecko.core.events import EventBus
from gecko.core.logging import get_logger
from gecko.core.exceptions import WorkflowError
from gecko.plugins.storage.interfaces import SessionInterface

# 导入子模块
from gecko.compose.workflow.models import WorkflowContext, CheckpointStrategy
from gecko.compose.workflow.graph import WorkflowGraph
from gecko.compose.workflow.executor import NodeExecutor
from gecko.compose.workflow.persistence import PersistenceManager
from gecko.compose.nodes import Next

logger = get_logger(__name__)


class Workflow:
    """
    DAG 工作流引擎
    
    特性:
    - 支持并行节点执行
    - 支持条件分支
    - 支持循环与动态跳转
    - 支持持久化与断点恢复
    """

    def __init__(
        self,
        name: str = "Workflow",
        event_bus: Optional[EventBus] = None,
        storage: Optional[SessionInterface] = None,
        max_steps: int = 100,
        checkpoint_strategy: Optional[str] = None, 
        max_history_retention: Optional[int] = None,
        enable_retry: bool = False,
        max_retries: int = 3,
        allow_cycles: bool = False,
        enable_parallel: bool = False,
    ):
        self.name = name
        self.event_bus = event_bus or EventBus()
        self.max_steps = max_steps

        # 动态获取最新配置
        current_settings = get_settings()

        strategy_val = checkpoint_strategy or current_settings.workflow_checkpoint_strategy
        retention_val = (
            max_history_retention 
            if max_history_retention is not None 
            else current_settings.workflow_history_retention
        )
        
        self._allow_cycles = allow_cycles
        self._enable_parallel = enable_parallel
        
        self._validation_errors = []

        # === 子组件组装 ===
        self.graph = WorkflowGraph()
        
        self.executor = NodeExecutor(
            enable_retry=enable_retry,
            max_retries=max_retries
        )
        
        self.persistence = PersistenceManager(
            storage=storage,
            strategy=CheckpointStrategy(strategy_val),
            history_retention=retention_val
        )

    # ================= 属性代理 (保持兼容性) =================
    
    @property
    def storage(self) -> Optional[SessionInterface]:
        return self.persistence.storage

    @storage.setter
    def storage(self, value: Optional[SessionInterface]):
        self.persistence.storage = value

    @property
    def allow_cycles(self) -> bool:
        return self._allow_cycles

    @allow_cycles.setter
    def allow_cycles(self, value: bool):
        self._allow_cycles = value
        # 修改配置后重置图验证状态
        self.graph._validated = False

    @property
    def checkpoint_strategy(self) -> CheckpointStrategy:
        return self.persistence.strategy

    @checkpoint_strategy.setter
    def checkpoint_strategy(self, value: Union[str, CheckpointStrategy]):
        self.persistence.strategy = CheckpointStrategy(value)

    # ================= 构建 API (Proxy to Graph) =================
    
    def add_node(self, name: str, func: Callable) -> "Workflow":
        """添加节点"""
        self.graph.add_node(name, func)
        return self

    def add_edge(self, source: str, target: str, condition: Optional[Callable] = None) -> "Workflow":
        """添加边 (支持条件)"""
        self.graph.add_edge(source, target, condition)
        return self

    def set_entry_point(self, name: str) -> "Workflow":
        """设置入口节点"""
        self.graph.set_entry_point(name)
        return self

    def to_mermaid(self) -> str:
        """生成 Mermaid 图表"""
        return self.graph.to_mermaid()
        
    def print_structure(self):
        print(self.to_mermaid())

    def validate(self) -> bool:
        """验证工作流结构"""
        valid, errors = self.graph.validate(
            allow_cycles=self.allow_cycles,
            enable_parallel=self._enable_parallel
        )
        self._validation_errors = errors
        return valid

    def add_parallel_group(self, *node_names: str) -> "Workflow":
        """
        [Deprecated] 定义并行组
        v0.4 引擎会自动通过图拓扑推导并行层级，此方法保留仅作兼容。
        """
        for name in node_names:
            if name not in self.graph.nodes:
                raise ValueError(f"Node '{name}' not found")
        return self

    def set_dependency(self, node: str, depends_on: Union[str, List[str]]) -> "Workflow":
        """显式设置节点依赖 (辅助构建图)"""
        deps = [depends_on] if isinstance(depends_on, str) else depends_on
        self.graph.node_dependencies.setdefault(node, set()).update(deps)
        self.graph._validated = False
        return self
        
    # 暴露内部方法供测试调用
    def _build_execution_layers(self, start_node: str):
        return self.graph.build_execution_layers(start_node)
    
    def _normalize_result(self, result: Any) -> Any:
        return self.executor._normalize_result(result)

    # ================= 执行入口 (Execution Core) =================

    async def execute_parallel(self, input_data: Any, session_id: Optional[str] = None) -> Any:
        """
        并行执行入口 (Phase 2 新增别名)
        """
        return await self.execute(input_data, session_id)

    async def execute(
        self, 
        input_data: Any, 
        session_id: Optional[str] = None, 
        start_node: Optional[str] = None,
        _resume_context: Optional[WorkflowContext] = None
    ) -> Any:
        """
        执行工作流 (主入口)
        
        Args:
            input_data: 初始输入数据
            session_id: 会话 ID (可选，用于持久化)
            start_node: [v0.4] 指定起始节点 (可选，用于 Resume 或特定入口)
            _resume_context: [v0.4] 注入已恢复的上下文 (用于 Resume)
        
        执行流程:
        1. 验证图结构 (强制启用并行支持)。
        2. 初始化或恢复上下文。
        3. 拓扑排序：构建静态分层执行计划 (Execution Plan)。
        4. 执行循环：逐层并行调度节点。
        5. 状态管理：处理状态合并、持久化与动态跳转。
        """
        # 1. 强制启用并行验证，允许分支结构
        self._enable_parallel = True 
        if not self.validate():
            raise WorkflowError(f"Workflow validation failed: {self._validation_errors}")

        # 2. 上下文准备
        if _resume_context:
            # 恢复模式：使用传入的上下文
            context = _resume_context
            # 注意：恢复时通常保留原有 input，除非 input_data 显式提供新值且逻辑允许覆盖
        else:
            # 全新模式：创建新上下文
            context = WorkflowContext(input=input_data)
            
        if session_id:
            context.metadata["session_id"] = session_id

        # 3. 规划执行路径
        # 优先使用传入的 start_node (Resume 场景)，否则使用图的 Entry Point
        entry = start_node or self.graph.entry_point
        if not entry:
            raise WorkflowError("No entry point defined")

        # 使用 Kahn 算法构建并行层级 (Static Plan)
        layers = self.graph.build_execution_layers(entry)
        
        # 将层级放入双端队列，作为初始执行计划
        execution_queue = deque(layers)
        
        # 记录计划到元数据 (用于调试/UI展示)
        context.metadata["execution_plan"] = [list(l) for l in layers]

        current_step = 0
        
        try:
            # 4. 主执行循环
            while execution_queue:
                # 安全检查：防止无限循环
                if current_step >= self.max_steps:
                    raise WorkflowError(f"Exceeded max steps: {self.max_steps}")

                # 取出当前层 (Set[node_name])
                layer = execution_queue.popleft()

                # 4.1 Pre-Commit (持久化 RUNNING 状态)
                # 记录即将执行的层，以便 Crash 后知道是在哪一步挂的
                if session_id:
                    await self.persistence.save_checkpoint(
                        session_id, current_step, list(layer), context, force=True # type: ignore
                    )

                # 4.2 [核心] 并行执行当前层
                # 返回: {node_name: {"output": ..., "state_diff": ...}}
                layer_results = await self._execute_layer_parallel(layer, context)
                
                # 4.3 合并结果与状态 (Merge)
                # 将并行节点的输出和状态变更合并回主 Context
                self._merge_layer_results(context, layer_results)
                
                # 4.4 处理动态跳转 (Next)
                # 策略: 如果层中任何节点返回了 Next，则中断静态计划，优先处理跳转
                jump_instruction = self._handle_dynamic_jump(layer_results, context)
                
                if jump_instruction:
                    target_node = jump_instruction.node
                    logger.info(f"Dynamic jump to '{target_node}', static plan interrupted.")
                    
                    # 清空当前静态队列
                    execution_queue.clear()
                    # 将目标节点作为新的一层加入，转为动态执行模式
                    execution_queue.append({target_node})
                    # 清除上下文中的指针，防止污染
                    context.clear_next_pointer()
                
                # 4.5 Post-Commit (持久化 SUCCESS 状态)
                # 记录本层已完成
                if session_id:
                    await self.persistence.save_checkpoint(
                        session_id, current_step, list(layer), context # type: ignore
                    )
                
                current_step += 1

            # 5. 最终保存 (Strategy=FINAL)
            if self.persistence.strategy == CheckpointStrategy.FINAL:
                await self.persistence.save_checkpoint(
                    session_id, 9999, None, context, force=True # type: ignore
                )
                
            return context.get_last_output()
            
        except Exception as e:
            logger.exception("Workflow execution failed")
            if not isinstance(e, WorkflowError):
                raise WorkflowError(f"Workflow execution failed: {e}") from e
            raise

    async def _execute_layer_parallel(self, layer: Set[str], context: WorkflowContext) -> Dict[str, Any]:
        """
        并行执行单层节点
        
        Args:
            layer: 当前层的节点集合
            context: 主上下文
            
        Returns:
            Dict[node_name, exec_result_wrapper]
        """
        results: Dict[str, Any] = {}
        
        # 使用 anyio.create_task_group 实现结构化并发
        # 如果任一任务抛出异常，整个 Group 会被取消
        async with anyio.create_task_group() as tg:
            for node_name in layer:
                node_func = self.graph.nodes[node_name]
                
                # [核心机制] Copy-On-Write (COW)
                # 为每个节点创建 Context 的深拷贝，防止并发修改 State 导致竞态。
                # 仅 input 保持只读引用，history/state 均被隔离。
                node_context = context.model_copy(deep=True)
                
                tg.start_soon(
                    self._run_node_wrapper, 
                    node_name, 
                    node_func, 
                    node_context, 
                    results
                )
        
        return results

    async def _run_node_wrapper(
        self, 
        name: str, 
        func: Callable, 
        ctx: WorkflowContext, 
        results: Dict[str, Any]
    ):
        """
        节点执行包装器 (Worker)
        
        职责:
        1. 运行时条件检查 (Condition Check)
        2. 调用 Executor 执行节点
        3. 捕获结果与状态变更 (Diff Calculation)
        """
        # 1. 运行时条件检查
        # 逻辑: 只要有一条指向本节点的边满足条件 (OR 逻辑)，则执行。
        # 如果所有入边的条件都不满足，则跳过。
        incoming_edges = []
        for src, edges in self.graph.edges.items():
            for target, cond in edges:
                if target == name:
                    incoming_edges.append((src, cond))
        
        if incoming_edges:
            should_run = False
            for src, cond in incoming_edges:
                # 只有当上游已执行 (在 history 中) 或者是 Start 节点时，条件才有意义
                # 如果上游节点未执行（被跳过），则该路径视为不通
                if src == self.graph.entry_point or src in ctx.history:
                    # 无条件边 -> 视为 True
                    if cond is None:
                        should_run = True
                        break
                    try:
                        # 评估条件 (支持 sync/async)
                        res = cond(ctx)
                        if inspect.isawaitable(res):
                            res = await res
                        if res:
                            should_run = True
                            break
                    except Exception as e:
                        # 条件执行报错视为不通过 (Fail Safe)
                        logger.error(f"Condition check failed for {src}->{name}: {e}")
            
            if not should_run:
                logger.info(f"Node {name} skipped due to conditions")
                return

        # 2. 执行节点
        try:
            output = await self.executor.execute_node(name, func, ctx)
            
            # 3. 计算 State Diff
            # 简单策略：直接返回当前节点的所有 state，由 merge 负责更新 (Last Write Wins)
            # 进阶策略：可以对比 ctx.state 和初始快照，只返回变更部分
            state_diff = ctx.state 
            
            # 写入结果容器 (线程安全，因为是在协程回调中写入 dict)
            results[name] = {
                "output": output,
                "state_diff": state_diff
            }
        except Exception as e:
            # 异常冒泡，TaskGroup 会捕获并取消其他任务
            raise e

    def _merge_layer_results(self, context: WorkflowContext, results: Dict[str, Any]):
        """
        合并并行结果回主上下文 (Synchronization Point)
        """
        layer_outputs = {}
        
        for node_name, res in results.items():
            output = res["output"]
            state_diff = res["state_diff"]
            
            # [Fix] 处理 Next 对象：确保 History 存储的是实际数据 Input
            # Executor 现在返回原始 Next 对象，这里需要解包
            actual_data = output
            if isinstance(output, Next):
                actual_data = output.input
            
            # 兼容性处理：如果 Executor 返回的是 dict 形式的 Next (被意外 normalize 了)
            elif isinstance(output, dict) and output.get("node") and "<Next" in str(output):
                 actual_data = output.get("input")

            # 更新 History
            context.history[node_name] = actual_data
            layer_outputs[node_name] = actual_data
            
            # 合并 State (Last Write Wins)
            # 并行节点若修改同一 Key，后处理者覆盖前者
            if state_diff:
                context.state.update(state_diff)
        
        # 更新 Last Output (供下一层使用)
        if not layer_outputs:
            return

        if len(layer_outputs) == 1:
            # 单节点：直接作为值
            context.history["last_output"] = list(layer_outputs.values())[0]
        else:
            # 多节点：聚合为字典 {node_name: output}
            context.history["last_output"] = layer_outputs

    def _handle_dynamic_jump(self, results: Dict[str, Any], context: WorkflowContext) -> Optional[Next]:
        """
        检查并处理动态跳转 (Next)
        
        Args:
            results: 当前层的执行结果
            
        Returns:
            找到的第一个 Next 指令 (如果有)
        """
        for res in results.values():
            val = res["output"]
            
            if isinstance(val, Next):
                # 将 Next.input 注入 state 的 _next_input，供下一个节点作为 input 使用
                if val.input is not None:
                    context.state["_next_input"] = val.input
                
                # 处理附带的 state 更新
                if val.update_state:
                    context.state.update(val.update_state)
                    
                return val
        return None

    async def resume(self, session_id: str) -> Any:
        """
        从存储恢复执行
        
        逻辑：
        1. 加载 Checkpoint 数据。
        2. 重建 WorkflowContext。
        3. 确定断点位置 (Next Pointer 优先，其次是 Last Node 的下游)。
        4. 调用 execute 从断点继续运行。
        """
        data = await self.persistence.load_checkpoint(session_id)
        if not data:
            raise ValueError(f"Session {session_id} not found")
            
        try:
            context = WorkflowContext.from_storage_payload(data["context"])
        except Exception as e:
            raise WorkflowError(f"Failed to reconstruct context: {e}") from e

        last_node = data.get("last_node")
        
        # 1. 动态跳转优先 (Next Pointer)
        if context.next_pointer:
            next_node = context.next_pointer.get("target_node")
            # 恢复 Next 携带的输入
            if context.next_pointer.get("input") is not None:
                context.state["_next_input"] = context.next_pointer["input"]
            
            logger.info(f"Resuming workflow from dynamic jump: {next_node}")
            return await self.execute(
                input_data=context.input,
                session_id=session_id,
                start_node=next_node,
                _resume_context=context # 注入恢复后的上下文
            )

        # 2. 静态流程恢复 (Last Node Successor)
        next_node = None
        if last_node:
             edges = self.graph.edges.get(last_node, [])
             if edges:
                 # 简单取第一条出边 (复杂分叉恢复需更完整状态记录)
                 next_node = edges[0][0] 
        
        if next_node:
            logger.info(f"Resuming workflow from static flow: {next_node}")
            return await self.execute(
                input_data=context.input,
                session_id=session_id,
                start_node=next_node,
                _resume_context=context
            )
            
        # 3. 无法确定下一跳，返回历史结果
        logger.warning(f"Could not determine next step from checkpoint {session_id}. Returning last output.")
        return context.get_last_output()
```

[8] gecko/compose/workflow/executor.py
```python
# gecko/compose/workflow/executor.py
"""
节点执行器 (Node Executor) - v0.4 Enhanced

职责：
1. 负责单个节点（Function / Agent / Team）的调度与执行。
2. 实现“智能参数绑定” (Smart Binding)：根据函数签名或类型提示自动注入 Context 或 Input。
3. 处理重试逻辑 (Retries) 与 异常捕获。
4. [v0.4 关键修复] 识别并透传 Next 控制流指令，防止被错误序列化。
"""
from __future__ import annotations

import asyncio
import inspect
import time
from typing import Any, Callable, Dict, Optional, Union

from pydantic import BaseModel

from gecko.core.exceptions import WorkflowError
from gecko.core.logging import get_logger
from gecko.core.message import Message
from gecko.core.utils import ensure_awaitable
from gecko.compose.nodes import Next
from gecko.compose.workflow.models import WorkflowContext, NodeExecution, NodeStatus

logger = get_logger(__name__)


class NodeExecutor:
    """
    节点执行引擎
    独立于 Workflow 主逻辑，专注于“如何执行一个节点”。
    它是无状态的（Stateless），可以安全地在并发环境中使用。
    """

    def __init__(self, enable_retry: bool = False, max_retries: int = 3):
        self.enable_retry = enable_retry
        self.max_retries = max_retries

    async def execute_node(
        self, 
        node_name: str, 
        node_func: Callable, 
        context: WorkflowContext
    ) -> Any:
        """
        执行节点并返回结果（包含状态记录与异常处理）
        
        Args:
            node_name: 节点名称
            node_func: 节点可调用对象
            context: 工作流上下文 (在并行执行中，这是主 Context 的深拷贝)
            
        Returns:
            执行结果（如果是 Next 指令则原样返回，否则返回标准化后的数据）
        """
        # 1. 记录执行开始状态 (Trace)
        # 注意：持久化层会利用 RUNNING 状态做 Pre-Commit
        execution = NodeExecution(node_name=node_name, status=NodeStatus.RUNNING)
        context.add_execution(execution)
        
        try:
            # 2. 执行核心逻辑 (根据配置决定是否重试)
            if self.enable_retry:
                result = await self._execute_with_retry(node_func, context)
            else:
                result = await self._dispatch_call(node_func, context)
            
            # 3. [v0.4 关键修复] 处理控制流指令 (Next)
            # 如果返回值是 Next 对象，必须原样返回给 Engine，
            # 绝不能进行 _normalize_result，否则 Engine 无法识别跳转意图。
            if isinstance(result, Next):
                execution.output_data = f"<Next -> {result.node}>"
                execution.status = NodeStatus.SUCCESS
                return result
            
            # 4. 结果标准化 (Normalization)
            # 将 Pydantic 对象、Message 对象转为 Dict/Str，便于存储和下游使用
            normalized = self._normalize_result(result)
            
            # 更新执行记录 (仅记录预览，防止大对象爆内存)
            execution.output_data = self._safe_preview(normalized)
            execution.status = NodeStatus.SUCCESS
            
            return normalized

        except Exception as e:
            # 5. 异常捕获
            execution.status = NodeStatus.FAILED
            execution.error = str(e)
            # 异常向上冒泡，由 Engine 或 TaskGroup 处理 (取消并行任务)
            raise e
        finally:
            execution.end_time = time.time()

    def _normalize_result(self, result: Any) -> Any:
        """
        标准化结果 (Pydantic Friendly)
        
        将复杂对象转换为可 JSON 序列化的结构，防止存储层报错。
        """
        if isinstance(result, BaseModel):
            return result.model_dump()
        if hasattr(result, "model_dump"):
            return result.model_dump()
        if hasattr(result, "dict"): # 兼容 Pydantic v1
            return result.dict()
        if isinstance(result, Message):
            return result.to_openai_format()
        return result

    def _safe_preview(self, data: Any, limit: int = 200) -> str:
        """生成数据的简略预览（用于日志/Trace）"""
        try:
            s = str(data)
            return s[:limit] + "..." if len(s) > limit else s
        except Exception:
            return "<Unprintable>"

    async def _execute_with_retry(self, func: Callable, context: WorkflowContext) -> Any:
        """执行带有指数退避的重试循环"""
        last_error = None
        for attempt in range(self.max_retries + 1):
            try:
                return await self._dispatch_call(func, context)
            except Exception as e:
                last_error = e
                # 如果不是最后一次尝试，则记录日志并等待
                if attempt < self.max_retries:
                    wait_time = 2 ** attempt
                    logger.warning(
                        f"Node execution failed, retrying ({attempt+1}/{self.max_retries})", 
                        error=str(e),
                        wait_s=wait_time
                    )
                    await asyncio.sleep(wait_time)
        
        # 重试耗尽，抛出最后一次异常
        raise last_error # type: ignore

    async def _dispatch_call(self, func: Callable, context: WorkflowContext) -> Any:
        """
        智能调度器 (Smart Dispatcher)
        
        根据 func 的类型（Agent 对象 vs 普通函数）和签名，
        自动决定如何注入参数。
        """
        # 1. 智能体对象 (Agent/Team 实现了 run 方法)
        if hasattr(func, "run"):
            return await self._run_intelligent_object(func, context)
            
        # 2. 普通函数 (Function/Lambda)
        return await self._run_function(func, context)

    async def _run_intelligent_object(self, obj: Any, context: WorkflowContext) -> Any:
        """
        运行 Agent/Team 对象
        
        策略：
        - 优先检查 state 中是否有 `_next_input` (由上一个节点的 Next 指令显式传递)。
        - 否则使用 `last_output`。
        - 否则使用 原始 `input`。
        """
        # pop 确保 _next_input 是一次性的
        # 注意：在并行执行中，context 是 deepcopy 的，pop 不会影响其他并发分支，是安全的
        if "_next_input" in context.state:
            inp = context.state.pop("_next_input")
        else:
            inp = context.get_last_output()
            
        return await obj.run(inp)

    async def _run_function(self, func: Callable, context: WorkflowContext) -> Any:
        """
        运行普通函数 (参数注入核心逻辑)
        
        策略：
        1. 分析函数签名。
        2. [v0.4 新增] 优先检查 Type Hint：如果参数类型是 WorkflowContext，注入 Context。
        3. 其次检查参数名：如果包含 `context` 或 `workflow_context`，注入 Context。
        4. 如果还有其他位置参数未被填充，注入当前 Input 数据。
        """
        sig = inspect.signature(func)
        kwargs = {}
        args = []
        
        # 1. 确定当前输入数据
        if "_next_input" in context.state:
            current_input = context.state.pop("_next_input")
        else:
            current_input = context.get_last_output()

        # 遍历参数寻找 Context 注入点
        params_to_skip = set(["self"])
        
        for name, param in sig.parameters.items():
            if name in params_to_skip:
                continue
            
            # 策略 A: 基于类型注解 (Type Hint)
            # 注意: 需要处理字符串注解的情况 (from __future__ import annotations)
            annotation = param.annotation
            is_context_type = False
            
            if annotation is not inspect.Parameter.empty:
                if annotation is WorkflowContext:
                    is_context_type = True
                elif isinstance(annotation, str) and "WorkflowContext" in annotation:
                    is_context_type = True
            
            # 策略 B: 基于参数名
            is_context_name = name in ("context", "workflow_context")
            
            if is_context_type or is_context_name:
                kwargs[name] = context
                params_to_skip.add(name)

        # 重新扫描以注入 Input (排除已注入 Context 的参数)
        # 简化逻辑：排除掉 kwargs 中的参数后，剩下的第一个参数给 Input
        remaining_params = [
            p for name, p in sig.parameters.items()
            if name not in kwargs and name != "self"
        ]
        
        if remaining_params:
            # 将 current_input 作为第一个剩余的位置参数传入
            args.append(current_input)
            
        # 4. 执行 (ensure_awaitable 兼容同步/异步函数)
        return await ensure_awaitable(func, *args, **kwargs)
```

[9] gecko/compose/workflow/graph.py
```python
# gecko/compose/workflow/graph.py
"""
DAG 图结构管理器 (v0.4 Enhanced)

职责：
1. 维护节点 (Nodes) 和 边 (Edges)
2. 提供拓扑排序算法，支持并行层级构建 (Phase 1 核心)
3. 进行静态校验（环检测、孤立点检测）
"""
from __future__ import annotations

from collections import deque, defaultdict
from typing import Callable, Dict, List, Optional, Set, Tuple

from gecko.core.exceptions import WorkflowCycleError
from gecko.core.logging import get_logger

logger = get_logger(__name__)


class WorkflowGraph:
    """工作流拓扑结构容器"""

    def __init__(self):
        # 节点注册表: name -> callable
        self.nodes: Dict[str, Callable] = {}
        # 边关系: source -> [(target, condition_func), ...]
        self.edges: Dict[str, List[Tuple[str, Optional[Callable]]]] = {}
        # 显式依赖: node -> {dependencies}
        self.node_dependencies: Dict[str, Set[str]] = {}
        
        self.entry_point: Optional[str] = None
        
        # 缓存验证结果
        self._validated = False
        self._validation_errors: List[str] = []

    def add_node(self, name: str, func: Callable):
        """添加节点"""
        if name in self.nodes:
            raise ValueError(f"Node '{name}' already exists")
        self.nodes[name] = func
        self._validated = False

    def add_edge(self, source: str, target: str, condition: Optional[Callable] = None):
        """添加边"""
        if source not in self.nodes:
            raise ValueError(f"Source node '{source}' not found")
        if target not in self.nodes:
            raise ValueError(f"Target node '{target}' not found")
        
        self.edges.setdefault(source, []).append((target, condition))
        self._validated = False

    def set_entry_point(self, name: str):
        """设置起始节点"""
        if name not in self.nodes:
            raise ValueError(f"Entry point '{name}' not found")
        self.entry_point = name
        self._validated = False

    # ================= 拓扑分析 (Phase 1 Core) =================

    def validate(self, allow_cycles: bool = False, enable_parallel: bool = False) -> Tuple[bool, List[str]]:
        """
        验证图结构合法性
        
        Args:
            allow_cycles: 是否允许环 (v0.4 暂不支持环的自动调度，通常为 False)
            enable_parallel: 是否启用并行模式。
                             若启用，允许一个节点有多个无条件出边 (分叉)。
        """
        if self._validated:
            return not self._validation_errors, self._validation_errors

        self._validation_errors.clear()

        # 1. 入口检查
        if not self.entry_point:
            self._validation_errors.append("No entry point defined")

        # 2. 环检测
        if not allow_cycles:
            try:
                self._detect_cycles()
            except WorkflowCycleError as e:
                self._validation_errors.append(str(e))

        # 3. 歧义分支检测 
        # [v0.4 优化] 如果启用了并行 (enable_parallel=True)，则允许无条件的多个出边 (Fork)
        # 只有在传统串行模式下，才认为多个无条件出边是歧义
        if not enable_parallel:
            for node, edges in self.edges.items():
                unconditional = [t for t, c in edges if c is None]
                if len(unconditional) > 1:
                    self._validation_errors.append(
                        f"Node '{node}' has ambiguous branching: {unconditional}"
                    )

        # 4. 连接性检查 (仅警告，不作为错误)
        self._check_connectivity()

        self._validated = True
        return not self._validation_errors, self._validation_errors
    
    def build_execution_layers(self, start_node: Optional[str]) -> List[Set[str]]:
        r"""
        [核心算法] 构建并行执行层级 (Kahn算法/拓扑排序变体)
        
        功能:
        将 DAG 图转换为一系列执行层。每一层中的节点都只依赖于上一层的输出，
        因此同一层内的节点天然无依赖，可安全并行执行。
        
        示例:
             A
            / \
           B   C
            \ /
             D
        输出: [{A}, {B, C}, {D}]
        
        Args:
            start_node: 以此节点为根，遍历所有可达节点。
            
        Returns:
            List[Set[str]]: 分层列表
        """
        if not start_node or start_node not in self.nodes:
            return []

        # 1. 初始化数据结构
        # 仅跟踪从 start_node 可达的子图，忽略不可达的孤立点
        in_degree: Dict[str, int] = defaultdict(int)
        adj_list: Dict[str, List[str]] = defaultdict(list)
        reachable_nodes: Set[str] = {start_node}
        
        # 2. BFS 遍历构建子图 (计算入度和邻接表)
        queue = deque([start_node])
        
        # 必须显式初始化 start_node 的入度为 0
        in_degree[start_node] = 0

        while queue:
            curr = queue.popleft()
            
            # 获取所有出边的目标节点
            targets = [target for target, _ in self.edges.get(curr, [])]
            
            for target in targets:
                adj_list[curr].append(target)
                in_degree[target] += 1
                
                if target not in reachable_nodes:
                    reachable_nodes.add(target)
                    queue.append(target)

        # 3. 拓扑分层 (Kahn's Algorithm)
        layers: List[Set[str]] = []
        
        # 初始层：所有入度为 0 的节点 (在子图中通常只有 start_node)
        current_layer = {node for node in reachable_nodes if in_degree[node] == 0}

        while current_layer:
            layers.append(current_layer)
            next_layer = set()
            
            for node in current_layer:
                # 遍历当前层节点的所有下游
                for neighbor in adj_list[node]:
                    in_degree[neighbor] -= 1
                    # 当依赖全部满足 (入度归零) 时，加入下一层
                    if in_degree[neighbor] == 0:
                        next_layer.add(neighbor)
            
            current_layer = next_layer

        return layers

    def _check_connectivity(self):
        """检查不可达节点（仅警告）"""
        if not self.entry_point:
            return

        reachable = set()
        queue = [self.entry_point]
        while queue:
            curr = queue.pop(0)
            if curr in reachable:
                continue
            reachable.add(curr)
            # 遍历出边
            for target, _ in self.edges.get(curr, []):
                queue.append(target)
        
        # 计算差集
        unreachable = set(self.nodes.keys()) - reachable
        if unreachable:
            logger.warning("Unreachable nodes detected", nodes=list(unreachable))

    def _detect_cycles(self):
        """DFS 环检测算法"""
        visited = set()
        recursion_stack = set()

        def dfs(node: str, path: List[str]):
            visited.add(node)
            recursion_stack.add(node)
            path.append(node)

            for neighbor, _ in self.edges.get(node, []):
                if neighbor not in visited:
                    dfs(neighbor, path)
                elif neighbor in recursion_stack:
                    cycle_start = path.index(neighbor)
                    cycle = " -> ".join(path[cycle_start:] + [neighbor])
                    raise WorkflowCycleError(f"Cycle detected: {cycle}")

            recursion_stack.remove(node)
            path.pop()

        for node in self.nodes:
            if node not in visited:
                dfs(node, [])
    
    def to_mermaid(self) -> str:
        """生成 Mermaid 流程图代码"""
        lines = ["graph TD"]
        for node in self.nodes:
            # 入口节点使用双圆圈
            shape_start = "((" if node == self.entry_point else "("
            shape_end = "))" if node == self.entry_point else ")"
            lines.append(f"    {node}{shape_start}{node}{shape_end}")
            
        for source, targets in self.edges.items():
            for target, condition in targets:
                label = "|condition|" if condition else ""
                lines.append(f"    {source} --{label}--> {target}")
        return "\n".join(lines)
```

[10] gecko/compose/workflow/models.py
```python
# gecko/compose/workflow/models.py
"""
Workflow 数据模型定义

职责：
1. 定义节点状态枚举 (NodeStatus)
2. 定义持久化策略枚举 (CheckpointStrategy)
3. 定义执行上下文 (WorkflowContext)，并实现核心的“瘦身”逻辑
"""
from __future__ import annotations

import time
import uuid
from enum import Enum
from typing import Any, Dict, List, Optional, Type, TypeVar

from pydantic import BaseModel, Field, PrivateAttr

T = TypeVar("T")


class NodeStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"


class CheckpointStrategy(str, Enum):
    """
    持久化策略
    """
    ALWAYS = "always"  # 每步保存 (开发环境推荐)
    FINAL = "final"    # 仅结束时保存 (生产环境高性能推荐)
    MANUAL = "manual"  # 手动控制


class NodeExecution(BaseModel):
    """节点执行轨迹记录 (Trace)"""
    node_name: str
    status: NodeStatus = NodeStatus.PENDING
    input_data: Any = None
    output_data: Any = None
    error: Optional[str] = None
    start_time: float = Field(default_factory=time.time)
    end_time: float = 0.0

    @property
    def duration(self) -> float:
        if self.end_time == 0.0:
            return 0.0
        return max(0.0, self.end_time - self.start_time)


class WorkflowContext(BaseModel):
    """
    工作流执行上下文
    
    优化：
    实现了 `to_storage_payload` 方法，用于在持久化时剥离监控数据和冗余历史，
    解决生产环境下的 "State Bloat" (状态爆炸) 问题。
    """
    execution_id: str = Field(
        default_factory=lambda: uuid.uuid4().hex,
        description="单次运行的唯一 ID"
    )
    input: Any = Field(..., description="工作流初始输入")
    state: Dict[str, Any] = Field(
        default_factory=dict,
        description="共享状态存储（业务核心数据，全量保存）"
    )
    history: Dict[str, Any] = Field(
        default_factory=dict, 
        description="节点历史输出记录（可能很大）"
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="元数据（如 session_id, trace_id）"
    )
    executions: List[NodeExecution] = Field(
        default_factory=list,
        description="完整执行轨迹（监控数据，持久化时可剥离）"
    )
    next_pointer: Optional[Dict[str, Any]] = Field(
        default=None, 
        description="Next 指令产生的动态跳转目标"
    )

    def add_execution(self, execution: NodeExecution):
        self.executions.append(execution)

    def get_last_output(self) -> Any:
        return self.history.get("last_output", self.input)
    
    def clear_next_pointer(self):
        self.next_pointer = None

    # ================= 核心优化：上下文瘦身 =================

    def to_storage_payload(self, max_history_steps: int = 10) -> Dict[str, Any]:
        """
        [High Priority Fix] 生成用于持久化的轻量级数据包
        
        策略：
        1. 移除 `executions`：轨迹属于监控数据，不存入 Redis/DB 这种运行态存储。
        2. 裁剪 `history`：仅保留最近 N 步，防止 O(N) 的 IO 膨胀。
        """
        # 1. 基础导出，排除 executions
        payload = self.model_dump(mode='python', exclude={'executions'})
        
        # 2. 裁剪 history
        if max_history_steps > 0 and len(self.history) > max_history_steps:
            all_keys = list(self.history.keys())
            keep_keys = all_keys[-max_history_steps:]
            
            # 必须保留 last_output，它是 Next 节点的默认输入
            if "last_output" in self.history and "last_output" not in keep_keys:
                keep_keys.append("last_output")
                
            payload['history'] = {k: self.history[k] for k in keep_keys}
            
        return payload

    @classmethod
    def from_storage_payload(cls, data: Dict[str, Any]) -> "WorkflowContext":
        """从存储数据重建 Context，自动补全缺失字段"""
        if "executions" not in data:
            data["executions"] = []
        return cls.model_validate(data)
    
    def get_summary(self) -> Dict[str, Any]:
        """获取执行摘要"""
        total_time = sum(e.duration for e in self.executions)
        is_failed = any(e.status == NodeStatus.FAILED for e in self.executions)
        return {
            "execution_id": self.execution_id,
            "total_nodes": len(self.executions),
            "total_time": total_time,
            "last_node": self.executions[-1].node_name if self.executions else None,
            "status": "failed" if is_failed else "completed"
        }

    def get_last_output_as(self, type_: Type[T]) -> T:
        """类型安全地获取上一步输出"""
        val = self.get_last_output()
        
        # 1. 直接类型匹配
        if isinstance(val, type_):
            return val
            
        # 2. Pydantic 转换
        if isinstance(val, dict) and hasattr(type_, "model_validate"):
            try:
                return type_.model_validate(val) # type: ignore
            except Exception:
                pass
                
        # 3. 简单类型转换
        try:
            return type_(val) # type: ignore
        except Exception as e:
            raise TypeError(f"Cannot convert last output {type(val)} to {type_}") from e
```

[11] gecko/compose/workflow/persistence.py
```python
# gecko/compose/workflow/persistence.py
"""
持久化管理器

职责：
1. 负责 WorkflowContext 的序列化与反序列化
2. 调用 safe_serialize_context 进行数据清洗 (CPU Bound -> ThreadPool)
3. 执行异步存储操作 (IO Bound)
"""
from __future__ import annotations

import time
from typing import Optional, Any

from anyio.to_thread import run_sync

from gecko.core.logging import get_logger
from gecko.core.utils import safe_serialize_context
from gecko.plugins.storage.interfaces import SessionInterface
from gecko.compose.workflow.models import WorkflowContext, CheckpointStrategy

logger = get_logger(__name__)


class PersistenceManager:
    def __init__(
        self, 
        storage: Optional[SessionInterface],
        strategy: CheckpointStrategy = CheckpointStrategy.ALWAYS,
        history_retention: int = 20
    ):
        self.storage = storage
        self.strategy = strategy
        self.history_retention = history_retention

    async def save_checkpoint(
        self,
        session_id: str,
        steps: int,
        current_node: Optional[str],
        context: WorkflowContext,
        force: bool = False
    ):
        """
        保存检查点 (优化版：瘦身 + 异步清洗)
        """
        if not self.storage or not session_id:
            return

        # 策略检查
        if not force:
            if self.strategy == CheckpointStrategy.MANUAL:
                return
            if self.strategy == CheckpointStrategy.FINAL:
                return

        try:
            # 1. 上下文瘦身 (Context Slimming)
            # 获取纯 Python 字典，移除冗余轨迹和过早历史
            raw_data = context.to_storage_payload(max_history_steps=self.history_retention)
            
            # 2. 深度清洗 (CPU 密集型 -> 卸载到线程池)
            # 清理不可序列化对象 (Lock, Socket 等)
            def _clean_task():
                return safe_serialize_context(raw_data)
            
            clean_context_data = await run_sync(_clean_task)

            # 3. 写入存储 (IO 密集型)
            payload = {
                "step": steps,
                "last_node": current_node,
                "context": clean_context_data,
                "updated_at": time.time(),
            }
            
            await self.storage.set(f"workflow:{session_id}", payload)
            
        except Exception as e:
            logger.warning("Failed to persist workflow state", session_id=session_id, error=str(e))

    async def load_checkpoint(self, session_id: str) -> Optional[dict]:
        """加载检查点数据"""
        if not self.storage:
            return None
        return await self.storage.get(f"workflow:{session_id}")
```

[12] gecko/config.py
```python
# gecko/config.py
"""
全局配置系统 (Final Production Version)

变更记录：
- [Add] Memory: 补充 LRU 缓存大小和摘要预留 Token 配置
- [Add] Telemetry: 补充服务名称与全局开关
"""

from __future__ import annotations

from typing import Optional, Literal

from pydantic import Field, field_validator
from pydantic_settings import BaseSettings, SettingsConfigDict


class GeckoSettings(BaseSettings):
    # ================= 1. Model & Inference (模型与推理) =================
    default_model: str = Field(default="gpt-4o", description="默认 LLM 模型名称")
    default_api_key: str = Field(default="", description="默认 API Key")
    default_base_url: Optional[str] = Field(default=None, description="自定义 API Base URL")
    default_temperature: float = Field(default=0.7, ge=0.0, le=2.0)
    default_model_timeout: float = Field(default=30.0, ge=5.0, description="LLM 请求超时(秒)")

    # ================= 2. Agent Execution (智能体运行时) =================
    max_turns: int = Field(default=10, ge=1, le=100, description="最大对话轮数")
    max_context_tokens: int = Field(default=4000, ge=100, description="上下文窗口限制")

    # ================= 3. Workflow Engine (工作流引擎) =================
    workflow_checkpoint_strategy: Literal["always", "final", "manual"] = Field(
        default="final", 
        description="持久化策略: always(每步), final(仅结束), manual(手动)"
    )
    workflow_history_retention: int = Field(
        default=20, 
        ge=1, 
        description="持久化时保留的历史记录步数(防止 IO 爆炸)"
    )

    # ================= 4. Storage & Database (存储层) =================
    default_storage_url: str = Field(
        default="sqlite:///./gecko_data.db", 
        description="数据库连接串 (sqlite/redis/postgres/chroma)"
    )
    storage_pool_size: int = Field(default=5, ge=1, description="连接池大小")
    storage_max_overflow: int = Field(default=10, ge=0, description="连接池最大溢出")

    # ================= 5. Memory Management (记忆模块) =================
    memory_summary_interval: float = Field(default=30.0, ge=5.0, description="摘要更新防抖间隔(秒)")
    # [新增]
    memory_cache_size: int = Field(default=2000, ge=100, description="Token 计数器的 LRU 缓存大小")
    # [新增]
    memory_summary_reserve_tokens: int = Field(default=500, ge=0, description="为摘要预留的 Token 数")

    # ================= 6. Telemetry & Observability (遥测) =================
    # [新增]
    telemetry_enabled: bool = Field(default=True, description="是否启用 OpenTelemetry")
    # [新增]
    telemetry_service_name: str = Field(default="gecko-app", description="Trace 服务名称")

    # ================= 7. System (系统层) =================
    log_level: str = Field(default="INFO", description="日志级别")
    log_format: Literal["text", "json"] = Field(default="text", description="日志格式")
    enable_cache: bool = Field(default=True, description="是否启用全局缓存")
    tool_execution_timeout: float = Field(default=30.0, ge=1.0, description="工具执行超时(秒)")

    model_config = SettingsConfigDict(
        env_prefix="GECKO_",
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="ignore",
    )

    @field_validator("log_level")
    @classmethod
    def validate_log_level(cls, v: str) -> str:
        valid = {"DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"}
        if v.upper() not in valid:
            raise ValueError(f"log_level must be one of {valid}")
        return v.upper()


# Singleton
_default_settings: Optional[GeckoSettings] = None

def get_settings(force_reload: bool = False) -> GeckoSettings:
    global _default_settings
    if _default_settings is None or force_reload:
        _default_settings = GeckoSettings()
    return _default_settings

def configure_settings(**overrides) -> GeckoSettings:
    global _default_settings
    _default_settings = GeckoSettings(**overrides)
    return _default_settings

def reset_settings():
    global _default_settings
    _default_settings = None

settings = get_settings()
```

[13] gecko/core/__init__.py
```python
```

[14] gecko/core/agent.py
```python
# gecko/core/agent.py  
from __future__ import annotations  
  
from typing import Any, AsyncIterator, Iterable, List, Optional, Type, Union  
  
from pydantic import BaseModel  
  
from gecko.core.events import AgentRunEvent, EventBus  
from gecko.core.events.types import AgentStreamEvent
from gecko.core.message import Message  
from gecko.core.output import AgentOutput  
from gecko.core.toolbox import ToolBox  
from gecko.core.memory import TokenMemory  
from gecko.core.engine.base import CognitiveEngine  
from gecko.core.engine.react import ReActEngine  
from gecko.core.logging import get_logger  
from gecko.core.exceptions import AgentError  
  
logger = get_logger(__name__)  
  
  
class Agent:  
    """  
    Agent 对象负责在模型、工具箱、记忆之间协调一次推理任务。  
    """  
  
    def __init__(  
        self,  
        model: Any,  
        toolbox: ToolBox,
        memory: TokenMemory,  
        engine_cls: Type[CognitiveEngine] = ReActEngine,  
        event_bus: Optional[EventBus] = None,  
        **engine_kwargs: Any,  
    ):  
        self.event_bus = event_bus or EventBus()
        self.toolbox = toolbox
        self.memory = memory
        self.engine = engine_cls(
            model=model,
            toolbox=toolbox,
            event_bus=self.event_bus,
            memory=memory,  
            **engine_kwargs
        )  
  
    # 修复缩进，使其成为类方法而不是 __init__ 的内部函数
    async def run(
        self,
        messages: str | Message | List[Message] | List[dict] | dict,
        response_model: Optional[Type[BaseModel]] = None,
        **kwargs: Any
    ) -> AgentOutput | BaseModel:
        """
        单次推理入口：对多种输入格式统一转换为 Message 列表
        """
        input_msgs = self._normalize_messages(messages)

        # 新增：拼接用户可读文本，供 Guardrails / 日志使用
        raw_text = "\n".join(m.get_text_content() for m in input_msgs)

        await self.event_bus.publish(
            AgentRunEvent(
                type="run_started",
                data={
                    "input": raw_text,
                    "input_count": len(input_msgs),
                },
            )
        )

        try:
            output = await self.engine.step(
                input_msgs, 
                response_model=response_model,
                **kwargs)
            payload = self._serialize_output(output)

            await self.event_bus.publish(
                AgentRunEvent(type="run_completed", data={"output": payload})
            )
            return output

        except Exception as e:
            logger.exception("Agent run failed")
            await self.event_bus.publish(
                AgentRunEvent(type="run_error", error=str(e))
            )
            raise

    async def stream(
        self, 
        messages: str | Message | List[Message] | List[dict] | dict
    ) -> AsyncIterator[str]:
        """
        流式推理（简易模式）：仅返回文本 Token。
        
        [修复] 适配 Engine 的 AgentStreamEvent 输出。
        """
        input_msgs = self._normalize_messages(messages)
        raw_text = "\n".join(m.get_text_content() for m in input_msgs)

        await self.event_bus.publish(
            AgentRunEvent(
                type="stream_started",
                data={"input": raw_text, "input_count": len(input_msgs)},
            )
        )
        
        try:
            # 迭代 Engine 产生的高级事件
            async for event in self.engine.step_stream(input_msgs): # type: ignore
                # 1. 如果是 Token，直接 yield 给用户
                if event.type == "token":
                    yield event.content
                
                # 2. 如果是 Error，记录日志 (是否抛出取决于策略，这里选择记录不中断流，除非是致命错误)
                elif event.type == "error":
                    logger.warning(f"Stream error event: {event.content}")
                    # 可选：yield f"[Error: {event.content}]"
                
                # 3. 其他事件 (tool_input, result) 可以通过 EventBus 转发，
                #    或者在这里忽略，仅供 stream_events 方法使用。
            
            await self.event_bus.publish(AgentRunEvent(type="stream_completed"))
            
        except Exception as e:
            logger.exception("Agent stream failed")
            await self.event_bus.publish(AgentRunEvent(type="stream_error", error=str(e)))
            raise

    async def stream_events(
        self, 
        messages: str | Message | List[Message] | List[dict] | dict
    ) -> AsyncIterator[AgentStreamEvent]:
        """
        [新增] 高级流式接口：返回完整的事件流 (Token, Tools, Result)。
        适用于需要展示“正在思考”、“正在调用工具”等状态的 UI。
        """
        input_msgs = self._normalize_messages(messages)
        
        try:
            async for event in self.engine.step_stream(input_msgs): # type: ignore
                yield event
        except Exception as e:
            # 即使发生异常，也尝试 yield 一个 error 事件给前端
            yield AgentStreamEvent(type="error", content=str(e))
            raise

    # ---------------- 辅助方法 ----------------  
    def _normalize_messages(  
        self,  
        messages: str | Message | List[Message] | List[dict] | dict  
    ) -> List[Message]:  
        """  
        支持以下输入：  
        1. 字符串 -> 单条 user 消息  
        2. Message -> [Message]  
        3. List[Message] -> 原样返回  
        4. dict -> 若包含 role/content 则构建 Message，否则视为 {"input": "..."}  
        5. List[dict] -> 每个 dict 转为 Message  
        """  
        if isinstance(messages, Message):  
            return [messages]  
  
        if isinstance(messages, str):  
            return [Message.user(messages)]  
  
        if isinstance(messages, dict):  
            if "role" in messages:  
                return [Message(**messages)]  
            text = messages.get("input") or str(messages)  
            return [Message.user(text)]  
  
        if isinstance(messages, list):  
            if not messages:  
                raise AgentError("消息列表为空")  
            if isinstance(messages[0], Message):  
                return messages  # type: ignore # 已经是标准 Message  
            normalized = []  
            for item in messages:  
                if isinstance(item, Message):  
                    normalized.append(item)  
                elif isinstance(item, dict):  
                    normalized.append(Message(**item))  
                else:  
                    raise AgentError(f"无法识别的消息元素类型: {type(item)}")  
            return normalized  
  
        raise AgentError(f"不支持的消息类型: {type(messages)}")  
  
    def _serialize_output(self, output: AgentOutput | BaseModel) -> dict:  
        if hasattr(output, "model_dump"):  
            return output.model_dump()  
        return {"content": str(output)}  
```

[15] gecko/core/builder.py
```python
# gecko/core/builder.py  
from __future__ import annotations  
  
from typing import Any, Sequence, Type  
  
from gecko.core.agent import Agent  
from gecko.core.memory import TokenMemory  
from gecko.core.toolbox import ToolBox  
from gecko.core.engine.base import CognitiveEngine  
from gecko.core.engine.react import ReActEngine  
from gecko.plugins.storage.interfaces import SessionInterface  
from gecko.plugins.tools.base import BaseTool  
from gecko.core.exceptions import ConfigurationError  
  
  
class AgentBuilder:  
    """  
    Agent 构建器（改进版）  
    关键改进：  
    1. system_prompt 等引擎参数统一通过 engine_kwargs 传递，避免与 Agent.__init__ 不匹配  
    2. 工具列表自动去重并校验是否继承 BaseTool  
    3. storage 必须实现 SessionInterface，否则在 TokenMemory 中使用会报错  
    4. 支持自定义 Engine 类 & 额外参数  
    """  
  
    def __init__(self):  
        self._model: Any | None = None  
        self._tools: list[BaseTool] = []  
        self._storage: SessionInterface | None = None  
        self._session_id: str = "default"  
        self._max_tokens: int = 4000  
        self._engine_cls: Type[CognitiveEngine] = ReActEngine  
        self._engine_kwargs: dict[str, Any] = {}  # 统一放置系统 Prompt、Hook 等  
        self._toolbox_config: dict[str, Any] = {}  
  
    # ---------------- 基础配置 ----------------  
    def with_model(self, model: Any) -> "AgentBuilder":  
        # 检查模型是否实现必要方法  
        missing = [m for m in ("acompletion",) if not hasattr(model, m)]  
        if missing:  
            raise ConfigurationError(  
                f"Model 缺少必要方法: {', '.join(missing)}",  
                context={"model": repr(model)}  
            )  
        self._model = model  
        return self  
  
    def with_tools(self, tools: Sequence[BaseTool]) -> "AgentBuilder":  
        for tool in tools:  
            if not isinstance(tool, BaseTool):  
                raise TypeError(f"Tool 必须继承 BaseTool，收到 {type(tool)}")  
            self._tools.append(tool)  
        return self  
  
    def with_storage(self, storage: SessionInterface | None) -> "AgentBuilder":  
        if storage and not isinstance(storage, SessionInterface):  
            raise TypeError(  
                "storage 必须实现 SessionInterface，用于 TokenMemory 持久化"  
            )  
        self._storage = storage  
        return self  
  
    def with_session_id(self, session_id: str) -> "AgentBuilder":  
        self._session_id = session_id  
        return self  
  
    def with_max_tokens(self, max_tokens: int) -> "AgentBuilder":  
        self._max_tokens = max_tokens  
        return self  
  
    def with_engine(  
        self,  
        engine_cls: Type[CognitiveEngine],  
        **engine_kwargs: Any  
    ) -> "AgentBuilder":  
        if not issubclass(engine_cls, CognitiveEngine):  
            raise TypeError("engine_cls 必须继承 CognitiveEngine")  
        self._engine_cls = engine_cls  
        self._engine_kwargs.update(engine_kwargs)  
        return self  
  
    def with_system_prompt(self, prompt: str) -> "AgentBuilder":  
        # 统一放入 engine_kwargs，确保 Engine 可接收  
        self._engine_kwargs["system_prompt"] = prompt  
        return self  
  
    def with_toolbox_config(self, **config: Any) -> "AgentBuilder":  
        """  
        允许调用者自定义 ToolBox 的并发/超时等参数  
        """  
        self._toolbox_config.update(config)  
        return self  
  
    # ---------------- 构建流程 ----------------  
    def build(self) -> Agent:  
        if not self._model:  
            raise ConfigurationError("构建 Agent 前必须调用 with_model 指定模型")  
  
        toolbox = self._build_toolbox()  
        memory = self._build_memory()  
  
        return Agent(  
            model=self._model,  
            toolbox=toolbox,  
            memory=memory,  
            engine_cls=self._engine_cls,  
            event_bus=self._engine_kwargs.pop("event_bus", None),  
            **self._engine_kwargs  # 其余参数直接传给 Engine  
        )  
  
    def _build_toolbox(self) -> ToolBox:  
        # 根据工具名称去重，后注册的同名工具会覆盖前者  
        deduped: dict[str, BaseTool] = {}  
        for tool in self._tools:  
            deduped[tool.name] = tool  
  
        return ToolBox(  
            tools=list(deduped.values()),  
            **self._toolbox_config  
        )  
  
    def _build_memory(self) -> TokenMemory:
        # [优化] 将 self._model (ModelProtocol) 注入到 TokenMemory
        # 这样 Memory 就能使用模型特定的 Tokenizer，而不是硬编码的 tiktoken
        return TokenMemory(
            session_id=self._session_id,
            storage=self._storage,
            max_tokens=self._max_tokens,
            model_driver=self._model  # 依赖注入
        )
```

[16] gecko/core/container.py
```python
# gecko/core/container.py
"""
轻量级依赖注入容器

提供服务的注册、解析和生命周期管理。
"""
from __future__ import annotations

import asyncio
import inspect
from typing import (
    Any, Callable, Dict, Generic, Optional, Type, TypeVar, Union, 
    get_type_hints, overload
)
from enum import Enum
from contextlib import asynccontextmanager

from gecko.core.logging import get_logger

logger = get_logger(__name__)

T = TypeVar("T")


class Lifetime(str, Enum):
    """服务生命周期"""
    TRANSIENT = "transient"    # 每次解析创建新实例
    SINGLETON = "singleton"    # 全局单例
    SCOPED = "scoped"          # 作用域内单例


class ServiceDescriptor(Generic[T]):
    """服务描述符"""
    
    def __init__(
        self,
        service_type: Type[T],
        implementation: Union[Type[T], Callable[..., T], T],
        lifetime: Lifetime = Lifetime.TRANSIENT,
    ):
        self.service_type = service_type
        self.implementation = implementation
        self.lifetime = lifetime
        
        # 单例实例缓存
        self._instance: Optional[T] = None
    
    def is_instance(self) -> bool:
        """是否是直接注册的实例"""
        return not (
            inspect.isclass(self.implementation) or 
            callable(self.implementation)
        )


class Container:
    """
    依赖注入容器
    
    示例:
        ```python
        container = Container()
        
        # 注册服务
        container.register(DatabaseService, SQLiteDatabase, Lifetime.SINGLETON)
        container.register_instance(Config, my_config)
        container.register_factory(HttpClient, lambda c: HttpClient(c.resolve(Config)))
        
        # 解析服务
        db = container.resolve(DatabaseService)
        
        # 作用域
        async with container.create_scope() as scope:
            scoped_service = scope.resolve(ScopedService)
        ```
    """
    
    def __init__(self, parent: Optional["Container"] = None):
        self._services: Dict[Type, ServiceDescriptor] = {}
        self._parent = parent
        self._scoped_instances: Dict[Type, Any] = {}
        self._is_scope = parent is not None
    
    # ==================== 注册方法 ====================
    
    def register(
        self,
        service_type: Type[T],
        implementation: Optional[Type[T]] = None,
        lifetime: Lifetime = Lifetime.TRANSIENT,
    ) -> "Container":
        """
        注册服务类型
        
        参数:
            service_type: 服务接口/基类
            implementation: 具体实现类（None 则使用 service_type 自身）
            lifetime: 生命周期
        """
        impl = implementation or service_type
        self._services[service_type] = ServiceDescriptor(
            service_type=service_type,
            implementation=impl,
            lifetime=lifetime,
        )
        logger.debug(
            "Service registered",
            service=service_type.__name__,
            implementation=impl.__name__ if inspect.isclass(impl) else str(impl),
            lifetime=lifetime.value
        )
        return self
    
    def register_singleton(
        self,
        service_type: Type[T],
        implementation: Optional[Type[T]] = None,
    ) -> "Container":
        """注册单例服务"""
        return self.register(service_type, implementation, Lifetime.SINGLETON)
    
    def register_scoped(
        self,
        service_type: Type[T],
        implementation: Optional[Type[T]] = None,
    ) -> "Container":
        """注册作用域服务"""
        return self.register(service_type, implementation, Lifetime.SCOPED)
    
    def register_instance(self, service_type: Type[T], instance: T) -> "Container":
        """
        注册已创建的实例（作为单例）
        """
        descriptor = ServiceDescriptor(
            service_type=service_type,
            implementation=instance,  # type: ignore
            lifetime=Lifetime.SINGLETON,
        )
        descriptor._instance = instance
        self._services[service_type] = descriptor
        logger.debug("Instance registered", service=service_type.__name__)
        return self
    
    def register_factory(
        self,
        service_type: Type[T],
        factory: Callable[["Container"], T],
        lifetime: Lifetime = Lifetime.TRANSIENT,
    ) -> "Container":
        """
        注册工厂函数
        """
        self._services[service_type] = ServiceDescriptor(
            service_type=service_type,
            implementation=factory,
            lifetime=lifetime,
        )
        logger.debug("Factory registered", service=service_type.__name__)
        return self
    
    # ==================== 解析方法 ====================
    
    def resolve(self, service_type: Type[T]) -> T:
        """
        解析服务实例
        
        参数:
            service_type: 服务类型
            
        返回:
            服务实例
            
        异常:
            KeyError: 服务未注册
        """
        # 查找描述符
        descriptor = self._get_descriptor(service_type)
        if descriptor is None:
            raise KeyError(f"Service not registered: {service_type.__name__}")
        
        return self._create_instance(descriptor)
    
    def resolve_optional(self, service_type: Type[T]) -> Optional[T]:
        """解析服务（未注册时返回 None）"""
        try:
            return self.resolve(service_type)
        except KeyError:
            return None
    
    def resolve_all(self, service_type: Type[T]) -> list[T]:
        """解析所有匹配的服务（包括子类）"""
        instances = []
        for registered_type, descriptor in self._services.items():
            if issubclass(registered_type, service_type):
                instances.append(self._create_instance(descriptor))
        return instances
    
    # ==================== 作用域 ====================
    
    @asynccontextmanager
    async def create_scope(self):
        """
        创建作用域容器
        
        示例:
            async with container.create_scope() as scope:
                service = scope.resolve(ScopedService)
        """
        scope = Container(parent=self)
        try:
            yield scope
        finally:
            # 清理作用域实例
            await scope._cleanup_scoped()
    
    async def _cleanup_scoped(self):
        """清理作用域内的实例"""
        for instance in self._scoped_instances.values():
            if hasattr(instance, "close"):
                try:
                    result = instance.close()
                    if asyncio.iscoroutine(result):
                        await result
                except Exception as e:
                    logger.warning("Error closing scoped instance", error=str(e))
        self._scoped_instances.clear()
    
    # ==================== 内部方法 ====================
    
    def _get_descriptor(self, service_type: Type) -> Optional[ServiceDescriptor]:
        """查找服务描述符（包括父容器）"""
        if service_type in self._services:
            return self._services[service_type]
        if self._parent:
            return self._parent._get_descriptor(service_type)
        return None
    
    def _create_instance(self, descriptor: ServiceDescriptor[T]) -> T:
        """创建服务实例"""
        # 单例：返回缓存的实例
        if descriptor.lifetime == Lifetime.SINGLETON:
            if descriptor._instance is not None:
                return descriptor._instance
        
        # 作用域：在当前作用域内缓存
        if descriptor.lifetime == Lifetime.SCOPED:
            if descriptor.service_type in self._scoped_instances:
                return self._scoped_instances[descriptor.service_type]
        
        # 创建实例
        impl = descriptor.implementation
        
        if descriptor.is_instance():
            # 直接注册的实例
            instance = impl
        elif callable(impl):
            if inspect.isclass(impl):
                # 类：自动注入构造函数参数
                instance = self._create_with_injection(impl)
            else:
                # 工厂函数
                instance = impl(self)
        else:
            raise TypeError(f"Cannot create instance from {impl}")
        
        # 缓存
        if descriptor.lifetime == Lifetime.SINGLETON:
            descriptor._instance = instance # type: ignore
        elif descriptor.lifetime == Lifetime.SCOPED:
            self._scoped_instances[descriptor.service_type] = instance
        
        return instance  # type: ignore
    
    def _create_with_injection(self, cls: Type[T]) -> T:
        """创建实例并自动注入依赖"""
        # 获取构造函数参数类型
        try:
            hints = get_type_hints(cls.__init__)
        except Exception:
            hints = {}
        
        # 排除 self 和 return
        hints.pop("return", None)
        
        # 解析依赖
        kwargs = {}
        for param_name, param_type in hints.items():
            if param_type in self._services or (self._parent and param_type in self._parent._services):
                kwargs[param_name] = self.resolve(param_type)
        
        return cls(**kwargs)
    
    # ==================== 工具方法 ====================
    
    def has(self, service_type: Type) -> bool:
        """检查服务是否已注册"""
        return self._get_descriptor(service_type) is not None
    
    def get_registered_services(self) -> list[Type]:
        """获取所有已注册的服务类型"""
        services = list(self._services.keys())
        if self._parent:
            services.extend(self._parent.get_registered_services())
        return services


# 全局默认容器
_default_container: Optional[Container] = None


def get_container() -> Container:
    """获取默认容器"""
    global _default_container
    if _default_container is None:
        _default_container = Container()
    return _default_container


def configure_container(container: Container) -> None:
    """设置默认容器"""
    global _default_container
    _default_container = container


# ==================== 导出 ====================

__all__ = [
    "Container",
    "Lifetime",
    "ServiceDescriptor",
    "get_container",
    "configure_container",
]
```

[17] gecko/core/deprecation.py
```python
# gecko/core/deprecation.py

import functools
import warnings
from typing import Optional, Callable

def deprecated(
    version: str,
    alternative: Optional[str] = None,
    removal_version: Optional[str] = None
) -> Callable:
    """
    标记函数为废弃
    
    示例:
        @deprecated("0.3.0", alternative="new_function", removal_version="1.0.0")
        def old_function():
            pass
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            msg = f"{func.__name__} is deprecated since v{version}."
            if alternative:
                msg += f" Use {alternative} instead."
            if removal_version:
                msg += f" Will be removed in v{removal_version}."
            
            warnings.warn(msg, DeprecationWarning, stacklevel=2)
            return func(*args, **kwargs)
        
        # 更新文档
        deprecation_note = f"\n\n.. deprecated:: {version}\n"
        if alternative:
            deprecation_note += f"   Use :func:`{alternative}` instead.\n"
        
        wrapper.__doc__ = (func.__doc__ or "") + deprecation_note
        return wrapper
    
    return decorator
```

[18] gecko/core/engine/base.py
```python
# gecko/core/engine/base.py
"""
认知引擎基类

定义 Agent 的推理和执行流程，所有引擎实现（ReAct、Chain、Tree 等）
都应继承此基类。

核心概念：
- CognitiveEngine: 抽象基类，定义引擎接口
- 支持普通推理和流式推理
- 支持结构化输出
- 提供 Hook 机制
- 统一的错误处理

优化点：
1. 强化类型注解（使用 ModelProtocol）
2. 完善抽象方法（step, step_stream, step_structured）
3. 添加 Hook 机制（before_step, after_step）
4. 提供工具方法（validate_input, log_execution）
5. 支持上下文管理器（资源管理）
"""
from __future__ import annotations

import asyncio
import time
from abc import ABC, abstractmethod
from typing import Any, AsyncIterator, Callable, Dict, List, Optional, Type, TypeVar

from pydantic import BaseModel

from gecko.core.events.bus import EventBus
from gecko.core.exceptions import AgentError, ModelError
from gecko.core.logging import get_logger
from gecko.core.memory import TokenMemory
from gecko.core.message import Message
from gecko.core.output import AgentOutput
from gecko.core.protocols import ModelProtocol, supports_streaming, validate_model
from gecko.core.toolbox import ToolBox

logger = get_logger(__name__)

T = TypeVar("T", bound=BaseModel)


# ====================== 执行统计 ======================

class ExecutionStats(BaseModel):
    """
    引擎执行统计
    
    用于性能监控和调试。
    """
    total_steps: int = 0
    total_time: float = 0.0
    total_tokens: int = 0
    tool_calls: int = 0
    errors: int = 0
    
    def add_step(self, duration: float, tokens: int = 0, had_error: bool = False):
        """记录一次步骤执行"""
        self.total_steps += 1
        self.total_time += duration
        self.total_tokens += tokens
        if had_error:
            self.errors += 1
    
    def add_tool_call(self):
        """记录一次工具调用"""
        self.tool_calls += 1
    
    def get_avg_step_time(self) -> float:
        """获取平均步骤时间"""
        return self.total_time / self.total_steps if self.total_steps > 0 else 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """转换为字典"""
        return {
            "total_steps": self.total_steps,
            "total_time": self.total_time,
            "avg_step_time": self.get_avg_step_time(),
            "total_tokens": self.total_tokens,
            "tool_calls": self.tool_calls,
            "errors": self.errors,
        }


# ====================== 认知引擎基类 ======================

class CognitiveEngine(ABC):
    """
    认知引擎抽象基类
    
    定义 Agent 的核心推理流程，所有具体引擎实现（ReAct、Chain、Tree 等）
    都应该继承此类。
    
    核心方法：
    - step(): 单次/多轮推理（必需）
    - step_stream(): 流式推理（可选）
    - step_structured(): 结构化输出（可选）
    
    Hook 方法：
    - before_step(): 步骤执行前
    - after_step(): 步骤执行后
    - on_error(): 错误处理
    
    生命周期：
    - initialize(): 初始化
    - cleanup(): 清理资源
    
    示例:
        ```python
        class MyEngine(CognitiveEngine):
            async def step(self, input_messages: List[Message]) -> AgentOutput:
                # 实现推理逻辑
                response = await self.model.acompletion(
                    messages=[m.to_openai_format() for m in input_messages]
                )
                return AgentOutput(content=response.choices[0].message["content"])
        
        # 使用
        engine = MyEngine(model=model, toolbox=toolbox, memory=memory)
        output = await engine.step([Message.user("Hello")])
        ```
    """
    
    def __init__(
        self,
        model: ModelProtocol,
        toolbox: ToolBox,
        memory: TokenMemory,
        event_bus: Optional[EventBus] = None,
        max_iterations: int = 10,
        enable_stats: bool = True,
        **kwargs
    ):
        """
        初始化认知引擎
        
        参数:
            model: 语言模型（必须实现 ModelProtocol）
            toolbox: 工具箱
            memory: 记忆管理器
            max_iterations: 最大迭代次数（防止死循环）
            enable_stats: 是否启用统计
            **kwargs: 子类的额外参数
        
        异常:
            TypeError: model 不符合 ModelProtocol
        """
        # 验证模型（鸭子类型检查）
        # 如果缺少必要方法，会由 validate_model 抛出带有 Missing methods 提示的 TypeError
        validate_model(model)
        self.model = model
        
        self.toolbox = toolbox
        self.event_bus = event_bus
        self.memory = memory
        self.max_iterations = max_iterations
        self.enable_stats = enable_stats
        
        # 统计信息
        self.stats = ExecutionStats() if enable_stats else None
        
        # Hook 函数（可由子类或外部设置）
        self.before_step_hook: Optional[Callable] = None
        self.after_step_hook: Optional[Callable] = None
        self.on_error_hook: Optional[Callable] = None
        
        # 存储额外的配置
        self._config = kwargs
        
        logger.debug(
            "Engine initialized",
            engine=self.__class__.__name__,
            model=type(model).__name__,
            max_iterations=max_iterations
        )
    
    # ====================== 核心抽象方法 ======================
    
    @abstractmethod
    async def step(
        self, 
        input_messages: List[Message],
        **kwargs
    ) -> AgentOutput:
        """
        执行推理步骤（必需实现）
        
        这是引擎的核心方法，定义了如何处理输入并生成输出。
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数（如 temperature, max_tokens 等）
        
        返回:
            AgentOutput: 执行结果
        
        异常:
            AgentError: 执行失败
            ModelError: 模型调用失败
        
        实现指南:
            1. 验证输入
            2. 调用 before_step_hook（如果有）
            3. 执行推理逻辑
            4. 调用 after_step_hook（如果有）
            5. 返回结果
        
        示例:
            ```python
            async def step(self, input_messages: List[Message]) -> AgentOutput:
                # 转换为 OpenAI 格式
                messages = [m.to_openai_format() for m in input_messages]
                
                # 调用模型
                response = await self.model.acompletion(messages=messages)
                
                # 构建输出
                return AgentOutput(
                    content=response.choices[0].message["content"],
                    usage=response.usage
                )
            ```
        """
        pass
    
    # ====================== 可选方法 ======================
    
    async def step_stream(
        self, 
        input_messages: List[Message],
        **kwargs
    ) -> AsyncIterator[str]:
        """
        流式推理（可选实现）
        
        如果引擎支持流式输出，应该重写此方法。
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数
        
        返回:
            AsyncIterator[str]: 文本流
        
        异常:
            NotImplementedError: 引擎不支持流式输出
        
        示例:
            ```python
            async def step_stream(self, input_messages: List[Message]):
                if not supports_streaming(self.model):
                    raise NotImplementedError("Model does not support streaming")
                
                messages = [m.to_openai_format() for m in input_messages]
                
                async for chunk in self.model.astream(messages=messages):
                    if chunk.content:
                        yield chunk.content
            ```
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} does not support streaming. "
            f"Override step_stream() to enable this feature."
        )
    
    async def step_structured(
        self,
        input_messages: List[Message],
        response_model: Type[T],
        **kwargs
    ) -> T:
        """
        结构化输出推理（可选实现）
        
        执行推理并将输出解析为 Pydantic 模型。
        
        参数:
            input_messages: 输入消息列表
            response_model: 目标 Pydantic 模型类
            **kwargs: 额外参数
        
        返回:
            T: 解析后的模型实例
        
        异常:
            NotImplementedError: 引擎不支持结构化输出
        
        示例:
            ```python
            from pydantic import BaseModel
            
            class Answer(BaseModel):
                question: str
                answer: str
                confidence: float
            
            result = await engine.step_structured(
                input_messages=[Message.user("What is AI?")],
                response_model=Answer
            )
            print(result.answer)
            ```
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} does not support structured output. "
            f"Override step_structured() to enable this feature."
        )
    
    # ====================== Hook 方法 ======================
    
    async def before_step(
        self, 
        input_messages: List[Message],
        **kwargs
    ) -> None:
        """
        步骤执行前的 Hook
        
        在推理开始前调用，可用于：
        - 日志记录
        - 输入验证
        - 状态初始化
        - 发送事件
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数
        
        注意:
            此方法不应修改输入，如需修改请在子类中重写
        """
        if self.before_step_hook:
            try:
                if asyncio.iscoroutinefunction(self.before_step_hook):
                    await self.before_step_hook(input_messages, **kwargs)
                else:
                    self.before_step_hook(input_messages, **kwargs)
            except Exception as e:
                logger.warning("before_step_hook failed", error=str(e))
    
    async def after_step(
        self,
        input_messages: List[Message],
        output: AgentOutput,
        **kwargs
    ) -> None:
        """
        步骤执行后的 Hook
        
        在推理完成后调用，可用于：
        - 日志记录
        - 结果验证
        - 统计更新
        - 发送事件
        
        参数:
            input_messages: 输入消息列表
            output: 执行结果
            **kwargs: 额外参数
        """
        if self.after_step_hook:
            try:
                if asyncio.iscoroutinefunction(self.after_step_hook):
                    await self.after_step_hook(input_messages, output, **kwargs)
                else:
                    self.after_step_hook(input_messages, output, **kwargs)
            except Exception as e:
                logger.warning("after_step_hook failed", error=str(e))
    
    async def on_error(
        self,
        error: Exception,
        input_messages: List[Message],
        **kwargs
    ) -> None:
        """
        错误处理 Hook
        
        在推理过程中发生错误时调用，可用于：
        - 错误日志记录
        - 错误恢复
        - 降级处理
        - 发送告警
        
        参数:
            error: 异常对象
            input_messages: 输入消息列表
            **kwargs: 额外参数
        """
        if self.on_error_hook:
            try:
                if asyncio.iscoroutinefunction(self.on_error_hook):
                    await self.on_error_hook(error, input_messages, **kwargs)
                else:
                    self.on_error_hook(error, input_messages, **kwargs)
            except Exception as e:
                logger.error("on_error_hook failed", error=str(e))
    
    # ====================== 工具方法 ======================
    
    def validate_input(self, input_messages: List[Message]) -> None:
        """
        验证输入消息
        
        参数:
            input_messages: 输入消息列表
        
        异常:
            ValueError: 输入无效
        """
        if not input_messages:
            raise ValueError("input_messages 不能为空")
        
        if not all(isinstance(m, Message) for m in input_messages):
            raise TypeError("所有输入必须是 Message 实例")
        
        logger.debug("Input validated", message_count=len(input_messages))
    
    def supports_streaming(self) -> bool:
        """
        检查引擎是否支持流式输出
        
        返回:
            是否支持
        """
        # 检查模型能力
        model_supports = supports_streaming(self.model)
        
        # 检查引擎是否重写了 step_stream
        engine_supports = (
            self.__class__.step_stream != CognitiveEngine.step_stream
        )
        
        return model_supports and engine_supports
    
    def get_config(self, key: str, default: Any = None) -> Any:
        """
        获取配置项
        
        参数:
            key: 配置键
            default: 默认值
        
        返回:
            配置值
        """
        return self._config.get(key, default)
    
    def set_config(self, key: str, value: Any) -> None:
        """
        设置配置项
        
        参数:
            key: 配置键
            value: 配置值
        """
        self._config[key] = value
    
    def get_stats(self) -> Optional[Dict[str, Any]]:
        """
        获取执行统计
        
        返回:
            统计信息字典，如果未启用统计则返回 None
        """
        return self.stats.to_dict() if self.stats else None
    
    def reset_stats(self) -> None:
        """重置统计信息"""
        if self.stats:
            self.stats = ExecutionStats()
            logger.debug("Stats reset")
    
    # ====================== 生命周期管理 ======================
    
    async def initialize(self) -> None:
        """
        初始化引擎
        
        在首次使用前调用，可用于：
        - 加载资源
        - 预热模型
        - 初始化连接
        
        子类可以重写此方法以添加自定义初始化逻辑。
        """
        logger.debug("Engine initialized", engine=self.__class__.__name__)
    
    async def cleanup(self) -> None:
        """
        清理资源
        
        在引擎不再使用时调用，可用于：
        - 关闭连接
        - 释放资源
        - 保存状态
        
        子类可以重写此方法以添加自定义清理逻辑。
        """
        logger.debug("Engine cleanup", engine=self.__class__.__name__)
    
    # ====================== 上下文管理器 ======================
    
    async def __aenter__(self):
        """异步上下文管理器入口"""
        await self.initialize()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """异步上下文管理器出口"""
        await self.cleanup()
        return False
    
    # ====================== 辅助方法 ======================
    
    async def _safe_execute(
        self,
        func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        安全执行函数（带错误处理和统计）
        
        参数:
            func: 要执行的函数
            *args: 位置参数
            **kwargs: 关键字参数
        
        返回:
            函数执行结果
        
        异常:
            原始异常（已记录日志和统计）
        """
        start_time = time.time()
        had_error = False
        
        try:
            if asyncio.iscoroutinefunction(func):
                result = await func(*args, **kwargs)
            else:
                result = func(*args, **kwargs)
            
            return result
        
        except Exception as e:
            had_error = True
            
            # 记录统计
            if self.stats:
                self.stats.errors += 1
            
            # 调用错误 Hook
            await self.on_error(e, kwargs.get("input_messages", []))
            
            # 记录日志
            logger.exception(
                "Engine execution failed",
                engine=self.__class__.__name__,
                error=str(e)
            )
            
            raise
        
        finally:
            # 记录执行时间
            duration = time.time() - start_time
            if self.stats:
                self.stats.add_step(duration, had_error=had_error)
    
    def __repr__(self) -> str:
        """字符串表示"""
        return (
            f"{self.__class__.__name__}("
            f"model={type(self.model).__name__}, "
            f"max_iterations={self.max_iterations}"
            f")"
        )


# ====================== 工具函数 ======================

def create_engine(
    engine_class: Type[CognitiveEngine],
    model: ModelProtocol,
    toolbox: ToolBox,
    memory: TokenMemory,
    **kwargs
) -> CognitiveEngine:
    """
    创建引擎实例（工厂函数）
    
    参数:
        engine_class: 引擎类
        model: 模型
        toolbox: 工具箱
        memory: 记忆
        **kwargs: 额外参数
    
    返回:
        引擎实例
    
    示例:
        ```python
        engine = create_engine(
            ReActEngine,
            model=openai_model,
            toolbox=toolbox,
            memory=memory,
            max_iterations=5
        )
        ```
    """
    if not issubclass(engine_class, CognitiveEngine):
        raise TypeError(
            f"engine_class 必须是 CognitiveEngine 的子类，"
            f"收到: {engine_class.__name__}"
        )
    
    return engine_class(
        model=model,
        toolbox=toolbox,
        memory=memory,
        **kwargs
    )


# ====================== 导出 ======================

__all__ = [
    "CognitiveEngine",
    "ExecutionStats",
    "create_engine",
]
```

[19] gecko/core/engine/buffer.py
```python
# gecko/core/engine/buffer.py
"""
流式缓冲区模块

核心职责：
1. 解决 OpenAI 协议中流式 Tool Call 分片传输、乱序到达的问题。
2. 解决 LLM 输出不规范 JSON (如 Markdown 包裹) 导致的解析崩溃问题。
"""
from __future__ import annotations

import json
import re
from typing import Dict, List, Any, Optional

from gecko.core.message import Message
from gecko.core.protocols import StreamChunk
from gecko.core.logging import get_logger

logger = get_logger(__name__)

class StreamBuffer:
    """
    流式响应聚合缓冲区。
    
    使用场景：
    在 Engine 的 Thinking 阶段，随着 LLM 流式吐出 token，此类负责实时聚合，
    并在最后产出一个结构完整的 Message 对象。
    """
    def __init__(self):
        self.content_parts: List[str] = []
        # tool_index -> tool_call_dict (处理并发或分片传输的工具调用)
        self.tool_calls_map: Dict[int, Dict[str, Any]] = {}
        
    def add_chunk(self, chunk: StreamChunk) -> Optional[str]:
        """
        接收一个流式块 (StreamChunk)，更新内部状态。
        
        返回：
            Optional[str]: 本次 chunk 中新增的文本内容（用于流式回显）。
            如果是纯工具调用的 chunk，返回 None。
        """
        delta = chunk.delta
        
        # 1. 聚合文本内容
        content = delta.get("content")
        if content:
            self.content_parts.append(content)
            
        # 2. 聚合工具调用 (处理 index 可能不连续或乱序的情况)
        if delta.get("tool_calls"):
            for tc in delta["tool_calls"]:
                idx = tc.get("index")
                if idx is None: 
                    continue
                
                # 初始化该索引的结构
                if idx not in self.tool_calls_map:
                    self.tool_calls_map[idx] = {
                        "id": "", 
                        "type": "function", 
                        "function": {"name": "", "arguments": ""}
                    }
                
                target = self.tool_calls_map[idx]
                
                # 增量拼接 ID
                if tc.get("id"): 
                    target["id"] += tc["id"]
                
                # 增量拼接函数名和参数
                func = tc.get("function", {})
                if func.get("name"): 
                    target["function"]["name"] += func["name"]
                if func.get("arguments"): 
                    target["function"]["arguments"] += func["arguments"]
        
        return content

    def build_message(self) -> Message:
        """
        构建最终的 Message 对象。
        
        关键逻辑：
        在此阶段会对收集到的 JSON 参数字符串进行“清洗”和“修复”，
        防止因为 Markdown 符号或引号问题导致后续工具执行失败。
        """
        full_content = "".join(self.content_parts)
        tool_calls_list = []
        
        # 按索引排序，确保工具调用顺序一致
        for idx in sorted(self.tool_calls_map.keys()):
            raw_tc = self.tool_calls_map[idx]
            
            # [生产级增强] 深度清洗参数 JSON
            raw_args = raw_tc["function"]["arguments"]
            cleaned_args = self._clean_arguments(raw_args)
            
            # 更新清洗后的参数
            raw_tc["function"]["arguments"] = cleaned_args
            
            tool_calls_list.append(raw_tc)
            
        return Message.assistant(
            content=full_content,
            # 只有当列表非空时才设值，符合 Pydantic 定义及 OpenAI 规范
            tool_calls=tool_calls_list if tool_calls_list else None
        )

    def _clean_arguments(self, raw_json: str) -> str:
        """
        清洗 LLM 输出的脏 JSON 字符串。
        
        常见问题修复：
        1. Markdown 代码块包裹 (```json ... ```)
        2. 首尾多余的误加引号 ('{...}')
        """
        if not raw_json: 
            return "{}"
            
        # 1. 尝试直接解析（这是最快路径，如果模型输出规范）
        try:
            json.loads(raw_json)
            return raw_json
        except json.JSONDecodeError:
            pass
            
        cleaned = raw_json.strip()
        
        # 2. 去除 Markdown 代码块
        # 匹配 ```json {...} ``` 或 ``` {...} ```
        match = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", cleaned)
        if match:
            cleaned = match.group(1)
            
        # 3. 简单修复：去除首尾多余的误加引号
        # 例如模型输出了: '{"arg": "val"}' (带单引号的字符串)
        if cleaned.startswith("'") and cleaned.endswith("'"):
            cleaned = cleaned[1:-1]
        elif cleaned.startswith('"') and cleaned.endswith('"'):
            # 只有当看起来是误加的外部引号时才去除
            cleaned = cleaned[1:-1]

        # 4. 再次尝试解析验证 (Best Effort)
        try:
            json.loads(cleaned)
            return cleaned
        except Exception:
            # 如果还无法解析，记录警告并返回原始内容，让上层 Engine/ToolBox 报错
            # 这样可以在 Tool Output 中反馈给 LLM，让它自己修正
            logger.warning(f"Failed to clean JSON arguments: {raw_json[:50]}...")
            return raw_json
```

[20] gecko/core/engine/react.py
```python
# gecko/core/engine/react.py
"""
ReAct 推理引擎 (Production Grade / 生产级)

架构设计：
1. 无状态设计 (Stateless): 
   Engine 实例不持有任何单次请求的状态，所有状态封装在 ExecutionContext 中。
   这使得 Engine 单例可以在多线程/异步环境下安全地处理并发请求。

2. 生命周期分解 (Lifecycle Decomposition):
   将复杂的 ReAct while 循环拆解为 _phase_think (思考), _phase_act (行动), 
   _phase_observe (观察) 三个独立阶段。子类（如 ReflexionEngine）可以重写特定阶段
   而不必复制整个循环逻辑。

3. 事件驱动 (Event Driven):
   统一使用 AgentStreamEvent 协议，消除了 yield 返回类型不明确的问题。
   同步接口 step 仅仅是流式接口 step_stream 的消费者。

4. 鲁棒性 (Robustness):
   - 集成 StreamBuffer 处理流式碎片及修复不规范 JSON。
   - 内置死循环熔断机制。
   - 内置观察值截断机制，防止 Context Window 爆炸。
"""
from __future__ import annotations

import json
from datetime import datetime
from typing import (
    Any,
    AsyncIterator,
    Callable,
    Dict,
    List,
    Optional,
    Type,
    TypeVar,
    Union,
    cast,
)

from pydantic import BaseModel

from gecko.core.engine.base import CognitiveEngine
from gecko.core.engine.buffer import StreamBuffer
from gecko.core.events.types import AgentStreamEvent
from gecko.core.protocols import StreamChunk 
from gecko.core.toolbox import ToolExecutionResult 
from gecko.core.exceptions import AgentError
from gecko.core.logging import get_logger
from gecko.core.memory import TokenMemory
from gecko.core.message import Message
from gecko.core.output import AgentOutput
from gecko.core.prompt import PromptTemplate
from gecko.core.structure import StructureEngine
from gecko.core.toolbox import ToolBox
from gecko.core.utils import ensure_awaitable

logger = get_logger(__name__)

T = TypeVar("T", bound=BaseModel)

# 默认的 ReAct 提示词模板
DEFAULT_REACT_TEMPLATE = """You are a helpful AI assistant.
Current Time: {{ current_time }}

Available Tools:
{% for tool in tools %}
- {{ tool['function']['name'] }}: {{ tool['function']['description'] }}
{% endfor %}

Answer the user's request. Use tools if necessary.
If you use a tool, just output the tool call format.
"""

class ExecutionContext:
    """
    执行上下文 (Runtime Context)
    
    承载单次 Agent.run/stream 的所有运行时状态。
    随请求创建，随请求销毁。
    """
    def __init__(self, messages: List[Message]):
        # 浅拷贝消息列表，防止污染传入的原始列表，但在处理过程中会追加新消息
        self.messages = messages.copy()
        self.turn = 0
        self.metadata: Dict[str, Any] = {}
        
        # --- 状态追踪：用于死循环检测与错误熔断 ---
        self.consecutive_errors: int = 0  # 连续工具错误次数
        self.last_tool_hash: Optional[int] = None # 上一次工具调用的指纹

    def add_message(self, message: Message) -> None:
        """追加消息到当前上下文历史"""
        self.messages.append(message)

    @property
    def last_message(self) -> Message:
        """获取历史中最后一条消息，用于检查 LLM 的最新输出"""
        if not self.messages:
            raise ValueError("Context is empty")
        return self.messages[-1]


class ReActEngine(CognitiveEngine):
    """
    生产级 ReAct 引擎实现。
    """

    def __init__(
        self,
        model: Any,
        toolbox: ToolBox,
        memory: TokenMemory,
        max_turns: int = 10,
        max_observation_length: int = 2000,
        system_prompt: Union[str, PromptTemplate, None] = None,
        # 生命周期钩子 Hooks
        on_turn_start: Optional[Callable[[ExecutionContext], Any]] = None,
        on_turn_end: Optional[Callable[[ExecutionContext], Any]] = None,
        **kwargs: Any,
    ):
        super().__init__(model, toolbox, memory, **kwargs)
        self.max_turns = max_turns
        self.max_observation_length = max_observation_length
        self.on_turn_start = on_turn_start
        self.on_turn_end = on_turn_end

        # 初始化 System Prompt 模板
        if system_prompt is None:
            self.prompt_template = PromptTemplate(template=DEFAULT_REACT_TEMPLATE)
        elif isinstance(system_prompt, str):
            self.prompt_template = PromptTemplate(template=system_prompt)
        else:
            self.prompt_template = system_prompt

        # 检测模型能力：是否支持原生 Function Calling
        self._supports_functions = getattr(self.model, "_supports_function_calling", True)

    # ================= 核心入口 (Public API) =================

    async def step( # type: ignore
        self,
        input_messages: List[Message],
        response_model: Optional[Type[T]] = None,
        max_retries: int = 0,
        **kwargs: Any,
    ) -> Union[AgentOutput, T]:
        """
        同步执行入口。
        
        逻辑流程：
        1. 作为一个消费者，迭代 `step_stream` 产生的事件流。
        2. 忽略中间的 Token 事件，只捕获 `result` 或处理 `error`。
        3. 如果指定了 `response_model`，则对结果进行结构化解析。
        4. 如果解析失败且配置了重试，则将错误反馈给模型并重新运行推理。
        
        参数:
            input_messages: 用户输入消息列表
            response_model: (可选) Pydantic 模型，用于强制结构化输出
            max_retries: 结构化解析失败时的最大重试次数
            **kwargs: 传递给 LLM 的额外参数
        """
        
        # 将 response_model 放入 kwargs 传递给 step_stream，以便底层构建 Tool Schema
        kwargs['response_model'] = response_model

        # 内部闭包：执行一次完整的推理流程，直到产生结果或报错
        async def _run_once(msgs: List[Message]) -> Optional[AgentOutput]:
            final_res = None
            async for event in self.step_stream(msgs, **kwargs):
                if event.type == "result" and event.data:
                    # 从事件载荷中提取 AgentOutput 对象
                    final_res = cast(AgentOutput, event.data.get("output"))
                elif event.type == "error":
                    # 遇到错误直接抛出，中断流程
                    logger.error(f"Engine step error: {event.content}")
                    raise AgentError(event.content)
            return final_res

        # 1. 首次运行
        # 浅拷贝列表，因为如果需要重试，我们会在这个列表上追加反馈消息
        current_messages = list(input_messages) 
        final_output = await _run_once(current_messages)
        
        if not final_output:
            return AgentOutput(content="[System Error] No output generated.")
            
        # 2. 结构化解析 + 自动重试循环
        # 如果不需要结构化输出，直接返回 AgentOutput
        if response_model:
            attempts = 0
            while True:
                try:
                    # 优先策略 A: 尝试从 Tool Calls 中解析 (OpenAI 模式)
                    # 如果 LLM 正确调用了我们注入的结构化工具，数据会在 tool_calls 中
                    if final_output.tool_calls:
                        return await StructureEngine.parse(
                            content="", 
                            model_class=response_model, 
                            raw_tool_calls=final_output.tool_calls
                        )
                    # 优先策略 B: 回退到 Content 解析 (JSON Mode / Text 提取)
                    return await StructureEngine.parse(final_output.content, response_model)
                
                except Exception as e:
                    # 解析失败，检查是否还有重试机会
                    if attempts >= max_retries:
                        raise AgentError(f"Structured parsing failed: {e}") from e
                    
                    attempts += 1
                    logger.warning(f"Structure parse failed, retrying ({attempts}/{max_retries})")
                    
                    # 构造反馈消息并追加到本次会话历史中
                    # 让模型看到自己上次的输出和对应的错误信息
                    current_messages.append(Message.assistant(
                        content=final_output.content,
                        tool_calls=final_output.tool_calls
                    ))
                    current_messages.append(Message.user(
                        f"Error parsing response: {e}. Please try again using the correct format."
                    ))
                    
                    # 再次运行 Engine (Retry)
                    final_output = await _run_once(current_messages)
                    if not final_output:
                         raise AgentError("Retry returned no output")

        return final_output

    async def step_stream( # type: ignore
        self, 
        input_messages: List[Message], 
        **kwargs: Any
    ) -> AsyncIterator[AgentStreamEvent]:
        """
        流式执行入口。
        
        生成 `AgentStreamEvent` 流，包含：
        - `token`: 实时生成的文本片段
        - `tool_input`: 工具调用意图和参数
        - `tool_output`: 工具执行结果
        - `result`: 最终生成的回复
        - `error`: 执行过程中的非致命错误或异常
        """
        # 1. 基础校验与 Hook
        self.validate_input(input_messages)
        await self.before_step(input_messages, **kwargs)

        # 2. 构建执行上下文 (Context) - 这是线程安全的局部变量
        context = await self._build_execution_context(input_messages)
        
        try:
            # 3. 进入生命周期主循环
            async for event in self._execute_lifecycle(context, **kwargs):
                yield event
                
        except Exception as e:
            logger.exception("Lifecycle execution crashed")
            await self.on_error(e, input_messages, **kwargs)
            # 将未捕获的异常转换为 Error 事件抛出给前端
            yield AgentStreamEvent(type="error", content=str(e))
            raise

    # ================= 生命周期主循环 (Lifecycle) =================

    async def _execute_lifecycle(
        self, 
        context: ExecutionContext, 
        **kwargs: Any
    ) -> AsyncIterator[AgentStreamEvent]:
        """
        ReAct 核心循环：Think -> Act -> Observe
        """
        
        # 预处理：检查是否在进行结构化输出
        # 如果是，我们需要获取那个“虚拟工具”的名称，以便拦截它
        response_model = kwargs.get('response_model')
        structure_tool_name = None
        if response_model and self._supports_functions:
            schema = StructureEngine.to_openai_tool(response_model)
            structure_tool_name = schema["function"]["name"]

        while context.turn < self.max_turns:
            context.turn += 1
            
            # Hook: 轮次开始
            if self.on_turn_start:
                await ensure_awaitable(self.on_turn_start, context)

            # ---------------- Phase 1: Think (Reasoning) ----------------
            buffer = StreamBuffer() # 创建新的流式缓冲区
            
            # 调用底层 LLM 生成流 (yield StreamChunk)
            async for chunk in self._phase_think(context, **kwargs):
                # 实时计算文本增量并加入缓冲区
                text_delta = buffer.add_chunk(chunk)
                if text_delta:
                    # 实时 yield 文本 Token 给前端
                    yield AgentStreamEvent(type="token", content=text_delta)
            
            # 从缓冲区构建完整的 Assistant 消息 (包含清洗过的 JSON 参数)
            assistant_msg = buffer.build_message()
            context.add_message(assistant_msg)

            # --- Safety Check: 死循环熔断 ---
            if self._detect_loop(context, assistant_msg):
                err_msg = "Infinite loop detected."
                yield AgentStreamEvent(type="error", content=err_msg)
                break # 中断执行

            # --- Decision: 检查是否调用了结构化输出工具 ---
            # 如果 LLM 调用了我们指定的结构化工具，说明它完成了任务
            # 我们拦截这个调用，不传给 ToolBox，直接作为结果返回
            tool_calls = assistant_msg.safe_tool_calls
            
            if structure_tool_name and tool_calls:
                # 查找目标工具
                target_call = next((tc for tc in tool_calls if tc["function"]["name"] == structure_tool_name), None)
                if target_call:
                    # 这是一个结构化输出结果
                    final_output = AgentOutput(
                        content="", # 内容在 tool_calls 里
                        tool_calls=[target_call], 
                        metadata={"is_structured": True}
                    )
                    yield AgentStreamEvent(type="result", data={"output": final_output})
                    break # 任务结束

            # --- Decision: 正常结束 ---
            # 如果没有工具调用，说明 LLM 输出了纯文本回复，任务结束
            if not tool_calls:
                final_output = AgentOutput(
                    content=str(assistant_msg.content),
                    tool_calls=[],
                )
                yield AgentStreamEvent(type="result", data={"output": final_output})
                break

            # ---------------- Phase 2: Act (Tool Execution) ----------------
            # 通知前端：即将执行工具
            yield AgentStreamEvent(
                type="tool_input", 
                data={"tools": tool_calls}
            )
            
            # 执行工具 (并发)
            tool_results = await self._phase_act(tool_calls)
            
            for res in tool_results:
                # 截断过长的输出，防止 Context Window 爆炸
                content_to_save = self._truncate_observation(res.result, res.tool_name)
                
                # 将结果写入上下文
                context.add_message(
                    Message.tool_result(res.call_id, content_to_save, res.tool_name)
                )
                
                # 通知前端：工具执行完成
                yield AgentStreamEvent(
                    type="tool_output", 
                    content=res.result,
                    data={"tool_name": res.tool_name, "is_error": res.is_error}
                )

            # ---------------- Phase 3: Observe (Reflection Hook) ----------------
            # 这是一个关键的扩展点，子类可以重写此方法实现 Reflexion 或 Human-in-the-loop
            should_continue = await self._phase_observe(context, tool_results)
            
            # Hook: 轮次结束
            if self.on_turn_end:
                await ensure_awaitable(self.on_turn_end, context)
            
            # Checkpoint: 持久化上下文 (防止进程崩溃导致进度丢失)
            await self._save_context(context)

            if not should_continue:
                stop_output = AgentOutput(content="Task stopped by system monitor.")
                yield AgentStreamEvent(type="result", data={"output": stop_output})
                break

    # ================= 阶段实现 (Protected Methods) =================
    # 这些方法设计为可被子类 (如 ReflexionEngine) 重写

    async def _phase_think(
        self, 
        context: ExecutionContext, 
        **kwargs: Any
    ) -> AsyncIterator[StreamChunk]:
        """
        Thinking 阶段：构造 Prompt 并调用 LLM。
        """
        messages_payload = [m.to_openai_format() for m in context.messages]
        
        # 1. 构建参数
        # 如果存在 response_model，_build_llm_params 会处理 tool_choice
        llm_params = self._build_llm_params(kwargs.get('response_model'), "auto")
        
        # 2. 合并用户传入的 kwargs (移除 response_model 防止污染)
        safe_kwargs = {k: v for k, v in kwargs.items() if k not in ['response_model']}
        llm_params.update(safe_kwargs)
        
        # 3. 强制开启流式
        llm_params["stream"] = True 
        
        # 4. 调用模型 (返回底层流生成器)
        stream_gen = self.model.astream(messages=messages_payload, **llm_params) # type: ignore
        async for chunk in stream_gen:
            yield chunk

    async def _phase_act(
        self, 
        tool_calls: List[Dict[str, Any]]
    ) -> List[ToolExecutionResult]:
        """
        Acting 阶段：标准化工具调用并并发执行。
        """
        # _normalize_tool_call 会尝试解析 JSON 字符串为 Dict
        flat_calls = [self._normalize_tool_call(tc) for tc in tool_calls]
        return await self.toolbox.execute_many(flat_calls)

    async def _phase_observe(
        self, 
        context: ExecutionContext, 
        results: List[ToolExecutionResult]
    ) -> bool:
        """
        Observing 阶段：分析执行结果，决定是否继续。
        
        默认策略：
        - 统计连续错误次数。
        - 如果连续 3 次工具调用失败，注入系统提示 (System Alert) 引导模型修正。
        """
        error_count = sum(1 for r in results if r.is_error)
        
        if error_count > 0:
            context.consecutive_errors += 1
        else:
            context.consecutive_errors = 0
            
        if context.consecutive_errors >= 3:
            logger.warning("Too many consecutive tool errors.")
            context.add_message(Message.user(
                "System Alert: The last 3 tool calls failed. "
                "Please stop repeating the same action. "
                "Analyze the error message and change your parameters or approach."
            ))
            # 重置计数器，给模型最后一次尝试修正的机会
            context.consecutive_errors = 0 
            return True
            
        return True

    # ================= 辅助方法 (Helpers) =================

    def _detect_loop(self, context: ExecutionContext, msg: Message) -> bool:
        """
        死循环检测：检查当前工具调用是否与上一次完全一致 (Hash 碰撞)。
        """
        if not msg.safe_tool_calls:
            return False
        
        try:
            # 序列化工具调用以计算指纹 (Name + Args)
            # sort_keys=True 确保字典顺序一致
            calls_dump = json.dumps(
                [
                    {
                        "name": tc.get("function", {}).get("name"),
                        "args": tc.get("function", {}).get("arguments"),
                    }
                    for tc in msg.safe_tool_calls
                ],
                sort_keys=True,
            )
            current_hash = hash(calls_dump)

            if context.last_tool_hash == current_hash:
                logger.warning("Infinite tool loop detected", calls=calls_dump)
                return True

            context.last_tool_hash = current_hash
            return False
        except Exception:
            # 序列化失败时不触发熔断
            return False

    def _truncate_observation(self, content: str, tool_name: str) -> str:
        """截断过长的工具输出，保留头部信息"""
        if len(content) > self.max_observation_length:
            logger.info(
                f"Truncating output for tool {tool_name}", 
                original_len=len(content)
            )
            return (
                content[: self.max_observation_length]
                + f"\n...(truncated, total {len(content)} chars)"
            )
        return content

    def _normalize_tool_call(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:
        """
        将 OpenAI 格式工具调用转换为 ToolBox 所需的扁平格式。
        在此处尝试解析 arguments JSON 字符串。
        """
        func_block = tool_call.get("function", {})
        name = func_block.get("name", "")
        raw_args = func_block.get("arguments", "{}")
        
        parsed_args: Dict[str, Any] = {}
        try:
            if isinstance(raw_args, str):
                parsed_args = json.loads(raw_args)
            elif isinstance(raw_args, dict):
                parsed_args = raw_args
        except json.JSONDecodeError as e:
            # 解析失败时，不抛出异常，而是传递特殊标记给 ToolBox
            # ToolBox 会识别此标记并返回友好的错误提示给 LLM
            parsed_args = {
                "__gecko_parse_error__": f"JSON format error: {str(e)}. Content: {raw_args}"
            }

        return {
            "id": tool_call.get("id", ""),
            "name": name,
            "arguments": parsed_args,
        }

    async def _build_execution_context(self, input_messages: List[Message]) -> ExecutionContext:
        """加载历史并构建 ExecutionContext"""
        history = await self._load_history()
        all_messages = history + input_messages
        
        # 自动注入 System Prompt（如果历史中不存在）
        has_system = any(m.role == "system" for m in all_messages)
        if not has_system:
            template_vars = {
                "tools": self.toolbox.to_openai_schema(),
                "current_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }
            system_content = self.prompt_template.format_safe(**template_vars)
            all_messages.insert(0, Message.system(system_content))
            
        return ExecutionContext(all_messages)

    async def _load_history(self) -> List[Message]:
        """从 Memory 加载历史消息"""
        if not self.memory.storage:
            return []
        try:
            data = await self.memory.storage.get(self.memory.session_id)
            if data and "messages" in data:
                return await self.memory.get_history(data["messages"])
        except Exception:
            return []
        return []

    async def _save_context(self, context: ExecutionContext, force: bool = False) -> None:
        """保存当前上下文到 Memory"""
        if not self.memory.storage:
            return
        try:
            messages_data = [m.to_openai_format() for m in context.messages]
            await self.memory.storage.set(
                self.memory.session_id, {"messages": messages_data}
            )
        except Exception as e:
            logger.warning("Failed to save context", error=str(e))

    def _build_llm_params(self, response_model: Any, strategy: str) -> Dict[str, Any]:
        """
        构建 LLM 调用参数 (Tools, Tool Choice)
        """
        params: Dict[str, Any] = {}
        tools_schema = self.toolbox.to_openai_schema()

        # 1. 结构化输出模式 (Structure Mode)
        if response_model and self._supports_functions:
            # 将 Response Model 转换为 Tool Schema
            structure_tool = StructureEngine.to_openai_tool(response_model)
            
            # 合并现有工具 (不能修改原列表)
            combined_tools = tools_schema + [structure_tool]
            params["tools"] = combined_tools
            
            # 强制调用该工具 (OpenAI `tool_choice` syntax)
            params["tool_choice"] = {
                "type": "function",
                "function": {"name": structure_tool["function"]["name"]}
            }
            
        # 2. 标准 ReAct 模式 (Standard Mode)
        elif tools_schema and self._supports_functions:
            params["tools"] = tools_schema
            params["tool_choice"] = "auto"
            
        return params
```

[21] gecko/core/error_codes.py
```python
# gecko/core/error_codes.py
"""
统一错误码系统

为框架提供结构化的错误分类和标识。
"""
from __future__ import annotations

from enum import Enum
from typing import Dict, NamedTuple, Optional


class ErrorCategory(str, Enum):
    """错误类别"""
    GENERAL = "general"
    MODEL = "model"
    TOOL = "tool"
    STORAGE = "storage"
    WORKFLOW = "workflow"
    VALIDATION = "validation"
    SECURITY = "security"


class ErrorInfo(NamedTuple):
    """错误信息"""
    category: ErrorCategory
    message: str
    retryable: bool
    severity: str  # "info", "warning", "error", "critical"


class ErrorCode(str, Enum):
    """错误码枚举"""
    # 通用
    UNKNOWN = "UNKNOWN"
    TIMEOUT = "TIMEOUT"
    CANCELLED = "CANCELLED"

    # 验证
    VALIDATION_FAILED = "VALIDATION_FAILED"
    INVALID_INPUT = "INVALID_INPUT"
    INVALID_CONFIG = "INVALID_CONFIG"

    # 模型
    MODEL_AUTH_FAILED = "MODEL_AUTH_FAILED"
    MODEL_RATE_LIMITED = "MODEL_RATE_LIMITED"
    MODEL_CONTEXT_EXCEEDED = "MODEL_CONTEXT_EXCEEDED"
    MODEL_UNAVAILABLE = "MODEL_UNAVAILABLE"

    # 工具
    TOOL_NOT_FOUND = "TOOL_NOT_FOUND"
    TOOL_EXECUTION_FAILED = "TOOL_EXECUTION_FAILED"
    TOOL_TIMEOUT = "TOOL_TIMEOUT"

    # 存储
    STORAGE_CONNECTION_FAILED = "STORAGE_CONNECTION_FAILED"
    STORAGE_READ_FAILED = "STORAGE_READ_FAILED"
    STORAGE_WRITE_FAILED = "STORAGE_WRITE_FAILED"

    # 工作流
    WORKFLOW_VALIDATION_FAILED = "WORKFLOW_VALIDATION_FAILED"
    WORKFLOW_CYCLE_DETECTED = "WORKFLOW_CYCLE_DETECTED"
    WORKFLOW_MAX_STEPS_EXCEEDED = "WORKFLOW_MAX_STEPS_EXCEEDED"

    # 安全
    SECURITY_INJECTION_DETECTED = "SECURITY_INJECTION_DETECTED"


# 错误码元数据
_ERROR_INFO: Dict[ErrorCode, ErrorInfo] = {
    ErrorCode.UNKNOWN: ErrorInfo(
        ErrorCategory.GENERAL, "An unknown error occurred", False, "error"
    ),
    ErrorCode.TIMEOUT: ErrorInfo(
        ErrorCategory.GENERAL, "Operation timed out", True, "warning"
    ),
    ErrorCode.CANCELLED: ErrorInfo(
        ErrorCategory.GENERAL, "Operation was cancelled", False, "info"
    ),
    ErrorCode.VALIDATION_FAILED: ErrorInfo(
        ErrorCategory.VALIDATION, "Validation failed", False, "error"
    ),
    ErrorCode.INVALID_INPUT: ErrorInfo(
        ErrorCategory.VALIDATION, "Invalid input provided", False, "error"
    ),
    ErrorCode.INVALID_CONFIG: ErrorInfo(
        ErrorCategory.VALIDATION, "Invalid configuration", False, "error"
    ),
    ErrorCode.MODEL_AUTH_FAILED: ErrorInfo(
        ErrorCategory.MODEL, "Model authentication failed", False, "error"
    ),
    ErrorCode.MODEL_RATE_LIMITED: ErrorInfo(
        ErrorCategory.MODEL, "Model rate limit exceeded", True, "warning"
    ),
    ErrorCode.MODEL_CONTEXT_EXCEEDED: ErrorInfo(
        ErrorCategory.MODEL, "Model context window exceeded", False, "error"
    ),
    ErrorCode.MODEL_UNAVAILABLE: ErrorInfo(
        ErrorCategory.MODEL, "Model service unavailable", True, "error"
    ),
    ErrorCode.TOOL_NOT_FOUND: ErrorInfo(
        ErrorCategory.TOOL, "Tool not found", False, "error"
    ),
    ErrorCode.TOOL_EXECUTION_FAILED: ErrorInfo(
        ErrorCategory.TOOL, "Tool execution failed", False, "error"
    ),
    ErrorCode.TOOL_TIMEOUT: ErrorInfo(
        ErrorCategory.TOOL, "Tool execution timed out", True, "warning"
    ),
    ErrorCode.STORAGE_CONNECTION_FAILED: ErrorInfo(
        ErrorCategory.STORAGE, "Storage connection failed", True, "error"
    ),
    ErrorCode.STORAGE_READ_FAILED: ErrorInfo(
        ErrorCategory.STORAGE, "Storage read operation failed", True, "error"
    ),
    ErrorCode.STORAGE_WRITE_FAILED: ErrorInfo(
        ErrorCategory.STORAGE, "Storage write operation failed", True, "error"
    ),
    ErrorCode.WORKFLOW_VALIDATION_FAILED: ErrorInfo(
        ErrorCategory.WORKFLOW, "Workflow validation failed", False, "error"
    ),
    ErrorCode.WORKFLOW_CYCLE_DETECTED: ErrorInfo(
        ErrorCategory.WORKFLOW, "Workflow cycle detected", False, "error"
    ),
    ErrorCode.WORKFLOW_MAX_STEPS_EXCEEDED: ErrorInfo(
        ErrorCategory.WORKFLOW, "Workflow max steps exceeded", False, "error"
    ),
    ErrorCode.SECURITY_INJECTION_DETECTED: ErrorInfo(
        ErrorCategory.SECURITY, "Potential injection attack detected", False, "critical"
    ),
}


def get_error_info(code: ErrorCode) -> ErrorInfo:
    """获取错误信息"""
    return _ERROR_INFO.get(
        code,
        ErrorInfo(ErrorCategory.GENERAL, str(code), False, "error")
    )


def is_retryable(code: ErrorCode) -> bool:
    """判断错误是否可重试"""
    return get_error_info(code).retryable


def get_message(code: ErrorCode) -> str:
    """获取错误消息"""
    return get_error_info(code).message


__all__ = [
    "ErrorCode",
    "ErrorCategory",
    "ErrorInfo",
    "get_error_info",
    "is_retryable",
    "get_message",
]
```

[22] gecko/core/events/__init__.py
```python
from gecko.core.events.types import BaseEvent
from gecko.core.events.bus import EventBus, EventHandler, Middleware
from gecko.core.events.presets import AgentRunEvent, WorkflowEvent, SessionEvent

__all__ = [
    "EventBus", "BaseEvent", 
    "AgentRunEvent", "WorkflowEvent", "SessionEvent",
    "EventHandler", "Middleware"
]
```

[23] gecko/core/events/bus.py
```python
"""事件总线逻辑"""
from __future__ import annotations
import asyncio
import inspect
from typing import Any, Awaitable, Callable, Dict, List, Optional, Set, Union
from gecko.core.logging import get_logger
from gecko.core.events.types import BaseEvent

logger = get_logger(__name__)

# EventHandler 可以是返回 None 的同步函数，或者返回 Awaitable 的函数
EventHandler = Callable[[BaseEvent], Union[Awaitable[None], None, Any]]
Middleware = Callable[[BaseEvent], Awaitable[Optional[BaseEvent]]]


class EventBus:
    """
    异步事件总线
    """
    
    def __init__(self):
        self._subscribers: Dict[str, List[EventHandler]] = {}
        self._middlewares: List[Middleware] = []
        self._background_tasks: Set[asyncio.Task] = set()
        self._running = True

    # --- 订阅管理 ---
    
    def subscribe(self, event_type: str, handler: EventHandler) -> "EventBus":
        """
        订阅事件
        支持通配符 "*" 订阅所有事件
        """
        if not callable(handler):
            raise TypeError(f"Event handler must be callable, got {type(handler)}")
        
        self._subscribers.setdefault(event_type, []).append(handler)
        logger.debug("Handler subscribed", event_type=event_type, handler=handler)
        return self

    def unsubscribe(self, event_type: str, handler: EventHandler) -> "EventBus":
        """取消订阅"""
        handlers = self._subscribers.get(event_type, [])
        if handler in handlers:
            handlers.remove(handler)
            logger.debug("Handler unsubscribed", event_type=event_type, handler=handler)
        return self

    def add_middleware(self, middleware: Middleware) -> "EventBus":
        """
        添加中间件
        """
        self._middlewares.append(middleware)
        return self

    # --- 发布事件 ---
    async def publish(self, event: BaseEvent, wait: bool = False):
        """
        发布事件
        
        参数:
            event: 事件对象
            wait: 是否等待所有处理器执行完毕
        """
        if not self._running:
            logger.warning("EventBus is shutting down, event ignored", event_type=event.type)
            return

        # 保存原始事件类型用于日志（因为中间件可能返回 None）
        original_type = event.type

        # 1. 执行中间件
        try:
            for mw in self._middlewares:
                event = await mw(event) # type: ignore
                if event is None:
                    logger.debug("Event blocked by middleware", event_type=original_type)
                    return
        except Exception as e:
            logger.error("Middleware error", error=str(e), event_type=original_type)
            return

        # 2. 获取订阅者
        type_handlers = self._subscribers.get(event.type, [])
        wildcard_handlers = self._subscribers.get("*", [])
        
        if not type_handlers and not wildcard_handlers:
            return
        
        # 3. 修复: 使用 id() 进行更健壮的去重
        # dict.fromkeys() 依赖对象的 __hash__，对于方法和 lambda 可能不可靠
        seen_ids = set()
        unique_handlers = []

        for h in type_handlers + wildcard_handlers:
            handler_id = id(h)
            if handler_id not in seen_ids:
                seen_ids.add(handler_id)
                unique_handlers.append(h)
        
        # 创建执行协程
        tasks = [self._execute_handler(h, event) for h in unique_handlers]

        if wait:
            await asyncio.gather(*tasks, return_exceptions=True)
        else:
            for coro in tasks:
                task = asyncio.create_task(coro)
                self._background_tasks.add(task)
                task.add_done_callback(self._background_tasks.discard)
    
    async def _execute_handler(self, handler: EventHandler, event: BaseEvent):
        """
        执行单个处理器（包含错误捕获）
        
        采用统一的调用方式：先调用，再判断返回值是否为 Awaitable。
        这兼容了 async def, def, 以及 async __call__ 对象。
        """
        try:
            result = handler(event)
            if inspect.isawaitable(result):
                await result
        except Exception as e:
            logger.exception(
                "Event handler failed", 
                event_type=event.type, 
                handler=getattr(handler, "__name__", str(handler)),
                error=str(e)
            )

    # --- 生命周期 ---

    async def shutdown(self, wait: bool = True):
        """
        关闭事件总线
        """
        self._running = False
        
        if wait and self._background_tasks:
            count = len(self._background_tasks)
            if count > 0:
                logger.info("Waiting for background tasks to finish", count=count)
                await asyncio.gather(*self._background_tasks, return_exceptions=True)
        
        self._background_tasks.clear()
        logger.info("EventBus shutdown completed")

    async def __aenter__(self):
        self._running = True
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.shutdown()
```

[24] gecko/core/events/presets.py
```python
"""预置系统事件"""
from gecko.core.events.types import BaseEvent

class AgentRunEvent(BaseEvent):
    """Agent 运行事件"""
    pass

class WorkflowEvent(BaseEvent):
    """Workflow 运行事件"""
    pass

class SessionEvent(BaseEvent):
    """会话变更事件"""
    pass
```

[25] gecko/core/events/types.py
```python
# gecko/core/events/types.py
"""
事件类型定义模块
定义了系统内流转的标准事件结构。
"""
from __future__ import annotations
import time
from typing import Any, Dict, Optional, Literal
from pydantic import BaseModel, Field

class BaseEvent(BaseModel):
    """
    通用事件基类
    用于 EventBus 的内部消息分发。
    """
    type: str
    timestamp: float = Field(default_factory=time.time)
    data: Dict[str, Any] = Field(default_factory=dict)
    error: Optional[str] = None
    
    model_config = {"arbitrary_types_allowed": True}

class AgentStreamEvent(BaseModel):
    """
    Agent 流式输出的标准协议。
    
    设计目的：
    解决流式生成过程中 yield 类型不统一的问题。
    前端或调用方应根据 `type` 字段决定如何渲染或处理。
    
    字段说明：
    - token:       LLM 生成的文本片段（增量）
    - tool_input:  Agent 决定调用工具，包含工具名和参数
    - tool_output: 工具执行完毕，包含执行结果
    - error:       执行过程中发生的非致命错误或警告
    - result:      执行结束，包含最终的 AgentOutput 对象
    """
    type: Literal["token", "tool_input", "tool_output", "error", "result"] = Field(
        ..., description="事件类型"
    )
    content: str = Field(default="", description="文本内容（如 Token 片段、错误详情、工具结果摘要）")
    data: Dict[str, Any] = Field(default_factory=dict, description="结构化载荷（如完整的工具调用对象、AgentOutput对象）")

    model_config = {
        "arbitrary_types_allowed": True,
        "populate_by_name": True
    }
```

[26] gecko/core/exceptions.py
```python
# gecko/core/exceptions.py
"""
统一异常与错误码系统

- 提供 GeckoError 及各子类异常（AgentError、ModelError、WorkflowError 等）
- 复用 gecko.core.error_codes 中定义的 ErrorCode / ErrorCategory 等
"""

from __future__ import annotations

from typing import Any, Optional

from gecko.core.error_codes import (  # 复用已有错误码定义
    ErrorCode,
    ErrorCategory,
    ErrorInfo,
    get_error_info,
    is_retryable,
    get_message,
)


class GeckoError(Exception):
    """
    Gecko 框架所有自定义异常的基类。

    属性:
        code: ErrorCode（逻辑错误码，默认为 UNKNOWN）
        context: 额外上下文信息（调试/日志）
        error_code: 兼容部分调用点传入的自定义字符串 error_code
    """

    def __init__(self, message: str, **kwargs: Any):
        # 标准化字段
        self.code: ErrorCode = kwargs.pop("code", ErrorCode.UNKNOWN)
        self.context: dict[str, Any] = kwargs.pop("context", {}) or {}
        # 某些调用点会传入 error_code="AUTH_ERROR" 这类字符串，这里兼容保留
        self.error_code: Any = kwargs.pop("error_code", None)

        super().__init__(message)

    def __str__(self) -> str:
        base = super().__str__()
        return f"{base} (code={self.code})"


class AgentError(GeckoError):
    """Agent 级别错误（输入不合法、内部状态异常等）"""
    pass


class ModelError(GeckoError):
    """模型调用相关错误（鉴权失败、限流、上下文超限等）"""
    pass


class ConfigurationError(GeckoError):
    """配置错误（缺少必要参数、无效配置项等）"""
    pass


class ToolError(GeckoError):
    """工具执行错误"""
    pass


class ToolNotFoundError(GeckoError):
    """工具未找到"""

    def __init__(self, tool_name: str, **kwargs: Any):
        msg = f"Tool '{tool_name}' not found"
        super().__init__(msg, **kwargs)
        self.context.setdefault("tool_name", tool_name)


class WorkflowError(GeckoError):
    """Workflow 编排/执行过程中的错误"""
    pass


class WorkflowCycleError(WorkflowError):
    """Workflow 结构中检测到环"""
    pass


class StorageError(GeckoError):
    """存储后端读写错误"""
    pass


__all__ = [
    # 异常类
    "GeckoError",
    "AgentError",
    "ModelError",
    "ConfigurationError",
    "ToolError",
    "ToolNotFoundError",
    "WorkflowError",
    "WorkflowCycleError",
    "StorageError",
    # 错误码相关（从 error_codes 复用）
    "ErrorCode",
    "ErrorCategory",
    "ErrorInfo",
    "get_error_info",
    "is_retryable",
    "get_message",
]
```

[27] gecko/core/logging.py
```python
# gecko/core/logging.py
"""
Gecko 结构化日志系统（改进版）

改进要点：
1. 优先使用 structlog（如未安装则自动降级到标准 logging）
2. 支持 trace_id / span_id / 额外上下文注入，便于分布式追踪与问题排查
3. 使用 ContextVar 保存追踪上下文，兼容多线程 / 异步场景
4. 修复：
   - ContextVar(default_factory=...) 的错误用法
   - structlog 不可用时 ContextLogger 与标准 logging 的兼容问题
"""

from __future__ import annotations

from contextlib import contextmanager
from contextvars import ContextVar
import logging
import sys
import uuid
import warnings
from typing import Any, Dict, Optional

# ========= 可选依赖：structlog =========

try:
    import structlog

    STRUCTLOG_AVAILABLE = True
except ImportError:
    # 如果未安装 structlog，则降级为标准 logging，同时给出一次性警告
    STRUCTLOG_AVAILABLE = False
    warnings.warn(
        "structlog 未安装，将回退到标准 logging。\n"
        "如需结构化日志，请安装：pip install structlog",
        ImportWarning,
    )

from gecko.config import settings

# ========= 追踪上下文变量 =========
#
# 说明：
#   - 使用 ContextVar 而不是全局变量，可以在异步/多线程环境下保持各自独立的上下文
#   - trace_id / span_id 用于分布式追踪
#   - extra_context 用于附加业务相关的上下文字段（如 user_id, request_id 等）

# 追踪 ID（链路级别）
trace_id_var: ContextVar[str] = ContextVar("trace_id", default="")

# Span ID（单次调用级别）
span_id_var: ContextVar[str] = ContextVar("span_id", default="")

# 额外上下文字段
# 注意：default 使用 {} 虽然是可变对象，但：
#   - ContextVar 的 default 只在「从未 set」时作为初始返回值
#   - 我们在使用时会立即 .copy() 并 set 回去，不会修改这个默认对象本身
extra_context_var: ContextVar[Dict[str, Any]] = ContextVar(
    "extra_context",
    default={},  # 默认空字典；实际使用时会 copy
)


def generate_trace_id() -> str:
    """
    生成追踪 ID（16 字节十六进制字符串）

    建议：在一次完整请求 / 会话的生命周期内复用同一个 trace_id，
    便于跨服务聚合日志。
    """
    return uuid.uuid4().hex[:16]


def generate_span_id() -> str:
    """
    生成 Span ID（8 字节十六进制字符串）

    通常每一个子操作 / 步骤使用一个新的 span_id，
    和 trace_id 一起用于构建调用树。
    """
    return uuid.uuid4().hex[:8]


# ========= 日志初始化 =========

_initialized = False  # 模块内全局标记，避免重复初始化


def setup_logging(
    level: Optional[str] = None,
    force: bool = False,
) -> None:
    """
    初始化日志系统。

    参数：
        level:
            日志级别字符串，如 "DEBUG" / "INFO" / "WARNING"。
            默认为 settings.log_level。
        force:
            为 True 时强制重新初始化（但目前不会清理旧 handler，仅重新配置）。

    设计说明：
        - 优先使用 structlog 进行结构化日志输出
        - 如未安装 structlog，则降级为标准 logging.basicConfig
        - 并统一降低部分高噪声三方库的日志级别到 WARNING
    """
    global _initialized

    # 如果已经初始化过，且未要求强制重置，则直接返回
    if _initialized and not force:
        return

    # ========= 屏蔽特定三方库的烦人警告 =========
    # LiteLLM / Pydantic 的序列化警告
    warnings.filterwarnings(
        "ignore",
        message=".*Pydantic serializer warnings.*",
        category=UserWarning,
    )
    # DuckDuckGo backend='api' 废弃警告
    warnings.filterwarnings(
        "ignore",
        message=".*backend='api' is deprecated.*",
        category=UserWarning,
    )

    # 解析日志级别：优先使用参数，其次 settings.log_level，兜底为 INFO
    level = level or settings.log_level
    log_level = getattr(logging, level.upper(), logging.INFO)

    # structlog 优先；否则回退到标准 logging
    if STRUCTLOG_AVAILABLE:
        _setup_structlog(log_level)
    else:
        _setup_standard_logging(log_level)

    # 降低易产生噪声的三方库日志级别
    for lib in ["httpx", "httpcore", "litellm", "openai"]:
        logging.getLogger(lib).setLevel(logging.WARNING)

    _initialized = True


def _setup_structlog(level: int) -> None:
    """
    配置 structlog 日志系统。

    说明：
        - 使用 PrintLoggerFactory 直接打印到 stdout
        - 使用 JSONRenderer 或 ConsoleRenderer 输出结构化日志
        - 不再使用 structlog.contextvars.merge_contextvars，
          上下文注入统一通过 ContextLogger 完成，避免混淆
    """
    assert STRUCTLOG_AVAILABLE  # 仅在 structlog 可用时调用

    structlog.configure(  # type: ignore
        processors=[
            # 注入日志级别字段
            structlog.processors.add_log_level,  # type: ignore
            # 渲染 stack_info（如设置了 stack_info=True）
            structlog.processors.StackInfoRenderer(),  # type: ignore
            # 将 exc_info 信息转换为可序列化结构（结合 logger.exception 使用）
            structlog.dev.set_exc_info,  # type: ignore
            # 增加时间戳字段，ISO 格式，UTC 时区
            structlog.processors.TimeStamper(fmt="iso", utc=True),  # type: ignore
            # 最终渲染器：JSON 或控制台友好格式
            (
                structlog.processors.JSONRenderer()  # type: ignore
                if settings.log_format == "json"  # type: ignore
                else structlog.dev.ConsoleRenderer()  # type: ignore
            ),
        ],
        # 根据日志级别过滤日志
        wrapper_class=structlog.make_filtering_bound_logger(level),  # type: ignore
        # 上下文字段的容器类型
        context_class=dict,
        # 输出到 stdout
        logger_factory=structlog.PrintLoggerFactory(file=sys.stdout),  # type: ignore
        # 首次使用时缓存 logger，提高性能
        cache_logger_on_first_use=True,
    )


def _setup_standard_logging(level: int) -> None:
    """
    配置标准 logging（降级方案）。

    说明：
        - 使用 logging.basicConfig 输出到 stdout
        - 日志格式较为传统，但配合 ContextLogger 仍可注入 extra 字段
    """
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        stream=sys.stdout,
    )


# ========= 获取基础 Logger =========

def get_logger(name: str) -> Any:
    """
    获取基础 Logger 实例（不带自动上下文注入）。

    返回：
        - structlog.BoundLogger（如果 structlog 可用）
        - logging.Logger（如果降级到标准 logging）

    使用示例：
        logger = get_logger(__name__)
        logger.info("event happened", user_id=123, action="login")

    注意：
        - 如果希望自动注入 trace_id / span_id / 额外上下文，
          推荐使用 get_context_logger 而不是直接使用 get_logger。
    """
    if not _initialized:
        setup_logging()

    if STRUCTLOG_AVAILABLE:
        return structlog.get_logger(name)  # type: ignore
    else:
        return logging.getLogger(name)


# ========= 追踪上下文管理器 =========

@contextmanager
def trace_context(
    trace_id: Optional[str] = None,
    span_id: Optional[str] = None,
    **extra: Any,
):
    """
    追踪上下文管理器。

    功能：
        - 自动生成或复用 trace_id
        - 自动生成 span_id
        - 注入额外上下文字段（extra），如 user_id, action 等
        - 与 ContextLogger 配合使用，所有日志自动带上上述字段

    示例：
        logger = get_context_logger(__name__)

        with trace_context(user_id="123", action="chat"):
            logger.info("Processing request")
            # 输出中会包含：trace_id, span_id, user_id, action

    嵌套行为：
        - 内层 trace_context 会基于外层 trace_id 或重新生成（如果显式传入）
        - 使用 ContextVar 的 token 机制，在退出时恢复外层上下文
    """
    # 1. 计算本次上下文的 trace_id 与 span_id
    #    - 优先使用传入参数
    #    - 其次复用当前上下文中的 trace_id
    #    - 最后才生成新的 trace_id
    tid = trace_id or trace_id_var.get() or generate_trace_id()
    sid = span_id or generate_span_id()

    # 2. 设置追踪 ID 到 ContextVar，并记录 token 以便退出时恢复
    trace_token = trace_id_var.set(tid)
    span_token = span_id_var.set(sid)

    # 3. 合并额外上下文（从当前 extra_context + 新传入的 extra）
    current_extra = extra_context_var.get().copy()  # copy 避免修改共享对象
    current_extra.update(extra)
    extra_token = extra_context_var.set(current_extra)

    try:
        # 在 with 体内可以拿到当前 trace/span，便于手工操作
        yield {"trace_id": tid, "span_id": sid}
    finally:
        # 4. 使用 token 恢复到进入 with 之前的上下文状态
        trace_id_var.reset(trace_token)
        span_id_var.reset(span_token)
        extra_context_var.reset(extra_token)


# ========= 带上下文注入的 Logger 包装器 =========

class ContextLogger:
    """
    带上下文的 Logger 包装器。

    功能：
        - 自动从 ContextVar 中取出 trace_id / span_id / extra_context
        - 对 structlog Logger：将上下文作为字段展开
        - 对标准 logging.Logger：通过 extra 参数注入上下文字段

    设计目标：
        - 对调用方提供统一的 API（debug/info/warning/error/exception）
        - 清晰区分「上下文字段」与「控制参数」（exc_info、stack_info 等）
    """

    def __init__(self, logger: Any):
        self._logger = logger
        # 简单的 duck-typing：
        # structlog 的 logger 一般有 bind 方法，标准 logging.Logger 没有
        self._is_structlog = hasattr(logger, "bind")

    # ---- 内部工具方法 ----

    def _enrich(self, **kwargs: Any) -> Dict[str, Any]:
        """
        构造完整的上下文字段字典。

        合并顺序：
            1. trace_id / span_id
            2. extra_context_var 中保存的额外上下文
            3. 调用方传入的业务字段（kwargs）
        """
        enriched: Dict[str, Any] = {}

        # 1. 追踪信息
        trace_id = trace_id_var.get()
        if trace_id:
            enriched["trace_id"] = trace_id

        span_id = span_id_var.get()
        if span_id:
            enriched["span_id"] = span_id

        # 2. 额外上下文
        extra_ctx = extra_context_var.get()
        if extra_ctx:
            enriched.update(extra_ctx)

        # 3. 调用方字段（后者覆盖前者）
        enriched.update(kwargs)

        return enriched

    def _log(self, method_name: str, message: str, **kwargs: Any) -> None:
        """
        通用日志调用入口。

        参数拆分：
            - 控制类参数（exc_info / stack_info / stacklevel）
            - 业务字段（将被合并到上下文字段中）

        对 structlog：
            logger.<level>(event=message, **enriched_fields, **control_kwargs)

        对标准 logging：
            logger.<level>(message, extra=enriched_fields, **control_kwargs)
        """
        # 从 kwargs 中拆分出控制参数（logging/structlog 认可的控制字段）
        control_keys = ("exc_info", "stack_info", "stacklevel")
        control_kwargs: Dict[str, Any] = {}
        business_kwargs: Dict[str, Any] = {}

        for k, v in kwargs.items():
            if k in control_keys:
                control_kwargs[k] = v
            else:
                business_kwargs[k] = v

        # 构造上下文字段
        enriched = self._enrich(**business_kwargs)
        log_method = getattr(self._logger, method_name)

        if self._is_structlog:
            # structlog：上下文字段直接展开成事件字段
            log_method(message, **enriched, **control_kwargs)
        else:
            # 标准 logging：
            #   - 上下文字段通过 extra 注入到 LogRecord.__dict__
            #   - 控制参数（exc_info 等）以正常方式传入
            log_method(message, extra=enriched, **control_kwargs)

    # ---- 对外日志方法 ----

    def debug(self, message: str, **kwargs: Any) -> None:
        """DEBUG 级别日志。"""
        self._log("debug", message, **kwargs)

    def info(self, message: str, **kwargs: Any) -> None:
        """INFO 级别日志。"""
        self._log("info", message, **kwargs)

    def warning(self, message: str, **kwargs: Any) -> None:
        """WARNING 级别日志。"""
        self._log("warning", message, **kwargs)

    def error(self, message: str, **kwargs: Any) -> None:
        """ERROR 级别日志。"""
        self._log("error", message, **kwargs)

    def exception(self, message: str, **kwargs: Any) -> None:
        """
        记录异常日志，自动附带当前异常堆栈。

        等价于：
            logger.error(..., exc_info=True)
        """
        # 强制携带 exc_info=True，兼容 structlog 和标准 logging 的习惯用法
        kwargs.setdefault("exc_info", True)
        self._log("exception", message, **kwargs)


def get_context_logger(name: str) -> ContextLogger:
    """
    获取带上下文自动注入能力的 Logger。

    使用示例：
        logger = get_context_logger(__name__)

        with trace_context(user_id="123"):
            logger.info("User action", action="click")

        # 结构化日志中会自动包含：
        #   - trace_id
        #   - span_id
        #   - user_id
        #   - action
    """
    base_logger = get_logger(name)
    return ContextLogger(base_logger)


# ========= 模块导入后自动初始化 =========
#
# 说明：
#   - 作为应用主工程使用时，自动初始化通常是方便的
#   - 如将 gecko 作为纯库使用，且需要自定义 logging 配置，
#     可以考虑未来通过配置开关控制是否自动初始化

setup_logging()
```

[28] gecko/core/memory/__init__.py
```python
# gecko/core/memory/__init__.py
"""
memory 包对外统一入口

拆分前：
    gecko/core/memory.py    （单文件）

拆分后：
    gecko/core/memory/      （包）
    ├─ __init__.py
    ├─ _executor.py
    ├─ base.py
    └─ summary.py

为保持向后兼容，对外仍然暴露同样的 API：

    from gecko.core.memory import TokenMemory, SummaryTokenMemory, shutdown_token_executor

因此，调用方无需修改任何 import 代码。
"""

from __future__ import annotations

from gecko.core.memory.base import TokenMemory
from gecko.core.memory.summary import SummaryTokenMemory
from gecko.core.memory.hybrid import HybridMemory
from gecko.core.memory._executor import shutdown_token_executor

__all__ = [
    "TokenMemory",
    "SummaryTokenMemory",
    "HybridMemory", 
    "shutdown_token_executor",
]
```

[29] gecko/core/memory/_executor.py
```python
# gecko/core/memory/_executor.py
"""
内部模块：Token 计算线程池管理

本模块只负责：
- 全局线程池的懒加载（get_token_executor）
- 全局线程池的关闭（shutdown_token_executor）

注意：
- 这是内部实现细节模块，正常情况下不建议从业务代码直接导入使用
- 对外统一通过 `gecko.core.memory.shutdown_token_executor` 暴露关闭接口
"""

from __future__ import annotations

import threading
from concurrent.futures import ThreadPoolExecutor
from typing import Optional

# 全局线程池实例（单进程单实例）
_TOKEN_EXECUTOR: Optional[ThreadPoolExecutor] = None
# 保护初始化过程的锁，避免多线程下重复创建线程池
_EXECUTOR_LOCK = threading.Lock()


def get_token_executor() -> ThreadPoolExecutor:
    """
    线程安全的懒加载全局 Token 计算线程池。

    设计说明：
    - 使用“双重检查 + 互斥锁”的方式保证只创建一个线程池实例
    - `max_workers` 不宜过大：
        * Token 计算属于 CPU 密集型
        * 少量线程即可充分利用多核
    - `thread_name_prefix` 用于调试和观察线程来源
    """
    global _TOKEN_EXECUTOR
    if _TOKEN_EXECUTOR is None:
        # 第一层检查：大多数情况下这里就直接通过，不会进锁
        with _EXECUTOR_LOCK:
            # 第二层检查：保证在锁保护下只初始化一次
            if _TOKEN_EXECUTOR is None:
                _TOKEN_EXECUTOR = ThreadPoolExecutor(
                    max_workers=2,
                    thread_name_prefix="gecko_token_",
                )
    return _TOKEN_EXECUTOR


def shutdown_token_executor() -> None:
    """
    关闭全局 Token 计算线程池。

    使用场景：
    - 单元测试结束后清理资源
    - 某些需要显式控制资源生命周期的场景

    注意：
    - 正常服务进程一般可以依赖解释器退出时自动回收线程池，
      无需主动调用本函数。
    - 这里使用 `wait=False`，以避免在关闭时长时间阻塞主线程。
    """
    global _TOKEN_EXECUTOR
    if _TOKEN_EXECUTOR is not None:
        _TOKEN_EXECUTOR.shutdown(wait=False)
        _TOKEN_EXECUTOR = None
```

[30] gecko/core/memory/base.py
```python
# gecko/core/memory/base.py
"""
TokenMemory 基础实现模块

职责：
- 提供 Token 感知的上下文管理能力：
  * Token 计数（tiktoken / model_driver / 字符估算）
  * LRU 缓存 Token 结果
  * 同步 / 异步批量计数
  * 按 max_tokens 对历史消息进行裁剪
  * 多模态（文本 + 图片） Token 估算

本文件对应原 memory.py 中的 TokenMemory 相关逻辑的完整拆分版。
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import threading
from collections import OrderedDict
from typing import Any, Callable, Dict, List, Optional, TYPE_CHECKING

from gecko.core.logging import get_logger
from gecko.core.message import Message
from gecko.plugins.storage.interfaces import SessionInterface

from gecko.core.memory._executor import get_token_executor

if TYPE_CHECKING:
    from gecko.core.protocols import ModelProtocol

logger = get_logger(__name__)


class TokenMemory:
    """
    Token 感知的记忆管理器（基础版）

    负责：
    - 在有限的 Context Window 内最大化保留有效对话历史
    - 为上层提供 Token 计数、历史裁剪等能力
    """

    def __init__(
        self,
        session_id: str,
        storage: Optional[SessionInterface] = None,
        max_tokens: int = 4000,
        model_name: str = "gpt-3.5-turbo",
        cache_size: int = 2000,
        max_message_length: int = 20000,
        enable_cache_for_batch: bool = True,
        model_driver: Optional["ModelProtocol"] = None,
        enable_async_counting: bool = True,
    ):
        """
        初始化 TokenMemory

        参数：
            session_id:
                会话 ID（一般用于区分不同用户/会话）
            storage:
                会话存储接口（当前未使用，预留给后续“自动存取记忆”能力）
            max_tokens:
                上下文最大 Token 限制（系统 + 历史 + 当前输入）
            model_name:
                用于 tiktoken 加载 encoder 的模型名（如：gpt-3.5-turbo, gpt-4 等）
            cache_size:
                LRU 缓存最大条目数（缓存 Key -> Token 数）
            max_message_length:
                单条消息最大字符长度（字符级防御性截断）
            enable_cache_for_batch:
                批量计数时是否默认启用缓存
            model_driver:
                可选的模型驱动，实现 count_tokens 接口，用于精确计数
            enable_async_counting:
                是否启用线程池进行异步批量 Token 计算
        """
        if max_tokens <= 0:
            raise ValueError(f"max_tokens must be positive, got {max_tokens}")
        if cache_size <= 0:
            raise ValueError(f"cache_size must be positive, got {cache_size}")

        self.session_id = session_id
        self.storage = storage
        self.max_tokens = max_tokens
        self.model_name = model_name
        self.cache_size = cache_size
        self.max_message_length = max_message_length
        self.enable_cache_for_batch = enable_cache_for_batch
        self.model_driver = model_driver
        self.enable_async_counting = enable_async_counting

        # LRU 缓存：key -> token_count
        self._token_cache: OrderedDict[str, int] = OrderedDict()
        # 读写都会发生，因此使用可重入锁保证线程安全
        self._cache_lock = threading.RLock()

        # 缓存统计信息（便于监控与调优）
        self._cache_hits = 0
        self._cache_misses = 0
        self._cache_evictions = 0

        # 延迟加载的 tokenizer 编码器
        self._encoding: Any = None
        self._tokenizer_failed = False
        self._encode_func: Optional[Callable[[str], List[int]]] = None

    # ====================== Tokenizer ======================

    @property
    def tokenizer(self) -> Any:
        """
        延迟加载 tiktoken encoder

        行为：
        - 优先：tiktoken.encoding_for_model(self.model_name)
        - 找不到模型：使用 cl100k_base 编码器
        - tiktoken 未安装或加载失败：标记 _tokenizer_failed=True，后续走字符估算路径
        """
        if self._encoding is not None:
            return self._encoding

        if self._tokenizer_failed:
            return None

        try:
            import tiktoken

            try:
                self._encoding = tiktoken.encoding_for_model(self.model_name)
            except KeyError:
                logger.warning(
                    f"Model {self.model_name} not found in tiktoken, using cl100k_base"
                )
                self._encoding = tiktoken.get_encoding("cl100k_base")

            self._encode_func = self._encoding.encode

        except ImportError:
            logger.warning(
                "tiktoken not installed. Token counting will use char estimation."
            )
            self._tokenizer_failed = True
        except Exception as e:
            logger.error(f"Failed to load tokenizer: {e}")
            self._tokenizer_failed = True

        return self._encoding

    def _get_encode_func(self) -> Optional[Callable[[str], List[int]]]:
        """
        获取 encode(text) 函数（确保 tokenizer 已加载）

        返回：
            encode 函数，若加载失败则返回 None。
        """
        if self._encode_func is None:
            _ = self.tokenizer  # 触发加载
        return self._encode_func

    # ====================== 缓存操作 ======================

    def _cache_get(self, key: str) -> Optional[int]:
        """
        线程安全的缓存读取（LRU）

        行为：
        - 命中：将 key 移至尾部（表示最近被访问），并增加 _cache_hits。
        - 未命中：返回 None。
        """
        with self._cache_lock:
            if key in self._token_cache:
                self._token_cache.move_to_end(key)
                self._cache_hits += 1
                return self._token_cache[key]
            return None

    def _cache_set(self, key: str, value: int) -> None:
        """
        线程安全的缓存写入（LRU）

        行为：
        - 写入或更新 key 后，将其移到尾部。
        - 认为发生了一次“需要计算”的请求，增加 _cache_misses。
        - 若缓存条目数超过 cache_size，则弹出最老的一条，并增加 _cache_evictions。
        """
        with self._cache_lock:
            self._token_cache[key] = value
            self._token_cache.move_to_end(key)
            self._cache_misses += 1

            while len(self._token_cache) > self.cache_size:
                self._token_cache.popitem(last=False)
                self._cache_evictions += 1

    def _make_cache_key(self, message: Message) -> str:
        """
        生成消息的缓存键（稳定且简短）

        策略：
        - 简单文本消息（无 tool_calls）：
            key = md5(f"{role}:{name}:{content}")
        - 含 tool_calls / 多模态：
            序列化为 JSON（model_dump_json / model_dump），再取 md5。
        """
        if isinstance(message.content, str) and not message.tool_calls:
            raw = f"{message.role}:{message.name or ''}:{message.content}"
            return hashlib.md5(raw.encode("utf-8")).hexdigest()

        try:
            raw_json = message.model_dump_json(
                include={"role", "content", "tool_calls", "name"},
                exclude_none=True,
            )
            return hashlib.md5(raw_json.encode("utf-8")).hexdigest()
        except Exception:
            data = message.model_dump(
                include={"role", "content", "tool_calls", "name"}
            )
            return hashlib.md5(
                json.dumps(data, sort_keys=True, default=str).encode("utf-8")
            ).hexdigest()

    # ====================== 文本截断工具 ======================

    def _truncate_text_to_tokens(self, text: str, limit_tokens: int) -> str:
        """
        将纯文本按 Token 限制进行截断，并返回截断后的文本。

        策略：
        - 无 encode：
            退化为字符估算，按 1 token ≈ 3.5 字符，截断后加后缀。
        - 有 encode：
            使用“二分 + encode 实测”精确控制 Token 数，
            并在截断后追加后缀 "...(truncated)"。
        """
        if not text or limit_tokens <= 0:
            return ""

        encode = self._get_encode_func()
        suffix = "...(truncated)"

        if encode is None:
            # 无 tokenizer：粗略按字符数估算
            char_limit = int(limit_tokens * 3.5)
            if len(text) <= char_limit:
                return text
            return text[:char_limit] + suffix

        # 若完整文本已经在预算内，直接返回
        if len(encode(text)) <= limit_tokens:
            return text

        left, right = 0, len(text)
        best_idx = 0

        # 二分搜索最长前缀
        while left <= right:
            mid = (left + right) // 2
            candidate = text[:mid] + suffix
            token_count = len(encode(candidate))
            if token_count <= limit_tokens:
                best_idx = mid
                left = mid + 1
            else:
                right = mid - 1

        if best_idx == 0:
            # 极端情况：连后缀本身都可能超限，保守处理
            if len(encode(suffix)) <= limit_tokens:
                return suffix
            return ""

        return text[:best_idx] + suffix

    # ====================== 单条计数 ======================

    def count_message_tokens(self, message: Message) -> int:
        """
        计算单条消息的 Token 数（带 LRU 缓存）

        流程：
        1. 生成缓存 key
        2. 若命中缓存，直接返回
        3. 未命中则调用 _count_tokens_impl 计算，并写入缓存
        """
        cache_key = self._make_cache_key(message)

        cached = self._cache_get(cache_key)
        if cached is not None:
            return cached

        count = self._count_tokens_impl(message)
        self._cache_set(cache_key, count)
        return count

    # ====================== 批量计数（同步） ======================

    def count_messages_batch(
        self,
        messages: List[Message],
        use_cache: Optional[bool] = None,
    ) -> List[int]:
        """
        批量计算消息的 Token 数（同步版本）

        参数：
            messages:
                消息列表
            use_cache:
                - None：跟随实例级配置 self.enable_cache_for_batch
                - True：强制使用缓存
                - False：强制禁用缓存
        """
        if not messages:
            return []

        should_use_cache = (
            use_cache if use_cache is not None else self.enable_cache_for_batch
        )

        if should_use_cache:
            return [self.count_message_tokens(m) for m in messages]
        else:
            encode_fn = self._get_encode_func()
            return [self._count_tokens_impl(m, encode=encode_fn) for m in messages]

    # ====================== 批量计数（异步） ======================

    async def count_messages_batch_async(
        self,
        messages: List[Message],
        use_cache: Optional[bool] = None,
    ) -> List[int]:
        """
        异步批量计算 Token 数（CPU 密集型计算卸载到线程池）

        参数：
            messages:
                消息列表
            use_cache:
                - None：跟随实例级配置 self.enable_cache_for_batch
                - True：强制使用缓存
                - False：强制禁用缓存
        """
        if not messages:
            return []

        should_use_cache = (
            use_cache if use_cache is not None else self.enable_cache_for_batch
        )

        cache_keys: List[str] = []
        cache_results: List[Optional[int]] = []

        # 第一阶段：批量缓存查询
        for msg in messages:
            key = self._make_cache_key(msg)
            cache_keys.append(key)

            if should_use_cache:
                cached = self._cache_get(key)
                cache_results.append(cached)
            else:
                cache_results.append(None)

        # 若全部命中缓存，直接返回
        if should_use_cache and all(r is not None for r in cache_results):
            return cache_results  # type: ignore

        # 第二阶段：收集需要计算的消息
        compute_indices: List[int] = []
        compute_messages: List[Message] = []

        for i, cached in enumerate(cache_results):
            if cached is None:
                compute_indices.append(i)
                compute_messages.append(messages[i])

        # 第三阶段：线程池中计算
        if compute_messages:
            computed = await self._compute_in_thread(compute_messages)

            # 第四阶段：结果回填 + 更新缓存
            for i, idx in enumerate(compute_indices):
                count = computed[i]
                cache_results[idx] = count
                if should_use_cache:
                    self._cache_set(cache_keys[idx], count)

        return cache_results  # type: ignore

    async def _compute_in_thread(self, messages: List[Message]) -> List[int]:
        """
        在线程池中执行 Token 计算。

        关键点：
        - 显式传入 encode_fn，确保在子线程中不会走 model_driver.count_tokens，
          避免在子线程里做网络调用或非线程安全操作。
        - 子线程中只进行本地 encode / 字符估算，属于纯 CPU 计算。
        """
        encode_fn = self._get_encode_func()

        def _compute() -> List[int]:
            return [self._count_tokens_impl(m, encode=encode_fn) for m in messages]

        if self.enable_async_counting:
            loop = asyncio.get_running_loop()
            return await loop.run_in_executor(get_token_executor(), _compute)
        else:
            return _compute()

    # ====================== Token 计算实现 ======================

    def _count_tokens_impl(
        self,
        message: Message,
        encode: Optional[Callable[[str], List[int]]] = None,
    ) -> int:
        """
        内部 Token 计算核心逻辑。

        优先级：
        1. 若 encode 为 None 且存在 model_driver：
           尝试调用 model_driver.count_tokens 进行精确计数。
        2. 若有 encode，则用 encode 精确计算。
        3. 否则退化为基于字符长度的大致估算。
        """
        # 1) 模型驱动计数（仅在未传入 encode 时使用）
        if self.model_driver is not None and encode is None:
            try:
                return self.model_driver.count_tokens([message.to_openai_format()])
            except Exception:
                # 若模型驱动计数失败，则退回本地估算
                pass

        # 2) 获取 encode 函数
        if encode is None:
            encode = self._get_encode_func()

        if encode is None:
            # 3) 字符估算路径
            # 粗略估计：1 token ≈ 4 个字符，并额外加上一些固定开销
            return len(message.get_text_content()) // 4 + 4

        num_tokens = 4  # per-message overhead（参考 OpenAI 官方估算）

        content = message.content

        if isinstance(content, str):
            num_tokens += len(encode(content))
        elif isinstance(content, list):
            # 多模态 / 结构化内容
            for block in content:
                # 支持对象与 dict 两种形式
                b_type = getattr(block, "type", None)
                b_text = getattr(block, "text", None)
                b_image = getattr(block, "image_url", None)

                if isinstance(block, dict):
                    b_type = block.get("type")
                    b_text = block.get("text")
                    b_image = block.get("image_url")

                if b_type == "text" and b_text:
                    num_tokens += len(encode(b_text))
                elif b_type == "image_url":
                    num_tokens += self._estimate_image_tokens(b_image)

        # tool_calls 计数
        if message.tool_calls:
            try:
                dump = json.dumps(message.tool_calls, ensure_ascii=False)
                num_tokens += len(encode(dump))
            except Exception:
                # 若序列化失败，给一个保守估值
                num_tokens += 100

        # name 开销
        if message.name:
            num_tokens += 1

        return num_tokens

    def _estimate_image_tokens(self, image_resource: Any) -> int:
        """
        粗略估算图片 Token 占用。

        说明：
        - detail == "low" 时使用较小估值（如缩略图、低分辨率）
        - 其他情况使用较大估值（偏保守，以避免低估）
        """
        if not image_resource:
            return 0
        detail = getattr(image_resource, "detail", "auto")
        if detail == "low":
            return 85
        return 1000

    # ====================== 历史加载 ======================

    async def get_history(
        self,
        raw_messages: List[Dict[str, Any]],
        preserve_system: bool = True,
    ) -> List[Message]:
        """
        加载并裁剪历史消息（基础版本）

        算法：
        1. 将 raw_messages（dict 列表）解析为 Message 对象，并做防御性截断。
        2. 若首条为 system 且 preserve_system=True，则单独保留为 system_msg。
        3. 对 system_msg 进行 Token 计数，若超 max_tokens，则强制截断。
        4. 从尾部开始累加剩余消息的 Token 数，直到即将超过 max_tokens 为止。
        5. 返回结果为：[system_msg?] + 最近若干条消息。
        """
        if not raw_messages:
            return []

        parsed: List[Message] = []
        for i, raw in enumerate(raw_messages):
            try:
                msg = Message(**raw)
                self._truncate_if_needed(msg)
                parsed.append(msg)
            except Exception as e:
                logger.warning(f"Skipping invalid message at index {i}: {e}")

        if not parsed:
            return []

        system_msg: Optional[Message] = None
        candidates = parsed

        if preserve_system and parsed[0].role == "system":
            system_msg = parsed[0]
            candidates = parsed[1:]

        used_tokens = 0
        if system_msg:
            sys_tokens = self.count_message_tokens(system_msg)
            if sys_tokens > self.max_tokens:
                self._force_truncate(system_msg, self.max_tokens)
                sys_tokens = self.count_message_tokens(system_msg)
            used_tokens = sys_tokens

        selected: List[Message] = []

        for msg in reversed(candidates):
            tokens = self.count_message_tokens(msg)
            if used_tokens + tokens > self.max_tokens:
                break
            selected.append(msg)
            used_tokens += tokens

        selected.reverse()

        result: List[Message] = []
        if system_msg:
            result.append(system_msg)
        result.extend(selected)
        return result

    def _truncate_if_needed(self, message: Message) -> None:
        """
        防御性截断（按字符数限制单条消息）

        说明：
        - 只对纯文本 content 生效
        - 用于避免异常超长输入导致内存/性能问题
        - 精确控制 Token 数的工作由 `_force_truncate` 负责
        """
        if isinstance(message.content, str):
            if len(message.content) > self.max_message_length:
                message.content = message.content[:self.max_message_length]

    def _force_truncate(self, message: Message, limit_tokens: int) -> None:
        """
        强制将消息内容截断到指定 Token 数以内（仅对纯文本 content 生效）
        """
        if not isinstance(message.content, str):
            return
        message.content = self._truncate_text_to_tokens(message.content, limit_tokens)

    # ====================== 工具方法 ======================

    def count_total_tokens(self, messages: List[Message]) -> int:
        """
        计算消息列表总 Token 数（使用批量计数，默认启用缓存）
        """
        return sum(self.count_messages_batch(messages, use_cache=True))

    def estimate_tokens(self, text: str) -> int:
        """
        快速估算一段文本的 Token 数。

        策略：
        - 有 encode：使用 encode 精算
        - 无 encode：退化为 len(text) // 4
        """
        if not text:
            return 0
        encode = self._get_encode_func()
        if encode:
            return len(encode(text))
        return len(text) // 4

    def clear_cache(self) -> None:
        """
        清空 Token 缓存及其统计数据。
        """
        with self._cache_lock:
            cleared = len(self._token_cache)
            self._token_cache.clear()
            self._cache_hits = 0
            self._cache_misses = 0
            self._cache_evictions = 0
        logger.info(f"Token cache cleared, {cleared} entries removed")

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        获取缓存统计信息（可用于监控与调优）
        """
        with self._cache_lock:
            size = len(self._token_cache)
            total = self._cache_hits + self._cache_misses
            hit_rate = self._cache_hits / total if total > 0 else 0.0

            return {
                "cache_size": size,
                "max_cache_size": self.cache_size,
                "cache_utilization": size / self.cache_size if self.cache_size > 0 else 0,
                "hits": self._cache_hits,
                "misses": self._cache_misses,
                "evictions": self._cache_evictions,
                "total_requests": total,
                "hit_rate": hit_rate,
            }

    def print_cache_stats(self) -> None:
        """
        打印缓存统计信息到标准输出（用于临时调试）
        """
        stats = self.get_cache_stats()
        print("\n" + "=" * 50)
        print("Token Cache Statistics".center(50))
        print("=" * 50)
        print(f"  Size:        {stats['cache_size']} / {stats['max_cache_size']}")
        print(f"  Utilization: {stats['cache_utilization']:.1%}")
        print(f"  Hits:        {stats['hits']}")
        print(f"  Misses:      {stats['misses']}")
        print(f"  Evictions:   {stats['evictions']}")
        print(f"  Hit Rate:    {stats['hit_rate']:.1%}")
        print("=" * 50 + "\n")

    def __repr__(self) -> str:
        return (
            f"TokenMemory(session_id='{self.session_id}', "
            f"max_tokens={self.max_tokens}, "
            f"cache={len(self._token_cache)}/{self.cache_size})"
        )
```

[31] gecko/core/memory/hybrid.py
```python
# gecko/core/memory/hybrid.py
"""
混合记忆系统 (Hybrid Memory)

结合短期 TokenMemory 和 长期 VectorStore 的记忆管理。
"""
from __future__ import annotations

import time
import asyncio
from typing import Any, Dict, List, Optional

from gecko.core.logging import get_logger
from gecko.core.memory.base import TokenMemory
from gecko.core.message import Message
from gecko.core.utils import ensure_awaitable
from gecko.plugins.knowledge.interfaces import EmbedderProtocol
from gecko.plugins.storage.interfaces import VectorInterface

logger = get_logger(__name__)


class HybridMemory(TokenMemory):
    """
    长短期混合记忆
    
    流程:
    1. Short-term: 保留最近对话 (基于 Token 限制)。
    2. Long-term:  基于 Query 进行向量检索，找回历史相关片段。
    """

    def __init__(
        self,
        session_id: str,
        vector_store: VectorInterface,
        embedder: EmbedderProtocol,
        short_term_limit: int = 2000,
        top_k_recall: int = 3,
        similarity_threshold: float = 0.7,
        **kwargs
    ):
        """
        初始化
        
        Args:
            session_id: 会话ID
            vector_store: 向量数据库接口
            embedder: Embedding 模型接口
            short_term_limit: 短期记忆的 Token 上限
            top_k_recall: 长期记忆召回条数
            similarity_threshold: 相似度阈值 (0-1)
        """
        # 基类负责管理短期记忆 (Short-term Buffer)
        super().__init__(session_id, max_tokens=short_term_limit, **kwargs)
        
        self.vector_store = vector_store
        self.embedder = embedder
        self.top_k_recall = top_k_recall
        self.similarity_threshold = similarity_threshold

    async def get_history( # type: ignore
        self, 
        raw_messages: List[Dict[str, Any]], 
        query: Optional[str] = None, 
        preserve_system: bool = True
    ) -> List[Message]:
        """
        获取上下文 (Short + Long Term)
        
        [优化] 并行执行短期记忆处理和长期记忆检索
        """
        
        # 定义长期记忆检索任务
        async def _fetch_long_term() -> List[Message]:
            # 如果没有 query 或者没有配置向量库，直接跳过
            if not query or not self.vector_store:
                return []
                
            try:
                # 向量化 Query
                query_vec = await ensure_awaitable(self.embedder.embed_query, query)
                
                # 搜索 (增加 session_id 过滤，确保只搜到当前用户的记忆)
                docs = await self.vector_store.search(
                    query_vec, 
                    top_k=self.top_k_recall, 
                    filters={"session_id": self.session_id}
                )
                
                relevant_snippets = []
                for doc in docs:
                    score = doc.get("score", 0.0)
                    if score >= self.similarity_threshold:
                        text = doc.get("text", "")
                        timestamp = doc.get("metadata", {}).get("timestamp", "unknown")
                        relevant_snippets.append(f"[{timestamp}] {text}")
                
                if relevant_snippets:
                    context_block = "\n---\n".join(relevant_snippets)
                    # 将召回的内容封装为 System Message
                    return [Message.system(
                        f"Relevant Context from Memory (Historical Data):\n{context_block}"
                    )]
            except Exception as e:
                logger.error(f"HybridMemory recall failed: {e}")
                return []
            return []

        # [核心优化] 并行执行
        short_term_msgs, long_term_msgs = await asyncio.gather(
            super().get_history(raw_messages, preserve_system),
            _fetch_long_term()
        )

        # 3. 组装上下文
        # 顺序建议: [System Prompt] -> [Long Term Context] -> [Short Term History]
        final_msgs = []
        
        # 提取原有的 System Prompt (如果有)
        if short_term_msgs and short_term_msgs[0].role == "system":
            final_msgs.append(short_term_msgs.pop(0))
        
        # 插入长期记忆
        final_msgs.extend(long_term_msgs)
        
        # 追加短期记忆
        final_msgs.extend(short_term_msgs)
        
        return final_msgs

    async def archive_message(self, message: Message):
        """
        手动归档一条消息到长期记忆 (Write Path)
        
        通常在 Agent 每一轮对话结束后调用。
        """
        text = message.get_text_content()
        # 忽略过短内容或非文本内容
        if not text or len(text) < 10: 
            return

        try:
            # 生成向量
            vec = await ensure_awaitable(self.embedder.embed_query, text)
            
            # 构造文档
            doc_id = f"{self.session_id}_{int(time.time()*1000)}"
            document = {
                "id": doc_id,
                "text": text,
                "embedding": vec,
                "metadata": {
                    "session_id": self.session_id,
                    "role": message.role,
                    "timestamp": time.time()
                }
            }
            
            # 写入向量库
            await self.vector_store.upsert([document])
            logger.debug("Archived message to vector store", doc_id=doc_id)
            
        except Exception as e:
            logger.error(f"Failed to archive message: {e}")
```

[32] gecko/core/memory/summary.py
```python
# gecko/core/memory/summary.py
"""
SummaryTokenMemory 摘要记忆实现模块 (Production Optimized)

优化日志：
- [Perf] 引入 min_update_interval (防抖)，避免频繁调用 LLM。
- [Perf] 支持 background_update (后台更新)，防止摘要生成阻塞主对话流程。
- [Safety] 保持 asyncio.Lock 确保同一时刻只有一个摘要任务在运行。
"""

from __future__ import annotations

import asyncio
import time
from typing import Any, Dict, List, Optional, TYPE_CHECKING

from gecko.core.logging import get_logger
from gecko.core.message import Message
from gecko.core.memory.base import TokenMemory

if TYPE_CHECKING:
    from gecko.core.protocols import ModelProtocol
    from gecko.core.prompt import PromptTemplate

logger = get_logger(__name__)


class SummaryTokenMemory(TokenMemory):
    """
    支持自动摘要的记忆管理器 (高性能版)。
    """

    def __init__(
        self,
        session_id: str,
        model: "ModelProtocol",
        summary_prompt: Optional[str] = None,
        summary_reserve_tokens: int = 500,
        # [新增] 生产级配置
        min_update_interval: float = 30.0,  # 两次摘要更新的最小间隔(秒)
        background_update: bool = True,     # 是否在后台异步更新摘要(推荐True)
        **kwargs,
    ):
        """
        初始化 SummaryTokenMemory

        参数：
            min_update_interval: 防抖窗口，默认 30 秒内只会触发一次摘要更新。
            background_update: 若为 True，get_history 不会等待摘要生成，直接返回旧摘要+新历史。
        """
        # 将模型驱动作为 model_driver 传给基类
        kwargs.setdefault("model_driver", model)
        super().__init__(session_id, **kwargs)

        self.model = model
        self.summary_reserve_tokens = summary_reserve_tokens
        self.current_summary: str = ""
        
        # [新增] 控制参数
        self.min_update_interval = min_update_interval
        self.background_update = background_update
        self._last_update_time: float = 0.0

        # 延迟导入 PromptTemplate
        from gecko.core.prompt import PromptTemplate  # type: ignore

        self.summary_template: "PromptTemplate" = PromptTemplate(
            template=summary_prompt
            or (
                "Condense the following conversation into a brief summary, "
                "preserving key information:\n\n{{ history }}\n\nSummary:"
            ),
            input_variables=["history"],
        )

        self._summary_lock: Optional[asyncio.Lock] = None

    def _get_summary_lock(self) -> asyncio.Lock:
        if self._summary_lock is None:
            self._summary_lock = asyncio.Lock()
        return self._summary_lock

    async def get_history(
        self,
        raw_messages: List[Dict[str, Any]],
        preserve_system: bool = True,
    ) -> List[Message]:
        """
        获取历史消息 (优化版)
        """
        if not raw_messages:
            return []

        # 1. 解析消息
        parsed: List[Message] = []
        for i, raw in enumerate(raw_messages):
            try:
                msg = Message(**raw)
                self._truncate_if_needed(msg)
                parsed.append(msg)
            except Exception as e:
                logger.warning(f"[SummaryTokenMemory] Skipping invalid message at index {i}: {e}")

        if not parsed:
            return []

        # 2. 分离 system 消息
        system_msg: Optional[Message] = None
        candidates = parsed

        if preserve_system and parsed[0].role == "system": # type: ignore
            system_msg = parsed[0]
            candidates = parsed[1:]

        # 3. 计算预算
        sys_tokens = 0
        if system_msg:
            sys_tokens = self.count_message_tokens(system_msg)
            if sys_tokens > self.max_tokens:
                self._force_truncate(system_msg, self.max_tokens)
                sys_tokens = self.count_message_tokens(system_msg)

        reserved = self.summary_reserve_tokens + sys_tokens
        available = self.max_tokens - reserved
        if available < 0:
            available = 0

        # 4. 拆分 Recent / ToSummarize
        used = 0
        recent: List[Message] = []
        to_summarize: List[Message] = []

        for msg in reversed(candidates):
            tokens = self.count_message_tokens(msg)
            if used + tokens <= available:
                recent.append(msg)
                used += tokens
            else:
                to_summarize.append(msg)

        recent.reverse()
        to_summarize.reverse()

        # 5. [核心优化] 触发摘要更新
        if to_summarize:
            await self._trigger_summary_update(to_summarize)

        # 6. 组装结果
        result: List[Message] = []
        if system_msg:
            result.append(system_msg)

        # 注入摘要 (使用当前内存中已有的摘要)
        if self.current_summary:
            summary_text = f"Previous context: {self.current_summary}"
            # 动态计算剩余给摘要的预算
            summary_budget = max(self.max_tokens - sys_tokens - used, 0)

            if summary_budget > 0:
                truncated_summary = self._truncate_text_to_tokens(summary_text, summary_budget)
                if truncated_summary:
                    result.append(Message.system(truncated_summary))

        result.extend(recent)
        return result

    async def _trigger_summary_update(self, messages: List[Message]) -> None:
        """
        触发摘要更新逻辑 (含防抖与后台执行)
        """
        now = time.time()
        
        # 1. 防抖检查
        if now - self._last_update_time < self.min_update_interval:
            logger.debug("Summary update skipped (debounce)")
            return

        # 2. 锁状态检查 (如果已有任务在跑，直接跳过，不做排队，避免堆积)
        lock = self._get_summary_lock()
        if lock.locked():
            logger.debug("Summary update skipped (locked)")
            return

        # 3. 执行策略
        if self.background_update:
            # 后台执行，不阻塞主流程
            # 创建 Task 会自动开始运行
            asyncio.create_task(self._update_summary_task(messages))
        else:
            # 阻塞执行 (旧行为)
            await self._update_summary_task(messages)

    async def _update_summary_task(self, messages: List[Message]) -> None:
        """实际执行摘要更新的逻辑"""
        lock = self._get_summary_lock()
        
        # 使用锁确保安全
        async with lock:
            try:
                logger.debug(f"Updating summary for {len(messages)} messages...")
                
                history_text = "\n".join(
                    f"{m.role}: {m.get_text_content()}" for m in messages
                )

                if self.current_summary:
                    history_text = f"Previous: {self.current_summary}\n\nNew:\n{history_text}"

                prompt = self.summary_template.format(history=history_text)

                response = await self.model.acompletion(
                    [{"role": "user", "content": prompt}]
                )
                content = response.choices[0].message.get("content", "")
                
                if content:
                    self.current_summary = content
                    self._last_update_time = time.time()
                    logger.info(f"Summary updated successfully, length: {len(content)}")
                    
            except Exception as e:
                logger.error(f"Failed to update summary: {e}")

    def clear_summary(self) -> None:
        self.current_summary = ""
        self._last_update_time = 0.0
```

[33] gecko/core/message/__init__.py
```python
from gecko.core.message.resources import MediaResource, ContentBlock
from gecko.core.message.model import Message, Role

__all__ = ["Message", "MediaResource", "ContentBlock", "Role"]
```

[34] gecko/core/message/model.py
```python
"""消息主体定义"""
from __future__ import annotations
import asyncio
from typing import Any, Dict, List, Literal, Optional, Union
from pydantic import BaseModel, Field, field_serializer, field_validator
from gecko.core.logging import get_logger
from gecko.core.message.resources import MediaResource, ContentBlock

logger = get_logger(__name__)
Role = Literal["system", "user", "assistant", "tool"]

class Message(BaseModel):
    """
    标准消息对象
    
    兼容 OpenAI Chat Completion API 格式
    
    示例:
        ```python
        # 简单文本消息
        msg = Message.user("Hello!")
        
        # 多模态消息
        msg = Message.user(
            text="What's in this image?",
            images=["./photo.jpg"]
        )
        
        # 助手消息
        msg = Message.assistant("I'm here to help!")
        
        # 工具返回消息
        msg = Message.tool_result(
            tool_call_id="call_123",
            content="Search results: ...",
            tool_name="search"
        )
        
        # 从 OpenAI 格式解析
        msg = Message.from_openai({
            "role": "user",
            "content": "Hello"
        })
        ```
    """
    role: Role
    content: Union[str, List[ContentBlock]] = Field(default="")
    name: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    tool_call_id: Optional[str] = None

    @field_validator("content", mode="before")
    @classmethod
    def validate_content(cls, v):
        """验证并规范化 content"""
        if v is None:
            return ""
        return v

    @field_serializer("content")
    def serialize_content(self, content: Union[str, List[ContentBlock]], _info):
        """序列化 content 为 OpenAI 格式"""
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            return [block.to_openai_format() for block in content]
        return str(content)

    # ===== 工厂方法 =====

    @classmethod
    def user(
        cls,
        text: str = "",
        images: Optional[List[str]] = None,
        name: Optional[str] = None
    ) -> Message:
        """
        创建用户消息
        
        参数:
            text: 文本内容
            images: 图片路径列表（URL 或本地文件）
            name: 用户名称（可选）
        
        返回:
            Message 实例
        
        示例:
            ```python
            # 纯文本
            msg = Message.user("Hello")
            
            # 文本 + 图片
            msg = Message.user(
                text="What's this?",
                images=["./photo.jpg", "https://example.com/img.png"]
            )
            ```
        """
        if not images:
            return cls(role="user", content=text, name=name)
        
        # 构建多模态内容
        blocks: List[ContentBlock] = []
        
        # 添加文本块
        if text:
            blocks.append(ContentBlock(type="text", text=text))
        
        # 添加图片块
        for img in images:
            try:
                # 判断是 URL 还是本地路径
                if img.startswith(("http://", "https://", "data:")):
                    resource = MediaResource(url=img)
                else:
                    resource = MediaResource.from_file(img)
                
                blocks.append(ContentBlock(type="image_url", image_url=resource))
            except Exception as e:
                logger.error("Failed to load image", path=img, error=str(e))
                # 继续处理其他图片
        
        return cls(role="user", content=blocks, name=name)

    @classmethod
    async def user_async(
        cls,
        text: str = "",
        images: Optional[List[str]] = None,
        name: Optional[str] = None
    ) -> Message:
        """
        创建用户消息（异步版本）
        
        对于大量或大文件图片，使用此方法避免阻塞
        """
        if not images:
            return cls(role="user", content=text, name=name)
        
        blocks: List[ContentBlock] = []
        
        if text:
            blocks.append(ContentBlock(type="text", text=text))
        
        # 异步加载所有图片
        async def _load_image(img: str) -> Optional[ContentBlock]:
            try:
                if img.startswith(("http://", "https://", "data:")):
                    resource = MediaResource(url=img)
                else:
                    resource = await MediaResource.from_file_async(img)
                return ContentBlock(type="image_url", image_url=resource)
            except Exception as e:
                logger.error("Failed to load image (async)", path=img, error=str(e))
                return None
        
        # 并发加载所有图片
        image_blocks = await asyncio.gather(*[_load_image(img) for img in images])
        blocks.extend([b for b in image_blocks if b is not None])
        
        return cls(role="user", content=blocks, name=name)

    @classmethod
    def assistant(
        cls, 
        content: str = "", 
        tool_calls: Optional[List[Dict[str, Any]]] = None, # 新增参数
        name: Optional[str] = None
    ) -> Message:
        """
        创建助手消息
        
        参数:
            content: 回复内容
            tool_calls: 工具调用列表
            name: 助手名称（可选）
        """
        return cls(
            role="assistant", 
            content=content, 
            tool_calls=tool_calls, 
            name=name
        )
    
    @property
    def safe_tool_calls(self) -> List[Dict[str, Any]]:
        """
        安全获取 tool_calls，确保返回列表而不是 None。
        用于减少 if msg.tool_calls is not None 的检查。
        """
        return self.tool_calls or []

    @classmethod
    def system(cls, content: str) -> Message:
        """
        创建系统消息
        
        参数:
            content: 系统提示词
        """
        return cls(role="system", content=content)

    @classmethod
    def tool_result(
        cls,
        tool_call_id: str,
        content: Any,
        tool_name: str
    ) -> Message:
        """
        创建工具返回消息
        
        参数:
            tool_call_id: 工具调用 ID
            content: 工具返回结果（任意类型，会自动序列化）
            tool_name: 工具名称
        
        返回:
            Message 实例
        """
        # 序列化 content
        if isinstance(content, str):
            serialized = content
        elif isinstance(content, (dict, list)):
            import json
            serialized = json.dumps(content, ensure_ascii=False, indent=2)
        else:
            serialized = str(content)
        
        return cls(
            role="tool",
            content=serialized,
            tool_call_id=tool_call_id,
            name=tool_name
        )

    @classmethod
    def from_openai(cls, payload: Dict[str, Any]) -> Message:
        """
        从 OpenAI API 格式解析消息
        
        参数:
            payload: OpenAI 格式的消息字典
        
        返回:
            Message 实例
        
        示例:
            ```python
            openai_msg = {
                "role": "assistant",
                "content": "Hello!",
                "tool_calls": [...]
            }
            msg = Message.from_openai(openai_msg)
            ```
        """
        try:
            return cls(**payload)
        except Exception as e:
            logger.error("Failed to parse OpenAI message", error=str(e), payload=payload)
            raise ValueError(f"无效的 OpenAI 消息格式: {e}") from e

    # ===== 转换方法 =====

    def to_openai_format(self) -> Dict[str, Any]:
        """
        转换为 OpenAI API 格式
        
        返回:
            符合 OpenAI 规范的字典
        """
        # 使用 Pydantic 的序列化（会调用 field_serializer）
        data = self.model_dump(exclude_none=True, mode="json")
        
        # 确保必要字段存在
        if "role" not in data:
            raise ValueError("消息缺少 role 字段")
        
        return data

    # ===== 工具方法 =====

    def get_text_content(self) -> str:
        """
        提取文本内容（忽略多模态部分）
        
        返回:
            纯文本内容
        
        用途:
            - 日志记录
            - 文本搜索
            - Token 估算
        """
        if isinstance(self.content, str):
            return self.content
        elif isinstance(self.content, list):
            text_parts = []
            for block in self.content:
                text = block.get_text_content()
                if text:
                    text_parts.append(text)
            return " ".join(text_parts)
        return ""

    def is_empty(self) -> bool:
        """
        检查消息是否为空
        
        返回:
            是否为空消息
        """
        if isinstance(self.content, str):
            return not self.content.strip()
        elif isinstance(self.content, list):
            return len(self.content) == 0
        return True

    def has_images(self) -> bool:
        """
        检查消息是否包含图片
        
        返回:
            是否包含图片
        """
        if isinstance(self.content, list):
            return any(block.type == "image_url" for block in self.content)
        return False

    def get_image_count(self) -> int:
        """
        获取图片数量
        
        返回:
            图片数量
        """
        if isinstance(self.content, list):
            return sum(1 for block in self.content if block.type == "image_url")
        return 0

    def clone(self) -> Message:
        """
        创建消息的深拷贝
        
        返回:
            新的 Message 实例
        """
        return Message.model_validate(self.model_dump())

    def truncate_content(self, max_length: int) -> Message:
        """
        截断消息内容（返回新消息）
        
        参数:
            max_length: 最大字符长度
        
        返回:
            截断后的新消息
        
        注意:
            仅截断文本内容，图片保持不变
        """
        if isinstance(self.content, str):
            if len(self.content) > max_length:
                truncated = self.content[:max_length] + "..."
                return Message(
                    role=self.role,
                    content=truncated,
                    name=self.name,
                    tool_calls=self.tool_calls,
                    tool_call_id=self.tool_call_id
                )
        
        # 多模态消息：截断文本块
        elif isinstance(self.content, list):
            new_blocks = []
            for block in self.content:
                if block.type == "text" and block.text:
                    if len(block.text) > max_length:
                        new_blocks.append(ContentBlock(
                            type="text",
                            text=block.text[:max_length] + "..."
                        ))
                    else:
                        new_blocks.append(block)
                else:
                    new_blocks.append(block)
            
            return Message(
                role=self.role,
                content=new_blocks,
                name=self.name,
                tool_calls=self.tool_calls,
                tool_call_id=self.tool_call_id
            )
        
        return self

    def __str__(self) -> str:
        """字符串表示（用于调试）"""
        text = self.get_text_content()
        preview = text[:50] + "..." if len(text) > 50 else text
        
        extra = []
        if self.has_images():
            extra.append(f"{self.get_image_count()} images")
        if self.tool_calls:
            extra.append(f"{len(self.tool_calls)} tool_calls")
        
        extra_str = f" ({', '.join(extra)})" if extra else ""
        
        return f"Message(role={self.role}, content='{preview}'{extra_str})"

    def __repr__(self) -> str:
        """详细表示"""
        return (
            f"Message("
            f"role={self.role!r}, "
            f"content={self.get_text_content()[:30]!r}, "
            f"has_images={self.has_images()}, "
            f"tool_calls={'Yes' if self.tool_calls else 'No'}"
            f")"
        )
```

[35] gecko/core/message/resources.py
```python
"""多模态资源定义"""
from __future__ import annotations
import asyncio
import base64
import mimetypes
from pathlib import Path
from typing import Any, Dict, Literal, Optional
from pydantic import BaseModel, model_validator
from gecko.core.logging import get_logger

logger = get_logger(__name__)

class MediaResource(BaseModel):
    """
    媒体资源（主要用于图片）
    
    支持：
    - URL（http/https）
    - Base64 编码的数据
    - 本地文件路径（通过工厂方法）
    
    示例:
        ```python
        # 从 URL
        img = MediaResource(url="https://example.com/image.jpg")
        
        # 从本地文件（同步）
        img = MediaResource.from_file("./image.png")
        
        # 从本地文件（异步）
        img = await MediaResource.from_file_async("./large_image.png")
        
        # 从 base64
        img = MediaResource(
            base64_data="iVBORw0KG...",
            mime_type="image/png"
        )
        ```
    """
    url: Optional[str] = None
    base64_data: Optional[str] = None
    mime_type: Optional[str] = None
    detail: Literal["auto", "low", "high"] = "auto"

    @model_validator(mode="after")
    def validate_source(self):
        """验证至少提供了一个数据源"""
        if not self.url and not self.base64_data:
            raise ValueError("必须提供 url 或 base64_data")
        return self

    @classmethod
    def from_file(
        cls,
        path: str,
        mime_type: Optional[str] = None,
        max_size_mb: int = 5,
        detail: Literal["auto", "low", "high"] = "auto"
    ) -> MediaResource:
        """
        从本地文件加载（同步版本）
        
        参数:
            path: 文件路径
            mime_type: MIME 类型（None 则自动推断）
            max_size_mb: 最大文件大小（MB）
            detail: 图片质量（OpenAI API 参数）
        
        返回:
            MediaResource 实例
        
        异常:
            FileNotFoundError: 文件不存在
            ValueError: 文件过大
        
        注意:
            这是同步方法，会阻塞事件循环。
            对于大文件，建议使用 from_file_async()
        """
        p = Path(path)
        
        # 检查文件是否存在
        if not p.exists():
            raise FileNotFoundError(f"文件不存在: {path}")
        
        if not p.is_file():
            raise ValueError(f"路径不是文件: {path}")
        
        # ✅ 优化：先检查文件大小，再读取
        file_size = p.stat().st_size
        max_size_bytes = max_size_mb * 1024 * 1024
        
        if file_size > max_size_bytes:
            raise ValueError(
                f"文件过大: {file_size / 1024 / 1024:.2f} MB "
                f"(最大 {max_size_mb} MB)"
            )
        
        # 读取并编码
        try:
            with open(p, "rb") as f:
                encoded = base64.b64encode(f.read()).decode("utf-8")
        except Exception as e:
            raise IOError(f"文件读取失败: {e}") from e
        
        # 推断 MIME 类型
        mime = mime_type or mimetypes.guess_type(p.name)[0] or "application/octet-stream"
        
        logger.debug(
            "Media loaded from file",
            path=path,
            size_kb=file_size / 1024,
            mime_type=mime
        )
        
        return cls(
            base64_data=encoded,
            mime_type=mime,
            detail=detail
        )

    @classmethod
    async def from_file_async(
        cls,
        path: str,
        mime_type: Optional[str] = None,
        max_size_mb: int = 5,
        detail: Literal["auto", "low", "high"] = "auto"
    ) -> MediaResource:
        """
        从本地文件加载（异步版本）
        
        对于大文件，使用此方法避免阻塞事件循环
        
        参数:
            同 from_file()
        
        返回:
            MediaResource 实例
        """
        p = Path(path)
        
        # 检查文件
        if not p.exists():
            raise FileNotFoundError(f"文件不存在: {path}")
        
        if not p.is_file():
            raise ValueError(f"路径不是文件: {path}")
        
        # 检查大小
        file_size = p.stat().st_size
        max_size_bytes = max_size_mb * 1024 * 1024
        
        if file_size > max_size_bytes:
            raise ValueError(
                f"文件过大: {file_size / 1024 / 1024:.2f} MB "
                f"(最大 {max_size_mb} MB)"
            )
        
        # ✅ 异步读取文件（在线程池中执行）
        def _read_file():
            with open(p, "rb") as f:
                return base64.b64encode(f.read()).decode("utf-8")
        
        try:
            encoded = await asyncio.to_thread(_read_file)
        except Exception as e:
            raise IOError(f"文件读取失败: {e}") from e
        
        # 推断 MIME 类型
        mime = mime_type or mimetypes.guess_type(p.name)[0] or "application/octet-stream"
        
        logger.debug(
            "Media loaded from file (async)",
            path=path,
            size_kb=file_size / 1024,
            mime_type=mime
        )
        
        return cls(
            base64_data=encoded,
            mime_type=mime,
            detail=detail
        )

    def to_openai_image_url(self) -> Dict[str, Any]:
        """
        转换为 OpenAI API 所需的 image_url 格式
        
        返回:
            符合 OpenAI 规范的字典
        """
        # 构建 URL
        if self.url:
            url_value = self.url
        elif self.base64_data:
            mime = self.mime_type or "image/jpeg"
            url_value = f"data:{mime};base64,{self.base64_data}"
        else:
            raise ValueError("MediaResource 缺少 URL 或 base64_data")
        
        return {
            "url": url_value,
            "detail": self.detail
        }

    def get_size_estimate(self) -> int:
        """
        估算数据大小（字节）
        
        返回:
            估算的字节数
        """
        if self.base64_data:
            # Base64 编码后的大小约为原始大小的 4/3
            return int(len(self.base64_data) * 3 / 4)
        elif self.url:
            # URL 无法估算实际大小
            return 0
        return 0


# ===== 内容块 =====

class ContentBlock(BaseModel):
    """
    消息内容块（用于多模态消息）
    
    支持：
    - 文本块
    - 图片块
    
    示例:
        ```python
        # 文本块
        text = ContentBlock(type="text", text="Hello")
        
        # 图片块
        image = ContentBlock(
            type="image_url",
            image_url=MediaResource(url="https://...")
        )
        ```
    """
    type: Literal["text", "image_url"]
    text: Optional[str] = None
    image_url: Optional[MediaResource] = None

    @model_validator(mode="after")
    def ensure_valid(self):
        """验证块的完整性"""
        if self.type == "text":
            if self.text is None:
                raise ValueError("文本块缺少 text 字段")
        elif self.type == "image_url":
            if self.image_url is None:
                raise ValueError("图片块缺少 image_url 字段")
        return self

    def to_openai_format(self) -> Dict[str, Any]:
        """转换为 OpenAI API 格式"""
        if self.type == "text":
            return {"type": "text", "text": self.text}
        elif self.type == "image_url":
            return {
                "type": "image_url",
                "image_url": self.image_url.to_openai_image_url() # type: ignore
            }
        else:
            raise ValueError(f"未知的内容类型: {self.type}")

    def get_text_content(self) -> str:
        """
        提取文本内容（用于调试/日志）
        
        返回:
            文本内容或占位符
        """
        if self.type == "text":
            return self.text or ""
        elif self.type == "image_url":
            return "[image]"
        return ""


# ===== 消息 =====

```

[36] gecko/core/metrics.py
```python
# gecko/core/metrics.py
"""
指标收集系统

提供轻量级的应用指标收集能力。
"""
from __future__ import annotations

import threading
import time
from collections import defaultdict
from contextlib import contextmanager
from dataclasses import dataclass, field
from typing import Any, Dict, Iterator, List, Optional


@dataclass
class MetricSample:
    """指标样本"""
    value: float
    timestamp: float = field(default_factory=time.time)
    labels: Dict[str, str] = field(default_factory=dict)


class Counter:
    """计数器（只增不减）"""

    def __init__(self, name: str, description: str = ""):
        self.name = name
        self.description = description
        self._values: Dict[tuple, float] = defaultdict(float)
        self._lock = threading.Lock()

    def inc(self, value: float = 1.0, **labels) -> None:
        """增加计数"""
        key = self._make_key(labels)
        with self._lock:
            self._values[key] += value

    def get(self, **labels) -> float:
        """获取当前值"""
        key = self._make_key(labels)
        with self._lock:
            return self._values.get(key, 0.0)

    def reset(self) -> None:
        """重置"""
        with self._lock:
            self._values.clear()

    @staticmethod
    def _make_key(labels: Dict[str, str]) -> tuple:
        return tuple(sorted(labels.items()))


class Gauge:
    """仪表（可增可减）"""

    def __init__(self, name: str, description: str = ""):
        self.name = name
        self.description = description
        self._values: Dict[tuple, float] = defaultdict(float)
        self._lock = threading.Lock()

    def set(self, value: float, **labels) -> None:
        """设置值"""
        key = self._make_key(labels)
        with self._lock:
            self._values[key] = value

    def inc(self, value: float = 1.0, **labels) -> None:
        """增加"""
        key = self._make_key(labels)
        with self._lock:
            self._values[key] += value

    def dec(self, value: float = 1.0, **labels) -> None:
        """减少"""
        key = self._make_key(labels)
        with self._lock:
            self._values[key] -= value

    def get(self, **labels) -> float:
        """获取值"""
        key = self._make_key(labels)
        with self._lock:
            return self._values.get(key, 0.0)

    @staticmethod
    def _make_key(labels: Dict[str, str]) -> tuple:
        return tuple(sorted(labels.items()))


class Histogram:
    """直方图（用于延迟等分布统计）"""

    DEFAULT_BUCKETS = (0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, float("inf"))

    def __init__(
        self,
        name: str,
        description: str = "",
        buckets: tuple = DEFAULT_BUCKETS
    ):
        self.name = name
        self.description = description
        self.buckets = tuple(sorted(buckets))

        self._counts: Dict[tuple, Dict[float, int]] = defaultdict(
            lambda: {b: 0 for b in self.buckets}
        )
        self._sums: Dict[tuple, float] = defaultdict(float)
        self._totals: Dict[tuple, int] = defaultdict(int)
        self._lock = threading.Lock()

    def observe(self, value: float, **labels) -> None:
        """记录观测值"""
        key = self._make_key(labels)
        with self._lock:
            self._sums[key] += value
            self._totals[key] += 1

            for bucket in self.buckets:
                if value <= bucket:
                    self._counts[key][bucket] += 1

    def get_stats(self, **labels) -> Dict[str, float]:
        """获取统计"""
        key = self._make_key(labels)
        with self._lock:
            total = self._totals.get(key, 0)
            sum_val = self._sums.get(key, 0.0)

            return {
                "count": total,
                "sum": sum_val,
                "avg": sum_val / total if total > 0 else 0.0,
            }

    @contextmanager
    def time(self, **labels) -> Iterator[None]:
        """计时上下文管理器"""
        start = time.perf_counter()
        try:
            yield
        finally:
            self.observe(time.perf_counter() - start, **labels)

    @staticmethod
    def _make_key(labels: Dict[str, str]) -> tuple:
        return tuple(sorted(labels.items()))


class MetricsRegistry:
    """
    指标注册中心
    
    示例:
        ```python
        metrics = MetricsRegistry()
        
        requests = metrics.counter("requests_total", "Total requests")
        latency = metrics.histogram("request_latency", "Request latency")
        
        requests.inc(endpoint="/api/chat")
        with latency.time(endpoint="/api/chat"):
            await process_request()
        ```
    """

    def __init__(self):
        self._counters: Dict[str, Counter] = {}
        self._gauges: Dict[str, Gauge] = {}
        self._histograms: Dict[str, Histogram] = {}
        self._lock = threading.Lock()

    def counter(self, name: str, description: str = "") -> Counter:
        """获取或创建计数器"""
        with self._lock:
            if name not in self._counters:
                self._counters[name] = Counter(name, description)
            return self._counters[name]

    def gauge(self, name: str, description: str = "") -> Gauge:
        """获取或创建仪表"""
        with self._lock:
            if name not in self._gauges:
                self._gauges[name] = Gauge(name, description)
            return self._gauges[name]

    def histogram(
        self,
        name: str,
        description: str = "",
        buckets: tuple = Histogram.DEFAULT_BUCKETS
    ) -> Histogram:
        """获取或创建直方图"""
        with self._lock:
            if name not in self._histograms:
                self._histograms[name] = Histogram(name, description, buckets)
            return self._histograms[name]

    def collect(self) -> Dict[str, Any]:
        """收集所有指标"""
        with self._lock:
            return {
                "counters": {
                    name: dict(c._values) for name, c in self._counters.items()
                },
                "gauges": {
                    name: dict(g._values) for name, g in self._gauges.items()
                },
                "histograms": {
                    name: h.get_stats() for name, h in self._histograms.items()
                },
            }

    def reset(self) -> None:
        """重置所有指标"""
        with self._lock:
            for c in self._counters.values():
                c.reset()
            self._gauges.clear()
            self._histograms.clear()


# 全局实例
_registry: Optional[MetricsRegistry] = None


def get_metrics() -> MetricsRegistry:
    """获取全局指标注册中心"""
    global _registry
    if _registry is None:
        _registry = MetricsRegistry()
    return _registry


__all__ = [
    "Counter",
    "Gauge",
    "Histogram",
    "MetricsRegistry",
    "MetricSample",
    "get_metrics",
]
```

[37] gecko/core/output/__init__.py
```python
# gecko/core/output/__init__.py
"""
Gecko Agent 输出模型包（模块化版本）

本包对原来的 gecko.core.output 单文件实现进行了拆分与扩展，但：
✅ 对外 API 完全兼容：
    from gecko.core.output import (
        TokenUsage,
        AgentOutput,
        create_text_output,
        create_tool_output,
        merge_outputs,
    )
依然可以正常使用，无需修改其他模块的 import。

当前子模块说明：
- token_usage.py
    定义 TokenUsage：
    - prompt_tokens / completion_tokens / total_tokens
    - 自动校验与补全 total_tokens
    - get_cost_estimate 成本估算

- agent_output.py
    定义 AgentOutput：
    - 标准对话输出模型（content + tool_calls + usage + raw + metadata）
    - 提供格式化、统计、转换为 OpenAI 消息等方法

- factories.py
    定义便捷工厂函数：
    - create_text_output  : 纯文本 AgentOutput
    - create_tool_output  : 带工具调用的 AgentOutput
    - create_json_output  : 结构化 JsonOutput

- merge.py
    定义输出合并逻辑：
    - merge_outputs : 将多个 AgentOutput 合并为一个

- json_output.py
    定义 JsonOutput：
    - 用于承载结构化 JSON / dict / list 形式的结果
    - 可转成 AgentOutput 便于统一处理

- streaming_output.py
    定义流式输出类型：
    - StreamingChunk   : 单个增量片段
    - StreamingOutput  : 管理多个片段并在结束时汇总为 AgentOutput

后续扩展：
- 若需要新增其他输出类型（例如：
  HtmlOutput、MarkdownOutput、RichMediaOutput 等），
  建议在本包内创建新的模块文件，并在本 __init__ 中统一导出。
"""

from .token_usage import TokenUsage
from .agent_output import AgentOutput
from .factories import (
    create_text_output,
    create_tool_output,
    create_json_output,
)
from gecko.core.output.merge import merge_outputs
from gecko.core.output.json_output import JsonOutput
from gecko.core.output.streaming_output import StreamingOutput, StreamingChunk

# __all__ 明确列出对外暴露的符号，便于 IDE 补全与文档生成
__all__ = [
    # 基础 usage 模型
    "TokenUsage",
    # 标准 Agent 输出模型
    "AgentOutput",
    # 常用工厂函数
    "create_text_output",
    "create_tool_output",
    "create_json_output",
    # 合并工具
    "merge_outputs",
    # 扩展输出类型：结构化 JSON
    "JsonOutput",
    # 扩展输出类型：流式输出
    "StreamingOutput",
    "StreamingChunk",
]
```

[38] gecko/core/output/agent_output.py
```python
# gecko/core/output/agent_output.py
"""
Agent 输出模型

本模块定义 Agent 执行后的标准输出结构：
- 最终回复内容（content）
- 工具调用信息（tool_calls）
- Token 使用统计（usage）
- 原始模型响应（raw）
- 附加元数据（metadata）

后续如果需要扩展不同形态的输出（如 StreamingOutput、JsonSchemaOutput 等），
可以在本模块旁新增新的模型，或从 AgentOutput 继承。
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, field_validator

from gecko.core.logging import get_logger
from gecko.core.output.token_usage import TokenUsage

logger = get_logger(__name__)


class AgentOutput(BaseModel):
    """
    Agent 执行结果

    属性:
        content: 最终文本回复（可能为空，如果只有工具调用）
        tool_calls: 工具调用列表（OpenAI 工具调用格式）
        usage: Token 使用统计（可选）
        raw:  原始模型响应（用于调试，不保证可 JSON 序列化）
        metadata: 附加元数据（例如模型名、耗时、include_raw 标记等）
    """

    content: str = Field(
        default="",
        description="最终文本回复",
    )
    tool_calls: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="工具调用列表（OpenAI 工具调用格式）",
    )
    usage: Optional[TokenUsage] = Field(
        default=None,
        description="Token 使用统计",
    )
    raw: Any = Field(
        default=None,
        description="原始模型响应（用于调试），类型不做强约束",
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="附加元数据（例如模型名、请求耗时、调试开关等）",
    )

    # 允许 raw 等字段为任意类型
    model_config = {"arbitrary_types_allowed": True}

    # ------ 字段校验器（构造阶段自动执行） ------ #

    @field_validator("tool_calls", mode="before")
    @classmethod
    def ensure_tool_calls(cls, value: Any) -> List[Dict[str, Any]]:
        """
        确保 tool_calls 始终是列表。

        兼容性与容错策略：
        - None            -> []
        - list            -> 原样返回
        - tuple / set     -> 转为 list，并记录一条 warning（提示调用方修正）
        - dict（单个调用） -> 包装为 [dict]，并记录 warning
        - 其他非法类型      -> 返回 []，记录 warning

        这样可以最大程度避免「调用方类型传错导致工具调用被静默丢弃」的问题，
        同时通过日志提示引导调用方修复。
        """
        if value is None:
            return []

        # 正常情况：已经是 list
        if isinstance(value, list):
            return value

        # 常见误用 1：tuple / set
        if isinstance(value, (tuple, set)):
            converted = list(value)
            logger.warning(
                "tool_calls converted to list from %s",
                type(value).__name__,
            )
            return converted

        # 常见误用 2：单个 dict
        if isinstance(value, dict):
            logger.warning("tool_calls single dict converted to list")
            return [value]

        # 其他异常类型：强制为空，但保留 warning 方便定位
        logger.warning(
            "tool_calls should be a list or dict-like, got %s",
            type(value).__name__,
        )
        return []

    @field_validator("content", mode="before")
    @classmethod
    def ensure_content(cls, value: Any) -> str:
        """
        确保 content 最终是字符串。

        规则：
        - None       -> ""
        - str        -> 原样返回
        - 其他类型    -> 调用 str()，以避免因为类型不符导致序列化报错。
        """
        if value is None:
            return ""
        if isinstance(value, str):
            return value
        # 容忍上层传入非字符串类型（如 Message 对象），尽量转换为 str
        return str(value)

    # ===== 检查方法 =====

    def has_content(self) -> bool:
        """
        检查是否有「非空白」文本内容。

        返回:
            bool: 是否有非空文本（会 strip 空白）
        """
        return bool(self.content and self.content.strip())

    def has_tool_calls(self) -> bool:
        """
        检查是否有工具调用。

        返回:
            bool: 是否包含工具调用
        """
        return len(self.tool_calls) > 0

    def tool_call_count(self) -> int:
        """
        获取工具调用数量。

        返回:
            int: 工具调用的数量
        """
        return len(self.tool_calls)

    def is_empty(self) -> bool:
        """
        检查输出是否完全为空。

        定义为：
        - 没有非空文本内容，且
        - 没有任何工具调用

        返回:
            bool: 是否既无内容也无工具调用
        """
        return not self.has_content() and not self.has_tool_calls()

    def has_usage(self) -> bool:
        """
        检查是否有 usage 信息。

        返回:
            bool: 是否包含 token 使用统计
        """
        return self.usage is not None

    # ===== 提取方法 =====

    def get_tool_names(self) -> List[str]:
        """
        提取所有被调用的工具名称。

        返回:
            List[str]: 工具名称列表
        """
        names: List[str] = []
        for call in self.tool_calls:
            func = call.get("function", {})
            name = func.get("name")
            if name:
                names.append(name)
        return names

    def get_tool_call_by_id(self, call_id: str) -> Optional[Dict[str, Any]]:
        """
        根据 ID 获取工具调用。

        参数:
            call_id: 工具调用 ID

        返回:
            dict | None: 工具调用字典，如果不存在返回 None
        """
        for call in self.tool_calls:
            if call.get("id") == call_id:
                return call
        return None

    def get_text_preview(self, max_length: int = 100) -> str:
        """
        获取内容预览（用于日志/显示）。

        参数:
            max_length: 最大预览长度（超出会截断并追加 '...'）

        返回:
            str: 截断后的文本预览
        """
        if not self.content:
            return ""

        if len(self.content) <= max_length:
            return self.content

        return self.content[:max_length] + "..."

    # ===== 转换方法 =====

    def to_dict(self) -> Dict[str, Any]:
        """
        转换为字典（便于序列化 / 日志记录）。

        注意：
        - usage 字段通过 Pydantic 的 model_dump 导出。
        - raw 字段默认不导出，只有在 metadata 中显式设置
          include_raw=True 且 raw is not None 时才导出。
        - raw 尽量保留原始结构（dict/list 等），由上层决定是否做 JSON 序列化。
        """
        data: Dict[str, Any] = {
            "content": self.content,
            "tool_calls": self.tool_calls,
            "metadata": self.metadata,
        }

        if self.usage is not None:
            # exclude_none=True 可以避免把 None 写进字典，减小日志体积
            data["usage"] = self.usage.model_dump(exclude_none=True)

        # raw 字段可能无法直接 JSON 序列化，仅在调试模式下包含
        if self.metadata.get("include_raw") and self.raw is not None:
            try:
                # 若 raw 已经是常见的 JSON 友好类型（dict/list/标量），直接保留结构
                if isinstance(self.raw, (dict, list, tuple, str, int, float, bool)):
                    data["raw"] = self.raw
                else:
                    # 非常规类型（如复杂对象），退而求其次转成字符串
                    data["raw"] = str(self.raw)
            except Exception:
                # 一旦转换异常，给出占位文本，避免序列化失败
                data["raw"] = "<non-serializable>"

        return data

    def to_message_dict(self) -> Dict[str, Any]:
        """
        转换为 OpenAI 消息格式（用于下一轮对话）。

        返回:
            dict: 符合 OpenAI API 的消息字典
        """
        msg: Dict[str, Any] = {
            "role": "assistant",
            # OpenAI 允许 content 为 null，此处用 None 表示「无内容，仅工具调用」
            "content": self.content or None,
        }

        if self.tool_calls:
            msg["tool_calls"] = self.tool_calls

        return msg

    # ===== 格式化输出 =====

    def format(self, include_metadata: bool = False) -> str:
        """
        格式化输出为可读文本（多段落），适合打印到控制台或日志。

        参数:
            include_metadata: 是否包含元数据段落

        返回:
            str: 格式化后的字符串
        """
        lines: List[str] = []

        # —— 内容 —— #
        if self.has_content():
            lines.append("=== 回复内容 ===")
            lines.append(self.content)
            lines.append("")

        # —— 工具调用 —— #
        if self.has_tool_calls():
            lines.append("=== 工具调用 ===")
            for i, call in enumerate(self.tool_calls, 1):
                func = call.get("function", {})
                name = func.get("name", "unknown")
                args = func.get("arguments", "{}")
                lines.append(f"{i}. {name}")
                lines.append(f"   参数: {args}")
            lines.append("")

        # —— Token 使用 —— #
        if self.usage:
            lines.append("=== Token 使用 ===")
            lines.append(f"输入: {self.usage.prompt_tokens}")
            lines.append(f"输出: {self.usage.completion_tokens}")
            lines.append(f"总计: {self.usage.total_tokens}")
            lines.append("")

        # —— 元数据 —— #
        if include_metadata and self.metadata:
            lines.append("=== 元数据 ===")
            for key, value in self.metadata.items():
                lines.append(f"{key}: {value}")
            lines.append("")

        return "\n".join(lines)

    def summary(self) -> str:
        """
        生成简短摘要（一行），适合在列表 / 简要日志中展示。

        返回:
            str: 一行摘要文本
        """
        parts: List[str] = []

        if self.has_content():
            preview = self.get_text_preview(30)
            parts.append(f"回复: {preview}")

        if self.has_tool_calls():
            parts.append(f"工具调用: {self.tool_call_count()}")

        if self.usage:
            parts.append(f"Tokens: {self.usage.total_tokens}")

        if not parts:
            return "空输出"

        return " | ".join(parts)

    # ===== 统计方法 =====

    def get_stats(self) -> Dict[str, Any]:
        """
        获取输出统计信息（结构化）。

        返回:
            dict: 包含各种统计数据的字典

        字段说明:
            - content_length: 文本内容长度（包括空白字符）
            - has_content: 是否存在非空白内容
            - tool_call_count: 工具调用数量
            - tool_names: 工具名称列表
            - is_empty: 是否既无内容也无工具调用
            - usage（可选）: token 使用统计快照
        """
        stats: Dict[str, Any] = {
            "content_length": len(self.content),  # 原始长度，用于粗略估算字数
            "has_content": self.has_content(),
            "tool_call_count": self.tool_call_count(),
            "tool_names": self.get_tool_names(),
            "is_empty": self.is_empty(),
        }

        if self.usage:
            stats["usage"] = {
                "prompt_tokens": self.usage.prompt_tokens,
                "completion_tokens": self.usage.completion_tokens,
                "total_tokens": self.usage.total_tokens,
            }

        return stats

    # ===== 字符串 & 布尔表示 =====

    def __str__(self) -> str:
        """使用 summary 作为默认字符串表示，便于日志阅读。"""
        return self.summary()

    def __repr__(self) -> str:
        """提供稍详细的调试信息。"""
        return (
            f"AgentOutput("
            f"content_length={len(self.content)}, "
            f"tool_calls={self.tool_call_count()}, "
            f"has_usage={self.has_usage()}"
            f")"
        )

    def __bool__(self) -> bool:
        """
        布尔值转换（是否有有效输出）。

        用于 if output: ... 这种写法，语义为「是否不为空输出」。
        """
        return not self.is_empty()
```

[39] gecko/core/output/factories.py
```python
# gecko/core/output/factories.py
"""
AgentOutput / JsonOutput 工厂函数模块

提供若干便捷方法，用于快速构造常见形式的输出：
- create_text_output : 仅包含文本内容的 AgentOutput
- create_tool_output : 包含工具调用的 AgentOutput
- create_json_output : 承载结构化 JSON 数据的 JsonOutput
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional

from gecko.core.output.agent_output import AgentOutput
from gecko.core.output.token_usage import TokenUsage
from gecko.core.output.json_output import JsonOutput   # 新增导入


def create_text_output(
    content: str,
    usage: Optional[TokenUsage] = None,
    **metadata: Any,
) -> AgentOutput:
    """
    快速创建纯文本输出（AgentOutput）。

    参数:
        content: 文本内容
        usage: Token 使用统计（可选）
        **metadata: 附加元数据，会存入 AgentOutput.metadata
    """
    return AgentOutput(
        content=content,
        usage=usage,
        metadata=metadata,
    )


def create_tool_output(
    tool_calls: List[Dict[str, Any]],
    content: str = "",
    usage: Optional[TokenUsage] = None,
    **metadata: Any,
) -> AgentOutput:
    """
    快速创建「包含工具调用」的输出（AgentOutput）。

    参数:
        tool_calls: 工具调用列表（OpenAI 工具调用格式）
        content: 可选的文本内容（一般为「我去调用某工具」之类的说明）
        usage: Token 使用统计（可选）
        **metadata: 附加元数据
    """
    return AgentOutput(
        content=content,
        tool_calls=tool_calls,
        usage=usage,
        metadata=metadata,
    )


def create_json_output(
    data: Any,
    usage: Optional[TokenUsage] = None,
    **metadata: Any,
) -> JsonOutput:
    """
    快速创建结构化 JSON 输出（JsonOutput）。

    参数:
        data: 结构化数据，一般为 dict/list/标量等
        usage: Token 使用统计（可选）
        **metadata: 附加元数据，会存入 JsonOutput.metadata

    返回:
        JsonOutput 实例

    典型场景：
        - 工具调用返回了结构化结果，想在框架内部完整保留
        - 中间推理结果是一个复杂 dict，而不是单纯文本
    """
    return JsonOutput(
        data=data,
        usage=usage,
        metadata=metadata,
    )
```

[40] gecko/core/output/json_output.py
```python
# gecko/core/output/json_output.py
"""
JsonOutput 结构化输出类型

使用场景：
- 对话/工具调用的最终结果是结构化数据（dict / list），
  希望保留为 JSON 形式，而不是直接转成文本。
- 例如：表格解析结果、配置项、校验报告等。

与 AgentOutput 的关系：
- JsonOutput 更偏向「数据结果」，AgentOutput 更偏向「对话消息」。
- 可以通过 to_agent_output() 将 JsonOutput 转换为可读文本形式。
"""

from __future__ import annotations

from typing import Any, Dict, Optional

from pydantic import BaseModel, Field, field_validator

from gecko.core.output.token_usage import TokenUsage
from gecko.core.output.agent_output import AgentOutput


class JsonOutput(BaseModel):
    """
    结构化 JSON 输出模型。

    属性:
        data:     结构化数据，一般为 dict / list，也可以是任意 JSON 兼容类型
        usage:    Token 使用统计（可选）
        metadata: 元数据（例如来源模型、生成时间、schema 版本等）
    """

    data: Any = Field(
        ...,
        description="结构化数据，要求是 JSON 友好的类型（dict/list/标量等）",
    )
    usage: Optional[TokenUsage] = Field(
        default=None,
        description="Token 使用统计（可选）",
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="附加元数据（如 schema 版本、生成时间等）",
    )

    model_config = {"arbitrary_types_allowed": True}

    @field_validator("data", mode="before")
    @classmethod
    def ensure_json_friendly(cls, value: Any) -> Any:
        """
        尽量保证 data 是 JSON 友好的类型。

        这里不做强制转换，只做简单的防御：
        - 若是 Pydantic BaseModel，则转为 dict
        - 其他类型交由上层决定是否可 JSON 序列化
        """
        try:
            from pydantic import BaseModel as _BaseModel  # 局部导入，避免循环引用
        except Exception:
            _BaseModel = None  # type: ignore

        if _BaseModel is not None and isinstance(value, _BaseModel):
            return value.model_dump()

        return value

    def to_dict(self) -> Dict[str, Any]:
        """
        转换为字典形式（适合日志/序列化）。

        返回:
            dict: 包含 data, metadata, usage 的结构
        """
        result: Dict[str, Any] = {
            "data": self.data,
            "metadata": self.metadata,
        }
        if self.usage is not None:
            result["usage"] = self.usage.model_dump(exclude_none=True)
        return result

    def summary(self, max_preview_length: int = 80) -> str:
        """
        生成简短摘要文本，通常用于日志 / CLI 显示。

        参数:
            max_preview_length: 预览字符串的最大长度

        返回:
            str: 摘要文本
        """
        # 尝试用简短的 repr 展示 data
        preview = repr(self.data)
        if len(preview) > max_preview_length:
            preview = preview[:max_preview_length] + "..."

        parts = [f"JSON: {preview}"]

        if self.usage is not None:
            parts.append(f"Tokens: {self.usage.total_tokens}")

        return " | ".join(parts)

    def to_agent_output(self, pretty: bool = False) -> AgentOutput:
        """
        将 JsonOutput 转换为 AgentOutput 形式（方便统一处理）。

        参数:
            pretty: 是否以缩进 JSON 的形式输出 content（便于人类阅读）

        返回:
            AgentOutput 实例，content 为 JSON 字符串
        """
        import json

        if pretty:
            content = json.dumps(self.data, ensure_ascii=False, indent=2)
        else:
            content = json.dumps(self.data, ensure_ascii=False)

        return AgentOutput(
            content=content,
            usage=self.usage,
            metadata={**self.metadata, "from": "JsonOutput"},
        )

    def __str__(self) -> str:
        """默认字符串表示，使用 summary。"""
        return self.summary()

    def __repr__(self) -> str:
        return f"JsonOutput(data_type={type(self.data).__name__}, has_usage={self.usage is not None})"
```

[41] gecko/core/output/merge.py
```python
# gecko/core/output/merge.py
"""
AgentOutput 聚合/合并工具模块

当前提供：
- merge_outputs : 将多个 AgentOutput 合并为一个（多 Agent 场景、流水线场景）

后续如果有更多聚合策略（例如按角色合并、按对话轮次分组），
也可以在本模块中扩展。
"""

from __future__ import annotations

from typing import Any, Dict, List, Optional

from gecko.core.output.agent_output import AgentOutput
from gecko.core.output.token_usage import TokenUsage


def merge_outputs(outputs: List[AgentOutput]) -> AgentOutput:
    """
    合并多个 AgentOutput（用于多 Agent 或多轮中间结果汇总）。

    合并策略（保持原有语义）：
        - 内容（content）：将所有「有非空内容」的输出按顺序用换行符拼接。
        - 工具调用（tool_calls）：简单拼接列表。
        - Usage：对 prompt_tokens / completion_tokens 做逐一累加，
                 total_tokens = 两者之和。
        - 元数据（metadata）：后者覆盖前者（dict 的 update 语义）。

    参数:
        outputs: AgentOutput 列表

    返回:
        AgentOutput: 合并后的输出对象

    示例:
        ```python
        output1 = AgentOutput(content="Part 1")
        output2 = AgentOutput(content="Part 2")
        merged = merge_outputs([output1, output2])
        print(merged.content)  # "Part 1\nPart 2"
        ```
    """
    if not outputs:
        # 空列表时返回一个空的 AgentOutput，避免上层还要做 None 判断
        return AgentOutput()

    if len(outputs) == 1:
        # 单元素直接返回原对象，避免不必要的拷贝
        return outputs[0]

    # —— 合并内容 —— #
    contents: List[str] = [o.content for o in outputs if o.has_content()]
    merged_content = "\n".join(contents)

    # —— 合并工具调用 —— #
    merged_tool_calls: List[Dict[str, Any]] = []
    for output in outputs:
        if output.tool_calls:
            merged_tool_calls.extend(output.tool_calls)

    # —— 合并 usage —— #
    merged_usage: Optional[TokenUsage] = None
    if any(o.has_usage() for o in outputs):
        total_prompt = sum(
            o.usage.prompt_tokens  # type: ignore[union-attr]
            for o in outputs
            if o.usage is not None
        )
        total_completion = sum(
            o.usage.completion_tokens  # type: ignore[union-attr]
            for o in outputs
            if o.usage is not None
        )
        merged_usage = TokenUsage(
            prompt_tokens=total_prompt,
            completion_tokens=total_completion,
            total_tokens=total_prompt + total_completion,
        )

    # —— 合并元数据 —— #
    merged_metadata: Dict[str, Any] = {}
    for output in outputs:
        # 后面的输出可以覆盖前面的同名 key，符合「后写优先」直觉
        merged_metadata.update(output.metadata)

    return AgentOutput(
        content=merged_content,
        tool_calls=merged_tool_calls,
        usage=merged_usage,
        metadata=merged_metadata,
    )
```

[42] gecko/core/output/streaming_output.py
```python
# gecko/core/output/streaming_output.py
"""
StreamingOutput 流式输出类型

使用场景：
- LLM/Agent 支持流式输出（stream=True），每次返回一个小片段（chunk）。
- 希望在框架内部以统一结构记录所有 chunk，并在结束时汇总成一个 AgentOutput。

核心结构：
- StreamingChunk : 表示一次增量输出
- StreamingOutput: 维护所有 chunk，并提供：
    - append_chunk(...)
    - iter_contents()
    - finalize() -> AgentOutput
"""

from __future__ import annotations

from typing import Any, Dict, Iterable, List, Optional

from pydantic import BaseModel, Field, field_validator

from gecko.core.output.token_usage import TokenUsage
from gecko.core.output.agent_output import AgentOutput


class StreamingChunk(BaseModel):
    """
    单个流式输出片段。

    属性:
        index:        片段序号（递增，便于调试/排序）
        content_delta: 本次新增的文本内容（增量）
        tool_calls_delta: 本次新增的工具调用（通常为空，部分模型支持流式工具调用）
        usage_delta:  本次片段对应的增量 token 使用统计（可选）
        raw:          原始 provider 返回的该片段响应（用于调试）
        metadata:     附加元数据（如时间戳、来源模型等）
    """

    index: int = Field(
        ...,
        ge=0,
        description="片段序号（从 0 开始递增）",
    )
    content_delta: str = Field(
        default="",
        description="本次新增的文本内容（增量）",
    )
    tool_calls_delta: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="本次新增的工具调用（少数模型支持流式工具调用）",
    )
    usage_delta: Optional[TokenUsage] = Field(
        default=None,
        description="本次片段对应的增量 token 使用统计（可选）",
    )
    raw: Any = Field(
        default=None,
        description="原始 provider 返回的该片段响应（用于调试）",
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="附加元数据（如时间戳、来源模型等）",
    )

    model_config = {"arbitrary_types_allowed": True}

    @field_validator("content_delta", mode="before")
    @classmethod
    def ensure_str(cls, value: Any) -> str:
        """
        内容增量统一转成字符串，防止 provider 返回奇怪类型。
        """
        if value is None:
            return ""
        if isinstance(value, str):
            return value
        return str(value)


class StreamingOutput(BaseModel):
    """
    流式输出聚合器。

    核心职责：
    - 持有多个 StreamingChunk
    - 提供追加接口（append_chunk）
    - 支持遍历所有文本增量（iter_contents）
    - 在流结束时汇总成一个完整的 AgentOutput（finalize）

    注意：
    - StreamingOutput 本身不直接提供 content 字段，
      而是通过 finalize() 生成最终 AgentOutput。
    """

    chunks: List[StreamingChunk] = Field(
        default_factory=list,
        description="按顺序记录的所有流式输出片段",
    )
    # 可选：整体级别的 usage（有的 provider 只在流末尾给总 usage）
    usage: Optional[TokenUsage] = Field(
        default=None,
        description="整体 Token 使用统计（可选，不同于各个 chunk 的增量）",
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="整体元数据（例如模型名称、会话 ID 等）",
    )

    model_config = {"arbitrary_types_allowed": True}

    # ===== 追加 & 遍历 =====

    def append_chunk(self, chunk: StreamingChunk) -> None:
        """
        追加一个流式片段。

        使用方式：
            streaming = StreamingOutput()
            streaming.append_chunk(StreamingChunk(index=0, content_delta="Hel"))
            streaming.append_chunk(StreamingChunk(index=1, content_delta="lo"))
        """
        self.chunks.append(chunk)

    def extend_chunks(self, chunks: Iterable[StreamingChunk]) -> None:
        """
        一次性追加多个片段。
        """
        self.chunks.extend(chunks)

    def iter_contents(self) -> Iterable[str]:
        """
        迭代每个片段的 content_delta，适合边消费边打印。

        示例：
            for delta in streaming.iter_contents():
                print(delta, end="", flush=True)
        """
        for chunk in self.chunks:
            if chunk.content_delta:
                yield chunk.content_delta

    # ===== 汇总逻辑 =====

    def _aggregate_content(self) -> str:
        """
        内部方法：将所有 chunk 的 content_delta 按 index 排序后拼接。
        """
        # 保险起见，按 index 排序一次（如果调用方保证按顺序 append，可忽略排序开销）
        sorted_chunks = sorted(self.chunks, key=lambda c: c.index)
        return "".join(c.content_delta for c in sorted_chunks if c.content_delta)

    def _aggregate_tool_calls(self) -> List[Dict[str, Any]]:
        """
        内部方法：合并所有 chunk 的 tool_calls_delta。
        """
        merged: List[Dict[str, Any]] = []
        for c in self.chunks:
            if c.tool_calls_delta:
                merged.extend(c.tool_calls_delta)
        return merged

    def _aggregate_usage(self) -> Optional[TokenUsage]:
        """
        内部方法：汇总 usage。

        策略：
        - 若整体 self.usage 已存在，直接返回（通常是 provider 在流末尾给的）。
        - 否则尝试对所有 chunk 的 usage_delta 做累加。
        """
        if self.usage is not None:
            return self.usage

        # 汇总 chunk 级别的 usage_delta
        deltas = [c.usage_delta for c in self.chunks if c.usage_delta is not None]
        if not deltas:
            return None

        total_prompt = sum(u.prompt_tokens for u in deltas)
        total_completion = sum(u.completion_tokens for u in deltas)

        return TokenUsage(
            prompt_tokens=total_prompt,
            completion_tokens=total_completion,
            total_tokens=total_prompt + total_completion,
        )

    def finalize(self) -> AgentOutput:
        """
        将所有流式片段汇总为一个完整的 AgentOutput。

        返回:
            AgentOutput 实例，其中：
                - content 为所有 content_delta 的拼接结果
                - tool_calls 为所有 tool_calls_delta 的合并结果
                - usage 为汇总后的 TokenUsage（若可用）
                - metadata 为 StreamingOutput.metadata
        """
        content = self._aggregate_content()
        tool_calls = self._aggregate_tool_calls()
        usage = self._aggregate_usage()

        return AgentOutput(
            content=content,
            tool_calls=tool_calls,
            usage=usage,
            metadata={**self.metadata, "from": "StreamingOutput"},
        )

    # ===== 辅助方法 =====

    def get_stats(self) -> Dict[str, Any]:
        """
        获取流式输出的统计信息（结构化）。

        返回:
            dict，包含：
                - chunk_count: 片段数量
                - total_content_length: 所有增量内容拼接后的长度
                - has_usage: 是否包含 usage（整体或增量）
        """
        aggregated_content = self._aggregate_content()
        has_usage = self.usage is not None or any(
            c.usage_delta is not None for c in self.chunks
        )

        return {
            "chunk_count": len(self.chunks),
            "total_content_length": len(aggregated_content),
            "has_usage": has_usage,
        }

    def __str__(self) -> str:
        stats = self.get_stats()
        return (
            f"StreamingOutput(chunks={stats['chunk_count']}, "
            f"total_content_length={stats['total_content_length']}, "
            f"has_usage={stats['has_usage']})"
        )

    def __repr__(self) -> str:
        return str(self)
```

[43] gecko/core/output/token_usage.py
```python
# gecko/core/output/token_usage.py
"""
Token 使用统计模型

本模块单独抽出 TokenUsage，便于：
- 与不同模型提供商的 usage 字段对齐/适配
- 后续扩展更多维度（如 cached_tokens、reasoning_tokens 等）
"""

from __future__ import annotations

from typing import Dict, Optional

from pydantic import BaseModel, Field, model_validator

from gecko.core.logging import get_logger

logger = get_logger(__name__)


class TokenUsage(BaseModel):
    """
    Token 使用统计

    设计目标：
    - 与 OpenAI API 的 usage 结构基本兼容
    - 在调用方缺省 total_tokens 时自动计算
    - 保留 provider 提供的原始 total_tokens（如有冲突，仅记录 warning）

    示例:
        ```python
        usage = TokenUsage(
            prompt_tokens=100,
            completion_tokens=50,
            total_tokens=150
        )
        ```
    """

    # —— 基本字段 —— #
    prompt_tokens: int = Field(
        default=0,
        ge=0,
        description="提示词（输入）消耗的 tokens",
    )
    completion_tokens: int = Field(
        default=0,
        ge=0,
        description="生成（输出）消耗的 tokens",
    )
    total_tokens: int = Field(
        default=0,
        ge=0,
        description="总消耗 tokens",
    )

    # —— 扩展字段（部分模型会提供更细粒度统计） —— #
    prompt_tokens_details: Optional[Dict[str, int]] = Field(
        default=None,
        description="提示词 tokens 详细信息，例如 {'cached_tokens': 10}",
    )
    completion_tokens_details: Optional[Dict[str, int]] = Field(
        default=None,
        description="生成 tokens 详细信息",
    )

    @model_validator(mode="after")
    def validate_total(self) -> "TokenUsage":
        """
        验证并补全 total_tokens 字段。

        逻辑说明：
        - calculated_total = prompt_tokens + completion_tokens
        - 如果 total_tokens 仍为 0 且 calculated_total > 0：
            认为调用方未显式提供 total_tokens，自动补全。
        - 如果 total_tokens 非 0 且与 calculated_total 不一致：
            不改写 total_tokens，只记录 warning 方便排查。
        - 如果三者都是 0：
            保持为 0，不做任何额外处理。
        """
        calculated_total = self.prompt_tokens + self.completion_tokens

        # 情况一：total_tokens 默认值 0，且显然有 token 消耗，则自动补全
        if self.total_tokens == 0 and calculated_total > 0:
            self.total_tokens = calculated_total

        # 情况二：provider 明确给了 total_tokens（非 0），但与计算值不一致，仅告警
        elif self.total_tokens != 0 and self.total_tokens != calculated_total:
            logger.warning(
                "Token usage total mismatch",
                total=self.total_tokens,
                calculated=calculated_total,
            )

        # 情况三：三者皆 0，保持 0，不做干预
        return self

    def get_cost_estimate(
        self,
        prompt_price_per_1k: float = 0.0,
        completion_price_per_1k: float = 0.0,
    ) -> float:
        """
        估算成本（美元）

        说明：
        - 使用 prompt_tokens 和 completion_tokens 分开计费，
          不使用 total_tokens，符合大多数厂商的计费模式。
        - 如果价格参数为 0，返回值自然为 0，不做额外处理。

        参数:
            prompt_price_per_1k: 输入 token 每 1000 个的价格
            completion_price_per_1k: 输出 token 每 1000 个的价格

        返回:
            估算成本（美元）
        """
        prompt_cost = (self.prompt_tokens / 1000) * prompt_price_per_1k
        completion_cost = (self.completion_tokens / 1000) * completion_price_per_1k
        return prompt_cost + completion_cost

    def __str__(self) -> str:
        """简洁的字符串表示，便于日志打印。"""
        return (
            f"TokenUsage("
            f"prompt={self.prompt_tokens}, "
            f"completion={self.completion_tokens}, "
            f"total={self.total_tokens}"
            f")"
        )
```

[44] gecko/core/prompt/__init__.py
```python
# gecko/core/prompt/__init__.py
"""
gecko.core.prompt 包初始化模块（对外兼容层）

重构背景：
- 原来 gecko.core.prompt 是一个单文件模块 prompt.py；
- 现在拆分为 prompt 包，下挂多个子模块（template, library, jinja_env 等）；
- 为了保持向后兼容，必须确保原有 import 写法继续生效。

兼容要求：
1. 以下写法仍然有效：
   - from gecko.core.prompt import PromptTemplate
   - from gecko.core.prompt import PromptLibrary, DEFAULT_REACT_PROMPT
   - import gecko.core.prompt as prompt; prompt.PromptTemplate(...)

2. 若老代码内部曾经使用过私有函数（不推荐但可能存在）：
   - from gecko.core.prompt import _get_jinja2_env, _check_jinja2
   可以通过轻量 alias 保留兼容性。

同时，为了给后续迭代留空间，这里还对外导出了：
- Prompt 组合器（PromptSection, PromptComposer）
- Prompt 验证 / Lint（PromptValidator, lint_prompt 等）
- Prompt 注册中心 / 版本管理（PromptRegistry, default_registry 等）
"""

from __future__ import annotations

# 核心模板 & 模板库
from gecko.core.prompt.template import PromptTemplate
from gecko.core.prompt.library import PromptLibrary, DEFAULT_REACT_PROMPT

# Jinja2 环境封装
from gecko.core.prompt.jinja_env import get_jinja2_env, check_jinja2

# Prompt 组合器
from gecko.core.prompt.composer import PromptSection, PromptComposer

# Prompt 验证 / Lint
from gecko.core.prompt.validators import (
    IssueSeverity,
    PromptIssue,
    PromptValidator,
    lint_prompt,
)

# Prompt 注册中心 / 版本管理
from gecko.core.prompt.registry import (
    PromptRecord,
    PromptRegistry,
    default_registry,
    register_prompt,
    get_prompt,
    list_prompts,
)


# ===== 向后兼容的私有函数别名 =====
# 老版本中的 `_get_jinja2_env()` / `_check_jinja2()` 若被其他模块引用，
# 这里提供别名以避免重构后出现 ImportError。
def _get_jinja2_env():
    """
    向后兼容的内部函数别名。

    新代码请直接使用：
        from gecko.core.prompt import get_jinja2_env
    """
    return get_jinja2_env()


def _check_jinja2():
    """
    向后兼容的内部函数别名。

    新代码请直接使用：
        from gecko.core.prompt import check_jinja2
    """
    return check_jinja2()


# ===== 修正 __module__ 提升兼容性 =====
# 这样在 repr / 日志 / 反射 / pickle 时，仍然表现为 "gecko.core.prompt.*"
PromptTemplate.__module__ = "gecko.core.prompt"
PromptLibrary.__module__ = "gecko.core.prompt"

# 视情况也可以把新类的 __module__ 统一成 gecko.core.prompt，方便调试/日志
PromptSection.__module__ = "gecko.core.prompt"
PromptComposer.__module__ = "gecko.core.prompt"
PromptValidator.__module__ = "gecko.core.prompt"
PromptRegistry.__module__ = "gecko.core.prompt"


__all__ = [
    # 核心模板
    "PromptTemplate",
    # 预定义模板库
    "PromptLibrary",
    "DEFAULT_REACT_PROMPT",
    # Jinja2 环境
    "get_jinja2_env",
    "check_jinja2",
    "_get_jinja2_env",
    "_check_jinja2",
    # 组合器
    "PromptSection",
    "PromptComposer",
    # 验证 / Lint
    "IssueSeverity",
    "PromptIssue",
    "PromptValidator",
    "lint_prompt",
    # 注册中心 / 版本管理
    "PromptRecord",
    "PromptRegistry",
    "default_registry",
    "register_prompt",
    "get_prompt",
    "list_prompts",
]
```

[45] gecko/core/prompt/composer.py
```python
# gecko/core/prompt/composer.py
"""
Prompt 组合器模块（Composer）

职责：
- 将多个 PromptTemplate 片段（Section）按顺序组合成一个完整 Prompt；
- 支持为每个 Section 配置前后缀、是否启用等信息；
- 支持一键生成新的 PromptTemplate，或直接渲染为字符串。

设计目标：
- 解耦“模板片段组织方式”和“模板本身的渲染逻辑”；
- 提供可扩展的 Section 描述结构，将来可以挂更多配置（权重、条件、元数据等）；
- 在不破坏 PromptTemplate 现有接口的前提下，提升 Prompt 结构化管理能力。
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Iterable, Set

from gecko.core.logging import get_logger
from .template import PromptTemplate

logger = get_logger(__name__)


@dataclass
class PromptSection:
    """
    单个 Prompt 片段（Section）的描述。

    字段说明：
        name:
            Section 名称，用于标识用途，例如 "system" / "examples" / "task" 等。
        template:
            实际负责渲染的 PromptTemplate。
        enabled:
            是否启用该 Section，为后续做 A/B 配置或动态开关留扩展空间。
        prefix:
            可选的前缀字符串，会拼接在该 Section 渲染结果之前。
        suffix:
            可选的后缀字符串，会拼接在该 Section 渲染结果之后。

    扩展空间（未来可以考虑加入）：
        - condition: Callable[[Dict[str, Any]], bool]，按上下文决定是否启用；
        - weight / priority: 在自动排序时使用；
        - metadata: 任意结构化信息用于分析或可视化。
    """

    name: str
    template: PromptTemplate
    enabled: bool = True
    prefix: str = ""
    suffix: str = ""

    def render(self, context: Dict[str, Any]) -> str:
        """
        渲染当前 Section。

        当前策略：
            - 默认使用严格模式的 format()；
            - 如有需要，可以在上层 Composer 中选择使用 format_safe()。
        """
        logger.debug("Rendering section", extra={"name": self.name})
        body = self.template.format(**context)
        return f"{self.prefix}{body}{self.suffix}"


class PromptComposer:
    """
    Prompt 组合器（Composer）

    功能：
        - 维护一组按顺序排列的 PromptSection；
        - 支持按顺序渲染，或组合成一个新的 PromptTemplate；
        - 支持全局分隔符（global_separator）统一控制 Section 之间的连接方式。

    常见用法：
        ```python
        composer = PromptComposer()

        composer.add_text_section(
            name="system",
            text="You are a helpful assistant.",
        )

        composer.add_template_section(
            name="task",
            template=PromptTemplate(
                template="User question: {{ question }}",
                input_variables=["question"],
            )
        )

        prompt_str = composer.render(question="What is AI?")
        # 或者：
        tpl = composer.to_template()
        prompt_str = tpl.format(question="What is AI?")
        ```
    """

    def __init__(
        self,
        sections: Optional[Iterable[PromptSection]] = None,
        global_separator: str = "\n\n",
    ) -> None:
        """
        初始化组合器。

        参数：
            sections:
                初始的 Section 列表，保持顺序；
            global_separator:
                在渲染多个 Section 时用于连接各个部分的全局分隔符。
        """
        self._sections: List[PromptSection] = list(sections) if sections else []
        self.global_separator: str = global_separator

    # ==================== Section 管理 ====================

    @property
    def sections(self) -> List[PromptSection]:
        """
        返回所有 Section（按顺序）。

        注意：
            - 返回的是实际内部列表的引用，如果需要严格封装，可以改为返回副本。
        """
        return self._sections

    def add_section(self, section: PromptSection) -> "PromptComposer":
        """
        添加一个已构造好的 PromptSection。

        返回：
            self（支持链式调用）
        """
        self._sections.append(section)
        return self

    def add_template_section(
        self,
        name: str,
        template: PromptTemplate,
        enabled: bool = True,
        prefix: str = "",
        suffix: str = "",
    ) -> "PromptComposer":
        """
        使用已有 PromptTemplate 添加一个 Section。
        """
        section = PromptSection(
            name=name,
            template=template,
            enabled=enabled,
            prefix=prefix,
            suffix=suffix,
        )
        return self.add_section(section)

    def add_text_section(
        self,
        name: str,
        text: str,
        enabled: bool = True,
        prefix: str = "",
        suffix: str = "",
    ) -> "PromptComposer":
        """
        使用纯文本添加一个 Section。

        实现方式：
            - 内部会创建一个只包含固定文本的 PromptTemplate，
              不需要任何 input_variables。
        """
        tpl = PromptTemplate(
            template=text,
            input_variables=[],
        )
        section = PromptSection(
            name=name,
            template=tpl,
            enabled=enabled,
            prefix=prefix,
            suffix=suffix,
        )
        return self.add_section(section)

    def enable_section(self, name: str) -> None:
        """
        按名称启用某个 Section。
        """
        for s in self._sections:
            if s.name == name:
                s.enabled = True

    def disable_section(self, name: str) -> None:
        """
        按名称禁用某个 Section。
        """
        for s in self._sections:
            if s.name == name:
                s.enabled = False

    # ==================== 渲染 / 组合 ====================

    def render(self, **context: Any) -> str:
        """
        直接渲染所有启用的 Section，返回拼接后的字符串。

        行为：
            - 按 Section 顺序遍历；
            - 对 enabled=True 的 Section 调用 section.render(context)；
            - 使用 global_separator 连接各个片段；
            - 不自动在末尾追加额外换行。
        """
        logger.debug(
            "Rendering composed prompt",
            extra={"sections": [s.name for s in self._sections]},
        )

        parts: List[str] = []
        for section in self._sections:
            if not section.enabled:
                continue
            rendered = section.render(context)
            # 跳过完全空字符串的片段，避免产生多余空段
            if rendered.strip():
                parts.append(rendered)

        return self.global_separator.join(parts)

    def to_template(self) -> PromptTemplate:
        """
        将当前组合器转换为一个新的 PromptTemplate。

        实现方式：
            - 将每个启用的 Section 的 template.template 按顺序拼接；
            - 使用 global_separator 作为片段之间的分隔符；
            - input_variables 取所有 Section 模板的并集；
            - template_format 目前简单选择第一个 Section 的模板格式
              （假设所有 Section 一致，若不一致可未来扩展更复杂逻辑）。

        注意：
            - 该方法只组合「模板字符串」，不会直接渲染；
            - 如需要预绑定部分变量，可以对返回的 PromptTemplate 调用 partial()。
        """
        if not self._sections:
            # 空组合器返回一个空模板，方便链路调用。
            return PromptTemplate(template="", input_variables=[])

        # 只考虑 enabled=True 的 Section
        enabled_sections = [s for s in self._sections if s.enabled]
        if not enabled_sections:
            return PromptTemplate(template="", input_variables=[])

        # 1) 拼接模板字符串
        template_parts: List[str] = []
        for s in enabled_sections:
            tmpl_str = s.template.template
            # 如果 Section 自己有前后缀，也拼进去
            full_str = f"{s.prefix}{tmpl_str}{s.suffix}"
            template_parts.append(full_str)

        combined_template_str = self.global_separator.join(template_parts)

        # 2) 合并所有 input_variables（取并集）
        all_vars: Set[str] = set()
        for s in enabled_sections:
            all_vars.update(s.template.input_variables)

        # 3) 决定最终 template_format（这里简单采用第一个 Section 的格式）
        template_format = enabled_sections[0].template.template_format

        logger.debug(
            "Composed PromptTemplate",
            extra={
                "template_length": len(combined_template_str),
                "input_variables": sorted(all_vars),
                "template_format": template_format,
            },
        )

        return PromptTemplate(
            template=combined_template_str,
            input_variables=sorted(all_vars),
            template_format=template_format,
        )


__all__ = ["PromptSection", "PromptComposer"]
```

[46] gecko/core/prompt/jinja_env.py
```python
# gecko/core/prompt/jinja_env.py
"""
Jinja2 环境封装模块

职责：
1. 懒加载检查当前环境是否已安装 jinja2；
2. 提供全局单例的 Jinja2 Environment 对象；
3. 作为 gecko.core.prompt 模块与 Jinja2 的解耦层，后续如果需要
   切换为 SandboxedEnvironment（安全沙箱）或增加过滤器，在本文件集中处理。

设计要点：
- 避免在 import 阶段就强依赖 jinja2，使用时再检测；
- 通过 module 内部的全局变量缓存 Environment，避免重复创建。
"""

from __future__ import annotations

from typing import Any, Optional

from gecko.core.logging import get_logger

logger = get_logger(__name__)

# 标记 jinja2 是否可用的缓存变量（None 表示尚未检测）
_jinja2_available: Optional[bool] = None
# 全局单例 Environment 实例缓存
_jinja2_env: Any = None


def check_jinja2() -> bool:
    """
    检查当前环境中是否可用 jinja2（懒加载）。

    返回:
        True  - 已安装 jinja2，后续可以正常使用 Jinja2 模板；
        False - 未安装 jinja2，使用相关功能会抛 ImportError。

    说明：
        - 只在第一次调用时真正 import jinja2，之后结果会缓存在 _jinja2_available 中；
        - 这样可以避免在不使用 Prompt 功能时引入多余依赖。
    """
    global _jinja2_available
    if _jinja2_available is None:
        try:
            import jinja2  # noqa: F401  # 仅用于验证模块存在
            _jinja2_available = True
        except ImportError:
            _jinja2_available = False
    return bool(_jinja2_available)


def get_jinja2_env():
    """
    获取全局 Jinja2 Environment 单例。

    行为：
        1. 若尚未创建 Environment，则调用 check_jinja2() 检查依赖是否存在；
        2. 若未安装 jinja2，抛出 ImportError，提示用户安装依赖；
        3. 若已安装，则创建 Environment 并缓存到模块级变量；
        4. 后续调用直接复用缓存对象。

    当前 Environment 配置：
        - undefined=StrictUndefined
          未定义变量会抛异常，避免静默错误；
        - autoescape=False
          Prompt 通常为纯文本，不需要 HTML 自动转义；
        - keep_trailing_newline=True
          保留模板末尾换行，避免拼接时格式错乱；
        - extensions=[]
          暂不启用额外扩展。

    可扩展点：
        - 如果未来需要严格的沙箱执行，可将 Environment 替换为 SandboxedEnvironment；
        - 如需增加自定义 filter / test，可在这里统一注册。
    """
    global _jinja2_env

    if _jinja2_env is None:
        if not check_jinja2():
            raise ImportError(
                "PromptTemplate 依赖 jinja2。\n"
                "请安装：pip install jinja2\n"
                "或：rye add jinja2"
            )

        from jinja2 import Environment, StrictUndefined

        _jinja2_env = Environment(
            undefined=StrictUndefined,
            autoescape=False,
            keep_trailing_newline=True,
            extensions=[],
        )
        logger.debug("Jinja2 environment initialized")

    return _jinja2_env


__all__ = ["check_jinja2", "get_jinja2_env"]
```

[47] gecko/core/prompt/library.py
```python
# gecko/core/prompt/library.py
"""
预定义 Prompt 模板库模块

职责：
- 提供一组常用的 Prompt 模板工厂方法，供业务层直接使用；
- 集中管理 ReAct / Chat / 摘要 / 提取 / 翻译等常见模板；
- 保持与旧版 prompt.py 中 PromptLibrary 行为兼容。

可扩展点：
- 若后续需要更多标准模板（例如 Code Review 模板、SQL 生成模板等），
  可以在该模块中继续扩展静态方法；
- 若数量较多，也可以按业务拆分多个子库，再在 __init__.py 中统一 re-export。
"""

from __future__ import annotations

from gecko.core.prompt.template import PromptTemplate


class PromptLibrary:
    """
    常用 Prompt 模板库。

    用法示例：
        ```python
        from gecko.core.prompt import PromptLibrary

        tpl = PromptLibrary.get_react_prompt()
        prompt_str = tpl.format(tools=[...], question="What is AI?")
        ```
    """

    @staticmethod
    def get_react_prompt() -> PromptTemplate:
        """
        获取 ReAct 风格推理模板。

        特点：
            - 支持工具列表展示；
            - 约定工具调用输出格式；
            - 引导模型 "Let's think step by step"。
        """
        template = """You are a helpful AI assistant with access to tools.

{% if tools %}
Available Tools:
{% for tool in tools %}
- {{ tool.name }}: {{ tool.description }}
{% endfor %}
{% endif %}

To use a tool, respond with a tool call in the following format:
Action: tool_name
Action Input: {"param": "value"}

Then wait for the observation before continuing.

Question: {{ question }}

Let's think step by step."""
        return PromptTemplate(
            template=template,
            input_variables=["tools", "question"],
        )

    @staticmethod
    def get_chat_prompt() -> PromptTemplate:
        """
        获取通用对话模板。

        模板结构：
            - 可选 system 提示（如系统角色设定）；
            - history 列表，用于多轮对话上下文；
              每个元素建议为：{"role": "...", "content": "..."}；
            - user_input 表示当前用户最新输入；
            - 模板以 "Assistant:" 结尾，方便模型补全。

        实现细节：
            - 在 Jinja2 StrictUndefined 模式下，未提供 system/history 会抛错；
            - 为保证“system / history 可选”的设计语义，这里采用：
                1) 在 input_variables 中声明 ["user_input", "system", "history"]；
                2) 使用 partial(system=None, history=[]) 绑定默认值；
            - 对调用方来说，只需要提供 user_input 即可完成渲染。
        """
        template = """{% if system %}{{ system }}

{% endif %}{% for message in history %}{{ message.role }}: {{ message.content }}
{% endfor %}User: {{ user_input }}
Assistant:"""

        base = PromptTemplate(
            template=template,
            input_variables=["user_input", "system", "history"],
        )
        return base.partial(system=None, history=[])

    @staticmethod
    def get_summarization_prompt() -> PromptTemplate:
        """
        获取摘要模板。

        约定：
            - max_words 控制摘要的词数上限；
            - text 为待摘要文本。
        """
        template = """Please summarize the following text in {{ max_words }} words or less:

{{ text }}

Summary:"""
        return PromptTemplate(
            template=template,
            input_variables=["text", "max_words"],
        )

    @staticmethod
    def get_extraction_prompt() -> PromptTemplate:
        """
        获取信息提取模板。

        约定：
            - fields: 需要提取的字段名称列表（字符串）；
            - text: 原始文本；
            - 要求模型以 JSON 格式返回提取结果。
        """
        template = """Extract the following information from the text:

{% for field in fields %}
- {{ field }}
{% endfor %}

Text: {{ text }}

Respond in JSON format."""
        return PromptTemplate(
            template=template,
            input_variables=["fields", "text"],
        )

    @staticmethod
    def get_translation_prompt() -> PromptTemplate:
        """
        获取翻译模板。

        约定：
            - source_lang: 源语言名称（如 "Chinese"）；
            - target_lang: 目标语言名称（如 "English"）；
            - text: 待翻译文本。
        """
        template = """Translate the following text from {{ source_lang }} to {{ target_lang }}:

{{ text }}

Translation:"""
        return PromptTemplate(
            template=template,
            input_variables=["source_lang", "target_lang", "text"],
        )


# ===== 原有默认模板（兼容旧接口）=====

DEFAULT_REACT_PROMPT = PromptTemplate(
    template="""You are a helpful AI assistant.
Current time: {{ current_time }}

{% if tools %}
You have access to the following tools:
{% for tool in tools %}
- {{ tool.name }}: {{ tool.description }}
{% endfor %}
{% endif %}

Answer the user's question using the tools if necessary.
""",
    input_variables=["current_time", "tools"],
)


__all__ = ["PromptLibrary", "DEFAULT_REACT_PROMPT"]
```

[48] gecko/core/prompt/registry.py
```python
# gecko/core/prompt/registry.py
"""
Prompt 注册中心 / 版本管理模块

职责：
- 提供一个可编程访问的 Prompt 配置中心，支持：
    - 按 name + version 注册、获取 PromptTemplate；
    - 为 Prompt 附带描述、标签、元数据；
    - 简单的“最新版本”解析逻辑；
- 提供一个全局 default_registry 和若干便捷函数，
  便于在项目中统一管理所有 Prompt。

设计目标：
- 初期以“内存级 Registry”为主，便于快速开发与测试；
- 结构设计为将来扩展为：
    - 从文件 / 数据库加载；
    - 与配置中心 / KV 存储对接；
    - 多环境（dev / staging / prod）隔离 等。
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Dict, Iterable, List, Optional, Set, Tuple

from gecko.core.logging import get_logger
from gecko.core.prompt.template import PromptTemplate

logger = get_logger(__name__)


@dataclass
class PromptRecord:
    """
    单条 Prompt 注册记录。

    字段说明：
        name:
            Prompt 名称，建议使用有层级语义的字符串，例如：
                - "chat.default"
                - "summarization.short"
                - "code.review"
        version:
            版本号字符串，例如：
                - "v1"
                - "1.0.0"
                - "2025-11-29"
              没有特别限定格式，由上层约定。
        template:
            实际的 PromptTemplate 实例。
        description:
            人类可读的描述，用于管理界面或文档展示。
        tags:
            标签集合，例如 {"chat", "production", "zh-CN"}；
        metadata:
            任意额外信息，例如 {"owner": "BMS team", "domain": "battery"}。
    """

    name: str
    version: str
    template: PromptTemplate
    description: Optional[str] = None
    tags: Set[str] = field(default_factory=set)
    metadata: Dict[str, Any] = field(default_factory=dict)


class PromptRegistry:
    """
    Prompt 注册中心（Registry）

    内部数据结构：
        - _store: Dict[name, Dict[version, PromptRecord]]

    核心方法：
        - register()       : 注册或更新某个 name/version 的记录；
        - get()            : 获取指定 name/version 的 PromptTemplate；
        - get_record()     : 获取完整 PromptRecord 对象；
        - remove()         : 删除指定记录；
        - list_records()   : 按条件列出所有记录；
        - resolve_version(): 决定在未指定 version 时选择哪一个版本。
    """

    def __init__(self) -> None:
        self._store: Dict[str, Dict[str, PromptRecord]] = {}

    # ========== 注册与更新 ==========

    def register(
        self,
        name: str,
        template: PromptTemplate,
        version: str = "default",
        description: Optional[str] = None,
        tags: Optional[Iterable[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        overwrite: bool = True,
    ) -> PromptRecord:
        """
        注册或更新一个 Prompt 记录。

        参数：
            name:
                Prompt 名称；
            template:
                PromptTemplate 实例；
            version:
                版本号字符串，默认 "default"；
            description:
                描述信息；
            tags:
                标签集合（可选）；
            metadata:
                任意元数据字典；
            overwrite:
                若为 False 且同名同版本已存在，则抛出 ValueError；
                若为 True，则覆盖原有记录。

        返回：
            新注册（或更新后）的 PromptRecord。
        """
        if name not in self._store:
            self._store[name] = {}

        existing = self._store[name].get(version)
        if existing and not overwrite:
            raise ValueError(
                f"Prompt '{name}' version '{version}' 已存在，且 overwrite=False。"
            )

        record = PromptRecord(
            name=name,
            version=version,
            template=template,
            description=description,
            tags=set(tags or []),
            metadata=dict(metadata or {}),
        )
        self._store[name][version] = record

        logger.info(
            "Registered prompt",
            extra={
                "name": name,
                "version": version,
                "tags": list(record.tags),
            },
        )

        return record

    # ========== 获取与删除 ==========

    def get(
        self,
        name: str,
        version: Optional[str] = None,
        raise_if_missing: bool = True,
    ) -> Optional[PromptTemplate]:
        """
        获取指定 name + version 的 PromptTemplate。

        参数：
            name:
                Prompt 名称；
            version:
                版本号；若为 None，则调用 resolve_version(name) 决定采用哪个版本；
            raise_if_missing:
                若为 True 且未找到，则抛 KeyError；
                若为 False 则返回 None。

        返回：
            PromptTemplate 实例或 None。
        """
        record = self.get_record(
            name=name,
            version=version,
            raise_if_missing=raise_if_missing,
        )
        return record.template if record else None

    def get_record(
        self,
        name: str,
        version: Optional[str] = None,
        raise_if_missing: bool = True,
    ) -> Optional[PromptRecord]:
        """
        获取完整 PromptRecord 记录。

        逻辑与 get() 类似，但返回的是包含描述、标签等信息的对象。
        """
        if name not in self._store:
            if raise_if_missing:
                raise KeyError(f"Prompt '{name}' 不存在。")
            return None

        versions_map = self._store[name]
        if version is None:
            # 未指定版本时调用内部版本解析逻辑
            version = self.resolve_version(name, versions_map)
            if version is None:
                if raise_if_missing:
                    raise KeyError(f"Prompt '{name}' 没有任何可用版本。")
                return None

        record = versions_map.get(version)
        if record is None and raise_if_missing:
            raise KeyError(f"Prompt '{name}' 的版本 '{version}' 不存在。")

        return record

    def remove(
        self,
        name: str,
        version: Optional[str] = None,
    ) -> None:
        """
        删除指定 name + version 的记录。

        行为：
            - 若 version 为 None，则删除该 name 下的所有版本；
            - 否则仅删除指定版本；
            - 若删除后该 name 无任何版本，会从 _store 中移除该键。
        """
        if name not in self._store:
            return

        if version is None:
            del self._store[name]
            logger.info("Removed all versions of prompt", extra={"name": name})
            return

        versions_map = self._store[name]
        if version in versions_map:
            del versions_map[version]
            logger.info(
                "Removed prompt version",
                extra={"name": name, "version": version},
            )

        if not versions_map:
            del self._store[name]

    # ========== 列表与搜索 ==========

    def list_records(
        self,
        name: Optional[str] = None,
        tags: Optional[Iterable[str]] = None,
    ) -> List[PromptRecord]:
        """
        列出所有匹配条件的 PromptRecord。

        参数：
            name:
                若提供，仅返回该 name 相关的记录；
            tags:
                若提供，仅返回包含所有这些标签的记录（子集关系）。

        返回：
            PromptRecord 列表（不保证顺序）。
        """
        results: List[PromptRecord] = []
        tag_set = set(tags or [])

        def record_matches(rec: PromptRecord) -> bool:
            if tag_set and not tag_set.issubset(rec.tags):
                return False
            return True

        if name:
            versions_map = self._store.get(name, {})
            for rec in versions_map.values():
                if record_matches(rec):
                    results.append(rec)
        else:
            for versions_map in self._store.values():
                for rec in versions_map.values():
                    if record_matches(rec):
                        results.append(rec)

        return results

    # ========== 版本解析策略 ==========

    def resolve_version(
        self,
        name: str,
        versions_map: Dict[str, PromptRecord],
    ) -> Optional[str]:
        """
        当调用 get(name, version=None) 时，决定选用哪个版本。

        默认策略：
            1. 若存在 "latest" 版本，则优先返回 "latest"；
            2. 若存在 "default" 版本，则返回 "default"；
            3. 否则，对版本号字符串做一次简单排序，返回“最大”的那个；
               （如果版本号为 "v1"、"v2" 或 "1.0"、"1.2.3"，通常能得到一个合理结果）

        可扩展：
            - 子类可重写本方法，实现更复杂的版本选择逻辑；
            - 例如解析 Semantic Version（semver）并按语义排序。
        """
        if not versions_map:
            return None

        if "latest" in versions_map:
            return "latest"
        if "default" in versions_map:
            return "default"

        # 简单按字典序排序版本号字符串
        all_versions = sorted(versions_map.keys())
        return all_versions[-1]


# ===== 全局默认 Registry + 便捷函数 =====

# 全局默认注册中心，适合大多场景使用。
default_registry = PromptRegistry()


def register_prompt(
    name: str,
    template: PromptTemplate,
    version: str = "default",
    description: Optional[str] = None,
    tags: Optional[Iterable[str]] = None,
    metadata: Optional[Dict[str, Any]] = None,
    overwrite: bool = True,
) -> PromptRecord:
    """
    使用全局 default_registry 注册一个 Prompt。

    用法示例：
        ```python
        from gecko.core.prompt import (
            PromptTemplate,
            register_prompt,
        )

        tpl = PromptTemplate(
            template="Hello {{ name }}",
            input_variables=["name"],
        )
        register_prompt(
            name="greeting.simple",
            version="v1",
            template=tpl,
            description="简单打招呼模板",
            tags={"greeting", "demo"},
        )
        ```
    """
    return default_registry.register(
        name=name,
        template=template,
        version=version,
        description=description,
        tags=tags,
        metadata=metadata,
        overwrite=overwrite,
    )


def get_prompt(
    name: str,
    version: Optional[str] = None,
    raise_if_missing: bool = True,
) -> Optional[PromptTemplate]:
    """
    从全局 default_registry 获取 PromptTemplate。

    参数：
        name:
            Prompt 名称；
        version:
            版本号；若为 None，则走默认版本解析策略；
        raise_if_missing:
            若为 True 且未找到，则抛 KeyError；否则返回 None。
    """
    return default_registry.get(
        name=name,
        version=version,
        raise_if_missing=raise_if_missing,
    )


def list_prompts(
    name: Optional[str] = None,
    tags: Optional[Iterable[str]] = None,
) -> List[PromptRecord]:
    """
    从全局 default_registry 列出所有匹配条件的 PromptRecord。
    """
    return default_registry.list_records(name=name, tags=tags)


__all__ = [
    "PromptRecord",
    "PromptRegistry",
    "default_registry",
    "register_prompt",
    "get_prompt",
    "list_prompts",
]
```

[49] gecko/core/prompt/template.py
```python
# gecko/core/prompt/template.py
"""
PromptTemplate 核心实现模块

职责：
- 定义 PromptTemplate 类，负责：
    - 模板字段与校验逻辑；
    - Jinja2 / str.format 两种渲染模式；
    - 模板语法验证；
    - 模板变量提取；
    - partial() 部分应用；
    - from_file / from_examples 等工厂方法。

设计要点：
- 使用 Pydantic BaseModel 方便与其他配置/模型统一管理；
- 使用 PrivateAttr 存放运行时缓存（编译模板、partial 参数、变量缓存）；
- 与 Jinja2 环境解耦，通过 jinja_env.get_jinja2_env() 访问；
- partial() 实现为“记录预绑定参数”，而不是立即渲染模板，避免破坏占位符。
"""

from __future__ import annotations

import re
from typing import Any, Dict, List, Optional, Set

from pydantic import BaseModel, Field, PrivateAttr, field_validator

from gecko.core.logging import get_logger
from gecko.core.prompt.jinja_env import get_jinja2_env

logger = get_logger(__name__)


class PromptTemplate(BaseModel):
    """
    Prompt 模板类

    支持两种模板格式：
        1. Jinja2（默认）：
           - 使用 {{ var }} / {% if %} / {% for %} 等语法；
           - 通过 get_jinja2_env() 获得严格模式（StrictUndefined）的 Environment；
        2. "f-string"（内部实现基于 str.format）：
           - 实际上是 Python 的 "Hello {name}".format(name="Alice") 形式；
           - 为保持历史兼容，字段名仍使用 "f-string"。

    典型用法（Jinja2）：
        ```python
        tpl = PromptTemplate(
            template="Hello, {{ name }}! You are {{ age }} years old.",
            input_variables=["name", "age"],
        )
        result = tpl.format(name="Alice", age=25)
        ```

    典型用法（partial 部分应用）：
        ```python
        tpl = PromptTemplate(
            template="Hello, {{ name }}! You are {{ age }} years old.",
            input_variables=["name", "age"],
        )
        p = tpl.partial(name="Alice")  # 预设 name
        result = p.format(age=25)      # 仅需传剩余变量
        ```

    属性说明：
        template:
            模板字符串。
        input_variables:
            渲染时要求调用方必须提供的变量列表；
            用于 format() 前的缺失变量检查。
        template_format:
            模板格式：
              - "jinja2"   : 使用 Jinja2 模板渲染；
              - "f-string" : 使用 str.format 渲染（名称为历史兼容保留）。
        validate_template:
            是否在模型初始化时立即验证模板语法。
    """

    # ------------ 公有字段（参与序列化） ------------

    template: str = Field(..., description="模板字符串")
    input_variables: List[str] = Field(
        default_factory=list,
        description="必需的输入变量列表",
    )
    template_format: str = Field(
        default="jinja2",
        description="模板格式（jinja2 / f-string，后者实际使用 str.format 语法）",
    )
    validate_template: bool = Field(
        default=True,
        description="是否在初始化时验证模板语法",
    )

    # ------------ 私有字段（运行时缓存，不参与序列化） ------------

    # 缓存已编译的 Jinja2 模板对象（避免重复 from_string）
    _compiled_template: Any = PrivateAttr(default=None)
    # partial() 预绑定的变量（部分应用参数）
    _partial_kwargs: Dict[str, Any] = PrivateAttr(default_factory=dict)
    # 模板中变量名的缓存集合（减少重复 AST 解析）
    _variables_cache: Optional[Set[str]] = PrivateAttr(default=None)

    # ===== Pydantic 钩子 & 字段校验 =====

    @field_validator("template_format", mode="before")
    @classmethod
    def validate_format(cls, v: str) -> str:
        """
        校验 template_format 字段是否合法。

        说明：
            - 为保持向后兼容，继续使用 "f-string" 命名；
            - 实现上并非 Python 原生 f-string，而是 str.format。
        """
        valid_formats = {"jinja2", "f-string"}
        if v not in valid_formats:
            raise ValueError(
                f"不支持的模板格式: {v}。支持的格式: {valid_formats}"
            )
        return v

    def model_post_init(self, __context: Any) -> None:  # type: ignore[override]
        """
        Pydantic v2 风格的初始化后钩子。

        在模型实例创建完成后：
            - 若 validate_template=True，则立即进行语法验证；
            - 语法错误在此阶段就会被抛出，避免运行期才发现问题。
        """
        if self.validate_template:
            self._validate_template_syntax()

    # ===== 模板语法验证相关方法 =====

    def _validate_template_syntax(self) -> None:
        """
        验证模板语法是否正确。

        - Jinja2 模式：尝试编译模板字符串为 Jinja2 Template 对象；
        - f-string 模式：做大括号数量匹配检查（基础检查）。
        """
        if self.template_format == "jinja2":
            try:
                env = get_jinja2_env()
                # from_string 本身会做语法解析，失败时抛出异常
                self._compiled_template = env.from_string(self.template)
                logger.debug("Template syntax validated")
            except Exception as e:
                error_msg = self._format_jinja2_error(str(e))
                raise ValueError(f"模板语法错误:\n{error_msg}") from e
        elif self.template_format == "f-string":
            self._validate_fstring_syntax()

    def _validate_fstring_syntax(self) -> None:
        """
        验证 "f-string"（实际为 str.format）模板语法的基础一致性。

        当前仅做：
            - 检查 '{' 和 '}' 的数量是否一致；
        若不一致，基本可以判定模板存在语法错误。
        """
        open_count = self.template.count("{")
        close_count = self.template.count("}")

        if open_count != close_count:
            raise ValueError(
                f"f-string 语法错误: 大括号不匹配 "
                f"({{ {open_count} 个, }} {close_count} 个)"
            )

    def _format_jinja2_error(self, error: str) -> str:
        """
        对 Jinja2 抛出的错误信息进行更加友好、便于排查的格式化。

        处理方式：
            - 仅保留前几行（最多 5 行）错误信息，避免过长；
            - 附带模板前 100 个字符的预览，快速定位问题位置。
        """
        lines = error.split("\n")
        formatted_lines: List[str] = []

        for line in lines[:5]:
            if line.strip():
                formatted_lines.append(f"  {line}")

        template_preview = self.template[:100].replace("\n", "\\n")
        formatted_lines.append(f"\n模板片段: {template_preview}...")

        return "\n".join(formatted_lines)

    # ===== 核心渲染接口 =====

    def format(self, **kwargs: Any) -> str:
        """
        严格模式下渲染模板。

        行为：
            1. 首先合并 partial() 预绑定参数与本次调用参数；
               - 调用参数优先级更高，可覆盖 partial 预设值；
            2. 基于 input_variables 检查是否缺少必需变量；
            3. 根据 template_format 调用对应的渲染实现。

        参数:
            **kwargs: 渲染时提供的变量。

        返回:
            渲染后的字符串。

        异常:
            - ValueError: 缺少必需变量或渲染失败。
        """
        merged_kwargs: Dict[str, Any] = {**self._partial_kwargs, **kwargs}

        # 检查必需变量
        missing = self._check_missing_variables(merged_kwargs)
        if missing:
            raise ValueError(
                f"缺少必需的模板变量: {', '.join(missing)}\n"
                f"需要: {self.input_variables}\n"
                f"提供: {list(merged_kwargs.keys())}"
            )

        # 根据格式渲染
        if self.template_format == "jinja2":
            return self._format_jinja2(**merged_kwargs)
        elif self.template_format == "f-string":
            return self._format_fstring(**merged_kwargs)
        else:
            raise ValueError(f"不支持的模板格式: {self.template_format}")

    def _check_missing_variables(self, kwargs: Dict[str, Any]) -> List[str]:
        """
        根据 input_variables 检查缺失的变量。

        注意：
            - 只检查 input_variables 中声明的必需变量；
            - 模板中实际引用的变量集合可能比 input_variables 多，二者不强制一致。
        """
        provided = set(kwargs.keys())
        required = set(self.input_variables)
        missing = required - provided
        return sorted(missing)

    def _format_jinja2(self, **kwargs: Any) -> str:
        """
        使用 Jinja2 渲染模板。

        缓存策略：
            - 若 _compiled_template 已存在，直接使用；
            - 否则调用 get_jinja2_env().from_string() 编译并缓存。
        """
        try:
            if self._compiled_template is None:
                env = get_jinja2_env()
                self._compiled_template = env.from_string(self.template)

            result = self._compiled_template.render(**kwargs)
            return result

        except Exception as e:
            error_msg = str(e)

            # 尝试识别「变量未定义」这种常见错误并给出更明确提示
            if "is undefined" in error_msg:
                match = re.search(r"'(\w+)' is undefined", error_msg)
                if match:
                    var_name = match.group(1)
                    raise ValueError(
                        f"模板变量 '{var_name}' 未定义。\n"
                        f"可用变量: {list(kwargs.keys())}"
                    ) from e

            # 兜底错误信息，附带模板片段
            raise ValueError(
                f"模板渲染失败: {error_msg}\n"
                f"模板: {self.template[:100]}..."
            ) from e

    def _format_fstring(self, **kwargs: Any) -> str:
        """
        使用 str.format 渲染模板（对应 template_format='f-string'）。

        说明：
            - 这里并不是 Python 的 f"..." 语法，而是
              `"Hello {name}".format(name="Alice")` 的形式。
        """
        try:
            return self.template.format(**kwargs)
        except KeyError as e:
            raise ValueError(
                f"缺少变量: {e}\n"
                f"可用变量: {list(kwargs.keys())}"
            ) from e
        except Exception as e:
            raise ValueError(f"f-string 格式化失败: {e}") from e

    def format_safe(self, **kwargs: Any) -> str:
        """
        宽松模式渲染模板（“尽量渲染成功”，不会因为缺变量直接抛异常）。

        行为：
            1. 合并 partial 预绑定参数与本次调用参数；
            2. 尝试解析模板中实际出现的变量名；
            3. 对未提供的变量填充合理的默认值：
                - 列表相关变量：history/messages/items/tools/examples -> []
                - 上下文/字符串类变量：system/context/prefix/suffix -> None
                - 其它变量："<MISSING: var_name>" 字样；
            4. 若最终渲染仍然失败，记录日志并返回
               "<TEMPLATE ERROR: ...>" 字符串，避免异常向外冒泡。

        适用场景：
            - 调试；
            - 容忍部分信息缺失的场景（例如日志/监控生成 Prompt）。
        """
        safe_kwargs: Dict[str, Any] = {**self._partial_kwargs, **kwargs}

        try:
            all_vars = self.get_variables_from_template()
        except Exception as e:
            logger.warning("Failed to extract variables", error=str(e))
            all_vars = set(self.input_variables)

        # 填充默认值
        for var in all_vars:
            if var not in safe_kwargs:
                if var in ("history", "messages", "items", "tools", "examples"):
                    safe_kwargs[var] = []
                elif var in ("system", "context", "prefix", "suffix"):
                    safe_kwargs[var] = None
                else:
                    safe_kwargs[var] = f"<MISSING: {var}>"

        try:
            if self.template_format == "jinja2":
                return self._format_jinja2(**safe_kwargs)
            elif self.template_format == "f-string":
                return self._format_fstring(**safe_kwargs)
            else:
                return "<TEMPLATE ERROR: 不支持的格式>"
        except Exception as e:
            logger.error("Safe format failed", error=str(e))
            return f"<TEMPLATE ERROR: {e}>"

    # ===== 变量提取相关方法 =====

    def get_variables_from_template(self) -> Set[str]:
        """
        从模板中提取所有「在模板语法中被引用」的变量名。

        - 对于 Jinja2 模板：使用 jinja2.meta.find_undeclared_variables；
        - 对于 f-string（str.format）模板：使用正则匹配 {var_name} 形式。

        注意：
            - 返回结果是模板层面实际用到的变量集合，
              不一定与 input_variables 完全一致（可以超集）。
        """
        if self._variables_cache is not None:
            return set(self._variables_cache)

        if self.template_format == "jinja2":
            variables = self._extract_jinja2_variables()
        elif self.template_format == "f-string":
            variables = self._extract_fstring_variables()
        else:
            variables = set()

        self._variables_cache = set(variables)
        return variables

    def _extract_jinja2_variables(self) -> Set[str]:
        """
        从 Jinja2 模板中提取变量名集合。

        通过：
            - env.parse(template) 得到 AST；
            - meta.find_undeclared_variables(ast) 获取「未在局部声明的标识符」集合。
        """
        try:
            env = get_jinja2_env()
            from jinja2 import meta

            ast = env.parse(self.template)
            variables = meta.find_undeclared_variables(ast)
            return set(variables)
        except Exception as e:
            logger.warning("Failed to extract Jinja2 variables", error=str(e))
            return set()

    def _extract_fstring_variables(self) -> Set[str]:
        """
        从 "f-string"（str.format）模板中提取变量名。

        当前实现较为简单：
            - 使用正则 \\{(\\w+)\\} 匹配 {var_name}；
            - 不支持复杂表达式（如 {user.name}、{value:.2f} 等）。

        如有更复杂需求，可替换为 string.Formatter().parse()。
        """
        pattern = r"\{(\w+)\}"
        matches = re.findall(pattern, self.template)
        return set(matches)

    # ===== 模板操作：partial / clone =====

    def partial(self, **kwargs: Any) -> "PromptTemplate":
        """
        部分填充变量（部分应用），返回一个新的 PromptTemplate 实例。

        实现方式：
            - 不立即渲染模板字符串；
            - 将传入的变量保存到 _partial_kwargs 中；
            - 在新的实例上更新 input_variables（去掉已绑定的变量）；
            - 后续调用 new_tpl.format(...) 时，会自动合并该 partial 参数。

        示例：
            ```python
            tpl = PromptTemplate(
                template="Hello {{ name }}, you are {{ age }}",
                input_variables=["name", "age"],
            )
            p = tpl.partial(name="Alice")
            p.format(age=25)  # -> "Hello Alice, you are 25"
            ```
        """
        remaining_vars = [v for v in self.input_variables if v not in kwargs]

        new_tpl = self.clone()
        new_tpl._partial_kwargs = {**self._partial_kwargs, **kwargs}
        new_tpl.input_variables = remaining_vars
        return new_tpl

    def clone(self) -> "PromptTemplate":
        """
        克隆当前 PromptTemplate 实例。

        - template 原样复制；
        - input_variables 复制一份列表；
        - _partial_kwargs 复制一份字典；
        - _compiled_template 直接复用（懒加载缓存）；
        - _variables_cache 复制或置为 None。

        用于：
            - 需要在不影响原实例的前提下做局部修改；
            - 例如不同场景对同一模板做 slight customization。
        """
        new_tpl = PromptTemplate(
            template=self.template,
            input_variables=self.input_variables.copy(),
            template_format=self.template_format,
            validate_template=False,  # 克隆时无需再验证语法
        )
        new_tpl._partial_kwargs = dict(self._partial_kwargs)
        new_tpl._compiled_template = self._compiled_template
        new_tpl._variables_cache = (
            set(self._variables_cache) if self._variables_cache is not None else None
        )
        return new_tpl

    # ===== 工厂方法：from_file / from_examples =====

    @classmethod
    def from_file(
        cls,
        path: str,
        input_variables: Optional[List[str]] = None,
        encoding: str = "utf-8",
    ) -> "PromptTemplate":
        """
        从文件加载模板。

        参数:
            path:
                模板文件路径。
            input_variables:
                若提供，则直接使用；若为 None，则在创建后自动从模板中提取变量；
            encoding:
                文件编码，默认 "utf-8"。

        返回:
            PromptTemplate 实例。

        说明：
            - 自动提取变量依赖 get_variables_from_template()；
            - 会将变量名排序后写回 input_variables。
        """
        from pathlib import Path

        file_path = Path(path)
        if not file_path.exists():
            raise FileNotFoundError(f"模板文件不存在: {path}")

        try:
            with open(file_path, "r", encoding=encoding) as f:
                template_str = f.read()
        except Exception as e:
            raise IOError(f"读取模板文件失败: {e}") from e

        prompt = cls(
            template=template_str,
            input_variables=input_variables or [],
        )

        if not input_variables:
            detected_vars = prompt.get_variables_from_template()
            prompt.input_variables = sorted(detected_vars)
            logger.info(
                "Auto-detected template variables",
                path=path,
                variables=prompt.input_variables,
            )

        return prompt

    @classmethod
    def from_examples(
        cls,
        examples: List[Dict[str, str]],
        template: str = "{{ input }}\n{{ output }}\n",
        separator: str = "\n---\n",
    ) -> "PromptTemplate":
        """
        从示例列表构造 few-shot 模板。

        参数:
            examples:
                示例列表，形如 [{"input": "...", "output": "..."}, ...]；
            template:
                单个示例的模板字符串，默认 "{{ input }}\\n{{ output }}\\n"；
            separator:
                示例之间的分隔符，默认 "\\n---\\n"。

        返回:
            PromptTemplate 实例，其 template 为所有示例拼接后的字符串。
        """
        example_template = cls(template=template, input_variables=[])

        rendered_examples: List[str] = []
        for ex in examples:
            rendered = example_template.format_safe(**ex)
            rendered_examples.append(rendered)

        full_template = separator.join(rendered_examples)

        return cls(
            template=full_template,
            input_variables=[],
        )

    # ===== 字符串表示 =====

    def __str__(self) -> str:
        """
        简洁字符串表示：显示模板前 50 字符 + 变量列表。

        便于在日志中快速查看。
        """
        preview = self.template[:50].replace("\n", "\\n")
        if len(self.template) > 50:
            preview += "..."
        return f"PromptTemplate('{preview}', vars={self.input_variables})"

    def __repr__(self) -> str:
        """
        详细字符串表示：适合调试与日志。

        包含：
            - 模板长度；
            - input_variables；
            - template_format。
        """
        return (
            "PromptTemplate("
            f"template_length={len(self.template)}, "
            f"input_variables={self.input_variables}, "
            f"format={self.template_format}"
            ")"
        )


__all__ = ["PromptTemplate"]
```

[50] gecko/core/prompt/validators.py
```python
# gecko/core/prompt/validators.py
"""
Prompt 验证 / Lint 模块

职责：
- 针对 PromptTemplate 提供一套基础的“静态检查”能力；
- 帮助发现潜在问题，如：
    - 模板中使用了但未声明的变量；
    - 声明了但实际上未使用的变量；
    - 使用了不在允许集合中的变量；
    - Prompt 过长；
    - 包含一些“反模式”词句（如 As an AI language model...）。

设计目标：
- 提供可扩展的 PromptValidator 类：
    - 可配置最大长度、禁用短语、开启/关闭不同规则；
    - validate() 返回结构化的 PromptIssue 列表；
- 提供顶层 lint_prompt() 便捷函数，方便快速使用。
"""

from __future__ import annotations

from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, Iterable, List, Optional, Set

from gecko.core.logging import get_logger
from gecko.core.prompt.template import PromptTemplate

logger = get_logger(__name__)


class IssueSeverity(str, Enum):
    """
    诊断问题的严重程度枚举。

    分级：
        - INFO    : 信息提示，一般不需要立即处理；
        - WARNING : 警告，建议关注并优化；
        - ERROR   : 错误，可能导致运行时异常或明显的不符合预期。
    """

    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"


@dataclass
class PromptIssue:
    """
    单条 Prompt 验证 / Lint 结果。

    字段说明：
        code:
            规则编码，用于程序化处理，如 "UNDECLARED_VAR" / "PROMPT_TOO_LONG"。
        message:
            人类可读的错误/警告描述。
        severity:
            严重等级（INFO / WARNING / ERROR）。
        hint:
            可选的修复建议或提示说明。
        extra:
            额外的上下文信息，如变量名列表、长度等，方便日志与调试。
    """

    code: str
    message: str
    severity: IssueSeverity
    hint: Optional[str] = None
    extra: Dict[str, Any] = None # type: ignore


class PromptValidator:
    """
    Prompt 验证器（Validator）

    通过组合多条简单规则，对 PromptTemplate 进行静态检查。

    可配置项：
        max_length:
            PromptTemplate.template 允许的最大字符数，超过则给出 WARNING 或 ERROR；
        length_severity:
            长度超限的严重等级（默认 WARNING）；
        banned_phrases:
            禁用短语列表，例如 ["As an AI language model"]；
        enabled_rules:
            启用的规则集合（为空则全部启用）。

    当前内置规则：
        - UNDECLARED_VAR:
            模板中使用的变量未在 input_variables 声明；
        - UNUSED_INPUT_VAR:
            input_variables 中声明的变量在模板中未使用；
        - UNKNOWN_VAR:
            模板中使用的变量不在 allowed_variables 集合中（若提供）；
        - PROMPT_TOO_LONG:
            模板长度超过 max_length；
        - BANNED_PHRASE:
            模板中包含 banned_phrases 中的短语。
    """

    # 规则编码常量，便于外部统一引用
    RULE_UNDECLARED_VAR = "UNDECLARED_VAR"
    RULE_UNUSED_INPUT_VAR = "UNUSED_INPUT_VAR"
    RULE_UNKNOWN_VAR = "UNKNOWN_VAR"
    RULE_PROMPT_TOO_LONG = "PROMPT_TOO_LONG"
    RULE_BANNED_PHRASE = "BANNED_PHRASE"

    def __init__(
        self,
        max_length: int = 4000,
        length_severity: IssueSeverity = IssueSeverity.WARNING,
        banned_phrases: Optional[Iterable[str]] = None,
        enabled_rules: Optional[Set[str]] = None,
    ) -> None:
        self.max_length = max_length
        self.length_severity = length_severity
        self.banned_phrases: List[str] = list(banned_phrases or [])
        # enabled_rules = None 表示启用所有规则
        self.enabled_rules = enabled_rules

    # ========== 对外入口 ==========

    def validate(
        self,
        template: PromptTemplate,
        allowed_variables: Optional[Set[str]] = None,
    ) -> List[PromptIssue]:
        """
        对给定模板执行所有启用的规则检查，返回问题列表。

        参数：
            template:
                要检查的 PromptTemplate；
            allowed_variables:
                若提供，表示「允许出现的变量名」全集，
                多用于检查“模板是否越权访问了不该用的变量”。

        返回：
            PromptIssue 列表；若为空列表，表示未发现问题。
        """
        issues: List[PromptIssue] = []

        used_vars = template.get_variables_from_template()
        declared_vars = set(template.input_variables)
        allowed = allowed_variables or set()

        # 规则 1：使用了但未声明的变量
        if self._is_rule_enabled(self.RULE_UNDECLARED_VAR):
            issues.extend(
                self._check_undeclared_variables(
                    used_vars=used_vars, declared_vars=declared_vars
                )
            )

        # 规则 2：声明了但未使用的变量
        if self._is_rule_enabled(self.RULE_UNUSED_INPUT_VAR):
            issues.extend(
                self._check_unused_input_variables(
                    used_vars=used_vars, declared_vars=declared_vars
                )
            )

        # 规则 3：变量不在 allowed_variables 集合中
        if allowed and self._is_rule_enabled(self.RULE_UNKNOWN_VAR):
            issues.extend(
                self._check_unknown_variables(
                    used_vars=used_vars, allowed_vars=allowed
                )
            )

        # 规则 4：Prompt 过长
        if self._is_rule_enabled(self.RULE_PROMPT_TOO_LONG):
            issues.extend(self._check_prompt_length(template))

        # 规则 5：包含禁用短语
        if self.banned_phrases and self._is_rule_enabled(self.RULE_BANNED_PHRASE):
            issues.extend(self._check_banned_phrases(template))

        return issues

    # ========== 单条规则实现 ==========

    def _is_rule_enabled(self, rule_code: str) -> bool:
        """
        判断某条规则是否启用。

        enabled_rules 为 None 表示全部启用。
        """
        if self.enabled_rules is None:
            return True
        return rule_code in self.enabled_rules

    def _check_undeclared_variables(
        self,
        used_vars: Set[str],
        declared_vars: Set[str],
    ) -> List[PromptIssue]:
        """
        检查：模板中使用的变量未在 input_variables 中声明。
        """
        undeclared = used_vars - declared_vars
        if not undeclared:
            return []

        return [
            PromptIssue(
                code=self.RULE_UNDECLARED_VAR,
                message=(
                    f"模板中使用了未声明的变量: {', '.join(sorted(undeclared))}。"
                ),
                severity=IssueSeverity.WARNING,
                hint="考虑将这些变量加入 input_variables，或修改模板不再使用它们。",
                extra={
                    "used_vars": sorted(used_vars),
                    "declared_vars": sorted(declared_vars),
                    "undeclared_vars": sorted(undeclared),
                },
            )
        ]

    def _check_unused_input_variables(
        self,
        used_vars: Set[str],
        declared_vars: Set[str],
    ) -> List[PromptIssue]:
        """
        检查：input_variables 中声明的变量在模板中未使用。
        """
        unused = declared_vars - used_vars
        if not unused:
            return []

        return [
            PromptIssue(
                code=self.RULE_UNUSED_INPUT_VAR,
                message=(
                    f"以下 input_variables 没有在模板中使用: {', '.join(sorted(unused))}。"
                ),
                severity=IssueSeverity.INFO,
                hint="可以考虑移除这些未使用变量，以简化调用方接口。",
                extra={
                    "used_vars": sorted(used_vars),
                    "declared_vars": sorted(declared_vars),
                    "unused_vars": sorted(unused),
                },
            )
        ]

    def _check_unknown_variables(
        self,
        used_vars: Set[str],
        allowed_vars: Set[str],
    ) -> List[PromptIssue]:
        """
        检查：模板变量不在 allowed_variables 集合中。

        用于场景：
            - 限制模板只能使用某些“白名单变量”；
            - 防止模板越权访问敏感上下文数据。
        """
        unknown = used_vars - allowed_vars
        if not unknown:
            return []

        return [
            PromptIssue(
                code=self.RULE_UNKNOWN_VAR,
                message=(
                    f"模板中使用了不在允许列表中的变量: {', '.join(sorted(unknown))}。"
                ),
                severity=IssueSeverity.WARNING,
                hint="请检查这些变量是否应该出现在模板中，或更新 allowed_variables。",
                extra={
                    "used_vars": sorted(used_vars),
                    "allowed_vars": sorted(allowed_vars),
                    "unknown_vars": sorted(unknown),
                },
            )
        ]

    def _check_prompt_length(self, template: PromptTemplate) -> List[PromptIssue]:
        """
        检查：模板字符串长度是否超过 max_length。
        """
        length = len(template.template)
        if length <= self.max_length:
            return []

        return [
            PromptIssue(
                code=self.RULE_PROMPT_TOO_LONG,
                message=(
                    f"模板长度为 {length} 字符，超过了配置的上限 {self.max_length}。"
                ),
                severity=self.length_severity,
                hint="建议拆分为多个模块化模板，或进行内容精简，以减少 token 消耗。",
                extra={
                    "length": length,
                    "max_length": self.max_length,
                },
            )
        ]

    def _check_banned_phrases(self, template: PromptTemplate) -> List[PromptIssue]:
        """
        检查：模板中是否包含禁用短语。
        """
        content = template.template
        hits = [p for p in self.banned_phrases if p in content]
        if not hits:
            return []

        return [
            PromptIssue(
                code=self.RULE_BANNED_PHRASE,
                message=(
                    f"模板中包含以下不建议出现的短语: {', '.join(hits)}。"
                ),
                severity=IssueSeverity.WARNING,
                hint="考虑改写这些内容，以避免模型重复套话或暴露实现细节。",
                extra={
                    "banned_phrases": self.banned_phrases,
                    "hits": hits,
                },
            )
        ]


# ========== 便捷函数 ==========

def lint_prompt(
    template: PromptTemplate,
    allowed_variables: Optional[Set[str]] = None,
    validator: Optional[PromptValidator] = None,
) -> List[PromptIssue]:
    """
    便捷函数：对模板执行一次 Lint，返回问题列表。

    参数：
        template:
            待检查的 PromptTemplate；
        allowed_variables:
            可选的变量白名单；
        validator:
            若希望复用已有 PromptValidator 实例，则传入；
            若为 None，则使用默认配置创建一个临时实例。

    用法示例：
        ```python
        from gecko.core.prompt import PromptTemplate, lint_prompt

        tpl = PromptTemplate(
            template="Hello {{ name }}!",
            input_variables=[],
        )
        issues = lint_prompt(tpl)
        for issue in issues:
            print(issue.code, issue.severity, issue.message)
        ```
    """
    if validator is None:
        validator = PromptValidator(
            banned_phrases=["As an AI language model"],  # 示例默认配置
        )
    return validator.validate(template, allowed_variables=allowed_variables)


__all__ = ["IssueSeverity", "PromptIssue", "PromptValidator", "lint_prompt"]
```

[51] gecko/core/protocols/__init__.py
```python
"""
协议模块入口
重新导出所有子模块内容，确保外部引用 gecko.core.protocols.ModelProtocol 依然有效。
"""
from gecko.core.protocols.base import check_protocol, get_missing_methods
from gecko.core.protocols.model import (
    ModelProtocol, StreamableModelProtocol, 
    CompletionResponse, CompletionChoice, CompletionUsage, StreamChunk,
    supports_streaming, supports_function_calling, supports_vision,
    get_model_name, validate_model
)
from gecko.core.protocols.storage import StorageProtocol, validate_storage
from gecko.core.protocols.tool import ToolProtocol, validate_tool
from gecko.core.protocols.embedder import EmbedderProtocol
from gecko.core.protocols.runnable import RunnableProtocol, StreamableRunnableProtocol
from gecko.core.protocols.vector import VectorStoreProtocol

# 类型别名 (保持兼容)
from typing import Dict, List, Any
ModelResponse = CompletionResponse
MessageDict = Dict[str, Any]
MessageList = List[MessageDict]
ToolCall = Dict[str, Any]
ToolCallList = List[ToolCall]
StorageValue = Dict[str, Any]
Vector = List[float]
VectorList = List[Vector]

__all__ = [
    "ModelProtocol", "StreamableModelProtocol",
    "CompletionResponse", "CompletionChoice", "CompletionUsage", "StreamChunk",
    "StorageProtocol", "ToolProtocol", "EmbedderProtocol",
    "RunnableProtocol", "StreamableRunnableProtocol", "VectorStoreProtocol",
    "check_protocol", "validate_model", "validate_storage", "validate_tool",
    "supports_streaming", "supports_function_calling", "supports_vision",
    "get_model_name", "get_missing_methods",
    "ModelResponse", "MessageDict", "ToolCall", "Vector"
]
```

[52] gecko/core/protocols/base.py
```python
"""
协议基础工具
提供运行时检查和验证辅助函数。
"""
from __future__ import annotations
import inspect as insp
from typing import Any, List

def check_protocol(obj: Any, protocol: type) -> bool:
    """
    检查对象是否实现了指定协议
    """
    return isinstance(obj, protocol)

def get_missing_methods(obj: Any, protocol: type) -> List[str]:
    """
    获取对象未实现的协议方法
    """
    missing = []
    # 获取协议的所有成员
    for name, value in insp.getmembers(protocol):
        if name.startswith("_"): continue
        
        if insp.isfunction(value) or insp.ismethod(value):
            if not hasattr(obj, name):
                missing.append(name)
            elif not callable(getattr(obj, name)):
                missing.append(name)
        elif insp.isdatadescriptor(value):
            if not hasattr(obj, name):
                missing.append(name)
    return missing
```

[53] gecko/core/protocols/embedder.py
```python
"""嵌入模型协议"""
from __future__ import annotations
from typing import List, Protocol, runtime_checkable

@runtime_checkable
class EmbedderProtocol(Protocol):
    async def embed(self, texts: List[str]) -> List[List[float]]: ...
    def get_dimension(self) -> int: ...
```

[54] gecko/core/protocols/model.py
```python
"""
LLM 模型相关协议与数据结构
"""
from __future__ import annotations
from typing import Any, Dict, List, Optional, Protocol, AsyncIterator, runtime_checkable
from pydantic import BaseModel, Field
from gecko.core.protocols.base import check_protocol, get_missing_methods

# ====================== 模型响应格式 ======================

class CompletionChoice(BaseModel):
    """单个补全选择"""
    index: int = 0
    message: Dict[str, Any]
    finish_reason: Optional[str] = None
    logprobs: Optional[Dict[str, Any]] = None

class CompletionUsage(BaseModel):
    """Token 使用统计"""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class CompletionResponse(BaseModel):
    """标准的模型补全响应格式"""
    id: str = Field(default="", description="响应 ID")
    object: str = Field(default="chat.completion", description="对象类型")
    created: int = Field(default=0, description="创建时间戳")
    model: str = Field(default="", description="模型名称")
    choices: List[CompletionChoice] = Field(default_factory=list)
    usage: Optional[CompletionUsage] = Field(default=None)
    system_fingerprint: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)

class StreamChunk(BaseModel):
    """流式响应的单个数据块"""
    id: str = ""
    object: str = "chat.completion.chunk"
    created: int = 0
    model: str = ""
    choices: List[Dict[str, Any]] = Field(default_factory=list)
    
    @property
    def delta(self) -> Dict[str, Any]:
        if self.choices:
            return self.choices[0].get("delta", {})
        return {}
    
    @property
    def content(self) -> Optional[str]:
        return self.delta.get("content")

# ====================== 模型协议 ======================

@runtime_checkable
class ModelProtocol(Protocol):
    """LLM 模型核心协议"""
    async def acompletion(self, messages: List[Dict[str, Any]], **kwargs) -> CompletionResponse:
        ...

    # [新增] 同步 Token 计数接口
    # 允许上层模块（如 Memory）在不阻塞 Event Loop 的前提下获取 Token 数
    def count_tokens(self, text_or_messages: str | List[Dict[str, Any]]) -> int:
        """
        计算输入内容的 Token 数量。
        应尽量使用本地 Tokenizer (如 tiktoken) 以保证性能。
        """
        ...

@runtime_checkable
class StreamableModelProtocol(ModelProtocol, Protocol):
    """支持流式输出的模型协议"""
    async def astream(self, messages: List[Dict[str, Any]], **kwargs) -> AsyncIterator[StreamChunk]:
        ...
        yield # type: ignore

# ====================== 工具函数 ======================

def supports_streaming(model: Any) -> bool:
    return isinstance(model, StreamableModelProtocol)

def supports_function_calling(model: Any) -> bool:
    if hasattr(model, "_supports_function_calling"):
        return model._supports_function_calling
    if hasattr(model, "supports_function_calling"):
        method = getattr(model, "supports_function_calling")
        if callable(method):
            return method() # type: ignore
    return False

def supports_vision(model: Any) -> bool:
    if hasattr(model, "_supports_vision"):
        return model._supports_vision
    if hasattr(model, "supports_vision"):
        method = getattr(model, "supports_vision")
        if callable(method):
            return method() # type: ignore
    return False

def get_model_name(model: Any) -> str:
    if hasattr(model, "model_name"): return model.model_name
    if hasattr(model, "name"): return model.name
    return model.__class__.__name__

def validate_model(model: Any) -> None:
    """
    验证模型是否满足 ModelProtocol 所需的方法/属性（鸭子类型检查）

    不再依赖 isinstance(model, ModelProtocol)，而是基于 get_missing_methods，
    这样自定义模型只要实现必要方法即可通过验证。
    """
    missing = get_missing_methods(model, ModelProtocol)
    if missing:
        raise TypeError(
            "Model does not implement ModelProtocol. "
            f"Missing methods: {', '.join(missing)}"
        )
```

[55] gecko/core/protocols/runnable.py
```python
"""可运行对象协议"""
from __future__ import annotations
from typing import Any, AsyncIterator, Protocol, runtime_checkable

@runtime_checkable
class RunnableProtocol(Protocol):
    async def run(self, input: Any) -> Any: ...

@runtime_checkable
class StreamableRunnableProtocol(RunnableProtocol, Protocol):
    async def stream(self, input: Any) -> AsyncIterator[str]:
        ...
        yield # type: ignore
```

[56] gecko/core/protocols/storage.py
```python
# gecko/core/protocols/storage.py
"""存储相关协议"""
from __future__ import annotations
from typing import Any, Dict, Optional, Protocol, runtime_checkable
from .base import get_missing_methods

@runtime_checkable
class StorageProtocol(Protocol):
    """存储后端协议"""
    async def get(self, key: str) -> Optional[Dict[str, Any]]: ...
    async def set(self, key: str, value: Dict[str, Any], ttl: Optional[int] = None) -> None: ...
    async def delete(self, key: str) -> bool: ...

def validate_storage(storage: Any) -> None:
    """
    验证存储是否满足 StorageProtocol 所需的方法/属性（鸭子类型检查）
    """
    missing = get_missing_methods(storage, StorageProtocol)
    if missing:
        raise TypeError(
            "Storage does not implement StorageProtocol. "
            f"Missing methods: {', '.join(missing)}"
        )
```

[57] gecko/core/protocols/tool.py
```python
# gecko/core/protocols/tool.py
"""工具相关协议"""
from __future__ import annotations
from typing import Any, Dict, Protocol, runtime_checkable
from gecko.core.protocols.base import get_missing_methods

@runtime_checkable
class ToolProtocol(Protocol):
    """工具协议"""
    name: str
    description: str
    parameters: Dict[str, Any]
    async def execute(self, arguments: Dict[str, Any]) -> str: ...

def validate_tool(tool: Any) -> None:
    """
    验证工具是否符合 ToolProtocol
    
    修复点：
    1. 补全 description 和 parameters 的类型/内容检查。
    2. 调整错误提示语序，以匹配单元测试的正则断言 (e.g. "non-empty 'name'").
    """
    # 1. 属性存在性检查
    for attr in ["name", "description", "parameters"]:
        if not hasattr(tool, attr):
            # 测试要求：ValueError(match="'name'") 或 ("Tool must have a 'name' attribute")
            raise ValueError(f"Tool must have a '{attr}' attribute")
    
    # 2. Name 内容检查
    # 测试要求正则: "non-empty 'name'"
    if not isinstance(tool.name, str) or not tool.name.strip():
        raise ValueError("Tool must have a non-empty 'name' attribute")
        
    # 3. Description 内容检查 (之前漏掉了)
    # 测试要求正则: "non-empty 'description'"
    if not isinstance(tool.description, str) or not tool.description.strip():
        raise ValueError("Tool must have a non-empty 'description' attribute")

    # 4. Parameters 类型检查 (之前漏掉了)
    # 测试要求正则: "'parameters'"
    if not isinstance(tool.parameters, dict):
        raise ValueError("Tool must have a 'parameters' dict attribute")

    # 5. 方法检查
    # 测试要求正则: "execute"
    if not hasattr(tool, "execute"):
        raise TypeError("Tool must have an 'execute' method")
    
    if not callable(getattr(tool, "execute")):
        raise TypeError("Tool 'execute' must be callable")
```

[58] gecko/core/protocols/vector.py
```python
"""向量存储协议"""
from __future__ import annotations
from typing import Any, Dict, List, Optional, Protocol, runtime_checkable

@runtime_checkable
class VectorStoreProtocol(Protocol):
    async def add(self, ids: List[str], vectors: List[List[float]], metadata: Optional[List[Dict[str, Any]]] = None) -> None: ...
    async def search(self, query_vector: List[float], top_k: int = 5, filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]: ...
    async def delete(self, ids: List[str]) -> None: ...
```

[59] gecko/core/resilience.py
```python
# gecko/core/resilience.py
"""
系统韧性模块 (Resilience)

核心功能：
- CircuitBreaker: 熔断器，防止故障级联，保护系统资源。
"""
from __future__ import annotations

import asyncio
import time
from enum import Enum
from typing import Any, Callable, Tuple, Type, TypeVar

from gecko.core.exceptions import GeckoError, ModelError
from gecko.core.logging import get_logger

logger = get_logger(__name__)

T = TypeVar("T")


class CircuitState(str, Enum):
    CLOSED = "closed"       # 正常状态：允许请求通过
    OPEN = "open"           # 熔断状态：拒绝请求，快速失败
    HALF_OPEN = "half_open" # 半开状态：允许少量请求尝试恢复


class CircuitOpenError(GeckoError):
    """熔断器开启异常，表示服务暂时不可用"""
    pass


class CircuitBreaker:
    """
    熔断器实现 (线程安全/协程安全)
    
    配置参数:
        failure_threshold: 触发熔断的连续失败次数
        recovery_timeout: 熔断后进入半开状态的冷却时间(秒)
        monitor_exceptions: 触发计数增加的异常类型元组
    """

    def __init__(
        self,
        failure_threshold: int = 5,
        recovery_timeout: float = 30.0,
        monitor_exceptions: Tuple[Type[Exception], ...] = (ModelError,)
    ):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.monitor_exceptions = monitor_exceptions

        self._state = CircuitState.CLOSED
        self._failure_count = 0
        self._last_failure_time = 0.0
        self._lock = asyncio.Lock()  # 保护状态变更的互斥锁

    async def call(self, func: Callable[..., Any], *args, **kwargs) -> Any:
        """
        通过熔断器执行函数
        
        Args:
            func: 目标异步函数
            *args, **kwargs: 传递给函数的参数
        """
        # 1. 状态检查 (加锁读取以保证一致性)
        async with self._lock:
            if self._state == CircuitState.OPEN:
                # 检查是否过了冷却期
                if time.time() - self._last_failure_time > self.recovery_timeout:
                    self._state = CircuitState.HALF_OPEN
                    logger.info("CircuitBreaker entering HALF_OPEN state")
                else:
                    raise CircuitOpenError("Service suspended due to high failure rate")

        try:
            # 2. 执行实际逻辑
            result = await func(*args, **kwargs)

            # 3. 成功后逻辑
            if self._state == CircuitState.HALF_OPEN:
                # 半开状态下成功，立即关闭熔断器（恢复正常）
                async with self._lock:
                    self._reset()
            elif self._failure_count > 0:
                # [性能优化] 只有在 CLOSED 状态且有失败计数时才获取锁进行重置
                # 避免每次成功调用都请求锁，提升高并发下的吞吐量
                async with self._lock:
                    if self._failure_count > 0: # Double check
                        self._failure_count = 0
            
            return result

        except self.monitor_exceptions as e:
            # 4. 捕获监控范围内的异常，记录失败
            await self._record_failure()
            raise e

    async def _record_failure(self):
        """记录失败并判断是否熔断"""
        async with self._lock:
            self._failure_count += 1
            self._last_failure_time = time.time()

            # 如果在半开状态失败，或者达到阈值，立即熔断
            if self._state == CircuitState.HALF_OPEN or self._failure_count >= self.failure_threshold:
                if self._state != CircuitState.OPEN:
                    self._state = CircuitState.OPEN
                    logger.error(
                        f"CircuitBreaker OPENED. Failures: {self._failure_count}/{self.failure_threshold}"
                    )

    def _reset(self):
        """重置熔断器状态"""
        self._state = CircuitState.CLOSED
        self._failure_count = 0
        logger.info("CircuitBreaker CLOSED (Service Recovered)")
```

[60] gecko/core/retry.py
```python
# gecko/core/retry.py
"""
统一重试策略模块

提供可配置的重试机制，支持：
- 多种退避策略
- 熔断器模式
- 异常过滤
- 执行回调
"""
from __future__ import annotations

import asyncio
import functools
import random
import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Awaitable, Callable, Optional, Tuple, Type, TypeVar, Union

from gecko.core.logging import get_logger

logger = get_logger(__name__)

T = TypeVar("T")


class BackoffStrategy(str, Enum):
    """退避策略"""
    CONSTANT = "constant"
    LINEAR = "linear"
    EXPONENTIAL = "exponential"


@dataclass
class RetryConfig:
    """
    重试配置
    
    示例:
        ```python
        config = RetryConfig(
            max_attempts=5,
            initial_delay=1.0,
            max_delay=30.0,
            backoff=BackoffStrategy.EXPONENTIAL,
            retryable=(ConnectionError, TimeoutError),
        )
        ```
    """
    max_attempts: int = 3
    initial_delay: float = 1.0
    max_delay: float = 60.0
    backoff: BackoffStrategy = BackoffStrategy.EXPONENTIAL
    multiplier: float = 2.0
    jitter: float = 0.1
    retryable: Tuple[Type[Exception], ...] = (Exception,)
    non_retryable: Tuple[Type[Exception], ...] = ()

    def calculate_delay(self, attempt: int) -> float:
        """计算第 N 次重试的延迟"""
        if attempt <= 0:
            return 0.0

        if self.backoff == BackoffStrategy.CONSTANT:
            base = self.initial_delay
        elif self.backoff == BackoffStrategy.LINEAR:
            base = self.initial_delay * attempt
        else:  # EXPONENTIAL
            base = self.initial_delay * (self.multiplier ** (attempt - 1))

        delay = min(base, self.max_delay)

        if self.jitter > 0:
            jitter_amount = delay * self.jitter
            delay += random.uniform(-jitter_amount, jitter_amount)
            delay = max(0.0, delay)

        return delay

    def should_retry(self, exception: Exception) -> bool:
        """判断是否应重试"""
        if isinstance(exception, self.non_retryable):
            return False
        return isinstance(exception, self.retryable)


@dataclass
class RetryState:
    """重试状态追踪"""
    attempt: int = 0
    total_delay: float = 0.0
    last_exception: Optional[Exception] = None
    started_at: float = field(default_factory=time.time)

    @property
    def elapsed(self) -> float:
        return time.time() - self.started_at


class Retrier:
    """
    重试执行器
    
    示例:
        ```python
        retrier = Retrier(RetryConfig(max_attempts=5))
        
        # 异步使用
        result = await retrier.run_async(unstable_function, arg1, arg2)
        
        # 同步使用
        result = retrier.run_sync(sync_function, arg1)
        ```
    """

    def __init__(
        self,
        config: Optional[RetryConfig] = None,
        on_retry: Optional[Callable[[int, Exception, float], None]] = None,
        on_success: Optional[Callable[[int], None]] = None,
        on_failure: Optional[Callable[[int, Exception], None]] = None,
    ):
        self.config = config or RetryConfig()
        self.on_retry = on_retry
        self.on_success = on_success
        self.on_failure = on_failure

    async def run_async(
        self,
        func: Callable[..., Awaitable[T]],
        *args: Any,
        **kwargs: Any
    ) -> T:
        """执行异步函数（带重试）"""
        state = RetryState()

        for attempt in range(1, self.config.max_attempts + 1):
            state.attempt = attempt

            try:
                result = await func(*args, **kwargs)
                self._call_success(attempt)
                return result

            except Exception as e:
                state.last_exception = e

                if not self.config.should_retry(e):
                    logger.debug(f"Exception not retryable: {type(e).__name__}")
                    raise

                if attempt >= self.config.max_attempts:
                    self._call_failure(attempt, e)
                    raise

                delay = self.config.calculate_delay(attempt)
                state.total_delay += delay

                self._call_retry(attempt, e, delay)
                logger.warning(
                    f"Retry {attempt}/{self.config.max_attempts} "
                    f"after {delay:.2f}s: {e}"
                )

                await asyncio.sleep(delay)

        # 不应到达此处
        raise state.last_exception  # type: ignore

    def run_sync(
        self,
        func: Callable[..., T],
        *args: Any,
        **kwargs: Any
    ) -> T:
        """执行同步函数（带重试）"""
        state = RetryState()

        for attempt in range(1, self.config.max_attempts + 1):
            state.attempt = attempt

            try:
                result = func(*args, **kwargs)
                self._call_success(attempt)
                return result

            except Exception as e:
                state.last_exception = e

                if not self.config.should_retry(e):
                    raise

                if attempt >= self.config.max_attempts:
                    self._call_failure(attempt, e)
                    raise

                delay = self.config.calculate_delay(attempt)
                state.total_delay += delay

                self._call_retry(attempt, e, delay)
                logger.warning(
                    f"Retry {attempt}/{self.config.max_attempts} "
                    f"after {delay:.2f}s: {e}"
                )

                time.sleep(delay)

        raise state.last_exception  # type: ignore

    def _call_retry(self, attempt: int, exc: Exception, delay: float) -> None:
        if self.on_retry:
            try:
                self.on_retry(attempt, exc, delay)
            except Exception as e:
                logger.warning(f"on_retry callback failed: {e}")

    def _call_success(self, attempt: int) -> None:
        if self.on_success:
            try:
                self.on_success(attempt)
            except Exception as e:
                logger.warning(f"on_success callback failed: {e}")

    def _call_failure(self, attempt: int, exc: Exception) -> None:
        if self.on_failure:
            try:
                self.on_failure(attempt, exc)
            except Exception as e:
                logger.warning(f"on_failure callback failed: {e}")


def retry(
    max_attempts: int = 3,
    initial_delay: float = 1.0,
    max_delay: float = 60.0,
    backoff: BackoffStrategy = BackoffStrategy.EXPONENTIAL,
    retryable: Tuple[Type[Exception], ...] = (Exception,),
    **config_kwargs
) -> Callable:
    """
    重试装饰器
    
    示例:
        ```python
        @retry(max_attempts=5, initial_delay=2.0)
        async def unstable_api_call():
            ...
        
        @retry(retryable=(ConnectionError,))
        def sync_operation():
            ...
        ```
    """
    config = RetryConfig(
        max_attempts=max_attempts,
        initial_delay=initial_delay,
        max_delay=max_delay,
        backoff=backoff,
        retryable=retryable,
        **config_kwargs
    )
    retrier = Retrier(config)

    def decorator(func: Callable) -> Callable:
        if asyncio.iscoroutinefunction(func):
            @functools.wraps(func)
            async def async_wrapper(*args, **kwargs):
                return await retrier.run_async(func, *args, **kwargs)
            return async_wrapper
        else:
            @functools.wraps(func)
            def sync_wrapper(*args, **kwargs):
                return retrier.run_sync(func, *args, **kwargs)
            return sync_wrapper

    return decorator


# ==================== 预设配置 ====================

class RetryPresets:
    """预设重试配置"""

    @staticmethod
    def api_calls() -> RetryConfig:
        """API 调用"""
        return RetryConfig(
            max_attempts=5,
            initial_delay=1.0,
            max_delay=30.0,
            backoff=BackoffStrategy.EXPONENTIAL,
            retryable=(ConnectionError, TimeoutError, OSError),
        )

    @staticmethod
    def llm_calls() -> RetryConfig:
        """LLM API 调用"""
        return RetryConfig(
            max_attempts=3,
            initial_delay=2.0,
            max_delay=60.0,
            backoff=BackoffStrategy.EXPONENTIAL,
            jitter=0.2,
        )

    @staticmethod
    def storage() -> RetryConfig:
        """存储操作"""
        return RetryConfig(
            max_attempts=3,
            initial_delay=0.5,
            max_delay=10.0,
            backoff=BackoffStrategy.EXPONENTIAL,
        )

    @staticmethod
    def quick() -> RetryConfig:
        """快速重试（2次）"""
        return RetryConfig(
            max_attempts=2,
            initial_delay=0.1,
            max_delay=1.0,
            backoff=BackoffStrategy.CONSTANT,
        )


__all__ = [
    "RetryConfig",
    "RetryState",
    "Retrier",
    "BackoffStrategy",
    "RetryPresets",
    "retry",
]
```

[61] gecko/core/session/__init__.py
```python
from gecko.core.session.schema import SessionMetadata
from gecko.core.session.entity import Session
from gecko.core.session.manager import SessionManager

__all__ = ["Session", "SessionManager", "SessionMetadata"]
```

[62] gecko/core/session/entity.py
```python
"""会话实体逻辑"""
from __future__ import annotations
import asyncio
import time
import uuid
from typing import Any, Dict, List, Optional
from gecko.core.logging import get_logger
from gecko.core.events import EventBus, SessionEvent
# 引用新的 protocols 路径
from gecko.plugins.storage.interfaces import SessionInterface 
from gecko.core.session.schema import SessionMetadata

logger = get_logger(__name__)

class Session:
    """
    会话对象
    
    管理内存中的会话状态，并提供可选的持久化支持。
    """
    
    def __init__(
        self,
        session_id: Optional[str] = None,
        state: Optional[Dict[str, Any]] = None,
        storage: Optional[SessionInterface] = None,
        ttl: Optional[int] = None,
        event_bus: Optional[EventBus] = None,
        auto_save: bool = False, 
        auto_save_debounce: float = 0.1,  # 新增: 防抖延迟
    ):
        self.session_id = session_id or self._generate_id()
        self.state: Dict[str, Any] = state or {}
        self.storage = storage
        self.event_bus = event_bus or EventBus()
        self.auto_save = auto_save
        self.auto_save_debounce = auto_save_debounce  # 新增
        
        # 元数据
        self.metadata = SessionMetadata(
            session_id=self.session_id,
            ttl=ttl
        )
        
        # 并发锁
        self._lock = asyncio.Lock()
        # 标记为已修改
        self._dirty = False
        self._save_scheduled = False  # ✅ 新增: 保存调度标记
        self._save_task: Optional[asyncio.Task] = None  # ✅ 新增: 保存任务引用
        
        logger.debug("Session created", session_id=self.session_id)
    
    @staticmethod
    def _generate_id() -> str:
        return f"session_{uuid.uuid4().hex[:16]}"
    
    # ===== 状态管理 (同步方法) =====
    
    def get(self, key: str, default: Any = None) -> Any:
        """
        获取状态值（不自动更新访问计数）
        """
        return self.state.get(key, default)
    
    def set(self, key: str, value: Any):
        """
        设置状态值（同步）
        """
        self.state[key] = value
        self.metadata.updated_at = time.time()
        self._dirty = True
        self._try_schedule_auto_save()
    
    def delete(self, key: str) -> bool:
        """删除状态值"""
        if key in self.state:
            del self.state[key]
            self.metadata.updated_at = time.time()
            self._dirty = True
            self._try_schedule_auto_save()
            return True
        return False
    
    def clear(self):
        """清空所有状态"""
        self.state.clear()
        self.metadata.updated_at = time.time()
        self._dirty = True
        self._try_schedule_auto_save()
    
    def update(self, data: Dict[str, Any]):
        """批量更新状态"""
        self.state.update(data)
        self.metadata.updated_at = time.time()
        self._dirty = True
        self._try_schedule_auto_save()
    
    def touch(self):
        """手动更新访问时间和计数"""
        self.metadata.touch()
    
    def _try_schedule_auto_save(self):
        """
        修复: 使用防抖机制调度自动保存
        """
        if not self.auto_save or not self.storage:
            return
        
        # 如果已有保存任务在等待，不重复调度
        if self._save_scheduled:
            return
        
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            # 没有运行中的事件循环
            return
        
        self._save_scheduled = True
        
        async def _debounced_save():
            """防抖保存任务"""
            try:
                # 等待防抖延迟
                await asyncio.sleep(self.auto_save_debounce)
                
                # 执行保存
                if self._dirty:  # 再次检查是否需要保存
                    await self.save()
            except asyncio.CancelledError:
                pass
            except Exception as e:
                logger.error("Auto-save failed", session_id=self.session_id, error=str(e))
            finally:
                self._save_scheduled = False
                self._save_task = None
        
        # 取消之前的任务（如果存在）
        if self._save_task and not self._save_task.done():
            self._save_task.cancel()
        
        self._save_task = loop.create_task(_debounced_save())

    async def _auto_save_task(self):
        """后台自动保存任务"""
        try:
            await self.save()
        except Exception as e:
            logger.error("Auto-save failed", session_id=self.session_id, error=str(e))

    # ===== 字典接口 =====
    
    def keys(self) -> List[str]:
        return list(self.state.keys())
    
    def values(self) -> List[Any]:
        return list(self.state.values())
    
    def items(self) -> List[tuple]:
        return list(self.state.items())
    
    def __contains__(self, key: str) -> bool:
        return key in self.state
    
    def __getitem__(self, key: str) -> Any:
        return self.state[key]
    
    def __setitem__(self, key: str, value: Any):
        self.set(key, value)
    
    # ===== 生命周期管理 =====
    
    def is_expired(self) -> bool:
        return self.metadata.is_expired()
    
    def extend_ttl(self, extra_seconds: int):
        if self.metadata.ttl is not None:
            self.metadata.ttl += extra_seconds
            self._dirty = True
    
    def renew(self):
        self.metadata.created_at = time.time()
        self._dirty = True
    
    # ===== 标签管理 =====
    
    def add_tag(self, tag: str):
        self.metadata.tags.add(tag)
        self._dirty = True
    
    def remove_tag(self, tag: str):
        self.metadata.tags.discard(tag)
        self._dirty = True
    
    def has_tag(self, tag: str) -> bool:
        return tag in self.metadata.tags
    
    # ===== 持久化 (异步方法) =====
    
    async def save(self, force: bool = False):
        """
        [优化] 保存会话到存储 (线程安全与一致性保证)
        
        策略：
        1. 持有锁时进行状态检查和数据快照 (Snapshotting)。
        2. 使用 utils.safe_serialize_context 确保数据深拷贝和清洗。
        3. 持有锁进行 IO 操作，确保写入顺序性（针对单个 Session 的并发保护）。
        """
        if not self.storage:
            return
        
        # 全程持有锁，防止在 Snapshotting 和 IO 之间状态发生变更
        # 虽然这会略微增加锁的持有时间，但对于单 Session 粒度来说，一致性优于微小的并发性能提升。
        async with self._lock:
            # 双重检查
            if not force and not self._dirty:
                return

            try:
                # 1. 准备数据快照 (CPU 密集型)
                # 引入 utils 中的序列化工具，它会递归处理并返回纯净的 dict/list 副本
                # 这实际上切断了 storage 数据与内存 self.state 的引用关系
                from gecko.core.utils import safe_serialize_context
                
                # 获取当前状态的字典表示
                raw_data = self.to_dict()
                
                # 清洗并深拷贝
                clean_data = safe_serialize_context(raw_data)
                
                # 2. 执行 IO (IO 密集型)
                # 这里的 await 会释放 GIL，但不会释放 self._lock (协程锁)
                # 因此其他协程无法在此期间修改 self.state 或发起新的 save
                await self.storage.set(self.session_id, clean_data)
                
                # 3. 状态重置
                # 只有成功写入后才清除 dirty 标记
                self._dirty = False
                
                logger.debug("Session saved", session_id=self.session_id)
                await self.event_bus.publish(SessionEvent(
                    type="session_saved",
                    data={"session_id": self.session_id}
                ))
                
            except Exception as e:
                logger.error(
                    "Failed to save session",
                    session_id=self.session_id,
                    error=str(e)
                )
                # 发生异常时，保持 dirty = True，以便下次重试
                # 抛出异常让上层知道保存失败
                raise
    
    async def load(self) -> bool:
        """从存储加载会话"""
        if not self.storage:
            return False
        
        async with self._lock:
            try:
                data = await self.storage.get(self.session_id)
                if not data:
                    return False
                
                self.from_dict(data)
                self._dirty = False
                
                logger.debug("Session loaded", session_id=self.session_id)
                
                await self.event_bus.publish(SessionEvent(
                    type="session_loaded",
                    data={"session_id": self.session_id}
                ))
                
                return True
            except Exception as e:
                logger.error(
                    "Failed to load session",
                    session_id=self.session_id,
                    error=str(e)
                )
                return False
    
    async def destroy(self):
        """销毁会话（从存储中删除）"""
        if self.storage:
            async with self._lock:
                try:
                    await self.storage.delete(self.session_id)
                    logger.info("Session destroyed", session_id=self.session_id)
                    
                    await self.event_bus.publish(SessionEvent(
                        type="session_destroyed",
                        data={"session_id": self.session_id}
                    ))
                except Exception as e:
                    logger.error(
                        "Failed to destroy session",
                        session_id=self.session_id,
                        error=str(e)
                    )
        
        self.state.clear()
    
    # ===== 序列化 =====
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "state": self.state,
            "metadata": self.metadata.model_dump(),
        }
    
    def from_dict(self, data: Dict[str, Any]):
        self.state = data.get("state", {})
        
        metadata_data = data.get("metadata", {})
        if metadata_data:
            self.metadata = SessionMetadata(**metadata_data)
    
    def clone(self, new_id: Optional[str] = None) -> Session:
        cloned = Session(
            session_id=new_id,
            state=self.state.copy(),
            storage=self.storage,
            ttl=self.metadata.ttl,
            event_bus=self.event_bus,
            auto_save=False,
        )
        cloned.metadata.tags = self.metadata.tags.copy()
        cloned.metadata.custom = self.metadata.custom.copy()
        return cloned
    
    def get_info(self) -> Dict[str, Any]:
        """
        [新增] 获取会话的统计信息
        """
        return {
            "session_id": self.session_id,
            "created_at": self.metadata.created_at,
            "updated_at": self.metadata.updated_at,
            "access_count": self.metadata.access_count,
            "state_keys": len(self.state),
            "ttl": self.metadata.ttl,
            "tags": list(self.metadata.tags)
        }
    
    # ===== 上下文管理器支持 =====
    
    async def __aenter__(self):
        await self.load()
        self.touch()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self._dirty:
            await self.save()
    
    def __repr__(self) -> str:
        return (
            f"Session("
            f"id={self.session_id}, "
            f"keys={len(self.state)}, "
            f"dirty={self._dirty}"
            f")"
        )

```

[63] gecko/core/session/manager.py
```python
"""会话管理器"""
from __future__ import annotations
import asyncio
from typing import Any, Dict, List, Optional
from gecko.plugins.storage.interfaces import SessionInterface
from gecko.core.session.entity import Session
from gecko.core.logging import get_logger

logger = get_logger(__name__)

class SessionManager:
    """会话管理器"""
    
    def __init__(
        self,
        storage: Optional[SessionInterface] = None,
        default_ttl: Optional[int] = None,
        auto_cleanup: bool = True,
        cleanup_interval: int = 300,
    ):
        self.storage = storage
        self.default_ttl = default_ttl
        self.auto_cleanup = auto_cleanup
        self.cleanup_interval = cleanup_interval
        
        self._sessions: Dict[str, Session] = {}
        self._lock = asyncio.Lock()
        
        self._cleanup_task: Optional[asyncio.Task] = None
        
        logger.info("SessionManager initialized", default_ttl=default_ttl)
    
    async def start(self):
        """启动管理器（主要是自动清理任务）"""
        if self.auto_cleanup and not self._cleanup_task:
            self._cleanup_task = asyncio.create_task(self._cleanup_loop())
            logger.debug("Cleanup task started", interval=self.cleanup_interval)

    async def _cleanup_loop(self):
        while True:
            try:
                await asyncio.sleep(self.cleanup_interval)
                await self.cleanup_expired()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error("Cleanup task error", error=str(e))
                await asyncio.sleep(60)

    async def create_session(
        self,
        session_id: Optional[str] = None,
        ttl: Optional[int] = None,
        **initial_state
    ) -> Session:
        """创建新会话"""
        async with self._lock:
            session = Session(
                session_id=session_id,
                state=initial_state,
                storage=self.storage,
                ttl=ttl or self.default_ttl,
                auto_save=True, 
            )
            
            self._sessions[session.session_id] = session
            
            if self.storage:
                await session.save()
            
            return session
    
    async def get_session(
        self,
        session_id: str,
        create_if_missing: bool = False
    ) -> Optional[Session]:
        """获取会话"""
        if session_id in self._sessions:
            session = self._sessions[session_id]
            if session.is_expired():
                await self.destroy_session(session_id)
                if create_if_missing:
                    return await self.create_session(session_id=session_id)
                return None
            session.touch()
            return session
        
        if self.storage:
            session = Session(
                session_id=session_id,
                storage=self.storage,
                auto_save=True,
            )
            if await session.load():
                if session.is_expired():
                    await session.destroy()
                    if create_if_missing:
                        return await self.create_session(session_id=session_id)
                    return None
                async with self._lock:
                    self._sessions[session_id] = session
                session.touch()
                return session
        
        if create_if_missing:
            return await self.create_session(session_id=session_id)
        
        return None
    
    async def destroy_session(self, session_id: str) -> bool:
        """销毁会话"""
        async with self._lock:
            session = self._sessions.pop(session_id, None)
            
            if session:
                await session.destroy()
                return True
            elif self.storage:
                try:
                    await self.storage.delete(session_id)
                    return True
                except Exception as e:
                    logger.error("Failed to destroy session", session_id=session_id, error=str(e))
        return False
    
    async def cleanup_expired(self) -> int:
        """清理所有过期会话"""
        expired_ids = []
        async with self._lock:
            for session_id, session in self._sessions.items():
                if session.is_expired():
                    expired_ids.append(session_id)
        
        for session_id in expired_ids:
            await self.destroy_session(session_id)
        
        if expired_ids:
            logger.info("Expired sessions cleaned", count=len(expired_ids))
        return len(expired_ids)
    
    def get_active_count(self) -> int:
        """获取活跃会话数量"""
        return len(self._sessions)
    
    def get_all_sessions(self) -> List[Session]:
        """获取所有活跃会话"""
        return list(self._sessions.values())
    
    async def shutdown(self):
        """关闭管理器"""
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        
        sessions = list(self._sessions.values())
        for session in sessions:
            await session.save(force=True)
        
        logger.info("SessionManager shutdown", sessions_saved=len(sessions))


    async def migrate_session(
        self,
        session_id: str,
        target_storage: SessionInterface,
        delete_source: bool = True
    ) -> bool:
        """
        迁移会话到新存储后端
        
        参数:
            session_id: 会话 ID
            target_storage: 目标存储
            delete_source: 是否删除源数据
        """
        session = await self.get_session(session_id)
        if not session:
            logger.warning("Session not found for migration", session_id=session_id)
            return False
        
        try:
            # 保存到目标
            data = session.to_dict()
            await target_storage.set(session_id, data)
            
            # 删除源
            if delete_source:
                await self.destroy_session(session_id)
            
            logger.info("Session migrated", session_id=session_id)
            return True
            
        except Exception as e:
            logger.error("Session migration failed", session_id=session_id, error=str(e))
            return False
    
    async def export_all_sessions(self) -> List[Dict[str, Any]]:
        """导出所有会话数据"""
        sessions = self.get_all_sessions()
        return [s.to_dict() for s in sessions]
    
    async def import_sessions(
        self,
        sessions_data: List[Dict[str, Any]],
        overwrite: bool = False
    ) -> int:
        """
        导入会话数据
        
        返回: 成功导入的数量
        """
        imported = 0
        
        for data in sessions_data:
            session_id = data.get("metadata", {}).get("session_id")
            if not session_id:
                continue
            
            # 检查是否存在
            existing = await self.get_session(session_id)
            if existing and not overwrite:
                continue
            
            # 创建新会话
            session = Session(
                session_id=session_id,
                storage=self.storage,
                auto_save=False
            )
            session.from_dict(data)
            await session.save(force=True)
            
            self._sessions[session_id] = session
            imported += 1
        
        logger.info("Sessions imported", count=imported)
        return imported
```

[64] gecko/core/session/schema.py
```python
"""会话元数据定义"""
from __future__ import annotations
import time
from typing import Any, Dict, Optional, Set
from pydantic import BaseModel, Field
from gecko.core.logging import get_logger

logger = get_logger(__name__)

class SessionMetadata(BaseModel):
    """会话元数据"""
    session_id: str = Field(..., description="会话 ID")
    created_at: float = Field(default_factory=time.time, description="创建时间")
    updated_at: float = Field(default_factory=time.time, description="更新时间")
    accessed_at: float = Field(default_factory=time.time, description="访问时间")
    access_count: int = Field(default=0, description="访问次数")
    ttl: Optional[int] = Field(default=None, description="生存时间（秒）")
    tags: Set[str] = Field(default_factory=set, description="标签")
    custom: Dict[str, Any] = Field(default_factory=dict, description="自定义数据")
    
    def is_expired(self) -> bool:
        """检查会话是否过期"""
        if self.ttl is None:
            return False
        age = time.time() - self.created_at
        return age > self.ttl
    
    def time_to_expire(self) -> Optional[float]:
        """获取距离过期的剩余时间（秒）"""
        if self.ttl is None:
            return None
        age = time.time() - self.created_at
        return max(0.0, self.ttl - age)
    
    def touch(self):
        """更新访问时间和计数"""
        self.accessed_at = time.time()
        self.access_count += 1
```

[65] gecko/core/structure/__init__.py
```python
# gecko/core/structure/__init__.py
"""
结构化输出子模块入口

本文件用于对外暴露稳定的公共接口，并隐藏内部实现细节。
通过这样的方式，我们可以在不破坏外部调用方的前提下，
自由地重构内部模块结构和实现。

对外主要暴露以下对象：
- StructureEngine: 核心结构化输出引擎
- StructureParseError: 结构化解析失败异常
- parse_structured_output: 同步封装的便捷函数
- extract_json_from_text: 轻量级 JSON 提取工具函数
- ExtractionStrategy / register_extraction_strategy: 策略插件扩展接口
"""

from gecko.core.structure.errors import StructureParseError
from gecko.core.structure.engine import StructureEngine
from gecko.core.structure.sync import parse_structured_output, extract_json_from_text
from gecko.core.structure.json_extractor import ExtractionStrategy, register_extraction_strategy

__all__ = [
    "StructureEngine",
    "StructureParseError",
    "parse_structured_output",
    "extract_json_from_text",
    "ExtractionStrategy",
    "register_extraction_strategy",
]
```

[66] gecko/core/structure/engine.py
```python
# gecko/core/structure/engine.py
"""
结构化输出引擎模块 (v0.4 Phase 3 Complete)

职责：
- 作为结构化解析的统一入口。
- 协调多种解析策略：Tool Call -> 文本提取 -> (新增) LLM 自愈修复。
- 提供 Schema 生成与差异比对工具。
"""
from __future__ import annotations

import json
from typing import Any, Dict, List, Optional, Type, TypeVar, TYPE_CHECKING

from pydantic import BaseModel

from gecko.core.logging import get_logger
from gecko.core.structure.errors import StructureParseError
from gecko.core.structure.json_extractor import extract_structured_data
from . import schema as schema_utils

if TYPE_CHECKING:
    from gecko.core.protocols import ModelProtocol

logger = get_logger(__name__)

T = TypeVar("T", bound=BaseModel)


class StructureEngine:
    """
    结构化输出引擎
    """

    # ================= Schema 工具方法 (代理) =================

    @staticmethod
    def to_openai_tool(model: type[BaseModel]) -> Dict[str, Any]:
        """
        将 Pydantic 模型转换为 OpenAI Tool Schema
        """
        return schema_utils.to_openai_tool(model)

    @classmethod
    def get_schema_diff(cls, data: Dict[str, Any], model_class: type[BaseModel]) -> Dict[str, Any]:
        """
        比较数据与模型 Schema 的差异
        """
        return schema_utils.get_schema_diff(data, model_class)

    # ================= 核心解析流程 =================

    @classmethod
    async def parse(
        cls,
        content: str,
        model_class: Type[T],
        raw_tool_calls: Optional[List[Dict[str, Any]]] = None,
        strict: bool = True,
        auto_fix: bool = True,
        model: Optional["ModelProtocol"] = None,
    ) -> T:
        """
        解析文本为 Pydantic 模型 (Async)

        解析策略优先级：
        1. Tool Call: 如果 LLM 使用了工具调用，直接解析参数。
        2. Text Extraction: 尝试从文本中提取 JSON (正则/Markdown/括号匹配)。
        3. LLM Repair: (新增) 如果提供了 model 参数，调用 LLM 尝试修复格式错误的 JSON。

        Args:
            content: LLM 的原始文本输出
            model_class: 目标 Pydantic 模型类
            raw_tool_calls: 原始工具调用列表 (OpenAI 格式)
            strict: 是否启用严格模式 (预留)
            auto_fix: 是否启用基础的字符串清理修复
            model: [Phase 3 新增] 用于自愈修复的模型实例

        Returns:
            T: Pydantic 模型实例

        Raises:
            StructureParseError: 当所有策略均失败时抛出
        """
        attempts: List[Dict[str, str]] = []

        # --- 策略 1: Tool Call 解析 ---
        if raw_tool_calls:
            for idx, call in enumerate(raw_tool_calls):
                try:
                    result = cls._parse_from_tool_call(call, model_class)
                    logger.info(
                        "Parsed from tool call",
                        model=model_class.__name__,
                        tool_call_index=idx,
                    )
                    return result
                except Exception as e:
                    attempts.append(
                        {
                            "strategy": f"tool_call_{idx}",
                            "error": str(e),
                        }
                    )

        # --- 策略 2-5: 纯文本 JSON 提取 ---
        # 包含：Direct JSON, Markdown Block, Braced {}, Bracket [], Cleaned JSON
        try:
            return extract_structured_data(
                text=content,
                model_class=model_class,
                strict=strict,
                auto_fix=auto_fix,
            )
        except Exception as e:
            # 合并子解析器的尝试记录
            if hasattr(e, "attempts") and getattr(e, "attempts"):
                attempts.extend(getattr(e, "attempts"))
            else:
                attempts.append({"strategy": "text_extraction", "error": str(e)})

        # --- 策略 6: LLM 自愈 (Self-Healing) ---
        if model:
            try:
                # 延迟导入以避免循环依赖
                from gecko.core.structure.repair import repair_json_with_llm
                
                # 获取最后一次报错信息作为参考，帮助 LLM 理解错误原因
                last_error = attempts[-1]['error'] if attempts else "Unknown parsing error"
                
                logger.info("Attempting LLM-based JSON repair...")
                
                # 调用修复逻辑 (Phase 3 核心)
                fixed_dict = await repair_json_with_llm(content, last_error, model)
                
                # 再次校验 Pydantic Schema
                result = model_class.model_validate(fixed_dict)
                
                logger.info(f"StructureEngine: Successfully repaired output for {model_class.__name__}")
                return result
                
            except Exception as repair_err:
                attempts.append({
                    "strategy": "llm_repair", 
                    "error": str(repair_err)
                })

        # --- 最终失败处理 ---
        error_details = "\n".join(
            f"  - {a['strategy']}: {a['error'][:100]}" for a in attempts
        )

        raise StructureParseError(
            f"无法解析为 {model_class.__name__}。尝试了 {len(attempts)} 种策略:\n{error_details}",
            attempts=attempts,
            raw_content=content,
        )

    @classmethod
    def _parse_from_tool_call(cls, call: Dict[str, Any], model_class: Type[T]) -> T:
        """
        从单个工具调用中解析
        """
        func = call.get("function", {})
        args = func.get("arguments", "")

        if isinstance(args, str):
            # 处理可能的 JSON 解析错误
            try:
                data = json.loads(args)
            except json.JSONDecodeError as e:
                raise ValueError(f"Invalid JSON in tool arguments: {e}")
        elif isinstance(args, dict):
            data = args
        else:
            raise ValueError(f"Invalid arguments type: {type(args)}")

        return model_class.model_validate(data)
```

[67] gecko/core/structure/errors.py
```python
# gecko/core/structure/errors.py
"""
错误类型定义模块

专门存放与结构化解析相关的自定义异常类型，避免在其他模块中
重复定义或产生循环依赖。
"""

from __future__ import annotations

from typing import Dict, List, Optional


class StructureParseError(ValueError):
    """
    结构化解析失败异常

    设计目标：
    - 在一次结构化解析过程中，我们会尝试多种解析策略（tool call、直接 JSON、
      markdown 代码块、括号匹配、清理重试等）。
    - 当所有策略都失败时，用一个统一的异常类型向调用方报告，并附带详细的错误轨迹。

    属性:
        message:
            主错误信息（继承自 ValueError 的 message）
        attempts:
            所有尝试的解析策略及其错误信息列表，
            每个元素形如 {"strategy": "...", "error": "..."}。
        raw_content:
            原始内容（通常是 LLM 的输出）。为了避免日志爆炸，应在外部使用时做长度截断。
    """

    def __init__(
        self,
        message: str,
        attempts: Optional[List[Dict[str, str]]] = None,
        raw_content: Optional[str] = None,
    ):
        super().__init__(message)
        # 保证属性始终存在，便于调用方无脑访问
        self.attempts: List[Dict[str, str]] = attempts or []
        self.raw_content: Optional[str] = raw_content

    def get_detailed_error(self) -> str:
        """
        将解析失败过程格式化为可读字符串

        用途：
            - 日志输出
            - 调试时打印详细信息
        """
        lines = [f"结构化解析失败: {self.args[0]}"]

        if self.attempts:
            lines.append("\n尝试的解析策略:")
            for i, attempt in enumerate(self.attempts, 1):
                strategy = attempt.get("strategy", "unknown")
                error = attempt.get("error", "unknown error")
                lines.append(f"  {i}. {strategy}: {error}")

        if self.raw_content:
            # 只展示前 200 字符，避免日志过长
            preview = self.raw_content[:200].replace("\n", "\\n")
            lines.append(f"\n原始内容预览: {preview}...")

        return "\n".join(lines)
```

[68] gecko/core/structure/json_extractor.py
```python
# gecko/core/structure/json_extractor.py
"""
JSON 提取与模型验证模块

负责：
1. 使用多种启发式策略（正则、括号匹配、Markdown块）从脏文本中提取 JSON。
2. 将提取出的数据转换为 Pydantic 模型或原生字典/列表。
3. 提供插件机制扩展解析策略。
"""
from __future__ import annotations

import json
import re
from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Optional, Type, TypeVar

from pydantic import BaseModel, ValidationError

from gecko.core.logging import get_logger
from .errors import StructureParseError

logger = get_logger(__name__)

# [修改] T 不再绑定 BaseModel，允许 dict/list 等任意类型
T = TypeVar("T")


# ============================================================================
# Strategy 插件接口定义
# ============================================================================

@dataclass
class ExtractionStrategy:
    """文本解析策略插件定义"""
    name: str
    func: Callable[[str, Type[T]], T] # type: ignore


# 全局插件策略列表
_EXTRA_STRATEGIES: List[ExtractionStrategy] = []


def register_extraction_strategy(strategy: ExtractionStrategy) -> None:
    """注册新的提取策略"""
    _EXTRA_STRATEGIES.append(strategy)


# ============================================================================
# 主入口函数
# ============================================================================

def extract_structured_data(
    text: str,
    model_class: Type[T],
    *,
    strict: bool = True,
    auto_fix: bool = True,
    max_text_length: int = 100000,
) -> T:
    """
    从文本中提取 JSON 并解析为指定类型（Pydantic 模型或 dict/list）。
    
    执行流程：
    1. 文本预处理与长度检查。
    2. 快速检查（Fast Fail）：如果不包含 '{' 或 '['，直接报错。
    3. 依次尝试内置策略：
       - Direct JSON (整体解析)
       - Markdown Code Block (```json ...```)
       - Braced Object ({...} 栈匹配)
       - Bracket Array ([...] 栈匹配)
       - Cleaned JSON (正则清理后重试)
    4. 尝试已注册的插件策略。
    5. 全部失败则抛出 StructureParseError，包含所有尝试的错误详情。
    """
    # 1. 防御性截断，防止正则 DoS
    if len(text) > max_text_length:
        logger.warning(
            "Text too long for JSON extraction, truncating",
            original_length=len(text),
            max_length=max_text_length,
        )
        text = text[:max_text_length]

    text = text.strip()
    attempts: List[Dict[str, str]] = []

    # 2. 快速失败检查
    if not text or ("{" not in text and "[" not in text):
        raise StructureParseError(
            "Content does not contain JSON-like structure (missing '{' or '[')",
            raw_content=text[:500] if text else "",
        )

    # --- 策略 A: 整体直接解析 ---
    try:
        data = json.loads(text)
        return _validate_model(data, model_class)
    except Exception as e:
        attempts.append({"strategy": "direct_json", "error": str(e)})

    # --- 策略 B: Markdown 代码块提取 ---
    # 匹配 ```json {...} ``` 或 ``` {...} ```
    markdown_pattern = r"```(?:\w+)?\s*([\s\S]*?)```"
    for idx, match in enumerate(re.finditer(markdown_pattern, text)):
        candidate = match.group(1).strip()
        try:
            data = json.loads(candidate)
            return _validate_model(data, model_class)
        except Exception as e:
            if idx < 3: # 仅记录前3次尝试，避免日志过大
                attempts.append({"strategy": f"markdown_{idx}", "error": str(e)})

    # --- 策略 C: 花括号对象提取 ({...}) ---
    obj_candidates = extract_braced_json(text)
    for idx, candidate in enumerate(obj_candidates):
        try:
            data = json.loads(candidate)
            return _validate_model(data, model_class)
        except Exception as e:
            if idx < 3:
                attempts.append({"strategy": f"braced_{idx}", "error": str(e)})

    # --- 策略 C2: 中括号数组提取 ([...]) ---
    array_candidates = extract_bracket_json(text)
    for idx, candidate in enumerate(array_candidates):
        try:
            data = json.loads(candidate)
            return _validate_model(data, model_class)
        except Exception as e:
            if idx < 3:
                attempts.append({"strategy": f"bracket_{idx}", "error": str(e)})

    # --- 策略 D: 简单清理后重试 ---
    if auto_fix:
        cleaned = clean_json_string(text)
        # 只有当清理产生了变化才重试
        if cleaned != text:
            try:
                data = json.loads(cleaned)
                logger.info("Parsed successfully after cleaning", model=model_class.__name__ if hasattr(model_class, "__name__") else "Unknown")
                return _validate_model(data, model_class)
            except Exception as e:
                attempts.append({"strategy": "cleaned_json", "error": str(e)})

    # --- 策略 E: 插件策略 ---
    for strategy in _EXTRA_STRATEGIES:
        try:
            return strategy.func(text, model_class) # type: ignore
        except Exception as e:
            attempts.append({"strategy": f"plugin_{strategy.name}", "error": str(e)})

    # --- 汇总失败 ---
    error_details = "\n".join(
        f"  - {a['strategy']}: {a['error'][:100]}" for a in attempts
    )
    raise StructureParseError(
        f"无法解析为 {getattr(model_class, '__name__', str(model_class))}。尝试了 {len(attempts)} 种策略:\n{error_details}",
        attempts=attempts,
        raw_content=text,
    )


# ============================================================================
# 辅助提取工具
# ============================================================================

def extract_braced_json(text: str, max_text_length: int = 100000, max_candidates: int = 5) -> List[str]:
    """使用栈提取 {...} 结构"""
    if len(text) > max_text_length:
        text = text[:max_text_length]

    candidates: List[str] = []
    stack: List[str] = []
    start: Optional[int] = None

    search_start = text.find("{")
    if search_start == -1:
        return []

    for idx, ch in enumerate(text[search_start:], start=search_start):
        if ch == "{":
            if not stack:
                start = idx
            stack.append(ch)
        elif ch == "}" and stack:
            stack.pop()
            if not stack and start is not None:
                candidates.append(text[start : idx + 1])
                start = None
                if len(candidates) >= max_candidates:
                    break

    candidates.sort(key=len, reverse=True)
    return candidates


def extract_bracket_json(text: str, max_text_length: int = 100000, max_candidates: int = 3) -> List[str]:
    """使用栈提取 [...] 结构"""
    if len(text) > max_text_length:
        text = text[:max_text_length]

    candidates: List[str] = []
    stack: List[str] = []
    start: Optional[int] = None

    search_start = text.find("[")
    if search_start == -1:
        return []

    for idx, ch in enumerate(text[search_start:], start=search_start):
        if ch == "[":
            if not stack:
                start = idx
            stack.append(ch)
        elif ch == "]" and stack:
            stack.pop()
            if not stack and start is not None:
                candidates.append(text[start : idx + 1])
                start = None
                if len(candidates) >= max_candidates:
                    break

    candidates.sort(key=len, reverse=True)
    return candidates


def clean_json_string(text: str) -> str:
    """清理 JSON 字符串中的注释和多余逗号"""
    # 移除单行注释
    text = re.sub(r"//.*?\n", "\n", text)
    # 移除块注释
    text = re.sub(r"/\*.*?\*/", "", text, flags=re.DOTALL)
    # 移除尾部多余逗号
    text = re.sub(r",(\s*[}\]])", r"\1", text)
    # 移除控制字符
    text = re.sub(r"[\x00-\x1f\x7f-\x9f]", "", text)
    return text.strip()


# ============================================================================
# 验证逻辑 (包含本次关键修复)
# ============================================================================

def _validate_model(data: Any, model_class: Type[T]) -> T:
    """
    验证并将数据转换为目标类型。
    
    [修复]：增加了对 dict 和 list 的原生支持，防止 Pydantic 抛出 AttributeError。
    """
    # 1. 原生类型检查：如果是 dict 或 list，直接返回数据
    # 这对 StructureEngine 的 LLM 修复逻辑至关重要
    if model_class is dict or model_class is list:
        if isinstance(data, model_class):
            return data # type: ignore
        raise ValueError(f"Data type {type(data)} does not match expected {model_class}")

    # 2. 泛型别名检查 (typing.Dict, typing.List 等)
    # 简单检查 origin 是否为 dict/list
    origin = getattr(model_class, "__origin__", None)
    if origin in (dict, list):
        return data # type: ignore

    # 3. Pydantic 模型校验
    try:
        # Pydantic V2 推荐方式
        return model_class.model_validate(data)  # type: ignore
    except AttributeError:
        # 兼容 dataclass 或 Pydantic V1 (fallback)
        try:
            return model_class(**data) # type: ignore
        except Exception as e:
             raise ValidationError(f"Failed to validate model: {e}") from e
    except ValidationError as e:
        logger.error(
            "Model validation failed",
            model=model_class.__name__,
            errors=e.errors(),
        )
        raise e


# ============================================================================
# YAML 插件 (可选)
# ============================================================================
try:
    import yaml # type: ignore
    
    def _yaml_fulltext_strategy(text: str, model_class: Type[T]) -> T:
        if not text.strip(): 
            raise ValueError("Empty text for YAML strategy")
        data = yaml.safe_load(text)
        return _validate_model(data, model_class)
        
    register_extraction_strategy(
        ExtractionStrategy(name="yaml_fulltext", func=_yaml_fulltext_strategy)
    )
except ImportError:
    pass
```

[69] gecko/core/structure/repair.py
```python
from __future__ import annotations

from typing import Any, Dict, Optional

from gecko.core.logging import get_logger
from gecko.core.protocols import ModelProtocol
from gecko.core.structure.json_extractor import extract_structured_data

logger = get_logger(__name__)

REPAIR_TEMPLATE = """
The following text was intended to be a valid JSON object but failed parsing:

```text
{broken_text}
```

Parser Error:
{error_msg}

Task:
Please fix the JSON formatting errors (e.g., missing quotes, trailing commas, unescaped characters).
Output ONLY the valid, minified JSON object. Do not add any markdown, explanations, or extra text.
"""

async def repair_json_with_llm(
    broken_text: str,
    error_msg: str,
    model: ModelProtocol,
    max_length: int = 2000
) -> Dict[str, Any]:
    """
    使用 LLM 修复损坏的 JSON 字符串
    
    Args:
        broken_text: 解析失败的原始文本
        error_msg: 解析器报错信息
        model: 用于修复的模型实例
        max_length: 发送给修复模型的最大文本长度 (防止 Context 爆炸)
    
    Returns:
        修复后的字典数据
        
    Raises:
        ValueError: 如果修复后仍然无法解析，或者解析结果不是字典
    """
    # 截断过长文本，保留头部信息作为修复依据
    if len(broken_text) > max_length:
        broken_text = broken_text[:max_length] + "...(truncated)"

    prompt = REPAIR_TEMPLATE.format(
        broken_text=broken_text,
        error_msg=error_msg
    )

    try:
        logger.info("Triggering LLM-based JSON repair...")
        
        # 调用模型 (构造简单的 OpenAI 格式消息)
        # temperature=0.0 对于格式修复至关重要
        response = await model.acompletion(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0, 
            json_mode=True   # 如果底层模型支持，开启 JSON 模式能显著提高成功率
        )
        
        fixed_content = response.choices[0].message.get("content", "")
        
        # 尝试从修复后的内容中提取
        # 我们复用 extract_structured_data 的提取逻辑 (正则/括号匹配)
        # 此时传入 dict 作为 model_class 可以绕过 Pydantic 校验，仅做 JSON Load
        fixed_data = extract_structured_data(
            text=fixed_content,
            model_class=dict, # type: ignore 
            auto_fix=True
        )
        
        # [优化] 强类型检查：确保返回的是字典
        if not isinstance(fixed_data, dict):
            raise ValueError(f"Repair returned {type(fixed_data)}, expected dict")
        
        logger.info("JSON repair successful")
        return fixed_data

    except Exception as e:
        logger.warning(f"JSON repair failed: {e}")
        raise ValueError(f"Repair failed: {e}") from e
```

[70] gecko/core/structure/schema.py
```python
# gecko/core/structure/schema.py
"""
Schema 工具模块

职责：
- 将 Pydantic 模型转换为 OpenAI Function Calling / tools 所需的 schema
- 计算实际数据与模型 schema 的差异（缺失字段、多余字段、简单类型不匹配）

注意：
- 这里只做“schema 层”的工具函数，不参与具体的 JSON 提取流程。
"""

from __future__ import annotations

import re
from typing import Any, Dict, List

from pydantic import BaseModel


def to_openai_tool(model: type[BaseModel]) -> Dict[str, Any]:
    """
    将 Pydantic 模型转换为 OpenAI Function Calling 所需的 tool schema

    参数:
        model: Pydantic 模型类

    返回:
        OpenAI tool 定义字典，例如：
        {
            "type": "function",
            "function": {
                "name": "...",
                "description": "...",
                "parameters": {...},  # JSON Schema
            },
        }

    设计要点：
        - 使用 Pydantic v2 的 `model_json_schema()` 生成 JSON Schema
        - 将 title 中的非单词字符去掉，生成合法的工具名称
        - 展开 `$defs` / `$ref`，生成更“扁平化”的 schema，方便 OpenAI 使用
    """
    schema = model.model_json_schema()

    # 提取模型名称（移除特殊字符，转换为小写）
    name = re.sub(r"\W+", "_", schema.get("title", "extract_data")).lower()

    # 去掉 title，工具 schema 中一般用不到
    schema.pop("title", None)

    # 如果模型包含子定义（$defs），则尝试展开 $ref 引用
    if "$defs" in schema:
        schema = _flatten_schema(schema)

    return {
        "type": "function",
        "function": {
            "name": name,
            "description": schema.get("description", f"Extract {name} data"),
            "parameters": schema,
        },
    }


def _flatten_schema(schema: Dict[str, Any]) -> Dict[str, Any]:
    """
    展开 schema 中的 $ref 引用（简化版实现）

    说明：
        - Pydantic 通常会把一些嵌套模型放在 `$defs` 中，并通过 `$ref` 引用。
        - 为了更好地兼容 OpenAI 的 tools，我们希望尽量把这些引用“展开”成一个
          更扁平的 JSON Schema。

    处理策略：
        - 支持 "$defs" + "$ref" 最常见的模式
        - 对于 { "$ref": "#/$defs/Xxx", <其他字段> }：
          1) 先递归解析 "$defs.Xxx"
          2) 将当前节点除 "$ref" 外的字段与目标 dict 做“浅合并”，保留额外信息
        - 不完整支持 allOf/anyOf/oneOf 等高级特性，仅覆盖普通 Pydantic 模型场景
    """
    defs = schema.pop("$defs", {})
    if not defs:
        return schema

    def resolve_ref(obj: Any) -> Any:
        # 递归解析对象里的 $ref
        if isinstance(obj, dict):
            if "$ref" in obj:
                ref_key = obj["$ref"].split("/")[-1]
                if ref_key in defs:
                    # 递归解析被引用定义
                    target = resolve_ref(defs[ref_key])
                    # 当前节点中除了 $ref 外，其余字段视为“补充信息”
                    extra = {k: v for k, v in obj.items() if k != "$ref"}
                    if isinstance(target, dict):
                        # 被引用定义优先，然后由当前节点覆盖
                        merged = {**target, **extra}
                        return resolve_ref(merged)
                    return target
                # 找不到对应 defs 时，退化为普通 dict 递归
            # 普通 dict：递归处理子字段
            return {k: resolve_ref(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [resolve_ref(item) for item in obj]
        return obj

    return resolve_ref(schema)  # type: ignore[return-value]


def get_schema_diff(
    data: Dict[str, Any],
    model_class: type[BaseModel],
) -> Dict[str, Any]:
    """
    比较数据与模型 Schema 的差异（基础版）

    参数:
        data: 实际数据（通常是 JSON 解析后的 dict）
        model_class: 期望的 Pydantic 模型

    返回:
        差异信息字典，包含：
            - missing_required: 缺失的必填字段名列表
            - extra_fields: 数据中多余的字段名列表
            - type_mismatches: 简单字段类型不匹配列表
    """
    schema = model_class.model_json_schema()
    required = set(schema.get("required", []))
    properties = schema.get("properties", {})

    data_keys = set(data.keys())
    schema_keys = set(properties.keys())

    return {
        "missing_required": list(required - data_keys),
        "extra_fields": list(data_keys - schema_keys),
        "type_mismatches": _check_type_mismatches(data, properties),
    }


def _check_type_mismatches(
    data: Dict[str, Any],
    properties: Dict[str, Any],
) -> List[Dict[str, str]]:
    """
    检查顶层字段类型不匹配（简化版）

    说明：
        - 仅检查顶层字段（不递归数组元素或嵌套对象）
        - 类型信息来源于 JSON Schema 中的 "type" 字段
        - 结果主要用于诊断和日志，不作为硬性约束
    """
    mismatches: List[Dict[str, str]] = []

    for key, value in data.items():
        if key not in properties:
            continue

        expected_type = properties[key].get("type")
        actual_type = type(value).__name__

        # 简化 JSON Schema type -> Python 类型映射
        type_map = {
            "string": str,
            "integer": int,
            "number": (int, float),
            "boolean": bool,
            "array": list,
            "object": dict,
        }

        expected_python_type = type_map.get(expected_type)
        if expected_python_type and not isinstance(value, expected_python_type):
            mismatches.append(
                {
                    "field": key,
                    "expected": expected_type,
                    "actual": actual_type,
                }
            )

    return mismatches
```

[71] gecko/core/structure/sync.py
```python
# gecko/core/structure/sync.py
"""
同步封装与轻量工具函数模块

职责：
- 提供同步版本的结构化解析函数 `parse_structured_output`，方便在纯同步脚本中使用
- 提供轻量级 JSON 提取函数 `extract_json_from_text`，用于简单场景

注意：
- 同步封装在已有事件循环中使用会抛出 RuntimeError，以避免错误地嵌套事件循环。
"""

from __future__ import annotations

import json
import asyncio
import re
from typing import Any, Dict, List, Optional, Type, TypeVar

from pydantic import BaseModel

from .engine import StructureEngine
from .json_extractor import extract_braced_json, extract_bracket_json

T = TypeVar("T", bound=BaseModel)


def parse_structured_output(
    content: str,
    model_class: Type[T],
    tool_calls: Optional[List[Dict[str, Any]]] = None,
) -> T:
    """
    同步版本的结构化输出解析（便捷函数）

    使用场景：
        - 在纯脚本 / 同步程序中使用结构化解析，不方便引入 async/await 时
        - 自动处理事件循环的创建与回收

    安全性考虑：
        - 如果当前存在正在运行的事件循环（例如在 FastAPI / Jupyter 中），
          本函数会抛出 RuntimeError，提示调用方改用异步接口：
          `await StructureEngine.parse(...)`
    """
    try:
        # 如果当前有运行中的事件循环，会抛 RuntimeError
        asyncio.get_running_loop()
    except RuntimeError:
        # 没有运行中的事件循环，可以安全地使用 asyncio.run
        return asyncio.run(
            StructureEngine.parse(
                content=content,
                model_class=model_class,
                raw_tool_calls=tool_calls,
            )
        )
    else:
        # 已在异步环境中，禁止同步封装，避免嵌套事件循环
        raise RuntimeError(
            "parse_structured_output() 不能在已有事件循环中调用，请使用 "
            "`await StructureEngine.parse(...)`。"
        )


def extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
    """
    从文本中提取第一个“看起来可靠”的 JSON 对象（轻量工具）

    参数:
        text: 文本内容（通常是 LLM 输出）

    返回:
        - 成功时返回 dict
        - 如果未找到合适的 JSON 对象，则返回 None

    实现策略（比 StructureEngine.parse 更轻量）：
        1. 直接整体 json.loads，且要求结果为 dict
        2. 从 ```json ...``` / ``` ...``` 代码块中尝试解析
        3. 使用 extract_braced_json 提取 `{...}` 片段并尝试解析
        4. 使用 extract_bracket_json 提取 `[...]` 片段：
           - 如果数组元素为对象，则返回第一个对象
    """
    text = (text or "").strip()
    if not text:
        return None

    # 1) 尝试整体解析
    try:
        obj = json.loads(text)
        if isinstance(obj, dict):
            return obj
    except json.JSONDecodeError:
        pass

    # 2) 尝试 Markdown 代码块
    pattern = r"```(?:json)?\s*([\s\S]*?)```"
    for match in re.finditer(pattern, text):
        try:
            obj = json.loads(match.group(1).strip())
            if isinstance(obj, dict):
                return obj
        except json.JSONDecodeError:
            continue

    # 3) 尝试 `{...}` 片段
    obj_candidates = extract_braced_json(text)
    for candidate in obj_candidates:
        try:
            obj = json.loads(candidate)
            if isinstance(obj, dict):
                return obj
        except json.JSONDecodeError:
            continue

    # 4) 尝试 `[...]` 片段（数组），返回第一个 dict 元素
    array_candidates = extract_bracket_json(text)
    for candidate in array_candidates:
        try:
            arr = json.loads(candidate)
            if isinstance(arr, list) and arr and isinstance(arr[0], dict):
                return arr[0]
        except json.JSONDecodeError:
            continue

    return None
```

[72] gecko/core/telemetry.py
```python
# gecko/core/telemetry.py
"""
OpenTelemetry 集成模块

提供分布式追踪能力，支持：
- 自动 Span 创建
- 上下文传播
- 与主流 APM 系统集成
"""

from __future__ import annotations

import functools
from contextlib import asynccontextmanager, contextmanager
from contextvars import ContextVar
from typing import Any, Callable, Dict, Optional, TypeVar, Union

from gecko.core.logging import get_logger

logger = get_logger(__name__)

from opentelemetry import trace
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider as SDKTracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, SpanExporter
from opentelemetry.trace import Span, StatusCode, Tracer

# 使用新的常量方式而非 ResourceAttributes
OTEL_AVAILABLE = True

T = TypeVar("T")

# 请求 ID 上下文
request_id_var: ContextVar[str] = ContextVar("gecko_request_id", default="")


class TelemetryConfig:
    """遥测配置"""

    def __init__(
        self,
        service_name: str = "gecko",
        service_version: str = "0.2.0",
        environment: str = "development",
        enabled: bool = True,
        exporter: Optional[SpanExporter] = None,
        # 新增配置项
        batch_export_interval_ms: int = 5000,
        max_export_batch_size: int = 512,
    ):
        self.service_name = service_name
        self.service_version = service_version
        self.environment = environment
        self.enabled = enabled and OTEL_AVAILABLE
        self.exporter = exporter
        self.batch_export_interval_ms = batch_export_interval_ms
        self.max_export_batch_size = max_export_batch_size


class GeckoTelemetry:
    """
    Gecko 遥测管理器

    示例:
```python
    # 初始化
    telemetry = GeckoTelemetry(TelemetryConfig(
        service_name="my-agent-service",
        environment="production"
    ))
    telemetry.setup()

    # 使用装饰器
    @telemetry.trace_async("process_request")
    async def process_request(data):
        ...

    # 使用上下文管理器
    async with telemetry.async_span("custom_operation") as span:
        if span:
            span.set_attribute("custom.key", "value")
        await do_something()
```
    """

    def __init__(self, config: Optional[TelemetryConfig] = None):
        self.config = config or TelemetryConfig()
        self._tracer: Optional[Tracer] = None
        self._provider: Optional[SDKTracerProvider] = None
        self._initialized = False

    def setup(self) -> None:
        """初始化 OpenTelemetry"""
        if not self.config.enabled:
            logger.info("Telemetry disabled")
            return

        if not OTEL_AVAILABLE:
            logger.warning(
                "OpenTelemetry not installed. Install with: "
                "pip install opentelemetry-api opentelemetry-sdk"
            )
            return

        try:
            # 创建 Resource - 使用字符串常量替代废弃的 ResourceAttributes
            resource = Resource.create(
                {
                    "service.name": self.config.service_name,
                    "service.version": self.config.service_version,
                    "deployment.environment": self.config.environment,
                }
            )

            # 创建 TracerProvider
            self._provider = SDKTracerProvider(resource=resource)

            # 添加 Exporter
            if self.config.exporter:
                processor = BatchSpanProcessor(
                    self.config.exporter,
                    schedule_delay_millis=self.config.batch_export_interval_ms,
                    max_export_batch_size=self.config.max_export_batch_size,
                )
                self._provider.add_span_processor(processor)

            # 设置全局 Provider
            trace.set_tracer_provider(self._provider)

            # 获取 Tracer
            self._tracer = trace.get_tracer(
                self.config.service_name, self.config.service_version
            )

            self._initialized = True
            logger.info(
                "Telemetry initialized",
                service=self.config.service_name,
                environment=self.config.environment,
            )

        except Exception as e:
            logger.error("Failed to initialize telemetry", error=str(e), exc_info=True)

    def shutdown(self) -> None:
        """关闭遥测并刷新所有 span"""
        if self._provider:
            try:
                self._provider.shutdown()
                logger.info("Telemetry shutdown successfully")
            except Exception as e:
                logger.error("Failed to shutdown telemetry", error=str(e))

    @property
    def tracer(self) -> Optional[Tracer]:
        """获取 Tracer"""
        return self._tracer

    @property
    def is_enabled(self) -> bool:
        """是否已启用"""
        return self._initialized and self._tracer is not None

    # ==================== Span 创建 ==================== 

    @contextmanager
    def span(
        self,
        name: str,
        attributes: Optional[Dict[str, Any]] = None,
        kind: Optional[trace.SpanKind] = None,
    ):
        """
        创建 Span(同步上下文管理器)
        
        Args:
            name: Span 名称
            attributes: 初始属性
            kind: Span 类型
        """
        if not self.is_enabled:
            yield None
            return

        span_kind = kind or trace.SpanKind.INTERNAL

        with self._tracer.start_as_current_span( # type: ignore
            name, kind=span_kind, attributes=attributes or {}
        ) as span:
            # 注入请求 ID
            request_id = request_id_var.get()
            if request_id:
                span.set_attribute("gecko.request_id", request_id)

            try:
                yield span
            except Exception as e:
                # 修复: 使用正确的 set_status 方式
                span.set_status(StatusCode.ERROR, str(e))
                span.record_exception(e)
                raise

    @asynccontextmanager
    async def async_span(
        self,
        name: str,
        attributes: Optional[Dict[str, Any]] = None,
        kind: Optional[trace.SpanKind] = None,
    ):
        """
        创建 Span(异步上下文管理器)
        
        Args:
            name: Span 名称
            attributes: 初始属性
            kind: Span 类型
        """
        if not self.is_enabled:
            yield None
            return

        span_kind = kind or trace.SpanKind.INTERNAL

        with self._tracer.start_as_current_span( # type: ignore
            name, kind=span_kind, attributes=attributes or {}
        ) as span:
            request_id = request_id_var.get()
            if request_id:
                span.set_attribute("gecko.request_id", request_id)

            try:
                yield span
            except Exception as e:
                span.set_status(StatusCode.ERROR, str(e))
                span.record_exception(e)
                raise

    # ==================== 装饰器 ====================

    def trace(
        self,
        name: Optional[str] = None,
        attributes: Optional[Dict[str, Any]] = None,
    ) -> Callable:
        """
        同步函数追踪装饰器
        
        Args:
            name: 自定义 Span 名称,默认使用函数名
            attributes: 初始属性
        """

        def decorator(func: Callable[..., T]) -> Callable[..., T]:
            span_name = name or f"{func.__module__}.{func.__qualname__}"

            @functools.wraps(func)
            def wrapper(*args: Any, **kwargs: Any) -> T:
                with self.span(span_name, attributes):
                    return func(*args, **kwargs)

            return wrapper

        return decorator

    def trace_async(
        self,
        name: Optional[str] = None,
        attributes: Optional[Dict[str, Any]] = None,
    ) -> Callable:
        """
        异步函数追踪装饰器
        
        Args:
            name: 自定义 Span 名称,默认使用函数名
            attributes: 初始属性
        """

        def decorator(func: Callable[..., T]) -> Callable[..., T]:
            span_name = name or f"{func.__module__}.{func.__qualname__}"

            @functools.wraps(func)
            async def wrapper(*args: Any, **kwargs: Any) -> T:
                async with self.async_span(span_name, attributes):
                    return await func(*args, **kwargs) # type: ignore

            return wrapper # type: ignore

        return decorator

    # ==================== 工具方法 ====================

    def set_request_id(self, request_id: str) -> None:
        """设置当前请求 ID"""
        request_id_var.set(request_id)

    def get_request_id(self) -> str:
        """获取当前请求 ID"""
        return request_id_var.get()

    def add_event(
        self, name: str, attributes: Optional[Dict[str, Any]] = None
    ) -> None:
        """向当前 Span 添加事件"""
        if not self.is_enabled:
            return

        current_span = trace.get_current_span()
        if current_span and current_span.is_recording():
            current_span.add_event(name, attributes=attributes or {})

    def set_attribute(self, key: str, value: Any) -> None:
        """向当前 Span 设置属性"""
        if not self.is_enabled:
            return

        current_span = trace.get_current_span()
        if current_span and current_span.is_recording():
            current_span.set_attribute(key, value)

    def get_current_span(self) -> Optional[Span]:
        """获取当前活动的 Span"""
        if not self.is_enabled:
            return None
        return trace.get_current_span()


# 全局遥测实例
_telemetry: Optional[GeckoTelemetry] = None


def get_telemetry() -> GeckoTelemetry:
    """获取全局遥测实例"""
    global _telemetry
    if _telemetry is None:
        _telemetry = GeckoTelemetry()
    return _telemetry


def configure_telemetry(config: TelemetryConfig) -> GeckoTelemetry:
    """配置并初始化遥测"""
    global _telemetry
    _telemetry = GeckoTelemetry(config)
    _telemetry.setup()
    return _telemetry


# ==================== 预置 Span 名称 ====================


class SpanNames:
    """预定义的 Span 名称常量"""

    # Agent
    AGENT_RUN = "gecko.agent.run"
    AGENT_STREAM = "gecko.agent.stream"

    # Engine
    ENGINE_STEP = "gecko.engine.step"
    ENGINE_TOOL_CALL = "gecko.engine.tool_call"

    # Workflow
    WORKFLOW_EXECUTE = "gecko.workflow.execute"
    WORKFLOW_NODE = "gecko.workflow.node"

    # Storage
    STORAGE_GET = "gecko.storage.get"
    STORAGE_SET = "gecko.storage.set"
    STORAGE_DELETE = "gecko.storage.delete"

    # Model
    MODEL_COMPLETION = "gecko.model.completion"
    MODEL_STREAM = "gecko.model.stream"
    MODEL_EMBEDDING = "gecko.model.embedding"


# ==================== 导出 ====================

__all__ = [
    "GeckoTelemetry",
    "TelemetryConfig",
    "SpanNames",
    "get_telemetry",
    "configure_telemetry",
    "request_id_var",
    "OTEL_AVAILABLE",
]
```

[73] gecko/core/toolbox.py
```python
# gecko/core/toolbox.py
"""
ToolBox - Agent 工具箱

核心功能：
1. 工具注册与管理（支持实例注入与注册表加载）
2. 单个/批量工具执行
3. 并发控制与超时管理
4. 执行统计与监控
5. OpenAI Function Calling Schema 生成

优化日志：
- [Refactor] 集成 ToolRegistry，支持通过字符串名称加载工具
- [Refactor] 适配新版 BaseTool 接口
- [Fix] 修复并发控制的信号量使用方式
- [Feat] 线程安全的统计数据
- [Fix] 补全 get_summary, reset_stats 及魔术方法
"""
from __future__ import annotations

import asyncio
import threading
import time
from collections import defaultdict
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union

from anyio import create_task_group, fail_after, Semaphore
from anyio import get_cancelled_exc_class

from gecko.config import settings
from gecko.core.exceptions import ToolError, ToolNotFoundError
from gecko.core.logging import get_logger
from gecko.plugins.tools.base import BaseTool, ToolResult
from gecko.plugins.tools.registry import ToolRegistry

logger = get_logger(__name__)


# ===== 返回值模型 =====

@dataclass
class ToolExecutionResult:
    """
    工具执行结果（ToolBox 层面的封装）
    
    包含工具本身的返回内容，以及 ToolBox 记录的执行元数据（耗时、ID等）。
    
    属性:
        tool_name: 工具名称
        call_id: 调用 ID（用于关联请求）
        result: 执行结果（成功时为字符串，失败时为错误信息）
        is_error: 是否执行失败
        duration: 执行耗时（秒）
        metadata: 附加信息
    """
    tool_name: str
    call_id: str
    result: str
    is_error: bool
    duration: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """转换为字典格式"""
        return {
            "tool_name": self.tool_name,
            "call_id": self.call_id,
            "result": self.result,
            "is_error": self.is_error,
            "duration": self.duration,
            "metadata": self.metadata,
        }


# ===== 工具箱主类 =====

class ToolBox:
    """
    Agent 工具箱
    
    负责工具的注册、执行、并发控制和统计。
    既支持直接传入 BaseTool 实例，也支持通过名称从 ToolRegistry 加载。
    
    示例:
        ```python
        # 混合加载工具
        toolbox = ToolBox(
            tools=["calculator", MyCustomTool()],
            max_concurrent=5
        )
        
        # 执行
        result = await toolbox.execute("calculator", {"expression": "1+1"})
        
        # 获取统计
        summary = toolbox.get_summary()
        print(f"Success Rate: {summary['overall_success_rate']:.2%}")
        ```
    """

    def __init__(
        self,
        tools: Optional[List[Union[BaseTool, str]]] = None,
        max_concurrent: int = 5,
        default_timeout: Optional[float] = None,
        enable_retry: bool = False,
        max_retries: int = 2,
    ):
        """
        初始化工具箱
        
        参数:
            tools: 初始工具列表（支持 BaseTool 实例或注册表中的字符串名称）
            max_concurrent: 最大并发执行数
            default_timeout: 默认超时时间（秒）
            enable_retry: 是否启用重试
            max_retries: 最大重试次数
        """
        # 工具存储
        self._tools: Dict[str, BaseTool] = {}
        
        # 配置
        self.max_concurrent = max_concurrent
        self.default_timeout = default_timeout or settings.tool_execution_timeout
        self.enable_retry = enable_retry
        self.max_retries = max_retries
        
        # 统计数据（线程安全）
        self._stats_lock = threading.Lock()
        self._execution_count: Dict[str, int] = defaultdict(int)
        self._error_count: Dict[str, int] = defaultdict(int)
        self._total_time: Dict[str, float] = defaultdict(float)
        
        # 注册初始工具
        if tools:
            for item in tools:
                self.add_tool(item)
    
    # ====================== 工具管理 ======================
    
    def add_tool(self, item: Union[BaseTool, str], **kwargs) -> "ToolBox":
        """
        添加工具（高层接口）
        
        支持：
        1. 字符串：从 ToolRegistry 加载
        2. 实例：直接注册
        
        参数:
            item: 工具名称或实例
            **kwargs: 如果是字符串加载，kwargs 将传递给工具构造函数
        """
        tool_instance: Optional[BaseTool] = None
        
        if isinstance(item, str):
            try:
                tool_instance = ToolRegistry.load_tool(item, **kwargs)
            except Exception as e:
                logger.error(f"Failed to load tool '{item}' from registry: {e}")
                # 此时可以选择抛出异常，或者仅记录错误跳过，这里选择抛出以便尽早发现配置错误
                raise ToolNotFoundError(f"Registry load failed for '{item}': {e}") from e
        elif isinstance(item, BaseTool):
            tool_instance = item
        else:
            raise TypeError(f"Tool must be BaseTool or str, got {type(item)}")
            
        if tool_instance:
            self.register(tool_instance)
            
        return self

    def register(self, tool: BaseTool, replace: bool = True) -> "ToolBox":
        """
        注册工具实例（底层接口）
        
        参数:
            tool: 工具实例
            replace: 是否替换同名工具
        """
        if not isinstance(tool, BaseTool):
            raise TypeError(f"Tool must inherit from BaseTool, got {type(tool)}")
        
        if tool.name in self._tools and not replace:
            raise ValueError(f"Tool '{tool.name}' already registered.")
        
        self._tools[tool.name] = tool
        
        # 初始化统计
        with self._stats_lock:
            if tool.name not in self._execution_count:
                self._execution_count[tool.name] = 0
                self._error_count[tool.name] = 0
                self._total_time[tool.name] = 0.0
        
        logger.debug("Tool registered", tool_name=tool.name)
        return self
    
    def unregister(self, tool_name: str) -> "ToolBox":
        """注销工具"""
        if tool_name in self._tools:
            del self._tools[tool_name]
            logger.info("Tool unregistered", tool_name=tool_name)
        return self
    
    def get(self, name: str) -> Optional[BaseTool]:
        """获取工具实例"""
        return self._tools.get(name)
    
    def list_tools(self) -> List[BaseTool]:
        """获取所有已注册工具"""
        return list(self._tools.values())
    
    def has_tool(self, name: str) -> bool:
        """检查工具是否存在"""
        return name in self._tools
    
    # ====================== Schema 生成 ======================
    
    def to_openai_schema(self) -> List[Dict[str, Any]]:
        """
        生成 OpenAI Function Calling Schema
        
        直接调用 BaseTool.openai_schema 属性
        """
        return [t.openai_schema for t in self._tools.values()]
    
    # ====================== 执行逻辑 ======================
    
    async def execute(
        self,
        name: str,
        arguments: Dict[str, Any],
        timeout: Optional[float] = None,
        call_id: str = "",
    ) -> str:
        """
        执行单个工具（简易版）
        
        返回:
            结果字符串
        """
        # [新增] 检查是否存在解析层传递下来的错误标记
        if "__gecko_parse_error__" in arguments:
            # 直接返回错误信息作为 Tool Output
            # 这比抛出异常更有效，因为 LLM 可以读取这个 Output 并尝试自我修正
            return f"System Error: Failed to parse arguments. {arguments['__gecko_parse_error__']} \
                Please correct your JSON format."

        result = await self.execute_with_result(name, arguments, timeout, call_id)
        if result.is_error:
            raise ToolError(
                f"Tool execution failed: {result.result}",
                context={"tool": name, "args": arguments}
            )
        return result.result
    
    async def execute_with_result(
        self,
        name: str,
        arguments: Dict[str, Any],
        timeout: Optional[float] = None,
        call_id: str = "",
    ) -> ToolExecutionResult:
        """
        执行单个工具（完整版）
        """
        # [修复] 优先处理 Engine 层传递下来的解析错误
        # 这种情况下 arguments 只有这个特殊的 key
        if "__gecko_parse_error__" in arguments:
            error_msg = (
                f"System Error: Failed to parse arguments JSON. "
                f"{arguments['__gecko_parse_error__']} "
                "Please check your output format and retry."
            )
            # 记录一次错误统计
            self._update_stats(name, 0.0, is_error=True)
            
            return ToolExecutionResult(
                tool_name=name,
                call_id=call_id,
                result=error_msg,
                is_error=True,
                duration=0.0
            )
        
        tool = self.get(name)
        if not tool:
            self._update_stats(name, 0, is_error=True)
            raise ToolNotFoundError(name)
        
        # ... (后续原有逻辑保持不变: retry, execute_once, stats update) ...
        actual_timeout = timeout or self.default_timeout
        start_time = time.time()
        
        if self.enable_retry:
            result_str, is_error = await self._execute_with_retry(
                tool, arguments, actual_timeout
            )
        else:
            result_str, is_error = await self._execute_once(
                tool, arguments, actual_timeout
            )
            
        duration = time.time() - start_time
        self._update_stats(name, duration, is_error)
        
        return ToolExecutionResult(
            tool_name=name,
            call_id=call_id,
            result=result_str,
            is_error=is_error,
            duration=duration
        )

    async def _execute_once(
        self,
        tool: BaseTool,
        arguments: Dict[str, Any],
        timeout: float
    ) -> tuple[str, bool]:
        """
        单次执行封装
        
        处理超时和异常，适配 BaseTool 的 ToolResult 返回值
        """
        try:
            with fail_after(timeout):
                # BaseTool.execute 已经处理了参数校验和内部异常，返回 ToolResult
                res: ToolResult = await tool.execute(arguments)
                return res.content, res.is_error
                
        except TimeoutError:
            return f"Execution timed out after {timeout}s", True
        except get_cancelled_exc_class():
            return "Execution cancelled", True
        except Exception as e:
            logger.exception("Unexpected tool execution error", tool=tool.name)
            return f"System error: {str(e)}", True

    async def _execute_with_retry(
        self,
        tool: BaseTool,
        arguments: Dict[str, Any],
        timeout: float
    ) -> tuple[str, bool]:
        """带重试的执行逻辑"""
        last_result = ""
        
        for attempt in range(self.max_retries + 1):
            content, is_error = await self._execute_once(tool, arguments, timeout)
            
            if not is_error:
                return content, False
            
            last_result = content
            
            # 如果是超时或系统错误，尝试重试
            # 如果是 BaseTool 返回的业务逻辑错误（如参数不对），通常重试无用，但在通用层我们还是给机会
            if attempt < self.max_retries:
                wait_time = 2 ** attempt
                logger.warning(
                    "Tool failed, retrying",
                    tool=tool.name,
                    attempt=attempt + 1,
                    error=content[:100]
                )
                await asyncio.sleep(wait_time)
        
        return last_result, True

    # ====================== 批量执行 ======================
    
    async def execute_many(
        self,
        tool_calls: List[Dict[str, Any]],
        timeout: Optional[float] = None
    ) -> List[ToolExecutionResult]:
        """
        并发批量执行
        
        参数:
            tool_calls: [{"name": "...", "arguments": {...}, "id": "..."}, ...]
            
        返回:
            结果列表（顺序与输入一致）
        """
        if not tool_calls:
            return []
            
        results: List[Optional[ToolExecutionResult]] = [None] * len(tool_calls)
        
        # 使用 anyio 信号量控制并发
        # max_concurrent <= 0 视为“无限制”(不使用信号量)
        semaphore: Optional[Semaphore]
        if self.max_concurrent and self.max_concurrent > 0:
            semaphore = Semaphore(self.max_concurrent)
        else:
            semaphore = None

        async def _run_single(idx: int, call: Dict[str, Any]):
            name = call.get("name", "")
            args = call.get("arguments", {})
            cid = call.get("id", "")
            
            if not name:
                results[idx] = ToolExecutionResult(
                    tool_name="unknown", call_id=cid, result="Missing tool name", is_error=True
                )
                return

            try:
                # 复用 execute_with_result 以获得完整的统计和重试支持
                results[idx] = await self.execute_with_result(name, args, timeout, cid)
            except Exception as e:
                results[idx] = ToolExecutionResult(
                    tool_name=name, call_id=cid, result=f"Batch error: {e}", is_error=True
                )

        async def _worker(idx: int, call: Dict[str, Any]):
            if semaphore is not None:
                async with semaphore:
                    await _run_single(idx, call)
            else:
                await _run_single(idx, call)

        async with create_task_group() as tg:
            for i, call in enumerate(tool_calls):
                tg.start_soon(_worker, i, call)
                
        # 过滤 None (理论上不应该存在)
        return [r for r in results if r is not None]

    # ====================== 统计与辅助 ======================
    
    def _update_stats(self, name: str, duration: float, is_error: bool):
        """更新统计数据（线程安全）"""
        with self._stats_lock:
            self._execution_count[name] += 1
            self._total_time[name] += duration
            if is_error:
                self._error_count[name] += 1
    
    def get_stats(self) -> Dict[str, Dict[str, Any]]:
        """
        获取详细统计快照
        
        修复: 在持有锁时先复制 keys，避免迭代过程中字典变化
        """
        with self._stats_lock:
            # ✅ 修复: 先获取 keys 快照
            tool_names = list(self._tools.keys())
            
            stats = {}
            for name in tool_names:
                # ✅ 修复: 防御性检查，工具可能在快照后被移除
                if name not in self._execution_count:
                    continue
                    
                cnt = self._execution_count[name]
                err = self._error_count[name]
                total = self._total_time[name]
                
                stats[name] = {
                    "calls": cnt,
                    "errors": err,
                    "avg_time": (total / cnt) if cnt > 0 else 0.0,
                    "success_rate": ((cnt - err) / cnt) if cnt > 0 else 1.0
                }
            return stats
            
    def print_stats(self):
        """打印格式化统计信息"""
        stats = self.get_stats()
        print("\n=== ToolBox Statistics ===")
        if not stats:
            print("No tools executed.")
        for name, data in stats.items():
            print(f"🔧 {name:<15} Calls: {data['calls']:<5} Errors: {data['errors']:<5} "
                  f"Avg: {data['avg_time']:.3f}s Rate: {data['success_rate']:.1%}")
        print("=" * 30 + "\n")
        
    def reset_stats(self):
        """
        重置所有统计数据
        """
        with self._stats_lock:
            self._execution_count.clear()
            self._error_count.clear()
            self._total_time.clear()
        logger.info("Statistics reset")

    def get_summary(self) -> Dict[str, Any]:
        """
        获取工具箱的全局摘要
        
        修复: 使用锁保护并快照数据
        """
        with self._stats_lock:
            # ✅ 修复: 在锁内完成所有计算
            tool_names = list(self._tools.keys())
            
            total_executions = sum(self._execution_count.get(n, 0) for n in tool_names)
            total_errors = sum(self._error_count.get(n, 0) for n in tool_names)
            total_time_sum = sum(self._total_time.get(n, 0.0) for n in tool_names)
        
        return {
            "tool_count": len(tool_names),
            "total_executions": total_executions,
            "total_errors": total_errors,
            "total_time": total_time_sum,
            "overall_success_rate": (
                (total_executions - total_errors) / total_executions
                if total_executions > 0 else 1.0
            ),
            "avg_time_per_call": (
                total_time_sum / total_executions
                if total_executions > 0 else 0.0
            ),
        }

    # ====================== 魔术方法 ======================

    def __repr__(self) -> str:
        return f"ToolBox(tools={len(self._tools)}, concurrent={self.max_concurrent})"
    
    def __len__(self) -> int:
        """返回已注册工具的数量"""
        return len(self._tools)
    
    def __contains__(self, tool_name: str) -> bool:
        """支持 'tool_name' in toolbox 语法"""
        return tool_name in self._tools
```

[74] gecko/core/tracing.py
```python
# gecko/core/tracing.py
"""
请求追踪模块（基于 logging 统一实现的版本）

本模块是对 gecko.core.logging 中「追踪上下文能力」的一个
**语义化薄封装**，主要解决两个问题：

1. 对上层调用者提供更语义化的 API：
   - generate_trace_id / generate_span_id
   - get_trace_id / get_span_id / get_context
   - trace_context / set_trace_context / clear_trace_context

2. 所有追踪状态（trace_id / span_id / extra_context）只维护一份：
   - 统一复用 gecko.core.logging 中已经定义好的 ContextVar
   - 不再在 tracing.py 内单独维护一套重复的 ContextVar
   - 避免「两份状态 + 同步」带来的复杂度和潜在不一致问题

设计要点：
- tracing.py 不再保存独立的状态，只是对 logging 中的能力进行包装
- 任何通过本模块设置的上下文，日志中都会自动体现（因为使用同一组 ContextVar）
- 任何通过 gecko.core.logging.trace_context 设置的上下文，
  也可以通过本模块的 get_trace_id / get_context 等函数访问到
"""

from __future__ import annotations

from contextlib import contextmanager
from typing import Any, Dict, Iterator, Optional

# 直接复用 logging 模块中的上下文变量与 ID 生成函数
from gecko.core.logging import (
    trace_id_var,        # ContextVar[str]
    span_id_var,         # ContextVar[str]
    extra_context_var,   # ContextVar[Dict[str, Any]]
    generate_trace_id as _generate_trace_id,
    generate_span_id as _generate_span_id,
    trace_context as _logging_trace_context,
)

# =========================================================
# 对外暴露的 ID 生成函数
# =========================================================

def generate_trace_id() -> str:
    """
    生成新的 trace_id。

    说明：
        - 直接复用 gecko.core.logging 中的实现
        - 返回值通常为 16 位十六进制字符串（uuid4 的截断）
        - 可用于与外部系统（如 APM、链路追踪系统）对接
    """
    return _generate_trace_id()


def generate_span_id() -> str:
    """
    生成新的 span_id。

    说明：
        - 直接复用 gecko.core.logging 中的实现
        - 返回值通常为 8 位十六进制字符串
        - 可用于标识单个子调用 / 子操作
    """
    return _generate_span_id()


# =========================================================
# 读取当前追踪上下文状态
# =========================================================

def get_trace_id() -> str:
    """
    获取当前上下文中的 trace_id。

    说明：
        - 如果当前没有设置 trace_id，则返回默认空字符串 ""
        - trace_id 的值来源：
            * 通过本模块的 trace_context / set_trace_context 设置
            * 或通过 gecko.core.logging.trace_context 直接设置
    """
    return trace_id_var.get()


def get_span_id() -> str:
    """
    获取当前上下文中的 span_id。

    说明：
        - 如果当前没有设置 span_id，则返回默认空字符串 ""
        - span_id 的值来源同 get_trace_id
    """
    return span_id_var.get()


def get_context() -> Dict[str, Any]:
    """
    获取当前追踪上下文的完整字典表示。

    返回字段（存在则返回，不存在则省略）：
        - "trace_id": 当前 trace_id
        - "span_id": 当前 span_id
        - 额外上下文字段（extra_context_var 中的内容），如：
            * "user_id"
            * "request_id"
            * "action"
            * ...

    示例：
        >>> get_context()
        {
            "trace_id": "abcd1234ef567890",
            "span_id": "1234abcd",
            "user_id": "u-001",
            "action": "login"
        }
    """
    ctx: Dict[str, Any] = {}

    trace_id = trace_id_var.get()
    if trace_id:
        ctx["trace_id"] = trace_id

    span_id = span_id_var.get()
    if span_id:
        ctx["span_id"] = span_id

    extra = extra_context_var.get()
    if extra:
        ctx.update(extra)

    return ctx


# =========================================================
# 追踪上下文管理器（语义化封装）
# =========================================================

@contextmanager
def trace_context(
    trace_id: Optional[str] = None,
    span_id: Optional[str] = None,
    **extra: Any,
) -> Iterator[Dict[str, Any]]:
    """
    追踪上下文管理器（统一封装，内部直接复用 logging.trace_context）。

    功能：
        - 自动生成或复用 trace_id / span_id
        - 设置额外的上下文字段（extra），如 user_id、request_id、action 等
        - 所有上下文状态都保存在 gecko.core.logging 中的 ContextVar 里
          -> 日志输出时会自动带上这些字段（使用 ContextLogger）

    参数：
        trace_id:
            可选。传入已有的 trace_id，用于链路复用；
            若不传，则优先使用当前上下文中的 trace_id，
            若仍为空，则调用 generate_trace_id() 生成新的。
        span_id:
            可选。传入已有的 span_id；
            若不传，则调用 generate_span_id() 生成新的。
        **extra:
            任意键值对，作为额外上下文字段注入，如：
                user_id="123",
                action="chat",
                request_id="req-001"

    返回（yield 的值）：
        一个 dict，包含：
            - "trace_id"
            - "span_id"
            - 以及传入的 extra 字段

    示例：
        >>> from gecko.core.tracing import trace_context, get_context
        >>> from gecko.core.logging import get_context_logger
        >>> logger = get_context_logger(__name__)

        with trace_context(user_id="123", action="chat") as ctx:
            # ctx = {"trace_id": "...", "span_id": "...", "user_id": "123", "action": "chat"}
            logger.info("Handling request")
            # 日志中会自动包含 trace_id / span_id / user_id / action

        # 上下文退出后，trace_id / span_id / extra 会自动恢复到进入 with 前的状态
    """
    # 这里不再重复实现 token/reset 逻辑，而是直接复用 logging.trace_context，
    # 确保两处行为完全一致，避免维护两份类似代码。
    #
    # 注意：_logging_trace_context 本身也是一个 @contextmanager，
    #       所以这里用嵌套 with 的方式做一层薄包装。
    with _logging_trace_context(trace_id=trace_id, span_id=span_id, **extra) as ctx:
        yield ctx


# =========================================================
# 非上下文管理器方式：直接设置 / 清除上下文
# =========================================================

def set_trace_context(
    trace_id: Optional[str] = None,
    span_id: Optional[str] = None,
    **extra: Any,
) -> None:
    """
    直接设置当前追踪上下文（非 with 方式）。

    典型使用场景：
        - 框架型代码在请求进入入口处统一设置上下文：
            * 从 HTTP 头、gRPC metadata 中解析出 trace_id
            * 根据请求信息设置 user_id / request_id 等
        - 后面业务代码不需要关心设置，只需通过日志或 get_context 使用即可

    参数：
        trace_id:
            可选。不为空时，直接覆盖当前 trace_id。
        span_id:
            可选。不为空时，直接覆盖当前 span_id。
        **extra:
            可选。传入的键值对会与当前 extra_context 合并（后者覆盖前者）。
    """
    if trace_id:
        trace_id_var.set(trace_id)

    if span_id:
        span_id_var.set(span_id)

    if extra:
        # 取出当前的 extra_context，copy 后合并新字段再 set 回去，
        # 避免对原字典的就地修改带来意外共享。
        current = dict(extra_context_var.get())
        current.update(extra)
        extra_context_var.set(current)


def clear_trace_context() -> None:
    """
    清除当前追踪上下文。

    行为：
        - 将 trace_id 重置为空字符串 ""
        - 将 span_id 重置为空字符串 ""
        - 将 extra_context 重置为一个新的空字典 {}

    注意：
        - 如果你在 with trace_context(...) 内调用 clear_trace_context()，
          上下文退出时（__exit__ 阶段）logging.trace_context 会使用 token.reset()
          将 ContextVar 恢复到进入 with 前的值，因此：
              * clear 的效果只在当前「层」中有效
              * 一旦退出 with，外层上下文会被恢复

        - 一般而言，clear_trace_context 更适合作为「最外层」的清理动作，
          例如在请求处理完毕、线程结束、协程回收时调用。
    """
    trace_id_var.set("")
    span_id_var.set("")
    extra_context_var.set({})


__all__ = [
    "generate_trace_id",
    "generate_span_id",
    "get_trace_id",
    "get_span_id",
    "get_context",
    "trace_context",
    "set_trace_context",
    "clear_trace_context",
]
```

[75] gecko/core/types.py
```python
# gecko/core/types.py
"""
类型定义模块

提供框架级别的类型定义。
"""
from __future__ import annotations

from typing import Any, Dict, Generic, List, Literal, Optional, TypeVar, Union

try:
    from typing import TypedDict, NotRequired
except ImportError:
    from typing_extensions import TypedDict, NotRequired


# ==================== OpenAI 消息格式 ====================

class ToolFunctionDict(TypedDict):
    """工具函数"""
    name: str
    arguments: str


class ToolCallDict(TypedDict):
    """工具调用"""
    id: str
    type: Literal["function"]
    function: ToolFunctionDict


class MessageDict(TypedDict, total=False):
    """消息字典"""
    role: str
    content: Optional[Union[str, List[Dict[str, Any]]]]
    name: NotRequired[str]
    tool_calls: NotRequired[List[ToolCallDict]]
    tool_call_id: NotRequired[str]


class UsageDict(TypedDict, total=False):
    """Token 使用"""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


# ==================== Result 类型 ====================

T = TypeVar("T")
E = TypeVar("E", bound=Exception)


class Result(Generic[T]):
    """
    结果包装器
    
    示例:
        ```python
        def divide(a: int, b: int) -> Result[float]:
            if b == 0:
                return Result.err("Division by zero")
            return Result.ok(a / b)
        
        result = divide(10, 2)
        if result.is_ok:
            print(result.unwrap())
        ```
    """

    __slots__ = ("_value", "_error", "_is_ok")

    def __init__(
        self,
        value: Optional[T] = None,
        error: Optional[str] = None,
        is_ok: bool = True
    ):
        self._value = value
        self._error = error
        self._is_ok = is_ok

    @classmethod
    def ok(cls, value: T) -> "Result[T]":
        """创建成功结果"""
        return cls(value=value, is_ok=True)

    @classmethod
    def err(cls, error: str) -> "Result[T]":
        """创建错误结果"""
        return cls(error=error, is_ok=False)

    @property
    def is_ok(self) -> bool:
        return self._is_ok

    @property
    def is_err(self) -> bool:
        return not self._is_ok

    @property
    def error(self) -> Optional[str]:
        return self._error

    def unwrap(self) -> T:
        """获取值（错误时抛异常）"""
        if not self._is_ok:
            raise ValueError(f"Cannot unwrap error: {self._error}")
        return self._value  # type: ignore

    def unwrap_or(self, default: T) -> T:
        """获取值或默认值"""
        return self._value if self._is_ok else default  # type: ignore

    def __repr__(self) -> str:
        if self._is_ok:
            return f"Ok({self._value!r})"
        return f"Err({self._error!r})"


# ==================== 类型别名 ====================

MessageList = List[MessageDict]
ToolCallList = List[ToolCallDict]
Embedding = List[float]
EmbeddingBatch = List[Embedding]


__all__ = [
    "MessageDict",
    "ToolCallDict",
    "ToolFunctionDict",
    "UsageDict",
    "Result",
    "MessageList",
    "ToolCallList",
    "Embedding",
    "EmbeddingBatch",
]
```

[76] gecko/core/utils.py
```python
# gecko/core/utils.py
"""
通用工具函数库

提供框架常用的工具函数，包括：
- 异步/同步统一处理
- 重试机制
- 超时控制
- 数据转换
- 字符串处理
- 装饰器

优化点：
1. 扩展工具函数集合
2. 添加超时和重试支持
3. 提供数据转换工具
4. 添加常用装饰器
"""
from __future__ import annotations

import asyncio
import functools
import hashlib
import inspect
import json
import time
from typing import Any, Awaitable, Callable, Dict, List, Optional, TypeVar, Union

from gecko.core.logging import get_logger

logger = get_logger(__name__)

T = TypeVar("T")
R = TypeVar("R")


# ===== 异步/同步统一处理 =====

async def ensure_awaitable(
    func: Callable[..., T | Awaitable[T]],
    *args,
    timeout: Optional[float] = None,
    **kwargs
) -> T:
    """
    统一处理同步/异步函数调用
    
    参数:
        func: 可调用对象（同步或异步）
        *args: 位置参数
        timeout: 超时时间（秒），None 表示无限制
        **kwargs: 关键字参数
    
    返回:
        函数执行结果
    
    异常:
        asyncio.TimeoutError: 超时
        Exception: 函数执行异常
    
    示例:
        ```python
        # 同步函数
        result = await ensure_awaitable(sync_func, arg1, arg2)
        
        # 异步函数
        result = await ensure_awaitable(async_func, arg1, arg2)
        
        # 带超时
        result = await ensure_awaitable(func, timeout=5.0)
        ```
    """
    # 确定是否为协程函数
    if asyncio.iscoroutinefunction(func):
        coro = func(*args, **kwargs)
    else:
        result = func(*args, **kwargs)
        if asyncio.iscoroutine(result):
            coro = result
        else:
            # 同步函数，直接返回结果
            return result # type: ignore
    
    # 执行异步函数（带可选超时）
    if timeout:
        try:
            return await asyncio.wait_for(coro, timeout=timeout)
        except asyncio.TimeoutError:
            logger.warning(
                "Function execution timeout",
                func=getattr(func, "__name__", str(func)),
                timeout=timeout
            )
            raise
    else:
        return await coro


def run_sync(coro: Awaitable[T]) -> T:
    """
    在同步上下文中运行异步函数
    
    参数:
        coro: 协程对象
    
    返回:
        执行结果
    
    示例:
        ```python
        async def async_func():
            return "result"
        
        # 在同步代码中调用
        result = run_sync(async_func())
        ```
    """
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        # 没有运行中的 loop，直接 run
        return asyncio.run(coro) # type: ignore
    
    if loop.is_running():
        # ⚠️ 检测到嵌套循环
        logger.debug(
            "Running async code in sync context with nested loop. "
            "Trying to use nest_asyncio."
        )
        try:
            import nest_asyncio
            nest_asyncio.apply()
            return loop.run_until_complete(coro)
        except ImportError:
            raise RuntimeError(
                "Detected nested event loop. "
                "Please install 'nest_asyncio' to allow nested usage: pip install nest_asyncio"
            )
    else:
        return loop.run_until_complete(coro)


# ===== 重试机制 =====

async def retry_async(
    func: Callable[..., Awaitable[T]],
    *args,
    max_attempts: int = 3,
    delay: float = 1.0,
    backoff: float = 2.0,
    exceptions: tuple = (Exception,),
    on_retry: Optional[Callable[[int, Exception], None]] = None,
    **kwargs
) -> T:
    """
    异步函数重试装饰器
    
    参数:
        func: 异步函数
        *args: 位置参数
        max_attempts: 最大尝试次数
        delay: 初始延迟（秒）
        backoff: 退避倍数
        exceptions: 需要重试的异常类型
        on_retry: 重试时的回调函数
        **kwargs: 关键字参数
    
    返回:
        函数执行结果
    
    异常:
        最后一次尝试的异常
    
    示例:
        ```python
        async def unstable_api_call():
            # 可能失败的 API 调用
            ...
        
        result = await retry_async(
            unstable_api_call,
            max_attempts=5,
            delay=2.0,
            backoff=2.0
        )
        ```
    """
    last_exception = None
    current_delay = delay
    
    func_name = getattr(func, "__name__", str(func))
    
    for attempt in range(1, max_attempts + 1):
        try:
            return await func(*args, **kwargs)
        except exceptions as e:
            last_exception = e
            
            if attempt >= max_attempts:
                logger.error(
                    "All retry attempts failed",
                    func=func_name,
                    attempts=max_attempts,
                    error=str(e)
                )
                raise
            
            logger.warning(
                "Function failed, retrying",
                func=func_name,
                attempt=attempt,
                max_attempts=max_attempts,
                delay=current_delay,
                error=str(e)
            )
            
            if on_retry:
                try:
                    # 新增：检测回调是否为异步函数
                    if inspect.iscoroutinefunction(on_retry):
                        await on_retry(attempt, e)
                    else:
                        on_retry(attempt, e)
                except Exception as cb_err:
                    logger.warning("Retry callback failed", error=str(cb_err))
            
            await asyncio.sleep(current_delay)
            current_delay *= backoff
    
    raise last_exception # type: ignore


def retry(
    max_attempts: int = 3,
    delay: float = 1.0,
    backoff: float = 2.0,
    exceptions: tuple = (Exception,)
):
    """
    重试装饰器（支持同步和异步）
    
    参数:
        max_attempts: 最大尝试次数
        delay: 初始延迟（秒）
        backoff: 退避倍数
        exceptions: 需要重试的异常类型
    
    示例:
        ```python
        @retry(max_attempts=5, delay=2.0)
        async def unstable_function():
            # 可能失败的操作
            ...
        ```
    """
    def decorator(func: Callable) -> Callable:
        if asyncio.iscoroutinefunction(func):
            @functools.wraps(func)
            async def async_wrapper(*args, **kwargs):
                return await retry_async(
                    func,
                    *args,
                    max_attempts=max_attempts,
                    delay=delay,
                    backoff=backoff,
                    exceptions=exceptions,
                    **kwargs
                )
            return async_wrapper
        else:
            @functools.wraps(func)
            def sync_wrapper(*args, **kwargs):
                last_exception = None
                current_delay = delay
                
                for attempt in range(1, max_attempts + 1):
                    try:
                        return func(*args, **kwargs)
                    except exceptions as e:
                        last_exception = e
                        if attempt >= max_attempts:
                            raise
                        time.sleep(current_delay)
                        current_delay *= backoff
                
                raise last_exception # type: ignore
            
            return sync_wrapper
    
    return decorator


# ===== 数据转换 =====

def safe_dict(obj: Any, max_depth: int = 3, _current_depth: int = 0) -> Any:
    """
    安全地将对象转换为字典（递归处理）
    
    参数:
        obj: 要转换的对象
        max_depth: 最大递归深度
        _current_depth: 当前深度（内部使用）
    
    返回:
        字典或可序列化的值
    
    示例:
        ```python
        class MyClass:
            def __init__(self):
                self.name = "test"
                self.value = 123
        
        obj = MyClass()
        data = safe_dict(obj)
        # {"name": "test", "value": 123}
        ```
    """
    if _current_depth >= max_depth:
        return str(obj)[:100]
    
    # Pydantic 模型
    if hasattr(obj, "model_dump"):
        try:
            return obj.model_dump()
        except Exception:
            pass
    
    # 字典
    if isinstance(obj, dict):
        return {
            str(k): safe_dict(v, max_depth, _current_depth + 1)
            for k, v in obj.items()
        }
    
    # 列表/元组
    if isinstance(obj, (list, tuple)):
        return [safe_dict(item, max_depth, _current_depth + 1) for item in obj]
    
    # 基本类型
    if isinstance(obj, (str, int, float, bool, type(None))):
        return obj
    
    # 对象属性
    if hasattr(obj, "__dict__"):
        return {
            k: safe_dict(v, max_depth, _current_depth + 1)
            for k, v in obj.__dict__.items()
            if not k.startswith("_")
        }
    
    # 其他情况：转字符串
    return str(obj)[:200]


def merge_dicts(*dicts: Dict, deep: bool = False) -> Dict:
    """
    合并多个字典
    
    参数:
        *dicts: 要合并的字典
        deep: 是否深度合并
    
    返回:
        合并后的字典
    
    示例:
        ```python
        d1 = {"a": 1, "b": {"x": 1}}
        d2 = {"b": {"y": 2}, "c": 3}
        
        # 浅合并
        result = merge_dicts(d1, d2)
        # {"a": 1, "b": {"y": 2}, "c": 3}
        
        # 深合并
        result = merge_dicts(d1, d2, deep=True)
        # {"a": 1, "b": {"x": 1, "y": 2}, "c": 3}
        ```
    """
    if not dicts:
        return {}
    
    result = {}
    
    for d in dicts:
        if not isinstance(d, dict):
            continue
        
        for key, value in d.items():
            if deep and key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = merge_dicts(result[key], value, deep=True)
            else:
                result[key] = value
    
    return result


# ===== 字符串处理 =====

def truncate(
    text: str,
    max_length: int = 100,
    suffix: str = "..."
) -> str:
    """
    截断文本
    
    参数:
        text: 文本
        max_length: 最大长度
        suffix: 后缀
    
    返回:
        截断后的文本
    """
    if len(text) <= max_length:
        return text
    
    return text[:max_length - len(suffix)] + suffix


def format_size(size_bytes: int) -> str:
    """
    格式化字节大小
    
    参数:
        size_bytes: 字节数
    
    返回:
        可读的大小字符串
    
    示例:
        ```python
        format_size(1024)       # "1.00 KB"
        format_size(1048576)    # "1.00 MB"
        format_size(1073741824) # "1.00 GB"
        ```
    """
    for unit in ["B", "KB", "MB", "GB", "TB"]:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0 # type: ignore
    return f"{size_bytes:.2f} PB"


def format_duration(seconds: float) -> str:
    """
    格式化时长
    
    参数:
        seconds: 秒数
    
    返回:
        可读的时长字符串
    
    示例:
        ```python
        format_duration(65)    # "1m 5s"
        format_duration(3665)  # "1h 1m 5s"
        ```
    """
    if seconds < 60:
        return f"{seconds:.1f}s"
    
    minutes = int(seconds // 60)
    secs = int(seconds % 60)
    
    if minutes < 60:
        return f"{minutes}m {secs}s"
    
    hours = minutes // 60
    minutes = minutes % 60
    
    return f"{hours}h {minutes}m {secs}s"


def compute_hash(text: str, algorithm: str = "md5") -> str:
    """
    计算文本的哈希值
    
    参数:
        text: 文本
        algorithm: 算法（md5/sha1/sha256）
    
    返回:
        哈希值（十六进制字符串）
    
    示例:
        ```python
        hash_value = compute_hash("Hello, World!")
        ```
    """
    if algorithm == "md5":
        hasher = hashlib.md5()
    elif algorithm == "sha1":
        hasher = hashlib.sha1()
    elif algorithm == "sha256":
        hasher = hashlib.sha256()
    else:
        raise ValueError(f"不支持的哈希算法: {algorithm}")
    
    hasher.update(text.encode("utf-8"))
    return hasher.hexdigest()


# ===== 性能监控 =====

class Timer:
    """
    简单的计时器（上下文管理器）
    
    示例:
        ```python
        with Timer("操作名称") as t:
            # 执行耗时操作
            do_something()
        
        print(f"耗时: {t.elapsed:.2f}s")
        ```
    """
    
    def __init__(self, name: str = "Timer", log: bool = True):
        self.name = name
        self.log = log
        self.start_time = None
        self.end_time = None
        self.elapsed = 0.0
    
    def __enter__(self):
        self.start_time = time.time()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.time()
        self.elapsed = self.end_time - self.start_time # type: ignore
        
        if self.log:
            logger.info(
                "Timer completed",
                name=self.name,
                elapsed=f"{self.elapsed:.3f}s"
            )
        
        return False

def safe_json_loads(text: str, default: Any = None) -> Any:
    """
    安全地解析 JSON
    """
    try:
        return json.loads(text)
    except (json.JSONDecodeError, TypeError):
        return default

def timing(func: Callable) -> Callable:
    """
    计时装饰器（支持同步和异步）
    
    示例:
        ```python
        @timing
        async def slow_function():
            await asyncio.sleep(1)
        ```
    """
    if asyncio.iscoroutinefunction(func):
        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            start = time.time()
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                elapsed = time.time() - start
                logger.info(
                    "Function executed",
                    func=func.__name__,
                    elapsed=f"{elapsed:.3f}s"
                )
        return async_wrapper
    else:
        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            start = time.time()
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                elapsed = time.time() - start
                logger.info(
                    "Function executed",
                    func=func.__name__,
                    elapsed=f"{elapsed:.3f}s"
                )
        return sync_wrapper


# ===== 函数签名工具 =====

def get_function_args(func: Callable) -> List[str]:
    """
    获取函数的参数名列表
    
    参数:
        func: 函数对象
    
    返回:
        参数名列表
    """
    sig = inspect.signature(func)
    return [
        name for name, param in sig.parameters.items()
        if param.kind in (
            inspect.Parameter.POSITIONAL_OR_KEYWORD,
            inspect.Parameter.KEYWORD_ONLY,
        )
    ]


def has_argument(func: Callable, arg_name: str) -> bool:
    """
    检查函数是否有指定参数
    
    参数:
        func: 函数对象
        arg_name: 参数名
    
    返回:
        是否存在该参数
    """
    return arg_name in get_function_args(func)


# ===== 其他工具 =====

def chunk_list(lst: List[T], chunk_size: int) -> List[List[T]]:
    """
    将列表分块
    
    参数:
        lst: 列表
        chunk_size: 每块大小
    
    返回:
        分块后的列表
    
    示例:
        ```python
        chunks = chunk_list([1, 2, 3, 4, 5], chunk_size=2)
        # [[1, 2], [3, 4], [5]]
        ```
    """
    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]


def flatten_list(nested: List[List[T]]) -> List[T]:
    """
    展平嵌套列表
    
    参数:
        nested: 嵌套列表
    
    返回:
        展平后的列表
    
    示例:
        ```python
        flat = flatten_list([[1, 2], [3, 4], [5]])
        # [1, 2, 3, 4, 5]
        ```
    """
    return [item for sublist in nested for item in sublist]


def deduplicate(
    items: List[T],
    key: Optional[Callable[[T], Any]] = None
) -> List[T]:
    """
    列表去重（保持顺序）
    
    参数:
        items: 列表
        key: 可选的键函数
    
    返回:
        去重后的列表
    
    示例:
        ```python
        # 简单去重
        unique = deduplicate([1, 2, 2, 3, 1])
        # [1, 2, 3]
        
        # 按属性去重
        users = [{"id": 1, "name": "A"}, {"id": 1, "name": "B"}]
        unique = deduplicate(users, key=lambda u: u["id"])
        ```
    """
    seen = set()
    result = []
    
    for item in items:
        k = key(item) if key else item
        
        # 处理不可哈希的情况
        try:
            if k not in seen:
                seen.add(k)
                result.append(item)
        except TypeError:
            # 不可哈希，使用 == 比较
            if not any(k == s for s in seen):
                seen.add(k)
                result.append(item)
    
    return result

def safe_serialize_context(data: Any) -> Any:
    """
    [新增] 高性能序列化清洗工具
    
    功能：
    1. 递归遍历对象结构 (Dict, List, Pydantic)。
    2. 检测不可序列化的对象 (如 Lock, Socket)。
    3. 将不可序列化对象替换为标记字典，而不是直接抛错。
    4. 纯 Python 操作，便于卸载到 ThreadPool 执行。
    """
    def _clean(obj, depth=0):
        # 防止无限递归
        if depth > 20: 
            return str(obj)

        # 1. 基础类型直通
        if obj is None or isinstance(obj, (str, int, float, bool)):
            return obj
        
        # 2. 列表/元组处理
        if isinstance(obj, (list, tuple)):
            return [_clean(x, depth + 1) for x in obj]
        
        # 3. 字典处理
        if isinstance(obj, dict):
            new_obj = {}
            for k, v in obj.items():
                # 递归清洗 Value
                new_obj[str(k)] = _clean(v, depth + 1)
            return new_obj
        
        # 4. Pydantic 对象 (转为 dict 后继续清洗)
        if hasattr(obj, "model_dump"):
            return _clean(obj.model_dump(mode='python'), depth + 1)
            
        # 5. 其他对象：尝试检测是否可 JSON 序列化
        try:
            json.dumps(obj)
            return obj
        except (TypeError, OverflowError):
            # 6. [关键] 不可序列化对象，转换为特殊标记
            # 这样 Resume 时虽然无法恢复该对象，但至少不会导致整个流程崩溃
            # 同时也给调试留下了线索
            return {
                "__gecko_unserializable__": True,
                "type": type(obj).__name__,
                "repr": str(obj)[:100]
            }

    return _clean(data)


# ===== 向后兼容导出 =====

__all__ = [
    # 异步工具
    "ensure_awaitable",
    "run_sync",
    # 重试
    "retry",
    "retry_async",
    # 数据转换
    "safe_dict",
    "merge_dicts",
    "safe_serialize_context",
    # 字符串
    "truncate",
    "format_size",
    "format_duration",
    "compute_hash",
    # 性能
    "Timer",
    "timing",
    # 函数工具
    "get_function_args",
    "has_argument",
    # 列表工具
    "chunk_list",
    "flatten_list",
    "deduplicate",
    "safe_json_loads",
]
```

[77] gecko/plugins/__init__.py
```python
```

[78] gecko/plugins/base.py
```python
# gecko/plugins/base.py  
  
"""  
占位：未来可在此定义所有插件的抽象基类或通用接口。  
目前实际内容请参考各子目录（tools/ storage/ knowledge 等）。  
"""  
```

[79] gecko/plugins/guardrails/__init__.py
```python
# gecko/plugins/guardrails/__init__.py
"""
Guardrails 总入口

- InputSanitizer / ScanResult / ThreatLevel: 基础扫描与清洗（sanitizer.py）
- InputSanitizerMiddleware / SanitizationResult: 事件总线中间件版本（input_sanitizer.py）
"""
from gecko.plugins.guardrails.sanitizer import (
    InputSanitizer,
    ScanResult,
    ThreatLevel,
)
from gecko.plugins.guardrails.input_sanitizer import (
    InputSanitizerMiddleware,
    SanitizationResult,
)

__all__ = [
    "InputSanitizer",
    "ScanResult",
    "ThreatLevel",
    "InputSanitizerMiddleware",
    "SanitizationResult",
]
```

[80] gecko/plugins/guardrails/input_sanitizer.py
```python
# gecko/plugins/guardrails/input_sanitizer.py
"""
输入安全清洗中间件

提供基础的 Prompt Injection 检测和清洗能力。
"""
from __future__ import annotations

import re
from typing import Any, Dict, List, Optional, Set
from enum import Enum

from pydantic import BaseModel, Field

from gecko.core.events import BaseEvent
from gecko.core.logging import get_logger
from gecko.plugins.guardrails.sanitizer import InputSanitizer as PublicInputSanitizer

logger = get_logger(__name__)

class ThreatLevel(str, Enum):
    """威胁等级"""
    NONE = "none"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class SanitizationResult(BaseModel):
    """清洗结果"""
    original_text: str
    sanitized_text: str
    threat_level: ThreatLevel = ThreatLevel.NONE
    detected_patterns: List[str] = Field(default_factory=list)
    was_modified: bool = False


class InputSanitizer:
    """
    输入清洗器
    
    检测并处理潜在的 Prompt Injection 攻击模式。
    
    使用示例:
        ```python
        sanitizer = InputSanitizer()
        result = sanitizer.sanitize("Ignore previous instructions and...")
        
        if result.threat_level in (ThreatLevel.HIGH, ThreatLevel.CRITICAL):
            logger.warning("Potential attack detected", patterns=result.detected_patterns)
        ```
    """
    
    # 高危模式（可能的指令覆盖尝试）
    HIGH_RISK_PATTERNS = [
        # 指令覆盖
        (r"ignore\s+(all\s+)?(previous|prior|above)\s+(instructions?|rules?|prompts?)", "instruction_override"),
        (r"disregard\s+(all\s+)?(previous|prior|above)", "instruction_override"),
        (r"forget\s+(everything|all|what)\s+(you|i)\s+(told|said)", "instruction_override"),
        
        # 角色劫持
        (r"you\s+are\s+now\s+(a|an|in)\s+\w+\s+mode", "role_hijack"),
        (r"pretend\s+(to\s+be|you\s+are)\s+(a|an)", "role_hijack"),
        (r"act\s+as\s+(if|though)\s+you", "role_hijack"),
        
        # 系统提示泄露
        (r"(show|reveal|display|print|output)\s+(your|the)\s+(system|initial)\s+(prompt|instructions?)", "prompt_leak"),
        (r"what\s+(are|is)\s+your\s+(system|initial|original)\s+(prompt|instructions?)", "prompt_leak"),
    ]
    
    # 中危模式（可疑但可能合法）
    MEDIUM_RISK_PATTERNS = [
        # 特殊标记
        (r"\[/?INST\]", "special_token"),
        (r"<\|?(system|user|assistant|im_start|im_end)\|?>", "special_token"),
        (r"###\s*(System|Human|Assistant|User):", "special_token"),
        
        # 编码绕过尝试
        (r"base64[:=]", "encoding_bypass"),
        (r"\\x[0-9a-fA-F]{2}", "hex_encoding"),
        (r"&#\d+;", "html_entity"),
    ]
    
    # 低危模式（需要上下文判断）
    LOW_RISK_PATTERNS = [
        (r"system:\s*\w+", "system_prefix"),
        (r"</?(s|b|i|u|code|pre)>", "html_tag"),
    ]

    
    def __init__(
        self,
        enable_sanitization: bool = True,
        log_detections: bool = True,
        block_high_risk: bool = False,
        custom_patterns: Optional[List[tuple]] = None,
    ):
        """
        初始化清洗器

        参数:
            enable_sanitization: 是否启用清洗（False 则仅检测）
            log_detections: 是否记录检测结果
            block_high_risk: 是否阻止高危输入（抛出异常）
            custom_patterns: 自定义检测模式列表

        自定义模式支持两种形式：
        - (pattern, name)                   -> 默认视为 HIGH
        - (pattern, name, ThreatLevel.xxx)  -> 使用指定威胁等级
        """
        self.enable_sanitization = enable_sanitization
        self.log_detections = log_detections
        self.block_high_risk = block_high_risk

        # 编译正则表达式（内置规则）
        self._high_patterns = [
            (re.compile(p, re.IGNORECASE), name)
            for p, name in self.HIGH_RISK_PATTERNS
        ]
        self._medium_patterns = [
            (re.compile(p, re.IGNORECASE), name)
            for p, name in self.MEDIUM_RISK_PATTERNS
        ]
        self._low_patterns = [
            (re.compile(p, re.IGNORECASE), name)
            for p, name in self.LOW_RISK_PATTERNS
        ]

        # 自定义模式的等级映射：
        # key: 模式名称 name, value: ThreatLevel
        # 仅用于覆盖高危检测中的默认 HIGH 等级
        self._custom_levels = {}

        # 添加自定义模式
        if custom_patterns:
            for item in custom_patterns:
                # 兼容两种形状：(pattern, name) / (pattern, name, level)
                if len(item) == 2:
                    pattern, name = item
                    level = ThreatLevel.HIGH  # 未指定等级时默认 HIGH
                elif len(item) == 3:
                    pattern, name, level = item
                else:
                    raise ValueError(
                        f"Invalid custom pattern tuple: {item!r}, "
                        "expected (pattern, name) or (pattern, name, level)"
                    )

                compiled = re.compile(pattern, re.IGNORECASE)
                # 统一归入高危模式集合，实际等级由 _custom_levels 控制
                self._high_patterns.append((compiled, name))
                self._custom_levels[name] = level


    @staticmethod
    def _max_level(current: ThreatLevel, new: ThreatLevel) -> ThreatLevel:
        """
        比较两个威胁等级，返回更高的那个。

        由于 ThreatLevel 的 value 是字符串，不能直接用字典序比较，
        在这里显式定义等级顺序。
        """
        order = {
            ThreatLevel.NONE: 0,
            ThreatLevel.LOW: 1,
            ThreatLevel.MEDIUM: 2,
            ThreatLevel.HIGH: 3,
            ThreatLevel.CRITICAL: 4,
        }
        return new if order[new] > order[current] else current

    def detect(self, text: str) -> SanitizationResult:
        """
        检测文本中的威胁模式
        """
        detected: List[str] = []
        threat_level = ThreatLevel.NONE
        
        # 高危检测
        for pattern, name in self._high_patterns:
            if pattern.search(text):
                detected.append(f"high:{name}")
                # 修复点：自定义模式使用自定义等级，内置模式默认 HIGH
                level = self._custom_levels.get(name, ThreatLevel.HIGH)
                threat_level = self._max_level(threat_level, level)
        
        # 中危检测
        if threat_level != ThreatLevel.HIGH:
            for pattern, name in self._medium_patterns:
                if pattern.search(text):
                    detected.append(f"medium:{name}")
                    threat_level = self._max_level(threat_level, ThreatLevel.MEDIUM)
        
        # 低危检测
        for pattern, name in self._low_patterns:
            if pattern.search(text):
                detected.append(f"low:{name}")
                threat_level = self._max_level(threat_level, ThreatLevel.LOW)
        
        # 特殊检测：异常长度
        if len(text) > 50000:
            detected.append("high:excessive_length")
            threat_level = self._max_level(threat_level, ThreatLevel.HIGH)
        
        # 特殊检测：重复字符（可能的 DoS）
        if self._has_excessive_repetition(text):
            detected.append("medium:repetition_attack")
            threat_level = self._max_level(threat_level, ThreatLevel.MEDIUM)
        
        return SanitizationResult(
            original_text=text,
            sanitized_text=text,
            threat_level=threat_level,
            detected_patterns=detected,
            was_modified=False
        )

    def sanitize(self, text: str) -> SanitizationResult:
        """
        检测并清洗文本
        
        参数:
            text: 待处理文本
            
        返回:
            SanitizationResult 包含清洗后的文本
            
        异常:
            ValueError: 当 block_high_risk=True 且检测到高危模式
        """
        result = self.detect(text)
        
        if self.log_detections and result.detected_patterns:
            logger.warning(
                "Input threat detected",
                threat_level=result.threat_level.value,
                patterns=result.detected_patterns
            )
        
        # 高危阻断
        if self.block_high_risk and result.threat_level in (ThreatLevel.HIGH, ThreatLevel.CRITICAL):
            raise ValueError(
                f"Input blocked due to security concerns: {result.detected_patterns}"
            )
        
        # 执行清洗
        if self.enable_sanitization and result.threat_level != ThreatLevel.NONE:
            sanitized = self._apply_sanitization(text, result.detected_patterns)
            result.sanitized_text = sanitized
            result.was_modified = (sanitized != text)
        
        return result

    def _apply_sanitization(self, text: str, detected: List[str]) -> str:
        """应用清洗规则"""
        sanitized = text
        
        # 移除特殊标记
        if any("special_token" in d for d in detected):
            sanitized = re.sub(r"\[/?INST\]", "", sanitized)
            sanitized = re.sub(r"<\|?(system|user|assistant|im_start|im_end)\|?>", "", sanitized)
            sanitized = re.sub(r"###\s*(System|Human|Assistant|User):", "", sanitized)
        
        # 转义可疑的系统前缀
        if any("system_prefix" in d for d in detected):
            sanitized = re.sub(r"(system:\s*)", r"[escaped] \1", sanitized, flags=re.IGNORECASE)
        
        # 截断过长文本
        if any("excessive_length" in d for d in detected):
            sanitized = sanitized[:50000] + "\n[Content truncated for security]"
        
        return sanitized

    def _has_excessive_repetition(self, text: str, threshold: float = 0.5) -> bool:
        """检测过度重复"""
        if len(text) < 100:
            return False
        
        # 检查字符分布
        char_counts: Dict[str, int] = {}
        for char in text[:1000]:  # 只检查前 1000 字符
            char_counts[char] = char_counts.get(char, 0) + 1
        
        if char_counts:
            max_count = max(char_counts.values())
            if max_count / min(len(text), 1000) > threshold:
                return True
        
        return False


class InputSanitizerMiddleware:
    """
    EventBus 中间件版本

    使用示例:
        ```python
        event_bus = EventBus()
        event_bus.add_middleware(InputSanitizerMiddleware())
        ```
    """

    def __init__(self, sanitizer: Optional[InputSanitizer] = None):
        """
        参数:
            sanitizer: 可选自定义清洗器。
                       - 为 None 时，默认使用 gecko.plugins.guardrails.sanitizer.InputSanitizer
                       - 若显式传入自定义实现，则按原样使用（保持向后兼容）
        """
        # 如果调用方显式传入 sanitizer，则保持兼容；
        # 否则使用公开的核心 InputSanitizer 实现，避免本模块和 sanitizer.py 两套规则长期分叉。
        self.sanitizer = sanitizer or PublicInputSanitizer()
    
    async def __call__(self, event: BaseEvent) -> Optional[BaseEvent]:
        """中间件入口"""
        if event.type in ("run_started", "stream_started"):
            input_data = event.data.get("input")
            
            if isinstance(input_data, str):
                result = self.sanitizer.sanitize(input_data)
                if result.was_modified:
                    event.data["input"] = result.sanitized_text
                    event.data["_security_modified"] = True
                    event.data["_threat_level"] = result.threat_level.value
        
        return event


# ==================== 导出 ====================

__all__ = [
    "InputSanitizer",
    "InputSanitizerMiddleware",
    "SanitizationResult",
    "ThreatLevel",
]
```

[81] gecko/plugins/guardrails/pii.py
```python
```

[82] gecko/plugins/guardrails/sanitizer.py
```python
# gecko/plugins/guardrails/sanitizer.py
"""
输入安全清洗器

检测和处理潜在的 Prompt Injection 攻击。
"""
from __future__ import annotations

import re
from dataclasses import dataclass, field
from enum import Enum
from typing import List, Optional, Pattern, Tuple

from gecko.core.logging import get_logger

logger = get_logger(__name__)


class ThreatLevel(str, Enum):
    """威胁等级"""
    NONE = "none"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"


@dataclass
class ScanResult:
    """扫描结果"""
    text: str
    threat_level: ThreatLevel = ThreatLevel.NONE
    detections: List[str] = field(default_factory=list)
    sanitized_text: Optional[str] = None

    @property
    def is_clean(self) -> bool:
        return self.threat_level == ThreatLevel.NONE

    @property
    def was_modified(self) -> bool:
        return self.sanitized_text is not None and self.sanitized_text != self.text


class InputSanitizer:
    """
    输入清洗器
    
    示例:
        ```python
        sanitizer = InputSanitizer()
        result = sanitizer.scan("Ignore previous instructions...")
        
        if result.threat_level == ThreatLevel.HIGH:
            raise SecurityError("Suspicious input detected")
        ```
    """

    # 检测模式: (pattern, name, threat_level)
    DEFAULT_PATTERNS: List[Tuple[str, str, ThreatLevel]] = [
        # 高危: 指令覆盖
        (r"ignore\s+(all\s+)?(previous|prior|above)\s+(instructions?|rules?)",
         "instruction_override", ThreatLevel.HIGH),
        (r"disregard\s+(everything|all|what)",
         "instruction_override", ThreatLevel.HIGH),
        (r"forget\s+(everything|all)\s+(you|i)\s+(know|said)",
         "instruction_override", ThreatLevel.HIGH),

        # 高危: 角色劫持
        (r"you\s+are\s+now\s+(a|an|in)\s+\w+\s+mode",
         "role_hijack", ThreatLevel.HIGH),
        (r"pretend\s+(to\s+be|you\s+are)",
         "role_hijack", ThreatLevel.HIGH),

        # 高危: 提示泄露
        (r"(show|reveal|print)\s+(your|the)\s+(system|initial)\s+prompt",
         "prompt_leak", ThreatLevel.HIGH),

        # 中危: 特殊标记
        (r"\[/?INST\]", "special_token", ThreatLevel.MEDIUM),
        (r"<\|?(system|user|assistant)\|?>", "special_token", ThreatLevel.MEDIUM),
        (r"###\s*(System|Human|Assistant):", "role_marker", ThreatLevel.MEDIUM),

        # 低危: 可疑前缀
        (r"^system:\s*", "system_prefix", ThreatLevel.LOW),
    ]

    def __init__(
        self,
        patterns: Optional[List[Tuple[str, str, ThreatLevel]]] = None,
        max_length: int = 50000,
    ):
        self.max_length = max_length

        # 编译模式
        pattern_list = patterns if patterns is not None else self.DEFAULT_PATTERNS
        self._patterns: List[Tuple[Pattern, str, ThreatLevel]] = []

        for pattern_str, name, level in pattern_list:
            try:
                compiled = re.compile(pattern_str, re.IGNORECASE)
                self._patterns.append((compiled, name, level))
            except re.error as e:
                logger.warning(f"Invalid pattern '{name}': {e}")

    def scan(self, text: str) -> ScanResult:
        """
        扫描文本中的威胁
        
        参数:
            text: 待扫描文本
            
        返回:
            ScanResult 扫描结果
        """
        result = ScanResult(text=text)

        # 长度检查
        if len(text) > self.max_length:
            result.detections.append("excessive_length")
            result.threat_level = ThreatLevel.HIGH

        # 模式匹配
        max_level = ThreatLevel.NONE

        for pattern, name, level in self._patterns:
            if pattern.search(text):
                result.detections.append(name)
                if self._level_value(level) > self._level_value(max_level):
                    max_level = level

        if self._level_value(max_level) > self._level_value(result.threat_level):
            result.threat_level = max_level

        return result

    def sanitize(self, text: str) -> ScanResult:
        """
        扫描并清洗文本
        
        参数:
            text: 待处理文本
            
        返回:
            ScanResult 包含清洗后的文本
        """
        result = self.scan(text)

        if result.is_clean:
            return result

        sanitized = text

        # 长度截断
        if len(sanitized) > self.max_length:
            sanitized = sanitized[:self.max_length] + "\n[truncated]"

        # 移除特殊标记
        if any(d in ("special_token", "role_marker") for d in result.detections):
            sanitized = re.sub(r"\[/?INST\]", "", sanitized)
            sanitized = re.sub(r"<\|?(system|user|assistant)\|?>", "", sanitized)
            sanitized = re.sub(r"###\s*(System|Human|Assistant):", "", sanitized)

        result.sanitized_text = sanitized

        if result.was_modified:
            logger.warning(
                "Input sanitized",
                threat_level=result.threat_level.value,
                detections=result.detections
            )

        return result

    @staticmethod
    def _level_value(level: ThreatLevel) -> int:
        """获取威胁等级数值（用于比较）"""
        return {"none": 0, "low": 1, "medium": 2, "high": 3}.get(level.value, 0)


__all__ = ["InputSanitizer", "ScanResult", "ThreatLevel"]
```

[83] gecko/plugins/knowledge/__init__.py
```python
# gecko/plugins/knowledge/__init__.py
from gecko.plugins.knowledge.document import Document
from gecko.plugins.knowledge.interfaces import EmbedderProtocol
from gecko.plugins.knowledge.embedders import OpenAIEmbedder, OllamaEmbedder
from gecko.plugins.knowledge.splitters import RecursiveCharacterTextSplitter
from gecko.plugins.knowledge.pipeline import IngestionPipeline
from gecko.plugins.knowledge.tool import RetrievalTool

__all__ = [
    "Document", 
    "EmbedderProtocol", 
    "OpenAIEmbedder", 
    "OllamaEmbedder",
    "RecursiveCharacterTextSplitter",
    "IngestionPipeline",
    "RetrievalTool"
]
```

[84] gecko/plugins/knowledge/base.py
```python
```

[85] gecko/plugins/knowledge/default.py
```python
```

[86] gecko/plugins/knowledge/document.py
```python
# gecko/plugins/knowledge/document.py
from __future__ import annotations
from uuid import uuid4
from typing import Dict, Any, Optional, List
from pydantic import BaseModel, Field

class Document(BaseModel):
    """
    Gecko 标准文档对象
    在 Pipeline 中流转的核心数据结构
    """
    id: str = Field(default_factory=lambda: str(uuid4()))
    text: str
    metadata: Dict[str, Any] = Field(default_factory=dict)
    embedding: Optional[List[float]] = None

    def to_dict(self) -> Dict[str, Any]:
        """转换为存储层所需的字典格式"""
        return {
            "id": self.id,
            "text": self.text,
            "metadata": self.metadata,
            "embedding": self.embedding
        }
```

[87] gecko/plugins/knowledge/embedders.py
```python
# gecko/plugins/knowledge/embedders.py
from __future__ import annotations
import os
from typing import List
from gecko.plugins.knowledge.interfaces import EmbedderProtocol


def _get_litellm():
    """
    惰性加载 litellm，避免导入 gecko.plugins.knowledge 时就强制依赖。

    在真正调用 embedding 时再检查依赖是否存在。
    """
    try:
        import litellm  # type: ignore
        return litellm
    except ImportError as e:
        raise ImportError(
            "OpenAIEmbedder / OllamaEmbedder requires 'litellm'. "
            "Install with: pip install litellm"
        ) from e

class OpenAIEmbedder(EmbedderProtocol):
    """
    基于 LiteLLM 的通用 Embedder
    支持 OpenAI, Azure, Ollama 等所有 LiteLLM 支持的 embedding 模型
    """
    def __init__(self, model: str = "text-embedding-3-small", dimension: int = 1536, **kwargs):
        self.model = model
        self._dimension = dimension
        self.kwargs = kwargs

    @property
    def dimension(self) -> int:
        return self._dimension

    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """批量文档嵌入（惰性加载 litellm）"""
        litellm = _get_litellm()

        # 替换换行符以提升某些模型的表现
        texts = [t.replace("\n", " ") for t in texts]
        response = await litellm.aembedding(
            model=self.model,
            input=texts,
            **self.kwargs
        )
        return [r["embedding"] for r in response.data]

    async def embed_query(self, text: str) -> List[float]:
        """单条查询嵌入（惰性加载 litellm）"""
        litellm = _get_litellm()

        text = text.replace("\n", " ")
        response = await litellm.aembedding(
            model=self.model,
            input=[text],
            **self.kwargs
        )
        return response.data[0]["embedding"]

# 预设 Ollama 配置
class OllamaEmbedder(OpenAIEmbedder):
    """
    Ollama 本地嵌入模型适配器
    """
    def __init__(self, model: str = "ollama/nomic-embed-text", base_url: str = "http://localhost:11434", dimension: int = 768):
        super().__init__(
            model=model, 
            dimension=dimension, 
            api_base=base_url
        )
```

[88] gecko/plugins/knowledge/interfaces.py
```python
# gecko/plugins/knowledge/interfaces.py
from __future__ import annotations
from typing import List, Protocol, runtime_checkable

@runtime_checkable
class EmbedderProtocol(Protocol):
    """
    嵌入模型协议
    负责将文本转换为向量
    """
    @property
    def dimension(self) -> int:
        """返回向量维度 (例如 1536)"""
        ...

    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """批量嵌入文档列表"""
        ...

    async def embed_query(self, text: str) -> List[float]:
        """嵌入单个查询语句"""
        ...

@runtime_checkable
class ReaderProtocol(Protocol):
    """
    文件读取协议
    """
    def load(self, file_path: str) -> str:
        """读取文件内容为字符串"""
        ...
```

[89] gecko/plugins/knowledge/pipeline.py
```python
# gecko/plugins/knowledge/pipeline.py
from typing import List, Optional
from gecko.plugins.knowledge.interfaces import EmbedderProtocol
from gecko.plugins.storage.interfaces import VectorInterface
from gecko.plugins.knowledge.splitters import RecursiveCharacterTextSplitter
from gecko.plugins.knowledge.readers import AutoReader
from gecko.core.utils import ensure_awaitable

class IngestionPipeline:
    """
    RAG 数据入库流水线
    Load -> Split -> Embed -> Store
    """
    def __init__(
        self,
        vector_store: VectorInterface,
        embedder: EmbedderProtocol,
        splitter = None
    ):
        self.vector_store = vector_store
        self.embedder = embedder
        self.splitter = splitter or RecursiveCharacterTextSplitter()

    async def run(self, file_paths: List[str], batch_size: int = 100):
        """
        执行入库流程
        :param file_paths: 文件路径列表
        :param batch_size: 向量库写入批次大小
        """
        print(f"🚀 开始处理 {len(file_paths)} 个文件...")
        
        # 1. Load
        raw_docs = []
        for path in file_paths:
            try:
                docs = AutoReader.read(path)
                raw_docs.extend(docs)
            except Exception as e:
                print(f"⚠️ 读取失败 {path}: {e}")

        # 2. Split
        chunks = self.splitter.split_documents(raw_docs)
        print(f"✂️ 切分为 {len(chunks)} 个片段")

        # 3. Embed & Store (Batch Processing)
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i : i + batch_size]
            texts = [doc.text for doc in batch]
            
            # 生成向量
            embeddings = await ensure_awaitable(self.embedder.embed_documents, texts)
            
            # 注入向量到文档对象
            docs_to_upsert = []
            for doc, emb in zip(batch, embeddings):
                doc.embedding = emb
                docs_to_upsert.append(doc.to_dict())
            
            # 写入数据库
            await self.vector_store.upsert(docs_to_upsert)
            print(f"💾 已存储批次 {i} - {i+len(batch)}")
            
        print("✅ 入库完成")
```

[90] gecko/plugins/knowledge/readers.py
```python
# gecko/plugins/knowledge/readers.py
import os
from pathlib import Path
from typing import List
from gecko.plugins.knowledge.document import Document
from gecko.plugins.knowledge.interfaces import ReaderProtocol

class TextReader(ReaderProtocol):
    """简单文本读取器 (.txt, .md, .py, etc)"""
    def load(self, file_path: str) -> str:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()

class PDFReader(ReaderProtocol):
    """PDF 读取器 (依赖 pypdf)"""
    def load(self, file_path: str) -> str:
        try:
            import pypdf
        except ImportError:
            raise ImportError("请安装 pypdf 以支持 PDF 读取: pip install pypdf")
            
        text = ""
        with open(file_path, "rb") as f:
            reader = pypdf.PdfReader(f)
            for page in reader.pages:
                text += page.extract_text() + "\n"
        return text

class AutoReader:
    """自动分发读取器"""
    _READERS = {
        ".txt": TextReader,
        ".md": TextReader,
        ".py": TextReader,
        ".json": TextReader,
        ".pdf": PDFReader
    }

    @classmethod
    def read(cls, file_path: str) -> List[Document]:
        path = Path(file_path)
        ext = path.suffix.lower()
        
        reader_cls = cls._READERS.get(ext)
        if not reader_cls:
            raise ValueError(f"不支持的文件类型: {ext}")
        
        content = reader_cls().load(str(path))
        return [Document(text=content, metadata={"source": str(path), "filename": path.name})]
```

[91] gecko/plugins/knowledge/splitters.py
```python
# gecko/plugins/knowledge/splitters.py
from typing import List
from gecko.plugins.knowledge.document import Document

class RecursiveCharacterTextSplitter:
    """
    递归字符切分器 (参考 LangChain 逻辑)
    尝试按顺序使用分隔符切分文本，直到块大小符合要求。
    """
    def __init__(
        self, 
        chunk_size: int = 1000, 
        chunk_overlap: int = 200,
        separators: List[str] | None = None
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or ["\n\n", "\n", " ", ""]

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """切分文档列表"""
        final_docs = []
        for doc in documents:
            chunks = self.split_text(doc.text)
            for i, chunk in enumerate(chunks):
                # 继承元数据，并增加切片信息
                new_meta = doc.metadata.copy()
                new_meta.update({"chunk_index": i, "source_id": doc.id})
                final_docs.append(Document(text=chunk, metadata=new_meta))
        return final_docs

    def split_text(self, text: str) -> List[str]:
        """
        切分单文本的核心逻辑

        设计要点：
        - 优先使用用户指定的分隔符（如空格、换行等）
        - 在保证单个 chunk 不超过 chunk_size 的前提下尽量“吃满”
        - ✅ 修复点：
          1) 判断是否还能追加下一段时，需要把分隔符的长度一起算进去
          2) 允许“刚好等于 chunk_size” 的情况（<=），避免不必要的碎片化
        """
        final_chunks: List[str] = []

        # 若整体长度不超过 chunk_size，直接返回整体
        if self._length(text) <= self.chunk_size:
            return [text]

        # 1. 选择分隔符（优先粗粒度）
        #    - 若指定了 separators，则按顺序优先选用第一个在文本中出现的
        #    - 若没有任何分隔符匹配，则退化为逐字符切分
        separator = self.separators[-1]
        for sep in self.separators:
            if sep == "":
                separator = ""
                break
            if sep in text:
                separator = sep
                break

        # 2. 基于分隔符切分
        splits = text.split(separator) if separator else list(text)

        # 3. 合并碎片
        good_splits: List[str] = []
        current_chunk = ""

        for s in splits:
            # 本次准备追加的内容实际会占用的长度：
            # - 自身长度 self._length(s)
            # - 如果当前已经有内容，还要加上分隔符长度（例如空格）
            to_add = self._length(s)
            if current_chunk:
                to_add += self._length(separator)

            # ✅ 修复点：
            #   使用 <=，允许“刚好等于 chunk_size”
            #   同时严格按“当前长度 + 分隔符 + 新内容”的总长度判断，
            #   避免出现 "hello world" 实际 11 个字符却被错误认为是 10 的情况。
            if self._length(current_chunk) + to_add <= self.chunk_size:
                # 可以继续往当前块追加
                if current_chunk:
                    current_chunk += separator + s
                else:
                    current_chunk = s
            else:
                # 当前块已经装不下更多内容了，先收集当前块，开启新块
                if current_chunk:
                    good_splits.append(current_chunk)
                current_chunk = s

        # 收尾：最后一个块如果非空就放进去
        if current_chunk:
            good_splits.append(current_chunk)

        return good_splits


    def _length(self, text: str) -> int:
        return len(text)
```

[92] gecko/plugins/knowledge/tool.py
```python
# gecko/plugins/knowledge/tool.py

from typing import Any, Type
from pydantic import BaseModel, Field, PrivateAttr
from gecko.plugins.tools.base import BaseTool, ToolResult
from gecko.plugins.storage.interfaces import VectorInterface
from gecko.plugins.knowledge.interfaces import EmbedderProtocol
from gecko.core.utils import ensure_awaitable


class RetrievalArgs(BaseModel):
    """
    RAG 检索工具的参数定义

    使用 Pydantic 模型作为 args_schema：
    - 便于 BaseTool.execute 统一做参数校验
    - 避免直接在 Tool 上手写 parameters 字典
    """
    query: str = Field(
        ...,
        description="用于在知识库中检索的查询语句"
    )


class RetrievalTool(BaseTool):
    """
    基于向量检索的知识库查询工具

    角色：
    - 作为 BaseTool 的一个具体实现，用于 RAG 检索场景
    - 通过 args_schema + _run 实现参数校验和业务逻辑
    """

    # 工具的基础元信息
    name: str = "knowledge_search"
    description: str = (
        "搜索内部知识库以获取相关信息。当问题涉及特定文档、报告或私有数据时使用。"
    )

    # ✅ 关键修复点：
    # 使用 args_schema 指定参数模型，避免在子类中定义与父类同名的字段 `parameters`
    # BaseTool.parameters 属性会自动基于此生成 OpenAI Tool Schema
    args_schema: Type[BaseModel] = RetrievalArgs  # type: ignore[assignment]

    # 使用 Pydantic PrivateAttr 来承载运行时依赖，避免被当作模型字段
    _vector_store: Any = PrivateAttr()
    _embedder: Any = PrivateAttr()
    _top_k: int = PrivateAttr(default=3)

    def __init__(
        self,
        vector_store: VectorInterface,
        embedder: EmbedderProtocol,
        top_k: int = 3,
        **data: Any,
    ) -> None:
        """
        初始化检索工具

        参数:
        - vector_store: 向量存储后端，须实现 VectorInterface
        - embedder: 向量化模型适配器，须实现 EmbedderProtocol
        - top_k: 检索返回的候选数量
        - data: 传递给 BaseTool/BaseModel 的其他字段（目前通常为空）
        """
        # 先初始化 BaseModel / BaseTool 部分（包括 args_schema 等）
        super().__init__(**data)

        # 再设置运行时依赖
        self._vector_store = vector_store
        self._embedder = embedder
        self._top_k = top_k

    async def _run(self, args: BaseModel) -> ToolResult:  # type: ignore[override]
        """
        具体业务逻辑实现

        输入:
        - args: 已通过 args_schema 校验过的参数对象 (RetrievalArgs)

        输出:
        - ToolResult: 统一结果封装

        修复点：
        - 对向量检索调用使用 ensure_awaitable 包装，兼容：
          * 同步函数 (return list)
          * 协程函数 (async def)
          * AsyncMock / MagicMock
        避免测试中使用 AsyncMock 时出现 "coroutine was never awaited"。
        """
        # 为了兼容潜在的 schema 变动，这里既支持属性访问也支持 model_dump
        query = getattr(args, "query", None) or args.model_dump().get("query")
        if not query:
            return ToolResult(content="错误：查询语句为空", is_error=True)

        # 1. 向量化查询
        #    embed_query 可能是同步函数、协程或 AsyncMock，所以统一通过 ensure_awaitable 调用
        query_vec = await ensure_awaitable(self._embedder.embed_query, query)

        # 2. 进行向量检索
        #    ✅ 修复点：search 同样使用 ensure_awaitable，而不是直接 `await self._vector_store.search(...)`
        results = await ensure_awaitable(
            self._vector_store.search,
            query_vec,
            top_k=self._top_k,
        )

        if not results:
            return ToolResult(content="未在知识库中找到相关内容。")

        # 3. 格式化结果
        context = "找到以下相关内容：\n\n"
        for i, res in enumerate(results, 1):
            metadata = res.get("metadata", {}) or {}
            source = metadata.get("filename", "unknown")
            snippet = res.get("text") or metadata.get("snippet", "")
            context += f"[{i}] 来源：{source}\n{snippet}\n\n"

        return ToolResult(content=context)
```

[93] gecko/plugins/models/__init__.py
```python
# gecko/plugins/models/__init__.py
"""
Gecko Models Plugin

提供统一的模型接入层。
架构：配置 (Config) -> 工厂 (Factory) -> 注册表 (Registry) -> 驱动 (Driver)
"""
try:
    import litellm  # type: ignore

    # ================= Global Configuration =================
    # 关闭 LiteLLM 的遥测和冗余打印，保持控制台整洁
    litellm.suppress_debug_info = True
    litellm.telemetry = False
    # ========================================================
except ImportError:
    # 未安装 litellm 时允许导入本模块，但实际使用驱动/Embedder 时仍会抛错
    litellm = None  # type: ignore

from gecko.plugins.models.base import AbstractModel, BaseChatModel, BaseEmbedder
from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.drivers.litellm_driver import LiteLLMDriver
from gecko.plugins.models.embedding import LiteLLMEmbedder
from gecko.plugins.models.factory import create_model
from gecko.plugins.models.presets.ollama import OllamaChat, OllamaEmbedder
from gecko.plugins.models.presets.openai import OpenAIChat, OpenAIEmbedder
from gecko.plugins.models.presets.zhipu import ZhipuChat

# 确保默认驱动被注册
import gecko.plugins.models.drivers.litellm_driver  # noqa: F401

__all__ = [
    "ModelConfig",
    "AbstractModel",
    "BaseChatModel",
    "BaseEmbedder",
    "LiteLLMDriver",
    "LiteLLMEmbedder",
    "create_model",
    "OpenAIChat",
    "OpenAIEmbedder",
    "OllamaChat",
    "OllamaEmbedder",
    "ZhipuChat",
]
```

[94] gecko/plugins/models/adapter.py
```python
# gecko/plugins/models/adapter.py
from __future__ import annotations

from typing import Any, List, Optional

from gecko.core.protocols import (
    CompletionChoice,
    CompletionResponse,
    CompletionUsage,
    StreamChunk,
)


def safe_access(obj: Any, key: str, default: Any = None) -> Any:
    """
    [工具] 通用属性/字典获取 (增强版)
    
    修复了 Pydantic getattr 抛错和 Mock 对象误判的问题。
    """
    if obj is None:
        return default

    # 1. 优先尝试字典访问 (最安全)
    try:
        return obj[key]
    except (TypeError, KeyError, IndexError, AttributeError):
        pass

    # 2. 尝试属性访问 (需捕获 AttributeError)
    try:
        # 注意：不要使用 hasattr，因为它在某些动态代理对象(如 Mock, Pydantic Lazy)上可能误判
        val = getattr(obj, key)
        return val if val is not None else default
    except (AttributeError, TypeError):
        pass

    return default


class LiteLLMAdapter:
    """
    LiteLLM 响应适配器 (Anti-Corruption Layer)
    
    职责：
    将 LiteLLM 返回的异构对象手动映射为 Gecko 的标准协议对象。
    避免调用 model_dump() 从而消除 Pydantic 序列化警告。
    """

    @staticmethod
    def to_gecko_response(resp: Any) -> CompletionResponse:
        """将 LiteLLM 响应转换为 Gecko CompletionResponse"""
        
        # 1. 提取 Choices
        choices: List[CompletionChoice] = []
        raw_choices = safe_access(resp, "choices", [])
        
        if isinstance(raw_choices, list):
            for c in raw_choices:
                # 提取 Message
                raw_msg = safe_access(c, "message", {})
                message_dict = {
                    "role": safe_access(raw_msg, "role", "assistant"),
                    "content": safe_access(raw_msg, "content", None),
                }
                
                # 提取 Tool Calls
                raw_tool_calls = safe_access(raw_msg, "tool_calls", None)
                if raw_tool_calls:
                    sanitized_tool_calls = []
                    for tc in raw_tool_calls:
                        sanitized_tool_calls.append({
                            "id": safe_access(tc, "id", ""),
                            "type": safe_access(tc, "type", "function"),
                            "function": {
                                "name": safe_access(safe_access(tc, "function"), "name", ""),
                                "arguments": safe_access(safe_access(tc, "function"), "arguments", "")
                            }
                        })
                    message_dict["tool_calls"] = sanitized_tool_calls

                choices.append(CompletionChoice(
                    index=safe_access(c, "index", 0),
                    finish_reason=safe_access(c, "finish_reason", None),
                    message=message_dict,
                    logprobs=safe_access(c, "logprobs", None)
                ))

        # 2. 提取 Usage
        usage = None
        raw_usage = safe_access(resp, "usage", None)
        if raw_usage:
            usage = CompletionUsage(
                prompt_tokens=safe_access(raw_usage, "prompt_tokens", 0),
                completion_tokens=safe_access(raw_usage, "completion_tokens", 0),
                total_tokens=safe_access(raw_usage, "total_tokens", 0)
            )

        # 3. 构建最终响应
        return CompletionResponse(
            id=safe_access(resp, "id", ""),
            object=safe_access(resp, "object", "chat.completion"),
            created=safe_access(resp, "created", 0),
            model=safe_access(resp, "model", ""),
            choices=choices,
            usage=usage,
            system_fingerprint=safe_access(resp, "system_fingerprint", None),
            metadata=safe_access(resp, "_hidden_params", {})
        )

    @staticmethod
    def to_gecko_chunk(chunk: Any) -> Optional[StreamChunk]:
        """
        将 LiteLLM 流式块转换为 StreamChunk
        """
        raw_choices = safe_access(chunk, "choices", [])
        
        # 过滤 Keep-Alive 空包
        # 有些 provider 会发送仅含 id 的包作为心跳，或者完全空的包
        if not raw_choices and not safe_access(chunk, "id"):
            return None

        mapped_choices = []
        if isinstance(raw_choices, list):
            for c in raw_choices:
                delta = safe_access(c, "delta", {})
                
                # [修复] 确保 delta 是字典，防止 AttributeError
                if not isinstance(delta, dict):
                    # 某些情况下 delta 可能是 Pydantic 对象
                    if hasattr(delta, "model_dump"):
                        delta = delta.model_dump()
                    elif hasattr(delta, "dict"):
                        delta = delta.dict()
                    else:
                        delta = {}

                mapped_choices.append({
                    "index": safe_access(c, "index", 0),
                    "delta": {
                        "role": safe_access(delta, "role", None),
                        "content": safe_access(delta, "content", None),
                        "tool_calls": safe_access(delta, "tool_calls", None)
                    },
                    "finish_reason": safe_access(c, "finish_reason", None)
                })

        return StreamChunk(
            id=safe_access(chunk, "id", ""),
            created=safe_access(chunk, "created", 0),
            model=safe_access(chunk, "model", ""),
            choices=mapped_choices
        )
```

[95] gecko/plugins/models/base.py
```python
# gecko/plugins/models/base.py
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any, AsyncIterator, Dict, List

from gecko.core.protocols import (
    CompletionResponse,
    EmbedderProtocol,
    StreamChunk,
    StreamableModelProtocol,
)
from gecko.plugins.models.config import ModelConfig


class AbstractModel(ABC):
    """所有模型的根基类"""

    def __init__(self, config: ModelConfig):
        self.config = config


class BaseChatModel(AbstractModel, StreamableModelProtocol):
    """
    Chat 模型驱动基类
    
    所有具体的驱动器 (Driver) 都必须继承此类并实现 `acompletion` 和 `astream`。
    """

    @abstractmethod
    async def acompletion(self, messages: List[Dict[str, Any]], **kwargs: Any) -> CompletionResponse:
        """单次生成"""
        ...

    @abstractmethod
    async def astream(self, messages: List[Dict[str, Any]], **kwargs: Any) -> AsyncIterator[StreamChunk]: # type: ignore
        """流式生成"""
        ...


class BaseEmbedder(AbstractModel, EmbedderProtocol):
    """
    Embedding 模型基类
    """
    
    def __init__(self, config: ModelConfig, dimension: int):
        super().__init__(config)
        self._dimension = dimension

    @property
    def dimension(self) -> int:
        """返回向量维度"""
        return self._dimension

    @abstractmethod
    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """批量嵌入"""
        ...

    @abstractmethod
    async def embed_query(self, text: str) -> List[float]:
        """单条查询嵌入"""
        ...
```

[96] gecko/plugins/models/chat.py
```python
# gecko/plugins/models/chat.py
from __future__ import annotations

import json
from typing import Any, AsyncIterator, Dict, List

import litellm
from pydantic import ValidationError

from gecko.core.exceptions import ModelError
from gecko.core.logging import get_logger
from gecko.core.protocols import CompletionResponse, StreamChunk
from gecko.plugins.models.base import BaseChatModel

logger = get_logger(__name__)

class LiteLLMChatModel(BaseChatModel):
    """
    基于 LiteLLM 的通用 Chat 模型实现
    """

    def _get_params(self, messages: List[Dict[str, Any]], stream: bool, **kwargs: Any) -> Dict[str, Any]:
        """构造 LiteLLM 调用参数，合并配置与运行时参数"""
        params = {
            "model": self.config.model_name,
            "messages": messages,
            "timeout": self.config.timeout,
            "stream": stream,
            **self.config.extra_kwargs,
            **kwargs,
        }

        if self.config.api_key:
            params["api_key"] = self.config.api_key
        if self.config.base_url:
            params["api_base"] = self.config.base_url

        return params

    def _sanitize_response(self, resp: Any) -> Dict[str, Any]:
        """
        [核心修复] 深度清洗 LiteLLM 响应对象
        
        目标：将不可靠的 Pydantic 对象转换为纯 Python 字典，
        避免因 litellm 内部 Schema 校验失败导致 crash。
        """
        # 策略 1: 尝试通过 JSON 序列化 (通常最稳健，因为会忽略 Pydantic 类型检查)
        try:
            if hasattr(resp, "model_dump_json"):
                # Pydantic v2 JSON 序列化
                return json.loads(resp.model_dump_json())
            if hasattr(resp, "json") and callable(resp.json):
                # Pydantic v1 / 传统 JSON 方法
                return json.loads(resp.json()) # type: ignore
        except Exception:
            pass

        # 策略 2: 尝试标准 model_dump (如果 JSON 失败)
        try:
            if hasattr(resp, "model_dump"):
                return resp.model_dump(mode='json') # mode='json' 强制转换类型
            if hasattr(resp, "dict"):
                return resp.dict()
        except Exception:
            pass

        # 策略 3: 手动暴力提取 (Ultimate Fallback)
        # 当 litellm 对象损坏无法 dump 时，手动提取关键字段
        try:
            data = {
                "id": getattr(resp, "id", ""),
                "object": getattr(resp, "object", "chat.completion"),
                "created": getattr(resp, "created", 0),
                "model": getattr(resp, "model", ""),
                "choices": [],
                "usage": None
            }
            
            # 提取 Choices
            raw_choices = getattr(resp, "choices", [])
            if isinstance(raw_choices, list):
                for c in raw_choices:
                    c_dict = {
                        "index": getattr(c, "index", 0),
                        "finish_reason": getattr(c, "finish_reason", "stop"),
                        "message": {}
                    }
                    # 提取 Message
                    msg = getattr(c, "message", None)
                    if msg:
                        c_dict["message"] = {
                            "role": getattr(msg, "role", "assistant"),
                            "content": getattr(msg, "content", None),
                            "tool_calls": getattr(msg, "tool_calls", None)
                        }
                    data["choices"].append(c_dict)
            
            # 提取 Usage
            usage = getattr(resp, "usage", None)
            if usage:
                data["usage"] = {
                    "prompt_tokens": getattr(usage, "prompt_tokens", 0),
                    "completion_tokens": getattr(usage, "completion_tokens", 0),
                    "total_tokens": getattr(usage, "total_tokens", 0)
                }
            
            return data
            
        except Exception as e:
            logger.error("Manual response extraction failed", error=str(e))
            # 如果到了这一步，说明对象完全不可读，返回空结构防止 crash
            return {"choices": [], "model": "unknown-error"}

    async def acompletion(self, messages: List[Dict[str, Any]], **kwargs: Any) -> CompletionResponse:
        try:
            kwargs.pop("stream", None)

            params = self._get_params(messages, stream=False, **kwargs)
            resp = await litellm.acompletion(**params)
            
            # 使用新的清洗逻辑
            data = self._sanitize_response(resp)
            
            return CompletionResponse(**data)

        except (ValidationError, TypeError) as e:
            logger.error("Failed to parse model response", error=str(e))
            raise ModelError(f"Response parsing failed: {e}") from e
        except Exception as e:
            self._handle_litellm_error(e)
            raise

    async def astream(self, messages: List[Dict[str, Any]], **kwargs: Any) -> AsyncIterator[StreamChunk]: # type: ignore
        try:
            kwargs.pop("stream", None)

            params = self._get_params(messages, stream=True, **kwargs)
            response_iterator = await litellm.acompletion(**params)
            
            async for chunk in response_iterator: # type: ignore
                # 流式 Chunk 同样需要清洗
                data = self._sanitize_response(chunk)
                # 确保 data 不为空且有 choices (防止空包)
                if data and data.get("choices") or data.get("id"):
                    yield StreamChunk(**data)

        except Exception as e:
            self._handle_litellm_error(e)

    def _handle_litellm_error(self, e: Exception) -> None:
        """统一异常映射"""
        msg = str(e)
        error_type = type(e).__name__
        
        if "AuthenticationError" in error_type:
            raise ModelError(f"Authentication failed: {msg}", error_code="AUTH_ERROR") from e
        if "RateLimitError" in error_type:
            raise ModelError(f"Rate limit exceeded: {msg}", error_code="RATE_LIMIT") from e
        if "ContextWindowExceededError" in error_type:
             raise ModelError(f"Context window exceeded: {msg}", error_code="CONTEXT_LIMIT") from e
             
        raise ModelError(f"Model execution failed ({error_type}): {msg}") from e
```

[97] gecko/plugins/models/config.py
```python
# gecko/plugins/models/config.py
from __future__ import annotations

from typing import Any, Dict, Optional

from pydantic import BaseModel, Field


class ModelConfig(BaseModel):
    """
    模型通用配置对象
    """
    model_name: str = Field(..., description="模型名称，如 'gpt-4o', 'ollama/llama3'")
    
    # 驱动类型 (不再是 Enum，支持字符串扩展)
    driver_type: str = Field(default="litellm", description="驱动类型: litellm, openai_native, etc.")
    
    # 连接配置
    api_key: Optional[str] = Field(default=None, description="API Key")
    base_url: Optional[str] = Field(default=None, description="API Base URL (本地模型必填)")
    timeout: float = Field(default=60.0, description="请求超时时间(秒)")
    max_retries: int = Field(default=2, description="最大重试次数")
    
    # 运行时参数透传 (Provider Specific)
    extra_kwargs: Dict[str, Any] = Field(default_factory=dict, description="透传给底层驱动的额外参数")
    
    # 能力标识 (Capability Flags)
    supports_vision: bool = Field(default=False, description="是否支持视觉输入")
    supports_audio: bool = Field(default=False, description="是否支持音频输入")
    supports_function_calling: bool = Field(default=True, description="是否支持工具调用")
```

[98] gecko/plugins/models/drivers/litellm_driver.py
```python
# gecko/plugins/models/drivers/litellm_driver.py
"""
LiteLLM 驱动 (v0.4 Phase 3 Complete)

集成特性：
1. Tokenizer 预加载与多级回退计数 (Tiktoken -> Char Estimation)。
2. 熔断器 (Circuit Breaker) 保护，防止服务雪崩。
3. 统一异常映射 (ProviderError 体系)。
4. 响应适配 (LiteLLMAdapter) 清洗 Pydantic 数据。
"""
from __future__ import annotations

from typing import Any, AsyncIterator, Dict, List, Union

import litellm

from gecko.core.exceptions import ModelError, GeckoError
from gecko.core.logging import get_logger
from gecko.core.protocols import CompletionResponse, StreamChunk
from gecko.core.resilience import CircuitBreaker
from gecko.plugins.models.adapter import LiteLLMAdapter
from gecko.plugins.models.base import BaseChatModel
from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.registry import register_driver
from gecko.plugins.models.exceptions import (
    AuthenticationError,
    RateLimitError,
    ContextWindowExceededError,
    ServiceUnavailableError,
    ProviderError
)

logger = get_logger(__name__)


@register_driver("litellm")
class LiteLLMDriver(BaseChatModel):
    """
    LiteLLM 通用驱动实现
    """
    
    def __init__(self, config: ModelConfig):
        super().__init__(config)
        self._tokenizer = None
        # 标记 tiktoken 是否可用/加载成功，用于 count_tokens 决策
        self._tiktoken_available: bool = True
        
        # 1. 预加载 Tokenizer (优化冷启动性能)
        self._preload_tokenizer()
        
        # 2. 初始化熔断器
        # 仅针对 服务不可用(5xx) 和 速率限制(429) 进行熔断
        # 认证错误(401)和上下文超限(400)属于不可恢复的业务/配置错误，不应触发熔断
        self._circuit_breaker = CircuitBreaker(
            failure_threshold=5,
            recovery_timeout=30.0,
            monitor_exceptions=(ServiceUnavailableError, RateLimitError)
        )

    def _preload_tokenizer(self):
        """
        预加载 Tokenizer 到内存
        
        尝试根据模型名称加载最合适的 tiktoken 编码器。
        如果失败，标记 _tiktoken_available = False。
        """
        model_lower = self.config.model_name.lower()

        try:
            import tiktoken
            self._tiktoken_available = True

            # 启发式匹配编码器
            if any(k in model_lower for k in ["gpt-4", "gpt-3.5", "gpt-4o"]):
                try:
                    self._tokenizer = tiktoken.encoding_for_model(self.config.model_name)
                except KeyError:
                    self._tokenizer = tiktoken.get_encoding("cl100k_base")
            
            elif "text-embedding" in model_lower:
                self._tokenizer = tiktoken.get_encoding("cl100k_base")
            
            elif any(k in model_lower for k in ["claude", "anthropic", "llama", "mistral", "glm"]):
                # 非 OpenAI 模型使用 cl100k_base 作为近似估算 (足够通用)
                self._tokenizer = tiktoken.get_encoding("cl100k_base")
            
            else:
                # 默认 fallback
                self._tokenizer = tiktoken.get_encoding("cl100k_base")

        except ImportError:
            self._tiktoken_available = False
            logger.debug("tiktoken not installed, token counting will use estimation")
        except Exception as e:
            self._tiktoken_available = False
            logger.debug(f"Tokenizer preload failed: {e}")

    def count_tokens(self, text_or_messages: Union[str, List[Dict[str, Any]]]) -> int:
        """
        计算 Token 数量 (多级回退策略)
        """
        try:
            text = self._to_text(text_or_messages)

            # 1. 优先使用本地 Tokenizer (最快，C++实现)
            if self._tokenizer is not None:
                return len(self._tokenizer.encode(text))

            # 2. 如果 tiktoken 根本不可用，直接走字符估算
            if not getattr(self, "_tiktoken_available", True):
                # 中文约 0.6 token/char, 英文约 0.25 -> 平均保守取 0.33
                return len(text) // 3

            # 3. 尝试调用 LiteLLM 的 encode (较慢，有 Python 开销)
            # 仅对短文本使用，避免阻塞 Event Loop
            if len(text) < 10000:
                try:
                    return len(litellm.encode(model=self.config.model_name, text=text)) # type: ignore
                except Exception:
                    pass

            # 4. 最终兜底：字符估算
            return len(text) // 3

        except Exception:
            # 最后的防线
            return len(str(text_or_messages)) // 3

    def _to_text(self, inp: Union[str, List[Dict[str, Any]]]) -> str:
        """辅助函数：将输入转为字符串"""
        if isinstance(inp, str):
            return inp
        if isinstance(inp, list):
            return "".join(str(m.get("content", "")) for m in inp)
        return str(inp)

    def _get_params(self, messages: List[Dict[str, Any]], stream: bool, **kwargs: Any) -> Dict[str, Any]:
        """构造 LiteLLM 调用参数"""
        params = {
            "model": self.config.model_name,
            "messages": messages,
            "timeout": self.config.timeout,
            "stream": stream,
            **self.config.extra_kwargs,
            **kwargs,
        }
        if self.config.api_key:
            params["api_key"] = self.config.api_key
        if self.config.base_url:
            params["api_base"] = self.config.base_url
        return params

    async def acompletion(self, messages: List[Dict[str, Any]], **kwargs: Any) -> CompletionResponse:
        """
        单次生成 (集成熔断器)
        """
        # 定义受保护的执行逻辑
        async def _execute():
            try:
                # 复制参数防止副作用
                call_kwargs = kwargs.copy()
                call_kwargs.pop("stream", None)

                params = self._get_params(messages, stream=False, **call_kwargs)
                resp = await litellm.acompletion(**params)
                
                # 使用适配器清洗响应数据
                return LiteLLMAdapter.to_gecko_response(resp)
            except Exception as e:
                # 将 LiteLLM 异常转换为 Gecko 异常并重新抛出
                self._handle_error(e)
                raise e

        # [Phase 3] 通过熔断器执行
        return await self._circuit_breaker.call(_execute)

    async def astream(self, messages: List[Dict[str, Any]], **kwargs: Any) -> AsyncIterator[StreamChunk]: # type: ignore
        """
        流式生成 (集成熔断器)
        
        注意：熔断器保护的是**连接建立**阶段。
        """
        # 定义生成器工厂
        async def _create_generator():
            call_kwargs = kwargs.copy()
            call_kwargs.pop("stream", None)
            params = self._get_params(messages, stream=True, **call_kwargs)
            return await litellm.acompletion(**params)

        try:
            # 1. 建立连接 (受熔断器保护)
            # 如果此时服务端 503/429，_create_generator 会抛错，触发熔断
            response_iterator = await self._circuit_breaker.call(_create_generator)
            
            # 2. 迭代数据 (流式传输)
            async for chunk in response_iterator: # type: ignore
                gecko_chunk = LiteLLMAdapter.to_gecko_chunk(chunk)
                if gecko_chunk:
                    yield gecko_chunk
        except Exception as e:
            self._handle_error(e)

    def _handle_error(self, e: Exception) -> None:
        """
        统一异常映射逻辑
        """
        # [Fix] 如果已经是框架内部异常（如 CircuitOpenError 或 已转换过的 ModelError），直接透传
        if isinstance(e, GeckoError):
            raise e

        msg = str(e)
        err_name = type(e).__name__
        
        logger.warning(f"LiteLLM raw error: {err_name} - {msg}")

        if "AuthenticationError" in err_name:
            raise AuthenticationError(f"Auth failed: {msg}") from e
            
        if "RateLimitError" in err_name:
            raise RateLimitError(f"Rate limit exceeded: {msg}") from e
            
        if "ContextWindowExceededError" in err_name:
            raise ContextWindowExceededError(f"Context limit exceeded: {msg}") from e
            
        if "ServiceUnavailableError" in err_name or "APIConnectionError" in err_name:
            raise ServiceUnavailableError(f"Service unavailable: {msg}") from e
            
        if "Timeout" in err_name:
            raise ServiceUnavailableError(f"Request timeout: {msg}") from e

        # 兜底
        raise ProviderError(f"Unknown provider error ({err_name}): {msg}") from e
```

[99] gecko/plugins/models/embedding.py
```python
# gecko/plugins/models/embedding.py
from __future__ import annotations

from typing import List

def _get_litellm():
    """
    惰性加载 litellm，避免导入 gecko.plugins.models.embedding 时强制要求安装。

    在实际调用 embedding 接口时再检查依赖是否存在。
    """
    try:
        import litellm  # type: ignore
        return litellm
    except ImportError as e:
        raise ImportError(
            "LiteLLMEmbedder requires 'litellm'. "
            "Install with: pip install litellm"
        ) from e

from gecko.core.exceptions import ModelError
from gecko.plugins.models.adapter import safe_access
from gecko.plugins.models.base import BaseEmbedder

class LiteLLMEmbedder(BaseEmbedder):
    """
    基于 LiteLLM 的通用 Embedding 模型实现
    """

    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        try:
            litellm = _get_litellm()

            # 预处理：移除换行符
            clean_texts = [t.replace("\n", " ") for t in texts]
            
            params = {
                "model": self.config.model_name,
                "input": clean_texts,
                "timeout": self.config.timeout,
                **self.config.extra_kwargs
            }
            
            if self.config.api_key:
                params["api_key"] = self.config.api_key
            if self.config.base_url:
                params["api_base"] = self.config.base_url

            resp = await litellm.aembedding(**params)
            
            # 使用 safe_access 进行健壮提取
            embeddings: List[List[float]] = []
            data_items = safe_access(resp, "data", [])
            
            if isinstance(data_items, list):
                for item in data_items:
                    emb = safe_access(item, "embedding")
                    if emb:
                        embeddings.append(emb)
            
            return embeddings

        except Exception as e:
            raise ModelError(f"Embedding failed: {str(e)}") from e

    async def embed_query(self, text: str) -> List[float]:
        res = await self.embed_documents([text])
        if not res:
            raise ModelError("Embedding returned empty result")
        return res[0]
```

[100] gecko/plugins/models/exceptions.py
```python
# gecko/plugins/models/exceptions.py
from gecko.core.exceptions import GeckoError, ErrorCode, ModelError

class ProviderError(ModelError):
    """
    模型服务商基础异常 (v0.4)
    继承自 ModelError 以保持向后兼容，确保上层业务逻辑能捕获所有模型错误。
    """
    def __init__(self, message: str, provider: str = "unknown", **kwargs):
        # 默认使用 MODEL_UNAVAILABLE，子类可覆盖
        code = kwargs.pop("code", ErrorCode.MODEL_UNAVAILABLE)
        super().__init__(message, code=code, **kwargs)
        self.context["provider"] = provider

class ContextWindowExceededError(ProviderError):
    """上下文窗口超限 (错误码: MODEL_CONTEXT_EXCEEDED)"""
    def __init__(self, message: str, max_tokens: int = 0, **kwargs):
        kwargs["code"] = ErrorCode.MODEL_CONTEXT_EXCEEDED
        super().__init__(message, **kwargs)
        self.context["max_tokens"] = max_tokens

class RateLimitError(ProviderError):
    """速率限制 (错误码: MODEL_RATE_LIMITED)"""
    def __init__(self, message: str, retry_after: float = 0.0, **kwargs):
        kwargs["code"] = ErrorCode.MODEL_RATE_LIMITED
        super().__init__(message, **kwargs)
        self.context["retry_after"] = retry_after

class AuthenticationError(ProviderError):
    """鉴权失败 (错误码: MODEL_AUTH_FAILED)"""
    def __init__(self, message: str, **kwargs):
        kwargs["code"] = ErrorCode.MODEL_AUTH_FAILED
        super().__init__(message, **kwargs)

class ServiceUnavailableError(ProviderError):
    """服务不可用 (5xx)"""
    pass
```

[101] gecko/plugins/models/factory.py
```python
# gecko/plugins/models/factory.py
from gecko.core.exceptions import ConfigurationError
from gecko.plugins.models.base import BaseChatModel
from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.registry import get_driver_class, list_drivers

# 导入默认驱动以触发注册
import gecko.plugins.models.drivers.litellm_driver  # noqa: F401


def create_model(config: ModelConfig) -> BaseChatModel:
    """
    根据配置创建模型实例 (工厂方法)
    """
    driver_cls = get_driver_class(config.driver_type)
    
    if not driver_cls:
        available = list_drivers()
        raise ConfigurationError(
            f"Driver '{config.driver_type}' not found. "
            f"Available drivers: {available}. "
            f"Have you registered it with @register_driver('{config.driver_type}')?"
        )
    
    return driver_cls(config)
```

[102] gecko/plugins/models/presets/ollama.py
```python
# gecko/plugins/models/presets/ollama.py
from __future__ import annotations

from typing import Any

from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.drivers.litellm_driver import LiteLLMDriver
from gecko.plugins.models.embedding import LiteLLMEmbedder


class OllamaChat(LiteLLMDriver):
    """Ollama 本地 Chat 模型预设"""
    
    def __init__(self, model: str = "llama3", base_url: str = "http://localhost:11434", **kwargs: Any):
        full_model_name = f"ollama/{model}" if not model.startswith("ollama/") else model
        
        config = ModelConfig(
            model_name=full_model_name,
            driver_type="litellm",
            base_url=base_url,
            api_key="ollama",
            timeout=kwargs.pop("timeout", 120.0),
            supports_function_calling=kwargs.pop("supports_function_calling", False),
            **kwargs
        )
        super().__init__(config)


class OllamaEmbedder(LiteLLMEmbedder):
    """Ollama 本地 Embedding 模型预设"""
    
    def __init__(
        self, 
        model: str = "nomic-embed-text", 
        base_url: str = "http://localhost:11434", 
        dimension: int = 768, 
        **kwargs: Any
    ):
        full_model_name = f"ollama/{model}" if not model.startswith("ollama/") else model
        
        config = ModelConfig(
            model_name=full_model_name,
            base_url=base_url,
            api_key="ollama",
            **kwargs
        )
        super().__init__(config, dimension=dimension)
```

[103] gecko/plugins/models/presets/openai.py
```python
# gecko/plugins/models/presets/openai.py
from __future__ import annotations

from typing import Any

from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.drivers.litellm_driver import LiteLLMDriver
from gecko.plugins.models.embedding import LiteLLMEmbedder


class OpenAIChat(LiteLLMDriver):
    """OpenAI Chat 模型预设"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o", **kwargs: Any):
        config = ModelConfig(
            model_name=model,
            driver_type="litellm",
            api_key=api_key,
            supports_vision=True,
            supports_function_calling=True,
            **kwargs
        )
        super().__init__(config)


class OpenAIEmbedder(LiteLLMEmbedder):
    """OpenAI Embedding 模型预设"""
    
    def __init__(self, api_key: str, model: str = "text-embedding-3-small", dimension: int = 1536, **kwargs: Any):
        config = ModelConfig(
            model_name=model,
            api_key=api_key,
            **kwargs
        )
        super().__init__(config, dimension=dimension)
```

[104] gecko/plugins/models/presets/zhipu.py
```python
# gecko/plugins/models/presets/zhipu.py
from __future__ import annotations

from typing import Any, AsyncIterator, Dict, List

from gecko.core.protocols import CompletionResponse, StreamChunk
from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.drivers.litellm_driver import LiteLLMDriver


class ZhipuChat(LiteLLMDriver):
    """
    智谱 AI (GLM) 预设
    
    继承自 LiteLLMDriver，复用其稳定的清洗逻辑和 OpenAI 协议。
    """
    
    def __init__(self, api_key: str, model: str = "glm-4-plus", **kwargs: Any):
        # 自动判断视觉支持
        is_vision = "v" in model.lower() or "vision" in model.lower()
        
        config = ModelConfig(
            model_name=model,
            # 显式指定使用 litellm 驱动
            driver_type="litellm",
            api_key=api_key,
            # 智谱官方 OpenAI 兼容接口
            base_url="https://open.bigmodel.cn/api/paas/v4/",
            # 强制 LiteLLM 使用 openai 协议处理
            extra_kwargs={"custom_llm_provider": "openai"},
            supports_vision=is_vision,
            supports_function_calling=True,
            **kwargs
        )
        super().__init__(config)
```

[105] gecko/plugins/models/registry.py
```python
# gecko/plugins/models/registry.py
from __future__ import annotations

from typing import Callable, Dict, Optional, Type, TypeVar

from gecko.core.logging import get_logger
from gecko.plugins.models.base import BaseChatModel

logger = get_logger(__name__)

_DRIVER_REGISTRY: Dict[str, Type[BaseChatModel]] = {}

# [修复] 定义一个泛型变量，限定范围是 BaseChatModel 的子类
T = TypeVar("T", bound=Type[BaseChatModel])

def register_driver(name: str) -> Callable[[T], T]:
    """
    装饰器：注册模型驱动
    
    使用泛型 T 确保类型推断能够透传：
    输入是具体的 Driver 类，返回的也是具体的 Driver 类（保留了具体实现信息）。
    """
    def decorator(cls: T) -> T:
        if name in _DRIVER_REGISTRY:
            logger.warning(f"Driver '{name}' already registered, overwriting with {cls.__name__}")
        _DRIVER_REGISTRY[name] = cls
        logger.debug(f"Registered model driver: {name}")
        return cls
    return decorator


def get_driver_class(name: str) -> Optional[Type[BaseChatModel]]:
    """获取驱动类"""
    return _DRIVER_REGISTRY.get(name)

def list_drivers() -> list[str]:
    """获取已注册的驱动名称列表"""
    return list(_DRIVER_REGISTRY.keys())
```

[106] gecko/plugins/registry.py
```python
# gecko/plugins/registry.py  
  
"""  
占位：预留统一插件注册机制的位置。  
  
TODO:  
    - 整合 tools/storage 等注册器  
    - 支持入口点加载第三方插件  
"""  
```

[107] gecko/plugins/storage/__init__.py
```python
# gecko/plugins/storage/__init__.py
"""
Gecko Storage 插件系统

提供统一的接口用于访问 Session (KV) 和 Vector (RAG) 存储。
"""
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.factory import create_storage
from gecko.plugins.storage.interfaces import SessionInterface, VectorInterface
from gecko.plugins.storage.registry import register_storage

__all__ = [
    "AbstractStorage",
    "create_storage",
    "SessionInterface",
    "VectorInterface",
    "register_storage",
]
```

[108] gecko/plugins/storage/abc.py
```python
# gecko/plugins/storage/abc.py
"""
存储抽象基类

定义所有存储后端必须实现的生命周期方法。
确保所有插件都有统一的初始化和关闭流程。
"""
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any, Dict


class AbstractStorage(ABC):
    """
    存储后端抽象基类
    
    所有具体的存储实现（SQLite, Redis, Chroma 等）都必须继承此类，
    并实现异步的生命周期管理方法。
    """
    
    def __init__(self, url: str, **kwargs: Any):
        """
        初始化存储后端配置
        
        参数:
            url: 连接字符串 (例如: sqlite:///./data.db)
            **kwargs: 额外的配置参数
        """
        self.url = url
        self.config = kwargs
        self._is_initialized = False

    @abstractmethod
    async def initialize(self) -> None:
        """
        异步初始化
        
        用于建立数据库连接、创建表结构、检查索引等耗时 IO 操作。
        必须确保此方法是幂等的（多次调用不会出错）。
        """
        pass

    @abstractmethod
    async def shutdown(self) -> None:
        """
        异步关闭
        
        用于释放连接池、关闭文件句柄、清理临时资源。
        """
        pass
    
    @property
    def is_initialized(self) -> bool:
        """检查是否已初始化"""
        return self._is_initialized

    async def __aenter__(self):
        """支持上下文管理器"""
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """退出上下文时自动关闭"""
        await self.shutdown()
```

[109] gecko/plugins/storage/backends/__init__.py
```python
```

[110] gecko/plugins/storage/backends/chroma.py
```python
# gecko/plugins/storage/backends/chroma.py
"""
ChromaDB 存储后端

更新日志：
- [Robustness] 增加 metadata 为 None 的防御性处理。
- [Robustness] 所有操作统一抛出 StorageError，屏蔽底层异常。
- [Feature] 实现 search 方法的 filters 参数支持 (Mapping dict to Chroma `where` clause)。
"""
from __future__ import annotations

from typing import Any, Dict, List, Optional, TYPE_CHECKING

# 仅用于类型检查的导入
if TYPE_CHECKING:
    from chromadb import ClientAPI, Collection # type: ignore

from gecko.core.exceptions import StorageError
from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.interfaces import SessionInterface, VectorInterface
from gecko.plugins.storage.mixins import (
    JSONSerializerMixin,
    ThreadOffloadMixin,
)
from gecko.plugins.storage.registry import register_storage
from gecko.plugins.storage.utils import parse_storage_url

logger = get_logger(__name__)


class IdentityEmbeddingFunction:
    """
    空实现的 Embedding Function。
    用于禁用 Chroma 内置的模型加载，提升性能。
    """
    def __call__(self, input: Any) -> Any:
        return [[0.0] for _ in input]

    def name(self) -> str:
        return "gecko_identity"


@register_storage("chroma")
class ChromaStorage(
    AbstractStorage,
    VectorInterface,
    SessionInterface,
    ThreadOffloadMixin,
    JSONSerializerMixin
):
    def __init__(self, url: str, **kwargs):
        super().__init__(url, **kwargs)
        scheme, path, params = parse_storage_url(url)
        
        self.persist_path = path
        self.collection_name = params.get("collection", "gecko_default")
        
        self.client: Optional[ClientAPI] = None
        self.vector_col: Optional[Collection] = None
        self.session_col: Optional[Collection] = None

    async def initialize(self) -> None:
        if self.is_initialized:
            return

        def _init_sync():
            try:
                import chromadb
                from chromadb.config import Settings

                logger.info("Initializing ChromaDB", path=self.persist_path)
                
                client = chromadb.PersistentClient(
                    path=self.persist_path,
                    settings=Settings(anonymized_telemetry=False)
                )
                
                ef : Any = IdentityEmbeddingFunction()
                
                v_col = client.get_or_create_collection(
                    name=self.collection_name,
                    metadata={"hnsw:space": "cosine"},
                    embedding_function=ef
                )
                
                s_col = client.get_or_create_collection(
                    name=f"{self.collection_name}_sessions",
                    embedding_function=ef
                )
                
                return client, v_col, s_col
            except Exception as e:
                raise StorageError(f"Failed to initialize ChromaDB: {e}") from e

        self.client, self.vector_col, self.session_col = await self._run_sync(_init_sync)
        self._is_initialized = True

    async def shutdown(self) -> None:
        self.client = None
        self.vector_col = None
        self.session_col = None
        self._is_initialized = False

    # ==================== VectorInterface 实现 ====================

    async def upsert(self, documents: List[Dict[str, Any]]) -> None:
        if not documents or not self.vector_col:
            return

        def _sync_upsert():
            try:
                if self.vector_col: 
                    # [修复] Chroma 不接受空字典作为 metadata，必须为 None 或 非空字典
                    metadatas = []
                    for d in documents:
                        m = d.get("metadata")
                        # 如果 m 是 None 或 {}，都转为 None
                        metadatas.append(m if m else None)
                    
                    self.vector_col.upsert(
                        ids=[d["id"] for d in documents],
                        embeddings=[d["embedding"] for d in documents],
                        metadatas=metadatas, # type: ignore
                        documents=[d.get("text", "") for d in documents]
                    )
            except Exception as e:
                raise StorageError(f"Chroma upsert failed: {e}") from e

        await self._run_sync(_sync_upsert)

    async def search(
        self, 
        query_embedding: List[float], 
        top_k: int = 5,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        if not self.vector_col:
            return []

        def _sync_search():
            try:
                if not self.vector_col:
                    return []
                
                # [Feature] 构造 Chroma filter
                # Chroma where 语法: {"field": "value"} 或 {"$and": [...]}
                chroma_filter = None
                if filters:
                    if len(filters) == 1:
                        chroma_filter = filters
                    else:
                        # 多个条件默认为 AND
                        chroma_filter = {"$and": [{k: v} for k, v in filters.items()]}

                results = self.vector_col.query(
                    query_embeddings=[query_embedding],
                    n_results=top_k,
                    where=chroma_filter, # type: ignore
                    include=["metadatas", "documents", "distances"]
                )
                
                parsed_results = []
                if not results["ids"]:
                    return []

                count = len(results["ids"][0])
                for i in range(count):
                    dist = results["distances"][0][i] # type: ignore
                    score = max(0.0, 1.0 - dist)

                    # [Fix] 处理 metadata 为 None 的情况，统一返回 {}
                    raw_meta = results["metadatas"][0][i] # type: ignore
                    meta = raw_meta if raw_meta is not None else {}
                    
                    parsed_results.append({
                        "id": results["ids"][0][i],
                        "text": results["documents"][0][i], # type: ignore
                        "metadata": meta,
                        "score": score
                    })
                
                return parsed_results
            except Exception as e:
                raise StorageError(f"Chroma search failed: {e}") from e

        return await self._run_sync(_sync_search)

    # ==================== SessionInterface 实现 ====================

    async def get(self, session_id: str) -> Optional[Dict[str, Any]]:
        if not self.session_col:
            return None

        def _sync_get():
            try:
                if not self.session_col:
                    return None
                result = self.session_col.get(
                    ids=[session_id],
                    include=["documents"] # type: ignore
                )
                if result["ids"] and result["documents"]:
                    return result["documents"][0]
                return None
            except Exception as e:
                raise StorageError(f"Chroma session get failed: {e}") from e

        json_str = await self._run_sync(_sync_get)
        return self._deserialize(json_str)

    async def set(self, session_id: str, state: Dict[str, Any]) -> None:
        if not self.session_col:
            return

        json_str = self._serialize(state)

        def _sync_set():
            try:
                if not self.session_col:
                    return
                self.session_col.upsert(
                    ids=[session_id],
                    documents=[json_str],
                    metadatas=[{"type": "session_state"}]
                )
            except Exception as e:
                raise StorageError(f"Chroma session set failed: {e}") from e

        await self._run_sync(_sync_set)

    async def delete(self, session_id: str) -> None:
        if not self.session_col:
            return

        def _sync_delete():
            try:
                if not self.session_col:
                    return
                self.session_col.delete(ids=[session_id])
            except Exception as e:
                raise StorageError(f"Chroma session delete failed: {e}") from e
        
        await self._run_sync(_sync_delete)
```

[111] gecko/plugins/storage/backends/lancedb.py
```python
# gecko/plugins/storage/backends/lancedb.py
"""
LanceDB 存储后端

LanceDB 是一个无服务器、基于 Arrow 的高性能向量数据库。
本实现通过 ThreadOffloadMixin 处理文件 I/O。

核心特性：
1. **自动建表**：首次 Upsert 时根据数据结构自动创建表。
2. **线程安全**：I/O 操作隔离。

更新日志：
- [Robustness] 增加 metadata 为 None 的防御性处理。
- [Robustness] 所有操作统一抛出 StorageError。
- [Feature] 实现 search 方法的 filters 参数支持 (SQL-like string construction)。
"""
from __future__ import annotations

from typing import Any, Dict, List, Optional

from gecko.core.exceptions import StorageError
from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.interfaces import VectorInterface
from gecko.plugins.storage.mixins import ThreadOffloadMixin
from gecko.plugins.storage.registry import register_storage
from gecko.plugins.storage.utils import parse_storage_url

logger = get_logger(__name__)


@register_storage("lancedb")
class LanceDBStorage(
    AbstractStorage,
    VectorInterface,
    ThreadOffloadMixin
):
    def __init__(self, url: str, **kwargs):
        super().__init__(url, **kwargs)
        scheme, path, params = parse_storage_url(url)
        
        self.db_path = path
        self.table_name = params.get("table", "gecko_vectors")
        self.embedding_dim = int(params.get("dim", 1536))
        
        self.db: Any = None
        self.table: Any = None

    async def initialize(self) -> None:
        if self.is_initialized:
            return

        def _init_sync():
            try:
                import lancedb
                logger.info("Connecting to LanceDB", path=self.db_path)
                
                self.db = lancedb.connect(self.db_path)
                
                if self.table_name in self.db.table_names():
                    self.table = self.db.open_table(self.table_name)
                    logger.debug(f"Opened existing table: {self.table_name}")
                else:
                    self.table = None
                    logger.debug(f"Table {self.table_name} will be created on first upsert")
            except Exception as e:
                raise StorageError(f"Failed to initialize LanceDB: {e}") from e

        await self._run_sync(_init_sync)
        self._is_initialized = True

    async def shutdown(self) -> None:
        self.db = None
        self.table = None
        self._is_initialized = False

    async def upsert(self, documents: List[Dict[str, Any]]) -> None:
        if not documents:
            return

        def _sync_upsert():
            try:
                data = []
                for doc in documents:
                    # [防御性处理] 确保 metadata 是字典
                    meta = doc.get("metadata")
                    if meta is None:
                        meta = {}
                    
                    item = {
                        "id": doc["id"],
                        "vector": doc["embedding"],
                        "text": doc.get("text", ""),
                        "metadata": meta
                    }
                    data.append(item)

                if self.table is None:
                    try:
                        self.table = self.db.create_table(self.table_name, data=data)
                        logger.info(f"Created LanceDB table: {self.table_name}")
                    except Exception as e:
                        # 处理并发创建冲突
                        if self.table_name in self.db.table_names():
                            self.table = self.db.open_table(self.table_name)
                            self.table.add(data)
                        else:
                            raise e
                else:
                    self.table.add(data)
            except Exception as e:
                raise StorageError(f"LanceDB upsert failed: {e}") from e

        await self._run_sync(_sync_upsert)

    async def search(
        self, 
        query_embedding: List[float], 
        top_k: int = 5,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        if self.table is None:
            return []

        def _sync_search():
            try:
                if self.table is None:
                    return []
                
                # 构建查询
                query = self.table.search(query_embedding).limit(top_k)
                
                # [Feature] 添加 where 子句
                # LanceDB metadata 存储为 struct，查询语法: "metadata.key = 'value'"
                if filters:
                    clauses = []
                    for k, v in filters.items():
                        if isinstance(v, str):
                            clauses.append(f"metadata.{k} = '{v}'")
                        else:
                            clauses.append(f"metadata.{k} = {v}")
                    
                    if clauses:
                        where_str = " AND ".join(clauses)
                        query = query.where(where_str)

                results = query.to_list()
                
                parsed_results = []
                for r in results:
                    distance = r.get("_distance", 0.0)
                    
                    parsed_results.append({
                        "id": r["id"],
                        "text": r["text"],
                        "metadata": r["metadata"],
                        "score": 1.0 / (1.0 + distance) 
                    })
                return parsed_results
            except Exception as e:
                raise StorageError(f"LanceDB search failed: {e}") from e

        return await self._run_sync(_sync_search)
```

[112] gecko/plugins/storage/backends/redis.py
```python
# gecko/plugins/storage/backends/redis.py
"""
Redis 存储后端

基于 redis-py (asyncio) 实现的高性能会话存储。
由于 Redis 客户端原生支持 asyncio，因此不需要使用 ThreadOffloadMixin。

核心特性：
1. **原生异步**：直接利用 asyncio Event Loop，性能极高。
2. **TTL 管理**：支持会话自动过期。

更新日志：
- [Robustness] 初始化失败时自动清理资源。
- [Robustness] 所有操作统一抛出 StorageError，屏蔽底层 redis 异常。
"""
from __future__ import annotations

from typing import Any, Dict, Optional

try:
    import redis.asyncio as redis
except ImportError:
    redis = None  # type: ignore

from gecko.core.exceptions import StorageError
from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.interfaces import SessionInterface
from gecko.plugins.storage.mixins import JSONSerializerMixin
from gecko.plugins.storage.registry import register_storage
from gecko.plugins.storage.utils import parse_storage_url
from gecko.config import get_settings

logger = get_logger(__name__)


@register_storage("redis")
class RedisStorage(
    AbstractStorage,
    SessionInterface,
    JSONSerializerMixin
):
    def __init__(self, url: str, **kwargs):
        super().__init__(url, **kwargs)
        if redis is None:
            raise ImportError(
                "Redis client not installed. Please install: pip install redis"
            )
        
        scheme, path, params = parse_storage_url(url)
        
        try:
            self.ttl = int(params.get("ttl", 3600 * 24 * 7))
        except ValueError:
            self.ttl = 3600 * 24 * 7
            
        self.prefix = kwargs.get("prefix", "gecko:session:")
        self.client: Optional[redis.Redis] = None # type: ignore

    async def initialize(self) -> None:
        if self.is_initialized:
            return

        logger.info("Connecting to Redis", url=self.url)

        settings = get_settings()
        
        try:
            # [优化] 配置 Redis 连接池
            # redis-py 的 from_url 支持 max_connections 参数
            max_connections = self.config.get("max_connections", settings.storage_pool_size * 2)
            
            self.client = redis.from_url( # type: ignore
                self.url,
                decode_responses=True,
                encoding="utf-8",
                max_connections=max_connections, # [新增]
                socket_timeout=5.0, # [新增] 防止网络卡死
                socket_connect_timeout=5.0
            )
            if self.client:
                await self.client.ping()
            self._is_initialized = True
            logger.debug("Redis connected successfully")
        except Exception as e:
            logger.error("Failed to connect to Redis", error=str(e))
            # 初始化失败时尝试清理
            await self.shutdown()
            raise StorageError(f"Failed to connect to Redis: {e}") from e

    async def shutdown(self) -> None:
        if self.client:
            try:
                await self.client.aclose()
            except Exception as e:
                logger.warning(f"Error closing Redis connection: {e}")
            finally:
                self.client = None
        self._is_initialized = False

    async def get(self, session_id: str) -> Optional[Dict[str, Any]]:
        if not self.client:
            raise StorageError("RedisStorage not initialized")
            
        key = f"{self.prefix}{session_id}"
        try:
            data = await self.client.get(key)
            return self._deserialize(data)
        except Exception as e:
            logger.error("Redis get failed", session_id=session_id, error=str(e))
            raise StorageError(f"Redis get failed: {e}") from e

    async def set(self, session_id: str, state: Dict[str, Any]) -> None:
        if not self.client:
            raise StorageError("RedisStorage not initialized")
            
        key = f"{self.prefix}{session_id}"
        json_str = self._serialize(state)
        
        try:
            await self.client.setex(key, self.ttl, json_str)
        except Exception as e:
            logger.error("Redis set failed", session_id=session_id, error=str(e))
            raise StorageError(f"Redis set failed: {e}") from e

    async def delete(self, session_id: str) -> None:
        if not self.client:
            raise StorageError("RedisStorage not initialized")
            
        key = f"{self.prefix}{session_id}"
        try:
            await self.client.delete(key)
        except Exception as e:
            logger.error("Redis delete failed", session_id=session_id, error=str(e))
            raise StorageError(f"Redis delete failed: {e}") from e
```

[113] gecko/plugins/storage/backends/sqlite.py
```python
# gecko/plugins/storage/backends/sqlite.py
from __future__ import annotations

import os
import time
from pathlib import Path
from typing import Any, Dict, Optional

from sqlalchemy import text
from sqlmodel import Field, Session, SQLModel, create_engine, select, delete

from gecko.core.exceptions import StorageError
from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.interfaces import SessionInterface
from gecko.plugins.storage.mixins import (
    AtomicWriteMixin,
    JSONSerializerMixin,
    ThreadOffloadMixin,
)
from gecko.plugins.storage.registry import register_storage
from gecko.plugins.storage.utils import parse_storage_url
from gecko.config import get_settings

logger = get_logger(__name__)


class SessionModel(SQLModel, table=True):
    """SQLModel 表定义"""
    __tablename__ = "gecko_sessions" # type: ignore
    session_id: str = Field(primary_key=True)
    state_json: str = Field(default="{}")
    # 新增字段用于 TTL 支持
    expire_at: Optional[float] = Field(default=None, index=True)


@register_storage("sqlite")
class SQLiteStorage(
    AbstractStorage,
    SessionInterface,
    ThreadOffloadMixin,
    AtomicWriteMixin,
    JSONSerializerMixin
):
    def __init__(self, url: str, **kwargs):
        super().__init__(url, **kwargs)
        
        scheme, path, params = parse_storage_url(url)
        
        self.db_path = path
        # [修复] 兼容 sqlite:///:memory: (path=/:memory:) 和 sqlite://:memory: (path=:memory:)
        self.is_memory = path in (":memory:", "/:memory:")

        # [Fix] 动态获取最新配置
        settings = get_settings()
        
        try:
            if self.is_memory:
                # 内存模式统一使用 sqlalchemy 可识别的 URL
                sqlalchemy_url = "sqlite:///:memory:"
                connect_args = {"check_same_thread": False}
                pool_args = {}
            else:
                # 文件模式
                try:
                    db_file = Path(self.db_path)
                    if not db_file.parent.exists():
                        db_file.parent.mkdir(parents=True, exist_ok=True)
                except Exception as e:
                    raise StorageError(f"Failed to configure SQLite path: {e}") from e

                # 仅在文件模式下启用文件锁
                self.setup_multiprocess_lock(self.db_path)

                sqlalchemy_url = f"sqlite:///{self.db_path}"
                connect_args = {"check_same_thread": False, "timeout": 30}
                
                pool_args = {
                    "pool_size": kwargs.get("pool_size", settings.storage_pool_size),
                    "max_overflow": kwargs.get("max_overflow", settings.storage_max_overflow),
                    "pool_recycle": 3600,
                }

            self.engine = create_engine(
                sqlalchemy_url,
                connect_args=connect_args,
                **pool_args
            )
        except Exception as e:
            if isinstance(e, StorageError):
                raise e
            raise StorageError(f"Failed to configure SQLite: {e}") from e

    async def initialize(self) -> None:
        if self.is_initialized:
            return

        try:
            # 确保表结构最新
            await self._run_sync(SQLModel.metadata.create_all, self.engine)
            
            if not self.is_memory:
                def _enable_wal():
                    with self.engine.connect() as conn:
                        conn.execute(text("PRAGMA journal_mode=WAL;"))
                        conn.execute(text("PRAGMA synchronous=NORMAL;"))
                
                await self._run_sync(_enable_wal)
                logger.debug("SQLite WAL mode enabled", path=self.db_path)
                
            self._is_initialized = True
            logger.info("SQLite storage initialized", url=self.url)
        except Exception as e:
            raise StorageError(f"Failed to initialize SQLite: {e}") from e

    async def shutdown(self) -> None:
        try:
            self.engine.dispose()
        except Exception as e:
            logger.warning(f"Error during SQLite shutdown: {e}")
        finally:
            self._is_initialized = False

    async def get(self, session_id: str) -> Optional[Dict[str, Any]]:
        # 性能优化：反序列化移入子线程
        def _sync_get_and_deserialize():
            try:
                with Session(self.engine) as session:
                    statement = select(SessionModel).where(
                        SessionModel.session_id == session_id
                    )
                    result = session.exec(statement).first()
                    
                    if not result:
                        return None
                    
                    # 懒惰过期检查
                    if result.expire_at and result.expire_at < time.time():
                        return None

                    return self._deserialize(result.state_json)
            except Exception as e:
                raise e

        try:
            return await self._run_sync(_sync_get_and_deserialize)
        except Exception as e:
            raise StorageError(f"SQLite get failed: {e}") from e

    async def set(self, session_id: str, state: Dict[str, Any]) -> None:
        # 性能优化：序列化移入子线程
        def _sync_serialize_and_set():
            # 获取文件锁（内存模式下 file_lock_guard 为空操作）
            with self.file_lock_guard():
                try:
                    # 1. 序列化
                    json_str = self._serialize(state)
                    
                    # 2. 计算过期时间
                    expire_at = None
                    metadata = state.get("metadata", {})
                    ttl = metadata.get("ttl")
                    if isinstance(ttl, (int, float)) and ttl > 0:
                        updated_at = metadata.get("updated_at", time.time())
                        expire_at = updated_at + ttl

                    # 3. 写入
                    with Session(self.engine) as session:
                        statement = select(SessionModel).where(
                            SessionModel.session_id == session_id
                        )
                        existing = session.exec(statement).first()
                        
                        if existing:
                            existing.state_json = json_str
                            existing.expire_at = expire_at
                            session.add(existing)
                        else:
                            new_rec = SessionModel(
                                session_id=session_id, 
                                state_json=json_str,
                                expire_at=expire_at
                            )
                            session.add(new_rec)
                        session.commit()
                except Exception as e:
                    raise e

        try:
            async with self.write_guard():
                await self._run_sync(_sync_serialize_and_set)
        except Exception as e:
            raise StorageError(f"SQLite set failed: {e}") from e

    async def delete(self, session_id: str) -> None:
        def _sync_delete():
            with self.file_lock_guard():
                try:
                    with Session(self.engine) as session:
                        statement = select(SessionModel).where(
                            SessionModel.session_id == session_id
                        )
                        result = session.exec(statement).first()
                        if result:
                            session.delete(result)
                            session.commit()
                except Exception as e:
                    raise e
        
        try:
            async with self.write_guard():
                await self._run_sync(_sync_delete)
        except Exception as e:
            raise StorageError(f"SQLite delete failed: {e}") from e

    async def cleanup_expired(self) -> int:
        """物理删除过期会话"""
        def _sync_cleanup():
            with self.file_lock_guard():
                try:
                    now = time.time()
                    with Session(self.engine) as session:
                        statement = delete(SessionModel).where(
                            SessionModel.expire_at != None, # type: ignore
                            SessionModel.expire_at < now # type: ignore
                        )
                        result = session.exec(statement) # type: ignore
                        session.commit()
                        return result.rowcount
                except Exception as e:
                    raise e
        
        try:
            async with self.write_guard():
                return await self._run_sync(_sync_cleanup)
        except Exception as e:
            logger.error("Failed to cleanup expired sessions", error=str(e))
            return 0
```

[114] gecko/plugins/storage/factory.py
```python
# gecko/plugins/storage/factory.py
"""
存储工厂

负责解析 URL，自动加载对应的后端模块，实例化并初始化存储对象。

更新日志：
- [Arch] 引入 importlib.metadata (entry_points) 支持第三方插件自动发现。
- [Refactor] 优化加载顺序：Entry Points -> Built-in -> Registry Check。
"""
from __future__ import annotations

import importlib
import inspect
import sys
# 引入 metadata 用于发现插件
from importlib.metadata import entry_points
from typing import Any, Optional, Type

from gecko.core.exceptions import ConfigurationError
from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
# [重要] 导入模块本身，防止闭包引用陈旧的 _STORAGE_REGISTRY 字典
import gecko.plugins.storage.registry as storage_registry
from gecko.plugins.storage.utils import parse_storage_url

logger = get_logger(__name__)

# 模块映射表：Scheme -> Module Path (内置后端)
_BACKEND_MODULES = {
    "sqlite": "gecko.plugins.storage.backends.sqlite",
    "redis": "gecko.plugins.storage.backends.redis",
    "chroma": "gecko.plugins.storage.backends.chroma",
    "lancedb": "gecko.plugins.storage.backends.lancedb",
    "postgres": "gecko.plugins.storage.backends.postgres",
    "qdrant": "gecko.plugins.storage.backends.qdrant",
    "milvus": "gecko.plugins.storage.backends.milvus",
}

# 插件组名称
PLUGIN_GROUP = "gecko.storage.backends"


def _load_from_entry_point(scheme: str) -> bool:
    """
    尝试从 Entry Points 加载插件
    
    返回: 是否成功加载
    """
    # 兼容 Python 3.10+ 和旧版本的 entry_points API
    try:
        # Python 3.10+
        eps = entry_points(group=PLUGIN_GROUP)
    except TypeError:
        # Python 3.8/3.9: entry_points() 返回字典
        eps = entry_points().get(PLUGIN_GROUP, []) # type: ignore

    # 查找匹配 scheme 的插件
    # 假设 entry point name 即为 scheme (例如: "mongo = my_package.mongo:MongoStorage")
    target_ep = next((ep for ep in eps if ep.name == scheme), None)

    if target_ep:
        try:
            logger.info(f"Loading storage plugin via entry point: {scheme}")
            # load() 会导入模块并返回对象 (类或函数)
            # 导入模块时，其内部的 @register_storage 装饰器应该会被触发
            target_ep.load()
            return True
        except Exception as e:
            logger.error(f"Failed to load plugin for '{scheme}': {e}")
            # 这里不抛出异常，允许回退到内置模块
            return False
    
    return False


async def create_storage(url: str, **kwargs: Any) -> AbstractStorage:
    """
    创建并初始化存储后端
    
    流程：
    1. 解析 URL 获取协议 (scheme)。
    2. 检查注册表是否已有对应类。
    3. 如果没有，按优先级尝试动态加载：
       a. Entry Points (第三方插件)
       b. Built-in Modules (内置支持)
    4. 再次检查注册表。
    5. 回退机制：手动扫描模块。
    6. 实例化并异步初始化。
    
    参数:
        url: 存储 URL (e.g., "sqlite:///data.db")
        **kwargs: 传递给存储后端的额外参数
    
    返回:
        已初始化的存储对象
        
    异常:
        ConfigurationError: 无法加载或初始化存储后端
    """
    try:
        scheme, _, _ = parse_storage_url(url)
        # 处理特殊变体 (如 postgres+pgvector -> postgres)
        clean_scheme = scheme.split("+")[0]
    except Exception as e:
        raise ConfigurationError(f"Invalid storage URL: {e}") from e

    # 1. 尝试从注册表获取
    cls: Optional[Type[AbstractStorage]] = storage_registry.get_storage_class(scheme)

    # 2. 如果未注册，尝试动态加载
    if not cls:
        module: Any = None
        loaded = False

        # 2a. 优先尝试 Entry Points (允许插件覆盖内置)
        if _load_from_entry_point(clean_scheme):
            loaded = True
        
        # 2b. 如果插件未处理，尝试内置模块
        if not loaded:
            module_path = _BACKEND_MODULES.get(clean_scheme)
            if not module_path:
                # 如果既没有插件也没有内置支持
                raise ConfigurationError(
                    f"Unknown storage scheme: '{scheme}'. "
                    f"Supported built-ins: {list(_BACKEND_MODULES.keys())}, "
                    f"Plugins: Check '{PLUGIN_GROUP}' entry points."
                )
            
            try:
                logger.debug("Lazy loading built-in backend", module=module_path)
                
                # [Fix] 增加对 None 的检查。
                # 如果测试 Mock 将模块置为 None，或者模块加载失败残留为 None，应视为未加载，
                # 从而进入 import_module 尝试加载（并触发预期的 ImportError）。
                if module_path in sys.modules and sys.modules[module_path] is not None:
                    module = importlib.reload(sys.modules[module_path])
                else:
                    module = importlib.import_module(module_path)
                
                loaded = True
            except ImportError as e:
                raise ConfigurationError(
                    f"Failed to load built-in backend for '{scheme}'.\n"
                    f"Missing dependency? Try installing: pip install gecko-ai[{clean_scheme}]\n"
                    f"Error: {e}"
                ) from e
        
        # 3. 再次尝试从注册表获取
        cls = storage_registry.get_storage_class(scheme)
        
        # 4. 回退机制：手动扫描模块 (针对内置模块加载后装饰器失效的罕见情况)
        if not cls and module:
            logger.warning(
                f"Registry lookup failed for {scheme}, scanning module..."
            )
            for name, obj in inspect.getmembers(module):
                if (inspect.isclass(obj) 
                    and issubclass(obj, AbstractStorage) 
                    and obj is not AbstractStorage):
                    
                    cls = obj
                    # 手动补注册
                    storage_registry._STORAGE_REGISTRY[scheme] = cls
                    logger.info(f"Manually registered {name} for {scheme}")
                    break
            
        if not cls:
            current_keys = list(storage_registry._STORAGE_REGISTRY.keys())
            raise ConfigurationError(
                f"Backend loaded but no storage class registered for '{scheme}'.\n"
                f"Current registry keys: {current_keys}"
            )

    # 5. 实例化与初始化
    instance: Optional[AbstractStorage] = None
    
    try:
        instance = cls(url, **kwargs)
        await instance.initialize()
        return instance
    except Exception as e:
        if instance:
            await instance.shutdown()
        raise ConfigurationError(f"Failed to initialize {cls.__name__}: {e}") from e
```

[115] gecko/plugins/storage/interfaces.py
```python
# gecko/plugins/storage/interfaces.py
"""
业务接口定义

定义 Session（会话存储）和 Vector（向量存储）的标准行为。

更新日志：
- [Feat] VectorInterface.search 增加 filters 参数，支持元数据过滤。
"""
from __future__ import annotations

from abc import abstractmethod
from typing import Any, Dict, List, Optional, Protocol, runtime_checkable


@runtime_checkable
class SessionInterface(Protocol):
    """
    Session 存储接口协议
    
    负责 Agent 的短期记忆（Conversation History）和状态（State）的持久化。
    """
    
    @abstractmethod
    async def get(self, session_id: str) -> Optional[Dict[str, Any]]:
        """
        获取会话状态
        
        参数:
            session_id: 会话唯一标识
            
        返回:
            状态字典，如果不存在返回 None
        """
        ...

    @abstractmethod
    async def set(self, session_id: str, state: Dict[str, Any]) -> None:
        """
        设置/更新会话状态
        
        参数:
            session_id: 会话唯一标识
            state: 要保存的状态字典
        """
        ...

    @abstractmethod
    async def delete(self, session_id: str) -> None:
        """
        删除会话
        
        参数:
            session_id: 会话唯一标识
        """
        ...


@runtime_checkable
class VectorInterface(Protocol):
    """
    Vector 存储接口协议 (RAG 用)
    
    负责文档的向量存储与检索。
    """
    
    @abstractmethod
    async def upsert(self, documents: List[Dict[str, Any]]) -> None:
        """
        插入或更新向量文档
        
        参数:
            documents: 文档列表，每项需包含 id, embedding, text, metadata
        """
        ...

    @abstractmethod
    async def search(
        self, 
        query_embedding: List[float], 
        top_k: int = 5,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        向量相似度搜索
        
        参数:
            query_embedding: 查询向量
            top_k: 返回结果数量
            filters: 元数据过滤条件 (Key-Value 精确匹配)
            
        返回:
            包含 text, metadata, score 的结果列表
        """
        ...
```

[116] gecko/plugins/storage/mixins.py
```python
# gecko/plugins/storage/mixins.py
"""
存储功能混入类 (Mixins)

包含解决异步阻塞、并发冲突和数据序列化的通用逻辑。

更新日志：
- [Arch] 引入 FileLock 实现跨进程文件锁，解决多进程环境下 SQLite/文件存储的并发安全问题。
- [Fix] 分离 asyncio.Lock (write_guard) 和 FileLock (file_lock_guard) 以避免死锁。
"""
from __future__ import annotations

import asyncio
import json
from contextlib import asynccontextmanager, contextmanager
from functools import partial
import os
from typing import Any, Callable, TypeVar, Optional

from anyio import to_thread

from gecko.core.exceptions import ConfigurationError
from gecko.core.logging import get_logger

logger = get_logger(__name__)

# 尝试导入 filelock，用于跨进程文件锁
try:
    from filelock import FileLock
    FILELOCK_AVAILABLE = True
except ImportError:
    FILELOCK_AVAILABLE = False

T = TypeVar("T")


class ThreadOffloadMixin:
    """
    [核心] 线程卸载混入类
    
    将同步的 IO 操作（如 sqlite3, chromadb, pandas 操作）卸载到
    独立的线程池中执行，防止阻塞主线程的 Event Loop。
    
    原理: 使用 anyio.to_thread.run_sync
    """
    
    async def _run_sync(self, func: Callable[..., T], *args: Any, **kwargs: Any) -> T:
        """
        在线程池中执行同步函数
        
        参数:
            func: 同步函数
            *args, **kwargs: 传递给函数的参数
            
        返回:
            函数执行结果
        """
        if kwargs:
            func = partial(func, **kwargs)
        
        return await to_thread.run_sync(func, *args)


class AtomicWriteMixin:
    """
    [核心] 原子写混入类 (增强版)
    
    提供双层锁机制：
    1. asyncio.Lock: 保证单进程内协程的互斥 (write_guard)。
    2. FileLock: 保证跨进程（如 Gunicorn Worker）对同一文件的互斥访问 (file_lock_guard)。
    
    使用方法：
        1. 在子类 __init__ 中调用 self.setup_multiprocess_lock(path) 启用文件锁。
        2. 在 async 方法中由于 await _run_sync 调用前使用 async with self.write_guard()。
        3. 在 _run_sync 调用的同步函数内部使用 with self.file_lock_guard()。
    """
    
    _write_lock: Optional[asyncio.Lock] = None
    _file_lock: Any = None

    @property
    def write_lock(self) -> asyncio.Lock:
        """懒加载获取协程锁"""
        if getattr(self, "_write_lock", None) is None:
            self._write_lock = asyncio.Lock()
        return self._write_lock # type: ignore

    def setup_multiprocess_lock(self, lock_path: str) -> None:
        """
        配置跨进程文件锁
        
        参数:
            lock_path: 锁文件路径（通常是数据库文件路径）
        """
        # 获取当前运行环境，默认为 development
        env = os.getenv("GECKO_ENV", "development").lower()
        
        if not FILELOCK_AVAILABLE:
            msg = (
                "filelock module not installed. "
                "Install with: pip install filelock"
            )
            # 生产环境强制检查
            if env == "production":
                raise ConfigurationError(
                    f"[CRITICAL] Running in PRODUCTION mode without filelock! "
                    f"This will cause data corruption in multi-worker setups. {msg}"
                )
            
            logger.warning(
                f"filelock module not installed. Cross-process safety is NOT guaranteed. {msg}"
            )
            return

        try:
            self._file_lock = FileLock(f"{lock_path}.lock") # type: ignore
            logger.debug("FileLock initialized", path=f"{lock_path}.lock")
        except Exception as e:
            if env == "production":
                raise ConfigurationError(f"Failed to initialize FileLock in production: {e}") from e
            logger.error("Failed to initialize FileLock", error=str(e))

    @asynccontextmanager
    async def write_guard(self):
        """
        写操作保护上下文 (Async)
        
        只负责进程内协程互斥。
        """
        async with self.write_lock:
            yield

    @contextmanager
    def file_lock_guard(self):
        """
        跨进程文件锁 (Sync)
        
        必须在 Worker 线程（_run_sync 调用的函数）内部使用。
        确保 acquire/release 在同一个线程中执行，避免 Reentrancy 问题。
        """
        if self._file_lock:
            with self._file_lock:
                yield
        else:
            yield


class JSONSerializerMixin:
    """
    JSON 序列化混入类
    
    提供标准化的数据序列化/反序列化方法。
    """
    
    def _serialize(self, data: Any) -> str:
        """序列化为 JSON 字符串"""
        try:
            # ensure_ascii=False 减少体积并保持中文可读性
            return json.dumps(data, ensure_ascii=False)
        except (TypeError, ValueError) as e:
            logger.error("Serialization failed", error=str(e))
            raise

    def _deserialize(self, data: str | bytes | None) -> Any:
        """从 JSON 字符串反序列化"""
        if not data:
            return None
        try:
            return json.loads(data)
        except json.JSONDecodeError as e:
            logger.error("Deserialization failed", error=str(e))
            return None
```

[117] gecko/plugins/storage/pool.py
```python
# gecko/plugins/storage/pool.py
"""
连接池抽象层

提供统一的连接池管理接口，支持：
- 异步连接获取/释放
- 连接健康检查
- 自动重连
- 资源限制
"""
from __future__ import annotations

import asyncio
from abc import ABC, abstractmethod
from collections import deque
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from typing import Any, AsyncIterator, Deque, Generic, Optional, TypeVar

from gecko.core.logging import get_logger

logger = get_logger(__name__)

T = TypeVar("T")  # 连接类型


@dataclass
class PoolConfig:
    """连接池配置"""
    min_size: int = 1
    max_size: int = 10
    max_idle_time: float = 300.0  # 空闲连接超时（秒）
    acquire_timeout: float = 30.0  # 获取连接超时
    health_check_interval: float = 60.0  # 健康检查间隔
    retry_on_failure: bool = True
    max_retries: int = 3


@dataclass
class PooledConnection(Generic[T]):
    """池化连接包装器"""
    connection: T
    created_at: float
    last_used_at: float
    use_count: int = 0
    is_healthy: bool = True


class ConnectionPool(ABC, Generic[T]):
    """
    异步连接池抽象基类
    
    子类需要实现：
    - _create_connection(): 创建新连接
    - _close_connection(): 关闭连接
    - _check_health(): 检查连接健康状态
    """
    
    def __init__(self, config: Optional[PoolConfig] = None):
        self.config = config or PoolConfig()
        
        self._pool: Deque[PooledConnection[T]] = deque()
        self._in_use: set[PooledConnection[T]] = set()
        self._lock = asyncio.Lock()
        self._available = asyncio.Semaphore(self.config.max_size)
        
        self._is_closed = False
        self._health_check_task: Optional[asyncio.Task] = None
        
        # 统计
        self._stats = {
            "connections_created": 0,
            "connections_closed": 0,
            "acquires": 0,
            "releases": 0,
            "health_checks": 0,
            "health_failures": 0,
        }
    
    @abstractmethod
    async def _create_connection(self) -> T:
        """创建新连接（子类实现）"""
        pass
    
    @abstractmethod
    async def _close_connection(self, conn: T) -> None:
        """关闭连接（子类实现）"""
        pass
    
    @abstractmethod
    async def _check_health(self, conn: T) -> bool:
        """检查连接健康状态（子类实现）"""
        pass
    
    async def initialize(self) -> None:
        """初始化连接池"""
        logger.info(
            "Initializing connection pool",
            min_size=self.config.min_size,
            max_size=self.config.max_size
        )
        
        # 创建最小数量的连接
        for _ in range(self.config.min_size):
            try:
                conn = await self._create_pooled_connection()
                self._pool.append(conn)
            except Exception as e:
                logger.error("Failed to create initial connection", error=str(e))
        
        # 启动健康检查任务
        if self.config.health_check_interval > 0:
            self._health_check_task = asyncio.create_task(self._health_check_loop())
    
    async def close(self) -> None:
        """关闭连接池"""
        self._is_closed = True
        
        # 取消健康检查
        if self._health_check_task:
            self._health_check_task.cancel()
            try:
                await self._health_check_task
            except asyncio.CancelledError:
                pass
        
        # 关闭所有连接
        async with self._lock:
            while self._pool:
                pooled = self._pool.popleft()
                await self._safe_close(pooled.connection)
            
            for pooled in self._in_use:
                await self._safe_close(pooled.connection)
            self._in_use.clear()
        
        logger.info("Connection pool closed", stats=self._stats)
    
    async def acquire(self) -> T:
        """
        获取连接
        
        返回:
            可用的连接
            
        异常:
            TimeoutError: 超时未能获取连接
            RuntimeError: 连接池已关闭
        """
        if self._is_closed:
            raise RuntimeError("Connection pool is closed")
        
        # 等待信号量（限制并发连接数）
        try:
            await asyncio.wait_for(
                self._available.acquire(),
                timeout=self.config.acquire_timeout
            )
        except asyncio.TimeoutError:
            raise TimeoutError(
                f"Failed to acquire connection within {self.config.acquire_timeout}s"
            )
        
        try:
            pooled = await self._get_or_create_connection()
            
            async with self._lock:
                self._in_use.add(pooled)
            
            pooled.use_count += 1
            pooled.last_used_at = asyncio.get_event_loop().time()
            
            self._stats["acquires"] += 1
            
            return pooled.connection
            
        except Exception:
            self._available.release()
            raise
    
    async def release(self, conn: T) -> None:
        """释放连接"""
        pooled = None
        
        async with self._lock:
            for p in self._in_use:
                if p.connection is conn:
                    pooled = p
                    break
            
            if pooled:
                self._in_use.remove(pooled)
        
        if pooled:
            # 检查是否应该回收
            if self._should_recycle(pooled):
                await self._safe_close(pooled.connection)
            else:
                async with self._lock:
                    self._pool.append(pooled)
        
        self._available.release()
        self._stats["releases"] += 1
    
    @asynccontextmanager
    async def connection(self) -> AsyncIterator[T]:
        """
        上下文管理器方式获取连接
        
        示例:
            async with pool.connection() as conn:
                await conn.execute(...)
        """
        conn = await self.acquire()
        try:
            yield conn
        finally:
            await self.release(conn)
    
    async def _get_or_create_connection(self) -> PooledConnection[T]:
        """获取或创建连接"""
        async with self._lock:
            # 尝试从池中获取
            while self._pool:
                pooled = self._pool.popleft()
                
                # 检查是否过期
                if self._is_expired(pooled):
                    await self._safe_close(pooled.connection)
                    continue
                
                # 简单健康检查
                if pooled.is_healthy:
                    return pooled
                else:
                    await self._safe_close(pooled.connection)
        
        # 创建新连接
        return await self._create_pooled_connection()
    
    async def _create_pooled_connection(self) -> PooledConnection[T]:
        """创建池化连接"""
        conn = await self._create_connection()
        now = asyncio.get_event_loop().time()
        
        self._stats["connections_created"] += 1
        
        return PooledConnection(
            connection=conn,
            created_at=now,
            last_used_at=now,
        )
    
    async def _safe_close(self, conn: T) -> None:
        """安全关闭连接"""
        try:
            await self._close_connection(conn)
            self._stats["connections_closed"] += 1
        except Exception as e:
            logger.warning("Error closing connection", error=str(e))
    
    def _is_expired(self, pooled: PooledConnection[T]) -> bool:
        """检查连接是否过期"""
        now = asyncio.get_event_loop().time()
        idle_time = now - pooled.last_used_at
        return idle_time > self.config.max_idle_time
    
    def _should_recycle(self, pooled: PooledConnection[T]) -> bool:
        """判断是否应该回收连接"""
        # 不健康的连接回收
        if not pooled.is_healthy:
            return True
        
        # 池已满，回收
        if len(self._pool) >= self.config.max_size:
            return True
        
        return False
    
    async def _health_check_loop(self) -> None:
        """后台健康检查任务"""
        while not self._is_closed:
            try:
                await asyncio.sleep(self.config.health_check_interval)
                await self._run_health_check()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error("Health check loop error", error=str(e))
    
    async def _run_health_check(self) -> None:
        """执行健康检查"""
        async with self._lock:
            connections_to_check = list(self._pool)
        
        for pooled in connections_to_check:
            self._stats["health_checks"] += 1
            
            try:
                is_healthy = await self._check_health(pooled.connection)
                pooled.is_healthy = is_healthy
                
                if not is_healthy:
                    self._stats["health_failures"] += 1
                    logger.warning("Connection health check failed")
                    
            except Exception as e:
                pooled.is_healthy = False
                self._stats["health_failures"] += 1
                logger.warning("Connection health check error", error=str(e))
    
    def get_stats(self) -> dict:
        """获取连接池统计"""
        return {
            **self._stats,
            "pool_size": len(self._pool),
            "in_use": len(self._in_use),
            "available": self.config.max_size - len(self._in_use),
        }


# ==================== 导出 ====================

__all__ = [
    "ConnectionPool",
    "PoolConfig",
    "PooledConnection",
]
```

[118] gecko/plugins/storage/registry.py
```python
# gecko/plugins/storage/registry.py
"""
存储插件注册器

负责管理 URL Scheme 到存储后端类的映射。
采用装饰器模式进行注册。
"""
from __future__ import annotations

from typing import Callable, Dict, Type

from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage

logger = get_logger(__name__)

# 存储后端类注册表
# Key: URL scheme (e.g., "sqlite", "redis")
# Value: Storage Class
_STORAGE_REGISTRY: Dict[str, Type[AbstractStorage]] = {}


def register_storage(scheme: str) -> Callable[[Type[AbstractStorage]], Type[AbstractStorage]]:
    """
    装饰器：注册存储后端实现
    
    参数:
        scheme: URL 协议前缀 (如 'sqlite', 'redis')
    
    示例:
        @register_storage("redis")
        class RedisStorage(AbstractStorage):
            ...
    """
    def decorator(cls: Type[AbstractStorage]) -> Type[AbstractStorage]:
        if scheme in _STORAGE_REGISTRY:
            logger.warning(
                "Storage scheme already registered, overwriting",
                scheme=scheme,
                existing=_STORAGE_REGISTRY[scheme].__name__,
                new=cls.__name__
            )
        
        _STORAGE_REGISTRY[scheme] = cls
        logger.debug("Registered storage backend", scheme=scheme, cls=cls.__name__)
        return cls
    
    return decorator


def get_storage_class(scheme: str) -> Type[AbstractStorage] | None:
    """获取已注册的存储类"""
    return _STORAGE_REGISTRY.get(scheme)
```

[119] gecko/plugins/storage/utils.py
```python
# gecko/plugins/storage/utils.py
"""
Storage 插件工具函数

提供统一的 URL 解析和验证逻辑。
"""
from __future__ import annotations

from urllib.parse import parse_qs, urlparse
from typing import Dict, Tuple, Optional


def parse_storage_url(url: str) -> Tuple[str, str, Dict[str, str]]:
    """
    解析存储 URL
    
    格式：scheme://path?param1=value1&param2=value2
    
    返回：(scheme, path, params)
    
    示例:
        "sqlite:///./data.db" -> ("sqlite", "./data.db", {})
        "sqlite://:memory:" -> ("sqlite", ":memory:", {})
    """
    if "://" not in url:
        raise ValueError(f"Invalid storage URL: '{url}'. Must include scheme.")
    
    parsed = urlparse(url)
    scheme = parsed.scheme
    
    # 解析路径
    if scheme == "sqlite":
        # 特殊处理 sqlite
        # sqlite:///foo.db -> path = /foo.db (urlparse behavior)
        # 我们需要去掉开头的 / 变成相对路径，或者保留绝对路径
        if parsed.netloc:
            # sqlite://:memory: -> netloc=':memory:'
            path = parsed.netloc
        else:
            # sqlite:///./data.db -> path='/./data.db'
            path = parsed.path
            if path.startswith("/") and len(path) > 1 and path[1] == ".":
                # 修正相对路径 /./data.db -> ./data.db
                path = path[1:]
            elif path.startswith("/"):
                 # 绝对路径保持不变，或者根据 OS 调整
                 # 这里简化处理，对于 Windows 可能需要更复杂的逻辑
                 pass
    else:
        path = f"{parsed.netloc}{parsed.path}"

    # 解析参数
    params: Dict[str, str] = {}
    if parsed.query:
        query_dict = parse_qs(parsed.query)
        params = {k: v[0] for k, v in query_dict.items()}
    
    return scheme, path, params
```

[120] gecko/plugins/tools/__init__.py
```python
# gecko/plugins/tools/__init__.py
"""
Gecko 工具插件系统

提供工具的定义、注册和发现机制。

核心组件：
- BaseTool: 所有工具的基类
- ToolResult: 标准化执行结果
- register_tool: 工具注册装饰器
- load_tool: 工具加载工厂
- ToolRegistry: 注册表管理类
"""
from gecko.plugins.tools.base import BaseTool, ToolResult
from gecko.plugins.tools.registry import ToolRegistry, load_tool, register_tool

__all__ = [
    "BaseTool",
    "ToolResult",
    "ToolRegistry",
    "register_tool",
    "load_tool",
]
```

[121] gecko/plugins/tools/base.py
```python
# gecko/plugins/tools/base.py
"""
工具基类定义

核心功能：
1. 定义统一的工具接口 BaseTool
2. 强制参数校验 (Pydantic)
3. 自动生成 OpenAI Function Schema
4. 统一的错误处理与结果封装
"""
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any, Dict, Type, Union

from pydantic import BaseModel, Field, ValidationError

from gecko.core.utils import ensure_awaitable


class ToolResult(BaseModel):
    """工具执行结果封装"""
    content: str = Field(..., description="工具执行的文本输出")
    is_error: bool = Field(default=False, description="是否执行出错")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="额外的元数据")


class BaseTool(BaseModel, ABC):
    """
    工具抽象基类
    
    所有自定义工具必须继承此类，并提供 args_schema 用于参数校验。
    
    示例:
        ```python
        class MyArgs(BaseModel):
            query: str
            
        class MyTool(BaseTool):
            name = "my_tool"
            description = "My awesome tool"
            args_schema = MyArgs
            
            async def _run(self, args: MyArgs) -> ToolResult:
                return ToolResult(content=f"Echo: {args.query}")
        ```
    """
    name: str = Field(..., description="工具唯一标识名称 (e.g., 'calculator')")
    description: str = Field(..., description="工具功能描述，用于 LLM 决策")
    
    # 排除 args_schema 不参与 BaseTool 本身的序列化，仅用于元编程
    args_schema: Type[BaseModel] = Field(..., exclude=True, description="参数定义的 Pydantic 模型类")

    @property
    def openai_schema(self) -> Dict[str, Any]:
        """
        自动生成 OpenAI Function Calling Schema
        """
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": self.args_schema.model_json_schema(),
            }
        }
    
    # 兼容旧代码的属性
    @property
    def parameters(self) -> Dict[str, Any]:
        return self.args_schema.model_json_schema()

    async def execute(self, arguments: Dict[str, Any]) -> ToolResult:
        """
        执行工具（模板方法）
        
        负责：
        1. 参数校验
        2. 异常捕获
        3. 调用具体的 _run 实现
        """
        try:
            # 1. Pydantic 校验
            validated_args = self.args_schema(**arguments)
        except ValidationError as e:
            return ToolResult(
                content=f"参数校验错误: {str(e)}",
                is_error=True
            )
        except Exception as e:
            return ToolResult(
                content=f"参数解析错误: {str(e)}",
                is_error=True
            )

        try:
            # 2. 执行业务逻辑 (支持同步/异步)
            result = await ensure_awaitable(self._run, validated_args)
            
            # 3. 结果标准化
            if isinstance(result, ToolResult):
                return result
            return ToolResult(content=str(result))
            
        except Exception as e:
            return ToolResult(
                content=f"工具执行内部错误: {str(e)}",
                is_error=True
            )

    @abstractmethod
    async def _run(self, args: BaseModel) -> Union[ToolResult, str]:
        """
        工具的具体实现逻辑
        
        参数:
            args: 已校验的 Pydantic 对象
        """
        pass
```

[122] gecko/plugins/tools/registry.py
```python
# gecko/plugins/tools/registry.py
"""
工具注册表

提供工具的自动发现、注册与工厂化创建能力。
"""
from __future__ import annotations

from typing import Dict, Type, List, Any, Optional

from gecko.core.logging import get_logger
from gecko.plugins.tools.base import BaseTool

logger = get_logger(__name__)


class ToolRegistry:
    """全局工具注册中心"""
    
    _registry: Dict[str, Type[BaseTool]] = {}

    @classmethod
    def register(cls, name: str):
        """
        装饰器：注册工具类
        
        示例:
            @register_tool("my_tool")
            class MyTool(BaseTool): ...
        """
        def decorator(tool_cls: Type[BaseTool]):
            if not issubclass(tool_cls, BaseTool):
                raise TypeError(f"Registered class {tool_cls.__name__} must inherit from BaseTool")
            
            if name in cls._registry:
                logger.warning(f"Tool '{name}' is being overwritten in registry.")
            
            cls._registry[name] = tool_cls
            logger.debug(f"Tool registered: {name} -> {tool_cls.__name__}")
            return tool_cls
        return decorator

    @classmethod
    def load_tool(cls, name: str, **kwargs: Any) -> BaseTool:
        """
        工厂方法：根据名称加载并实例化工具
        
        参数:
            name: 工具名称
            **kwargs: 传递给工具构造函数的参数
            
        异常:
            ValueError: 工具未找到
        """
        if name not in cls._registry:
            raise ValueError(f"Tool '{name}' not found in registry. Available: {list(cls._registry.keys())}")
        
        tool_cls = cls._registry[name]
        try:
            # 实例化工具
            # 注意：BaseTool 是 Pydantic 模型，kwargs 会被 validate
            return tool_cls(**kwargs)
        except Exception as e:
            raise ValueError(f"Failed to instantiate tool '{name}': {e}") from e

    @classmethod
    def list_tools(cls) -> List[str]:
        """列出所有已注册工具"""
        return list(cls._registry.keys())


# 便捷导出
register_tool = ToolRegistry.register
load_tool = ToolRegistry.load_tool
```

[123] gecko/plugins/tools/standard/__init__.py
```python
# gecko/plugins/tools/standard/__init__.py
"""
Gecko 标准工具库

包含一组经过安全审查和优化的内置工具。
导入此模块时，会自动将工具注册到 ToolRegistry。
"""
from gecko.plugins.tools.standard.calculator import CalculatorTool
from gecko.plugins.tools.standard.duckduckgo import DuckDuckGoSearchTool

# 可以在此定义 lazy_load 逻辑，目前为了简单直接导入以触发注册
__all__ = [
    "CalculatorTool",
    "DuckDuckGoSearchTool",
]
```

[124] gecko/plugins/tools/standard/calculator.py
```python
# gecko/plugins/tools/standard/calculator.py
"""
安全计算器工具

基于 AST 解析的安全数学表达式计算，防止代码注入。
"""
from __future__ import annotations

import ast
import math
import operator
from typing import Type, Union, Dict

from pydantic import BaseModel, Field

from gecko.plugins.tools.base import BaseTool, ToolResult
from gecko.plugins.tools.registry import register_tool


class CalculatorArgs(BaseModel):
    expression: str = Field(
        ..., 
        description="数学表达式，支持加减乘除、幂运算及常用数学函数 (sqrt, log, sin, etc.)。例如: '2 + 2 * sqrt(4)'"
    )


@register_tool("calculator")
class CalculatorTool(BaseTool):
    name: str = "calculator"
    description: str = "用于执行精确的数学计算。"
    args_schema: Type[BaseModel] = CalculatorArgs

    async def _run(self, args: CalculatorArgs) -> ToolResult: # type: ignore
        expr = args.expression.strip()
        
        # 长度限制防止 DoS
        if len(expr) > 500:
             return ToolResult(content="错误：表达式过长", is_error=True)

        try:
            result = self._safe_eval(expr)
            return ToolResult(content=str(result))
        except Exception as e:
            return ToolResult(content=f"计算错误: {str(e)}", is_error=True)

    def _safe_eval(self, expr: str) -> Union[int, float]:
        """
        基于 AST 的安全求值
        仅允许特定的节点类型和函数。
        """
        # 支持的操作符
        # 支持的操作符（仅数学相关，显式禁用位运算等）
        operators = {
            ast.Add: operator.add,
            ast.Sub: operator.sub,
            ast.Mult: operator.mul,
            ast.Div: operator.truediv,
            ast.Pow: operator.pow,
            ast.USub: operator.neg,
        }

        # 支持的函数
        functions = {
            "sqrt": math.sqrt,
            "log": math.log,
            "ln": math.log,
            "sin": math.sin,
            "cos": math.cos,
            "tan": math.tan,
            "abs": abs,
            "round": round,
            "ceil": math.ceil,
            "floor": math.floor,
            "pi": math.pi,
            "e": math.e,
        }

        def _eval(node):
            # 1. 数字
            if isinstance(node, ast.Constant): 
                if isinstance(node.value, (int, float)):
                    return node.value
                raise ValueError(f"不支持的常量类型: {type(node.value)}")

            # 2. 二元运算 (a + b)
            elif isinstance(node, ast.BinOp):
                op_type = type(node.op)
                if op_type in operators:
                    left = _eval(node.left)
                    right = _eval(node.right)

                    # [新增] 安全防御：限制幂运算的指数大小
                    if op_type == ast.Pow:
                        # 检查指数是否为数字且过大 (例如限制为 1000)
                        if isinstance(right, (int, float)) and right > 1000:
                            raise ValueError(f"指数过大: {right} (最大允许 1000)")

                    return operators[op_type](left, right)
            
                raise ValueError(f"不支持的操作符: {op_type}")

            # 3. 一元运算 (-a)
            elif isinstance(node, ast.UnaryOp):
                op_type = type(node.op)
                if op_type in operators:
                    return operators[op_type](_eval(node.operand))
                raise ValueError(f"不支持的一元操作符: {op_type}")

            # 4. 函数调用 (sqrt(4))
            elif isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name):
                    func_name = node.func.id
                    if func_name in functions:
                        # 递归计算参数
                        args = [_eval(arg) for arg in node.args]
                        return functions[func_name](*args)
                    raise ValueError(f"禁止调用的函数: {func_name}")
                raise ValueError("不支持的复杂函数调用")

            # 5. 变量名 (pi, e)
            elif isinstance(node, ast.Name):
                if node.id in functions:
                    val = functions[node.id]
                    if isinstance(val, (int, float)):
                        return val
                raise ValueError(f"未知变量: {node.id}")
            
            # [优化建议] 显式拦截属性访问和下标访问，给出更明确的提示
            elif isinstance(node, (ast.Attribute, ast.Subscript, ast.List, ast.Dict, ast.Tuple)):
                raise ValueError("出于安全考虑，禁止使用属性访问、下标或复杂数据结构")

            raise ValueError(f"非法表达式结构: {type(node)}")

        # 解析并求值
        tree = ast.parse(expr, mode='eval')
        return _eval(tree.body)
```

[125] gecko/plugins/tools/standard/duckduckgo.py
```python
# gecko/plugins/tools/standard/duckduckgo.py
from __future__ import annotations

from typing import Any, Dict, List, Type

from anyio.to_thread import run_sync
from pydantic import BaseModel, Field

from gecko.core.logging import get_logger
from gecko.plugins.tools.base import BaseTool, ToolResult
from gecko.plugins.tools.registry import register_tool

logger = get_logger(__name__)


class DuckDuckGoArgs(BaseModel):
    query: str = Field(..., description="搜索关键词", min_length=1, max_length=200)
    max_results: int = Field(default=5, description="返回的最大结果数量 (1-10)", ge=1, le=10)


@register_tool("duckduckgo_search")
class DuckDuckGoSearchTool(BaseTool):
    name: str = "duckduckgo_search"
    description: str = (
        "使用 DuckDuckGo 搜索引擎搜索互联网信息。"
        "当需要获取实时新闻、具体事实或不知道的信息时使用。"
        "无需 API Key。"
    )
    args_schema: Type[BaseModel] = DuckDuckGoArgs

    async def _run(self, args: DuckDuckGoArgs) -> ToolResult: # type: ignore
        # 检查依赖
        try:
            from duckduckgo_search import DDGS
        except ImportError:
            return ToolResult(
                content=(
                    "错误：未安装 duckduckgo_search 库。\n"
                    "请运行: pip install duckduckgo-search"
                ),
                is_error=True
            )

        query = args.query.strip()
        
        # 定义同步执行函数
        def _search_sync() -> List[Dict[str, str]]:
            results = []
            try:
                # 使用上下文管理器确保 session 关闭
                with DDGS() as ddgs:
                    # [修改] 移除 backend='api'，让库自动选择 (通常是 html 或 api)
                    # text() 方法在新版中可能返回 None 或生成器
                    raw_results = ddgs.text(
                        keywords=query,
                        max_results=args.max_results
                    )
                    if raw_results:
                        results = list(raw_results)
            except Exception as e:
                logger.error("DuckDuckGo search failed", error=str(e))
                raise e
            return results

        try:
            # 异步非阻塞执行
            raw_data = await run_sync(_search_sync)

            if not raw_data:
                return ToolResult(content=f"未找到关于 '{query}' 的相关结果，或搜索服务暂时不可用。")

            return self._format_results(raw_data)

        except Exception as e:
            return ToolResult(
                content=f"搜索工具执行异常: {str(e)}",
                is_error=True
            )

    def _format_results(self, results: List[Dict[str, Any]]) -> ToolResult:
        """格式化搜索结果为易读文本"""
        lines = []
        for i, r in enumerate(results, 1):
            title = r.get('title', 'No Title')
            link = r.get('href', '#')
            body = r.get('body', '')
            
            lines.append(f"[{i}] {title}")
            lines.append(f"    Link: {link}")
            lines.append(f"    Snippet: {body}\n")
        
        formatted_text = "DuckDuckGo 搜索结果:\n" + "\n".join(lines)
        
        return ToolResult(
            content=formatted_text,
            metadata={"source": "duckduckgo", "count": len(results)}
        )
```

[126] gecko/utils/cleanup.py
```python
# gecko/utils/cleanup.py
import atexit
import asyncio

def register_litellm_cleanup():
    """在进程退出时优雅关闭 LiteLLM 异步客户端，避免 RuntimeWarning"""
    def _cleanup():
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # 如果循环还在运行，调度清理任务
                loop.create_task(_close_clients())
            else:
                # 循环已关闭，直接新开一个临时循环执行清理
                asyncio.run(_close_clients())
        except Exception:
            pass  # 防止清理本身抛错

    async def _close_clients():
        try:
            import litellm
            # LiteLLM 官方提供的异步关闭方法（v1.40+ 支持）
            if hasattr(litellm, "async_http_handler"):
                if litellm.async_http_handler: # type: ignore
                    await litellm.async_http_handler.client.close() # type: ignore
            # 兼容旧版本
            if hasattr(litellm, "http_client"):
                if litellm.http_client: # type: ignore
                    await litellm.http_client.close() # type: ignore
        except Exception:
            pass

    atexit.register(_cleanup)

# 自动注册（模块导入即生效）
register_litellm_cleanup()
```

[127] gecko/version.py
```python
# gecko/version.py
"""
版本管理模块

通过单一来源 (Single Source of Truth) 管理 Gecko 框架版本号：
- 避免 __init__.py、CLI 等多个地方手写版本号导致不一致
- 方便在打包/CI 时统一维护
"""

__version__: str = "0.3.1"  # 请根据真实发布版本号修改
__author__: str = "Xuemzhan"
__email__: str = "zxm0813@gmail.com"
__license__: str = "MIT"
```

