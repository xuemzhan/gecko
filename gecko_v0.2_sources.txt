[1] gecko/__init__.py
```python
# gecko/__init__.py
from __future__ import annotations

from gecko.core.agent import Agent
from gecko.core.builder import AgentBuilder
from gecko.core.message import Message

# 自动清理 LiteLLM 异步客户端，彻底消除 RuntimeWarning
import atexit
import asyncio
import litellm # type: ignore

def _cleanup_litellm():
    async def _close():
        try:
            if hasattr(litellm, "async_http_handler") and litellm.async_http_handler:
                await litellm.async_http_handler.client.close()
        except:
            pass
    try:
        asyncio.run(_close())
    except:
        pass

atexit.register(_cleanup_litellm)

__version__ = "0.1.0"
__all__ = ["Agent", "AgentBuilder", "Message"]
```

[2] gecko/compose/__init__.py
```python
# gecko/compose/__init__.py
"""
Gecko Compose 模块

提供多智能体编排能力：
- Workflow: DAG 工作流引擎
- Team: 并行多智能体执行
- step: 节点装饰器
- ensure_awaitable: 同步/异步统一调用工具
- Next: 控制流指令
"""
from gecko.compose.workflow import Workflow
from gecko.compose.team import Team
from gecko.compose.nodes import step, ensure_awaitable, Next

__all__ = ["Workflow", "Team", "step", "ensure_awaitable", "Next"]
```

[3] gecko/compose/nodes.py
```python
# gecko/compose/nodes.py
"""
Workflow 节点定义与辅助工具

核心功能：
1. Next: 控制流指令，用于动态跳转节点
2. step: 节点装饰器，用于标记和增强函数元数据

优化日志：
- [Fix] 使用 functools.wraps 保留被装饰函数的元数据 (签名、文档等)
- [Refactor] 移除本地 ensure_awaitable，引用 gecko.core.utils
- [Feat] step 装饰器自动将同步函数转为异步，统一调用行为
"""
from __future__ import annotations

import functools
from typing import Any, Callable, Dict, Optional

from pydantic import BaseModel, Field

from gecko.core.utils import ensure_awaitable


class Next(BaseModel):
    """
    控制流指令：用于 Workflow 节点返回值中，指示引擎跳转到特定节点。
    
    示例:
        ```python
        def check_score(score: int):
            if score > 60:
                return Next(node="Pass", input="Good job")
            return Next(node="Fail", input="Try again")
        ```
    """
    node: str = Field(..., description="下一个节点的名称")
    input: Optional[Any] = Field(default=None, 
        iption="传递给下一个节点的输入数据。如果为 None，则保持上下文中的 last_output 不变。"
    ) # type: ignore
    # [New] 允许在跳转时更新 Context.state # type: ignore
    update_state: Dict[str, Any] = Field(
        default_factory=dict,
        description="需要合并到 WorkflowContext.state 的字典"
    )


def step(name: Optional[str] = None):
    """
    节点装饰器
    
    功能：
    1. 标记函数为 Workflow 节点
    2. 允许自定义节点名称 (metadata)
    3. 统一将同步函数包装为异步函数
    
    参数:
        name: 自定义节点名称（可选，默认使用函数名）
        
    示例:
        ```python
        @step(name="DataFetcher")
        def fetch_data(url: str):
            return requests.get(url).text
            
        # 在 Workflow 中使用
        workflow.add_node("fetch", fetch_data)
        ```
    """
    def decorator(func: Callable):
        # 1. 保留原始函数的元数据 (name, doc, signature)
        # 这对于 Workflow 的智能参数注入 (Smart Binding) 至关重要
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            # 2. 统一转为异步执行
            return await ensure_awaitable(func, *args, **kwargs)
        
        # 3. 附加标识位和名称
        setattr(wrapper, "_is_step", True)
        setattr(wrapper, "_step_name", name or func.__name__)
        
        return wrapper
    return decorator
```

[4] gecko/compose/team.py
```python
# gecko/compose/team.py
"""
Team 多智能体并行引擎

提供 Map-Reduce 模式的并行执行能力，将单一任务分发给多个 Agent/Function 执行，
并聚合结果。适用于 "专家评审团"、"多路赛马"、"并发搜索" 等场景。

核心功能：
1. 高效并行：基于 AnyIO TaskGroup 实现异步并发。
2. 流量整形：支持 max_concurrent 限制，防止触发 LLM Rate Limit。
3. 容错机制：单个成员失败不熔断整体任务 (Partial Success)。
4. 智能绑定：自动解析 WorkflowContext，支持数据流转。

优化日志：
- [Fix] 增加 max_concurrent 信号量控制，防止 API 速率超限
- [Fix] 完善异常捕获边界，确保 TaskGroup 稳定性
- [Refactor] 统一输入解析与结果标准化逻辑

优化日志：
- [Refactor] 引入 MemberResult 标准化返回结果
- [Refactor] 移除输入处理中的隐式拆包逻辑 (Magic Unpacking)
- [Fix] 异常处理改为对象封装，不再返回错误字符串
"""
from __future__ import annotations

from typing import Any, Callable, Generic, List, Optional, TypeVar, Union, TYPE_CHECKING

import anyio
from pydantic import BaseModel, Field

from gecko.core.agent import Agent
from gecko.core.logging import get_logger
from gecko.core.message import Message
from gecko.core.output import AgentOutput
from gecko.core.utils import ensure_awaitable

if TYPE_CHECKING:
    from gecko.compose.workflow import WorkflowContext

logger = get_logger(__name__)

R = TypeVar("R")


class MemberResult(BaseModel, Generic[R]):
    """
    标准化成员执行结果
    """
    result: Optional[R] = Field(default=None, description="执行成功时的返回值")
    error: Optional[str] = Field(default=None, description="执行失败时的错误信息")
    member_index: int = Field(..., description="成员在 Team 中的索引")
    is_success: bool = Field(default=True, description="是否执行成功")

    @property
    def value(self) -> R:
        """
        获取结果，如果是错误则抛出异常 (便捷方法)
        """
        if not self.is_success:
            raise RuntimeError(f"Member execution failed: {self.error}")
        return self.result # type: ignore


class Team:
    """
    多智能体协作组 (Parallel Execution Engine)
    """

    def __init__(
        self, 
        members: List[Union[Agent, Callable]],
        name: str = "Team",
        max_concurrent: int = 0,
        return_full_output: bool = False
    ):
        self.members = members
        self.name = name
        self.max_concurrent = max_concurrent
        self.return_full_output = return_full_output

    # ========================= 接口协议 =========================

    async def __call__(self, context_or_input: Any) -> List[MemberResult]:
        """
        实现 Callable 协议
        """
        return await self.run(context_or_input)

    async def run(self, context_or_input: Any) -> List[MemberResult]:
        """
        执行 Team 逻辑
        
        返回:
            MemberResult 列表，包含每个成员的执行状态和结果
        """
        # 1. 解析输入 (不再进行隐式内容提取)
        inp = self._resolve_input(context_or_input)
        
        member_count = len(self.members)
        logger.info(
            "Team execution started", 
            team=self.name, 
            member_count=member_count,
            max_concurrent=self.max_concurrent or "unlimited"
        )

        # 2. 初始化容器
        # 使用 MemberResult 占位，初始状态设为失败，防止未执行的情况
        results: List[Optional[MemberResult]] = [None] * member_count
        
        # 3. 准备并发控制
        semaphore = anyio.Semaphore(self.max_concurrent) if self.max_concurrent > 0 else None

        # 4. 定义 Worker
        async def _worker(idx: int, member: Any):
            if semaphore:
                await semaphore.acquire()
            
            try:
                # 执行成员逻辑
                raw_result = await self._execute_member(member, inp)
                # 结果标准化
                processed = self._process_result(raw_result)
                
                results[idx] = MemberResult(
                    member_index=idx,
                    result=processed,
                    is_success=True
                )
            except Exception as e:
                logger.error(
                    "Team member execution failed",
                    team=self.name,
                    member_index=idx,
                    error=str(e)
                )
                results[idx] = MemberResult(
                    member_index=idx,
                    error=str(e),
                    is_success=False
                )
            finally:
                if semaphore:
                    semaphore.release()

        # 5. 启动并发任务组
        async with anyio.create_task_group() as tg:
            for idx, member in enumerate(self.members):
                tg.start_soon(_worker, idx, member)

        # 6. 结果整理
        # 理论上 task_group 结束时所有 results 都已被赋值，这里做一次非空断言过滤
        final_results = [r for r in results if r is not None]
        
        # 统计
        success_count = sum(1 for r in final_results if r.is_success)
        fail_count = member_count - success_count
        
        logger.info(
            "Team execution completed",
            team=self.name,
            success=success_count,
            failed=fail_count
        )

        return final_results

    # ========================= 内部逻辑 =========================

    def _resolve_input(self, context_or_input: Any) -> Any:
        """
        智能输入解析 (Refactored: Removed Magic Unpacking)
        
        仅保留从 WorkflowContext 中提取数据的逻辑，移除针对 dict 内容的自动拆包。
        """
        # 1. 检查是否为 WorkflowContext (Duck Typing)
        if (
            hasattr(context_or_input, "history") 
            and hasattr(context_or_input, "input")
            and isinstance(getattr(context_or_input, "history", None), dict)
        ):
            ctx = context_or_input
            history = getattr(ctx, "history", {})
            state = getattr(ctx, "state", {})
            
            # 优先级: 
            # 1. 显式传递的 _next_input (Next 指令)
            # 2. 上一步输出 (last_output)
            # 3. 全局初始输入 (input)
            val = state.pop("_next_input", None) or history.get("last_output", getattr(ctx, "input"))
            
            # [Removed] Data Handover 清洗逻辑
            # 以前这里会检查 dict["content"]，现在移除，保持原始数据完整性
            return val
            
        # 2. 普通输入直接返回
        return context_or_input

    async def _execute_member(self, member: Any, inp: Any) -> Any:
        """执行单个成员"""
        if hasattr(member, "run"):
            return await member.run(inp)
        
        if callable(member):
            return await ensure_awaitable(member, inp)
            
        raise TypeError(f"Member {member} is not executable (must be Agent or Callable)")

    def _process_result(self, result: Any) -> Any:
        """结果标准化处理"""
        if self.return_full_output:
            if isinstance(result, (BaseModel, AgentOutput, Message)):
                return result.model_dump()
            return result
            
        # 默认模式：仅提取核心文本内容
        if isinstance(result, AgentOutput):
            return result.content
        if isinstance(result, Message):
            return result.content
        # [Modified] 这里的 dict 检查仅针对 AgentOutput.model_dump() 后的结构
        # 普通的 dict 不会被提取 content，除非它是 AgentOutput 的序列化形式
        # 但为了安全起见，我们仅处理明确的对象类型，对于 dict 保持原样，
        # 或者我们可以保留这里的逻辑如果确信 dict 是由 Agent 产生的。
        # 为了贯彻 "No Magic"，建议此处如果用户返回的是 dict，就返回 dict。
        # AgentOutput/Message 对象在上一步已经被识别处理了。
        
        return result

    def __repr__(self) -> str:
        return f"Team(name='{self.name}', members={len(self.members)}, concurrency={self.max_concurrent})"
```

[5] gecko/compose/workflow.py
```python
# gecko/compose/workflow.py
"""
Workflow 引擎

提供基于 DAG（有向无环图）的任务编排能力，支持复杂的控制流和状态管理。

核心功能：
1. 节点编排：支持普通函数、Agent、Team 等多种节点类型混编
2. 状态管理：基于 Pydantic 的强类型上下文，支持完整的序列化与持久化
3. 控制流：支持条件分支、循环（通过 Next 指令）
4. 智能绑定：自动根据函数签名注入 Context 或 Input
5. 可观测性：详细的节点执行轨迹与统计

优化日志：
- [Fix] 修复参数注入逻辑，支持同时接收 Input 和 Context 的函数签名
- [Fix] 将 WorkflowContext 升级为 BaseModel，彻底解决持久化数据截断问题
- [Fix] 增加分支歧义检测，确保逻辑确定性
- [Fix] 优化 Agent 间的数据流转 (Data Handover)，避免 Prompt 污染

优化日志：
- [Feat] 支持 allow_cycles 配置，允许定义有环图
- [Feat] 支持处理 Next.update_state
- [Feat] 新增 CheckpointStrategy 控制持久化频率
- [Feat] 重构 _execute_loop 支持自定义起始点
- [Feat] 新增 resume 方法实现断点恢复
"""
from __future__ import annotations

import asyncio
import inspect
import time
import uuid
# 明确导入 run_sync，避免模块/函数混淆
from anyio.to_thread import run_sync
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, TypeVar, Union, Set

from pydantic import BaseModel, Field, PrivateAttr

from gecko.compose.nodes import Next
from gecko.core.events import BaseEvent, EventBus
from gecko.core.exceptions import WorkflowCycleError, WorkflowError
from gecko.core.logging import get_logger
from gecko.core.message import Message
from gecko.core.utils import ensure_awaitable, safe_serialize_context
from gecko.plugins.storage.interfaces import SessionInterface

logger = get_logger(__name__)

T = TypeVar("T")


# ========================= 事件定义 =========================

class WorkflowEvent(BaseEvent):
    """Workflow 专用事件对象"""
    pass


# ========================= 状态模型 =========================

class NodeStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"


class NodeExecution(BaseModel):
    """
    节点执行记录（轨迹追踪）
    """
    node_name: str
    status: NodeStatus = NodeStatus.PENDING
    input_data: Any = None
    output_data: Any = None
    error: Optional[str] = None
    start_time: float = Field(default_factory=time.time)
    end_time: float = 0.0

    @property
    def duration(self) -> float:
        """计算执行耗时"""
        if self.end_time == 0.0:
            return 0.0
        return max(0.0, self.end_time - self.start_time)


class WorkflowContext(BaseModel):
    """
    工作流执行上下文
    
    使用 Pydantic 模型确保类型安全和原生序列化支持。
    """
    execution_id: str = Field(
        default_factory=lambda: uuid.uuid4().hex,
        description="单次运行的唯一 ID"
    )
    input: Any = Field(..., description="工作流初始输入")
    state: Dict[str, Any] = Field(
        default_factory=dict, 
        description="共享状态存储（用户自定义）"
    )
    history: Dict[str, Any] = Field(
        default_factory=dict, 
        description="节点历史输出记录"
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="元数据（如 session_id, trace_id）"
    )
    executions: List[NodeExecution] = Field(
        default_factory=list,
        description="完整执行轨迹"
    )
    # [新增] 动态跳转指针，用于断点恢复
    # 格式: {"target_node": "NodeB", "input": ...}
    next_pointer: Optional[Dict[str, Any]] = Field(
        default=None, 
        description="Next 指令产生的动态跳转目标"
    )

    def clear_next_pointer(self):
        """消费完指针后清除"""
        self.next_pointer = None

    def add_execution(self, execution: NodeExecution):
        """添加执行记录"""
        self.executions.append(execution)

    def get_last_output(self) -> Any:
        """获取上一个节点的输出（如果无历史则返回初始输入）"""
        return self.history.get("last_output", self.input)
    
    def get_last_output_as(self, type_: Type[T]) -> T:
        """
        类型安全地获取上一步输出
        尝试将输出转换为指定类型，或进行断言。
        """
        val = self.get_last_output()
        
        # 1. 如果已经是该类型，直接返回
        if isinstance(val, type_):
            return val
            
        # 2. 尝试 Pydantic 转换 (如果是 dict -> Model)
        if isinstance(val, dict) and hasattr(type_, "model_validate"):
            try:
                return type_.model_validate(val) # type: ignore
            except Exception:
                pass
                
        # 3. 简单的类型转换 (如 int, str)
        try:
            return type_(val) # type: ignore
        except Exception as e:
            raise TypeError(f"Cannot convert last output {type(val)} to {type_}") from e

    def get_summary(self) -> Dict[str, Any]:
        """获取执行摘要"""
        total_time = sum(e.duration for e in self.executions)
        is_failed = any(e.status == NodeStatus.FAILED for e in self.executions)
        return {
            "execution_id": self.execution_id,
            "total_nodes": len(self.executions),
            "total_time": total_time,
            "last_node": self.executions[-1].node_name if self.executions else None,
            "status": "failed" if is_failed else "completed"
        }

# [New] 持久化策略枚举
class CheckpointStrategy(str, Enum):
    ALWAYS = "always"  # 每步保存 (默认, 最安全)
    FINAL = "final"    # 仅在结束时保存 (性能最好)
    MANUAL = "manual"  # 不自动保存 (需用户手动处理)

# ========================= 工作流引擎 =========================

class Workflow:
    """
    DAG 工作流引擎
    """

    def __init__(
        self,
        name: str = "Workflow",
        event_bus: Optional[EventBus] = None,
        storage: Optional[SessionInterface] = None,
        max_steps: int = 100,
        enable_retry: bool = False,
        max_retries: int = 3,
        allow_cycles: bool = False, # [New] 允许循环
        checkpoint_strategy: Union[str, CheckpointStrategy] = CheckpointStrategy.ALWAYS, # [New]
    ):
        self.name = name
        self.event_bus = event_bus or EventBus()
        self.storage = storage
        self.max_steps = max_steps
        self.enable_retry = enable_retry
        self.max_retries = max_retries
        self.allow_cycles = allow_cycles 
        self.checkpoint_strategy = CheckpointStrategy(checkpoint_strategy)

        # 内部存储
        self._nodes: Dict[str, Callable] = {}
        # edges: source -> [(target, condition_func), ...]
        self._edges: Dict[str, List[Tuple[str, Optional[Callable]]]] = {}
        self._entry_point: Optional[str] = None

        # 验证状态
        self._validated = False
        self._validation_errors: List[str] = []

    # ========================= 构建 API =========================

    def add_node(self, name: str, func: Callable) -> "Workflow":
        """
        添加节点
        
        参数:
            name: 节点唯一名称
            func: 可调用对象（函数、Agent、Team）
        """
        if name in self._nodes:
            raise ValueError(f"Node '{name}' already exists")
        self._nodes[name] = func
        self._validated = False
        logger.debug("Node added", workflow=self.name, node=name)
        return self

    def add_edge(
        self,
        source: str,
        target: str,
        condition: Optional[Callable[[WorkflowContext], bool]] = None,
    ) -> "Workflow":
        """
        添加边（支持条件分支）
        
        参数:
            source: 源节点名称
            target: 目标节点名称
            condition: 转移条件函数（接收 Context 返回 bool）
        """
        if source not in self._nodes:
            raise ValueError(f"Source node '{source}' not found")
        if target not in self._nodes:
            raise ValueError(f"Target node '{target}' not found")

        self._edges.setdefault(source, []).append((target, condition))
        self._validated = False
        logger.debug("Edge added", source=source, target=target, conditional=bool(condition))
        return self

    def set_entry_point(self, name: str) -> "Workflow":
        """设置入口节点"""
        if name not in self._nodes:
            raise ValueError(f"Node '{name}' not found")
        self._entry_point = name
        self._validated = False
        return self

    # ========================= 验证逻辑 =========================

    def validate(self) -> bool:
        """验证工作流结构的合法性"""
        if self._validated:
            return len(self._validation_errors) == 0

        self._validation_errors.clear()

        # 1. 检查入口
        if not self._entry_point:
            self._validation_errors.append("No entry point defined")
        elif self._entry_point not in self._nodes:
            self._validation_errors.append(f"Entry point '{self._entry_point}' not in nodes")

        # 2. 检查歧义分支 (同一节点存在多个无条件出边)
        for node, edges in self._edges.items():
            unconditional_edges = [t for t, c in edges if c is None]
            if len(unconditional_edges) > 1:
                self._validation_errors.append(
                    f"Node '{node}' has ambiguous edges: multiple unconditional targets {unconditional_edges}"
                )

        # 3. 检查死循环 (静态检测)
        # [Updated] 仅在不允许循环时检测环
        if not self.allow_cycles:
            try:
                self._detect_cycles()
            except WorkflowCycleError as e:
                self._validation_errors.append(str(e))

        # 4. 连通性警告
        self._check_connectivity()

        self._validated = True
        if self._validation_errors:
            logger.error("Workflow validation failed", errors=self._validation_errors)
            return False

        logger.info("Workflow validation passed", name=self.name)
        return True

    def _detect_cycles(self):
        """DFS 检测环"""
        visited = set()
        recursion_stack = set()

        def dfs(node: str, path: List[str]):
            visited.add(node)
            recursion_stack.add(node)
            path.append(node)

            for neighbor, _ in self._edges.get(node, []):
                if neighbor not in visited:
                    dfs(neighbor, path)
                elif neighbor in recursion_stack:
                    cycle_start = path.index(neighbor)
                    cycle = " -> ".join(path[cycle_start:] + [neighbor])
                    raise WorkflowCycleError(f"Cycle detected: {cycle}")

            recursion_stack.remove(node)
            path.pop()

        for node in self._nodes:
            if node not in visited:
                dfs(node, [])

    def _check_connectivity(self):
        """检查不可达节点（仅警告）"""
        if not self._entry_point:
            return

        reachable = set()
        queue = [self._entry_point]
        while queue:
            curr = queue.pop(0)
            if curr in reachable:
                continue
            reachable.add(curr)
            for target, _ in self._edges.get(curr, []):
                queue.append(target)
        
        unreachable = set(self._nodes.keys()) - reachable
        if unreachable:
            logger.warning("Unreachable nodes detected", nodes=list(unreachable))

    # ========================= 执行引擎 =========================

    async def execute(self, input_data: Any, session_id: Optional[str] = None) -> Any:
        """
        执行工作流
        
        参数:
            input_data: 初始输入数据
            session_id: 会话 ID（用于持久化和状态恢复）
            
        返回:
            最终输出（Context 中的 last_output）
        """
        if not self.validate():
            raise WorkflowError(f"Workflow validation failed:\n" + "\n".join(self._validation_errors))

        # 初始化上下文
        context = WorkflowContext(input=input_data)
        if session_id:
            context.metadata["session_id"] = session_id

        await self.event_bus.publish(
            WorkflowEvent(
                type="workflow_started", 
                data={"name": self.name, "execution_id": context.execution_id}
            )
        )

        try:
            # [Update] 默认从入口点开始，步数为 0
            await self._execute_loop(context, session_id, start_node=self._entry_point, start_step=0)

            # 最终保存 (如果策略是 FINAL)
            if self.storage and session_id and self.checkpoint_strategy == CheckpointStrategy.FINAL:
                await self._persist_state(session_id, 9999, None, context, force=True)

            # await self._execute_loop(context, session_id)
            
            result = context.get_last_output()
            await self.event_bus.publish(
                WorkflowEvent(
                    type="workflow_completed",
                    data={"name": self.name, "summary": context.get_summary()},
                )
            )
            return result
            
        except Exception as e:
            logger.exception("Workflow execution failed")
            await self.event_bus.publish(
                WorkflowEvent(type="workflow_error", error=str(e), data={"name": self.name})
            )
            raise

    async def resume(self, session_id: str) -> Any:
        """
        从存储中恢复执行
        """
        if not self.storage:
            raise ValueError("Cannot resume: Storage not configured")
        
        # 1. 加载状态
        saved_data = await self.storage.get(f"workflow:{session_id}")
        if not saved_data:
            raise ValueError(f"Session '{session_id}' not found")
        
        logger.info("Resuming workflow", session_id=session_id, last_node=saved_data.get("last_node"))
        
        # 2. 重建 Context
        try:
            context = WorkflowContext(**saved_data["context"])
        except Exception as e:
            raise WorkflowError(f"Failed to reconstruct context: {e}") from e
            
        last_node = saved_data.get("last_node")
        current_step = saved_data.get("step", 0)
        
        # 3. 确定下一步
        next_node = None

        # [优化] 优先检查是否存在动态跳转指针
        if context.next_pointer:
            logger.info("Resuming from dynamic Next pointer", target=context.next_pointer.get("target_node"))
            next_node = context.next_pointer.get("target_node")
            
            # 恢复可能存在的输入传递
            if context.next_pointer.get("input"):
                context.state["_next_input"] = context.next_pointer["input"]
            
            # 消费指针 (已使用，清除以避免重复)
            context.clear_next_pointer()

        elif last_node:
            # 只有在没有动态指针时，才回退到基于静态图的推导
            next_node = await self._find_next_node(last_node, context)
            if not next_node:
                logger.info("Workflow already completed (no next node)", session_id=session_id)
                return context.get_last_output()
        else:
            # 这是一个全新的会话（或者刚初始化未执行）
            next_node = self._entry_point
        
        # 4. 继续执行循环
        try:
            await self._execute_loop(
                context, 
                session_id, 
                start_node=next_node, 
                start_step=current_step
            )
            
            # 最终保存
            if self.checkpoint_strategy == CheckpointStrategy.FINAL:
                await self._persist_state(session_id, current_step, None, context, force=True)
                
            return context.get_last_output()
            
        except Exception as e:
            logger.exception("Resume execution failed")
            raise

    async def _execute_loop(self, 
                            context: WorkflowContext,  
                            session_id: Optional[str], 
                            start_node: Optional[str],
                            start_step: int):
        """核心执行循环"""
        current_node = start_node
        steps = start_step

        while current_node and steps < self.max_steps:
            steps += 1

            # 1. 如果是从 next_pointer 恢复的（Resume 场景），跳过执行，直接流转
            # 但这里逻辑比较绕，更清晰的是：如果 next_pointer 存在，说明上一步是 Next 指令，
            # 且已经持久化了，我们应该直接使用 next_pointer 指向的节点作为 current_node。
            # 这在 resume() 方法中处理更合适，这里保持循环逻辑。

            # 执行节点
            logger.debug("Executing step", step=steps, node=current_node)
            # 执行节点逻辑
            result = await self._execute_node_safe(current_node, context)

            # 准备持久化所需的临时变量
            # 记录当前节点为“已完成节点”
            persist_node = current_node 
            next_target = None

            # 处理流转逻辑
            if isinstance(result, Next):
                # === 动态跳转处理 ===
                next_target = result.node
                
                # 更新 Input / State
                if result.input is not None:
                    normalized = self._normalize_result(result.input)
                    context.history["last_output"] = normalized
                    context.state["_next_input"] = normalized
                
                if result.update_state:
                    context.state.update(result.update_state)
                    
                # [关键优化] 记录动态指针，确保持久化时包含此信息
                context.next_pointer = {
                    "target_node": next_target,
                    "input": context.state.get("_next_input")
                }
                
                # 即使是跳转，也需要在 history 中留痕，证明此节点已执行完毕
                # 这里记录一个特殊的标识，方便调试
                context.history[current_node] = f"<Next -> {next_target}>"

            else:
                # === 静态流转处理 ===
                normalized = self._normalize_result(result)
                context.history[current_node] = normalized
                context.history["last_output"] = normalized
                
                # 既然走了静态流程，确保清除之前的指针（防御性编程）
                context.clear_next_pointer()
                
                # 基于静态图寻找下一跳
                next_target = await self._find_next_node(current_node, context)

            # [优化] 立即持久化 (Atomic Checkpoint)
            # 此时 context 包含了最新的 history 和 next_pointer
            # 即使下一秒 Crash，resume 时也能通过 next_pointer 找到 next_target
            if self.storage and session_id:
                await self._persist_state(session_id, steps, persist_node, context)

            # 推进到下一个节点
            current_node = next_target

        if steps >= self.max_steps:
            raise WorkflowError(f"Exceeded max steps: {self.max_steps}", context={"last": current_node})

    async def _execute_node_safe(self, node_name: str, context: WorkflowContext) -> Any:
        """节点执行包装器：负责状态记录、重试和错误处理"""
        execution = NodeExecution(node_name=node_name, status=NodeStatus.RUNNING)
        await self.event_bus.publish(WorkflowEvent(type="node_started", data={"node": node_name}))

        try:
            node_func = self._nodes[node_name]
            
            # 执行逻辑（含重试）
            if self.enable_retry:
                result = await self._execute_with_retry(node_func, context)
            else:
                result = await self._run_any_node(node_func, context)
            
            # 如果是 Next 对象，不在这里进行序列化，直接返回给 Loop 处理
            if isinstance(result, Next):
                execution.output_data = f"Next(node={result.node})"
                execution.status = NodeStatus.SUCCESS
            else:
                # 规范化结果以便记录在 trace 中
                normalized = self._normalize_result(result)
                execution.output_data = normalized
                result = normalized

            execution.status = NodeStatus.SUCCESS
            
            await self.event_bus.publish(
                WorkflowEvent(
                    type="node_completed", 
                    data={"node": node_name, "duration": execution.duration}
                )
            )
            return result

        except Exception as e:
            execution.status = NodeStatus.FAILED
            execution.error = str(e)
            
            await self.event_bus.publish(
                WorkflowEvent(type="node_error", error=str(e), data={"node": node_name})
            )
            # 重新抛出异常以中断 loop (或者触发全局错误处理)
            raise WorkflowError(f"Node '{node_name}' failed: {e}") from e
            
        finally:
            execution.end_time = time.time()
            context.add_execution(execution)

    # ========================= 节点调度逻辑 (核心) =========================

    async def _run_any_node(self, node_callable: Callable, context: WorkflowContext) -> Any:
        """
        通用节点执行器 (Duck Typing & Smart Binding)
        
        支持:
        1. Agent/Team (具备 run 方法的对象)
        2. 普通函数 (自动注入 context/input/none)
        
        修复：智能处理参数绑定，支持同时需要 Input 和 Context 的场景
        """
        # 1. 智能体对象 (Agent or Team)
        if hasattr(node_callable, "run") and callable(node_callable.run): # type: ignore
            return await self._run_intelligent_object(node_callable, context)
        
        # 2. 普通可调用对象
        if callable(node_callable):
            sig = inspect.signature(node_callable)
            params = sig.parameters
            kwargs = {}
            args = []
            
            # 获取当前输入数据
            current_input = context.state.pop("_next_input", None) or context.get_last_output()
            
            # A. 注入 Context
            if "context" in params:
                kwargs["context"] = context
            elif "workflow_context" in params:
                kwargs["workflow_context"] = context
                
            # B. 注入 Input (Input Injection)
            # 找到除 context, self 之外的参数，通常是第一个位置参数或特定的参数名
            # 这里采取简单策略：如果有剩余参数位置，则将 Input 作为第一个位置参数传入
            
            # 过滤掉已处理的 context 参数
            remaining_params = [
                name for name, p in params.items() 
                if name not in ("context", "workflow_context", "self")
                and p.kind in (inspect.Parameter.POSITIONAL_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD)
            ]
            
            if remaining_params:
                # 如果有剩余参数，假设第一个是用来接收 input 的
                args.append(current_input)
            elif not kwargs:
                # 如果既不需要 Context 也没有其他参数（无参函数），直接调用
                pass
                
            # 执行
            return await ensure_awaitable(node_callable, *args, **kwargs)
            
        raise WorkflowError(f"Node '{node_callable}' is not callable")

    async def _run_intelligent_object(self, obj: Any, context: WorkflowContext) -> Any:
        """
        执行 Agent/Team 对象
        
        优化: 智能处理数据流转 (Data Handover)
        优化: [Refactored Phase 1] 移除数据流转的魔法清洗逻辑
        """
        # 1. 获取输入 (优先使用 Next 传递的，否则用上一步输出)
        raw_input = context.state.pop("_next_input", None) or context.get_last_output()
        
        # 2. 数据清洗 (Data Handover Fix)
        ## 以前这里会尝试从 dict 中提取 content。
        ## 现在我们假设 Agent.run 能够处理 raw_input (因为 Agent.run 支持 dict 输入)
        ## 或者上一个节点的输出就是 Agent 期望的格式。
        # 如果上一个节点返回的是 AgentOutput (dict)，且当前 Agent 需要文本输入，
        # 我们尝试提取 content，避免将整个 JSON 结构扔给 LLM。
        # agent_input = raw_input
        # if isinstance(raw_input, dict):
        #     # 如果有 content 且没有 role (说明不是 Message 对象，而是 Output 字典)
        #     if "content" in raw_input and "role" not in raw_input:
        #         agent_input = raw_input["content"]
        
        # 3. 执行
        output = await obj.run(raw_input)
        
        # 4. 结果处理
        if hasattr(output, "model_dump"):
            return output.model_dump()
        return output

    async def _execute_with_retry(self, func: Callable, context: WorkflowContext) -> Any:
        """重试逻辑"""
        last_error = None
        for attempt in range(self.max_retries):
            try:
                return await self._run_any_node(func, context)
            except Exception as e:
                last_error = e
                logger.warning(
                    "Node retry triggered",
                    attempt=attempt + 1,
                    error=str(e)
                )
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
        raise last_error # type: ignore

    # ========================= 辅助方法 =========================

    async def _find_next_node(self, current: str, context: WorkflowContext) -> Optional[str]:
        """
        寻找下一个节点
        
        包含歧义检测：不允许同时满足多个无条件路径
        """
        edges = self._edges.get(current, [])
        candidates = []
        
        for target, condition in edges:
            should_go = False
            if condition is None:
                should_go = True
            else:
                try:
                    if inspect.iscoroutinefunction(condition):
                        should_go = await condition(context)
                    else:
                        should_go = condition(context)
                except Exception as e:
                    logger.error("Condition evaluation failed", source=current, target=target, error=str(e))
            
            if should_go:
                candidates.append(target)
        
        if len(candidates) == 0:
            return None
        
        # 歧义检测
        if len(candidates) > 1:
            raise WorkflowError(
                f"Ambiguous branching from '{current}': multiple conditions met for targets {candidates}. "
                "Workflow logic must be deterministic."
            )
            
        return candidates[0]

    def _normalize_result(self, result: Any) -> Any:
        """
        标准化结果 (Pydantic Friendly)
        """
        if isinstance(result, BaseModel):
            return result.model_dump()
        if hasattr(result, "model_dump"):
            return result.model_dump()
        if isinstance(result, Message):
            return result.to_openai_format()
        return result

    async def _persist_state(
        self,
        session_id: str,
        steps: int,
        current_node: Optional[str],
        context: WorkflowContext,
        force: bool = False
    ):
        """
        状态持久化
        
        使用 Pydantic 的 mode='json' 确保完整序列化，不进行截断。
        参数 force: 是否强制保存（忽略策略）

        [优化] 异步非阻塞状态持久化
        1. 获取原始数据 (mode='python') 避免 Pydantic 报错。
        2. 在线程池中执行深层清洗 (CPU 密集型)。
        3. 异步写入存储。
        """
        # 如果策略是 MANUAL 且非强制，跳过
        if not force and self.checkpoint_strategy == CheckpointStrategy.MANUAL:
            return
            
        # 如果策略是 FINAL 且非强制，也跳过 (FINAL 只在 execute 结束或 resume 结束时 force=True 调用)
        if not force and self.checkpoint_strategy == CheckpointStrategy.FINAL:
            return

        try:
            # 1. 快速获取 Python 原生字典 (包含未序列化的 Lock 等对象)
            # 这一步很快，因为不涉及 JSON 转换
            raw_data = context.model_dump(mode='python')
            
            # 2. [核心优化] 将耗时的清洗工作卸载到线程池
            # 避免 Context 很大时阻塞 Event Loop
            def _heavy_clean_task():
                return safe_serialize_context(raw_data)
            
            clean_context_data = await run_sync(_heavy_clean_task)

            # 3. 写入存储 (clean_context_data 已经是纯净的 dict)
            await self.storage.set( # type: ignore
                f"workflow:{session_id}",
                {
                    "step": steps,
                    "last_node": current_node,
                    "context": clean_context_data,
                    "updated_at": time.time(),
                },
            )
        except Exception as e:
            logger.warning("Failed to persist workflow state", session_id=session_id, error=str(e))

    # ========================= 可视化 =========================

    def to_mermaid(self) -> str:
        """生成 Mermaid 流程图代码"""
        lines = ["graph TD"]
        for node in self._nodes:
            # 入口节点使用双圆圈
            shape_start = "((" if node == self._entry_point else "("
            shape_end = "))" if node == self._entry_point else ")"
            lines.append(f"    {node}{shape_start}{node}{shape_end}")
            
        for source, targets in self._edges.items():
            for target, condition in targets:
                label = "|condition|" if condition else ""
                lines.append(f"    {source} --{label}--> {target}")
        return "\n".join(lines)

    def print_structure(self):
        """打印工作流结构"""
        print(f"\n=== Workflow: {self.name} ===")
        print(f"Entry Point: {self._entry_point}")
        print(f"\nNodes ({len(self._nodes)}):")
        for node in self._nodes:
            print(f"  - {node}")

        print(f"\nEdges ({sum(len(v) for v in self._edges.values())}):")
        for source, targets in self._edges.items():
            for target, condition in targets:
                cond_str = " [conditional]" if condition else ""
                print(f"  - {source} -> {target}{cond_str}")
        print()
```

[6] gecko/config.py
```python
# gecko/config.py  
"""  
配置系统（改进版）  
  
- 增加 configure_settings / reset_settings，便于测试重载  
- 使用 Lazy 初始化，防止导入 config 时立刻读取 .env  
- Docstring 更新，避免示例与 Agent API 不匹配  
"""  
  
from __future__ import annotations  
  
from typing import Optional  
  
from pydantic import Field, field_validator  
from pydantic_settings import BaseSettings, SettingsConfigDict  
  
  
class GeckoSettings(BaseSettings):  
    default_model: str = Field(default="gpt-3.5-turbo")  
    default_api_key: str = Field(default="")  
    default_base_url: Optional[str] = None  
    default_temperature: float = Field(default=0.7, ge=0.0, le=2.0)  
  
    max_turns: int = Field(default=5, ge=1, le=50)  
    max_context_tokens: int = Field(default=4000, ge=100)  
  
    default_storage_url: str = Field(default="sqlite://./gecko_data.db")  
    log_level: str = Field(default="INFO")  
    log_format: str = Field(default="text")  
  
    enable_cache: bool = True  
    tool_execution_timeout: float = Field(default=30.0, ge=1.0)  
  
    model_config = SettingsConfigDict(  
        env_prefix="GECKO_",  
        env_file=".env",  
        env_file_encoding="utf-8",  
        case_sensitive=False,  
        extra="ignore",  
    )  
  
    @field_validator("log_level")  
    @classmethod  
    def validate_log_level(cls, v: str) -> str:  
        valid = {"DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"}  
        if v.upper() not in valid:  
            raise ValueError(f"log_level must be one of {valid}")  
        return v.upper()  
  
    @field_validator("log_format")  
    @classmethod  
    def validate_log_format(cls, v: str) -> str:  
        if v not in {"text", "json"}:  
            raise ValueError("log_format must be 'text' or 'json'")  
        return v  
  
  
_default_settings: Optional[GeckoSettings] = None  
  
  
def get_settings(force_reload: bool = False) -> GeckoSettings:  
    global _default_settings  
    if _default_settings is None or force_reload:  
        _default_settings = GeckoSettings()  
    return _default_settings  
  
  
def configure_settings(**overrides) -> GeckoSettings:  
    """  
    允许测试/脚本传入覆盖参数，例如：  
        configure_settings(default_model="gpt-4")  
    """  
    global _default_settings  
    _default_settings = GeckoSettings(**overrides)  
    return _default_settings  
  
  
def reset_settings():  
    global _default_settings  
    _default_settings = None  
  
  
settings = get_settings()  
```

[7] gecko/core/__init__.py
```python
```

[8] gecko/core/agent.py
```python
# gecko/core/agent.py  
from __future__ import annotations  
  
from typing import Any, Iterable, List, Optional, Type, Union  
  
from pydantic import BaseModel  
  
from gecko.core.events import AgentRunEvent, EventBus  
from gecko.core.message import Message  
from gecko.core.output import AgentOutput  
from gecko.core.toolbox import ToolBox  
from gecko.core.memory import TokenMemory  
from gecko.core.engine.base import CognitiveEngine  
from gecko.core.engine.react import ReActEngine  
from gecko.core.logging import get_logger  
from gecko.core.exceptions import AgentError  
  
logger = get_logger(__name__)  
  
  
class Agent:  
    """  
    Agent 对象负责在模型、工具箱、记忆之间协调一次推理任务。  
    """  
  
    def __init__(  
        self,  
        model: Any,  
        toolbox: ToolBox,  
        memory: TokenMemory,  
        engine_cls: Type[CognitiveEngine] = ReActEngine,  
        event_bus: Optional[EventBus] = None,  
        **engine_kwargs: Any,  
    ):  
        self.event_bus = event_bus or EventBus()  
        self.toolbox = toolbox  
        self.memory = memory  
        self.engine = engine_cls(  
            model=model,  
            toolbox=toolbox,  
            memory=memory,  
            **engine_kwargs  
        )  
  
    async def run(  
        self,  
        messages: str | Message | List[Message] | List[dict] | dict,  
        response_model: Optional[Type[BaseModel]] = None  
    ) -> AgentOutput | BaseModel:  
        """  
        单次推理入口：对多种输入格式统一转换为 Message 列表  
        """  
        input_msgs = self._normalize_messages(messages)  
  
        await self.event_bus.publish(  
            AgentRunEvent(type="run_started", data={"input_count": len(input_msgs)})  
        )  
  
        try:  
            output = await self.engine.step(input_msgs, response_model=response_model)  
            payload = self._serialize_output(output)  
  
            await self.event_bus.publish(  
                AgentRunEvent(type="run_completed", data={"output": payload})  
            )  
            return output  
  
        except Exception as e:  
            logger.exception("Agent run failed")  
            await self.event_bus.publish(  
                AgentRunEvent(type="run_error", error=str(e))  
            )  
            raise  
  
    async def stream(self, messages: str | Message | List[Message] | List[dict] | dict):  
        """  
        流式推理：共用同一套输入标准化逻辑  
        """  
        input_msgs = self._normalize_messages(messages)  
  
        await self.event_bus.publish(AgentRunEvent(type="stream_started"))  
        try:  
            async for chunk in self.engine.step_stream(input_msgs):   # type: ignore
                yield chunk  
            await self.event_bus.publish(AgentRunEvent(type="stream_completed"))  
        except Exception as e:  
            logger.exception("Agent stream failed")  
            await self.event_bus.publish(AgentRunEvent(type="stream_error", error=str(e)))  
            raise  
  
    # ---------------- 辅助方法 ----------------  
    def _normalize_messages(  
        self,  
        messages: str | Message | List[Message] | List[dict] | dict  
    ) -> List[Message]:  
        """  
        支持以下输入：  
        1. 字符串 -> 单条 user 消息  
        2. Message -> [Message]  
        3. List[Message] -> 原样返回  
        4. dict -> 若包含 role/content 则构建 Message，否则视为 {"input": "..."}  
        5. List[dict] -> 每个 dict 转为 Message  
        """  
        if isinstance(messages, Message):  
            return [messages]  
  
        if isinstance(messages, str):  
            return [Message.user(messages)]  
  
        if isinstance(messages, dict):  
            if "role" in messages:  
                return [Message(**messages)]  
            text = messages.get("input") or str(messages)  
            return [Message.user(text)]  
  
        if isinstance(messages, list):  
            if not messages:  
                raise AgentError("消息列表为空")  
            if isinstance(messages[0], Message):  
                return messages  # type: ignore # 已经是标准 Message  
            normalized = []  
            for item in messages:  
                if isinstance(item, Message):  
                    normalized.append(item)  
                elif isinstance(item, dict):  
                    normalized.append(Message(**item))  
                else:  
                    raise AgentError(f"无法识别的消息元素类型: {type(item)}")  
            return normalized  
  
        raise AgentError(f"不支持的消息类型: {type(messages)}")  
  
    def _serialize_output(self, output: AgentOutput | BaseModel) -> dict:  
        if hasattr(output, "model_dump"):  
            return output.model_dump()  
        return {"content": str(output)}  
```

[9] gecko/core/builder.py
```python
# gecko/core/builder.py  
from __future__ import annotations  
  
from typing import Any, Sequence, Type  
  
from gecko.core.agent import Agent  
from gecko.core.memory import TokenMemory  
from gecko.core.toolbox import ToolBox  
from gecko.core.engine.base import CognitiveEngine  
from gecko.core.engine.react import ReActEngine  
from gecko.plugins.storage.interfaces import SessionInterface  
from gecko.plugins.tools.base import BaseTool  
from gecko.core.exceptions import ConfigurationError  
  
  
class AgentBuilder:  
    """  
    Agent 构建器（改进版）  
    关键改进：  
    1. system_prompt 等引擎参数统一通过 engine_kwargs 传递，避免与 Agent.__init__ 不匹配  
    2. 工具列表自动去重并校验是否继承 BaseTool  
    3. storage 必须实现 SessionInterface，否则在 TokenMemory 中使用会报错  
    4. 支持自定义 Engine 类 & 额外参数  
    """  
  
    def __init__(self):  
        self._model: Any | None = None  
        self._tools: list[BaseTool] = []  
        self._storage: SessionInterface | None = None  
        self._session_id: str = "default"  
        self._max_tokens: int = 4000  
        self._engine_cls: Type[CognitiveEngine] = ReActEngine  
        self._engine_kwargs: dict[str, Any] = {}  # 统一放置系统 Prompt、Hook 等  
        self._toolbox_config: dict[str, Any] = {}  
  
    # ---------------- 基础配置 ----------------  
    def with_model(self, model: Any) -> "AgentBuilder":  
        # 检查模型是否实现必要方法  
        missing = [m for m in ("acompletion",) if not hasattr(model, m)]  
        if missing:  
            raise ConfigurationError(  
                f"Model 缺少必要方法: {', '.join(missing)}",  
                context={"model": repr(model)}  
            )  
        self._model = model  
        return self  
  
    def with_tools(self, tools: Sequence[BaseTool]) -> "AgentBuilder":  
        for tool in tools:  
            if not isinstance(tool, BaseTool):  
                raise TypeError(f"Tool 必须继承 BaseTool，收到 {type(tool)}")  
            self._tools.append(tool)  
        return self  
  
    def with_storage(self, storage: SessionInterface | None) -> "AgentBuilder":  
        if storage and not isinstance(storage, SessionInterface):  
            raise TypeError(  
                "storage 必须实现 SessionInterface，用于 TokenMemory 持久化"  
            )  
        self._storage = storage  
        return self  
  
    def with_session_id(self, session_id: str) -> "AgentBuilder":  
        self._session_id = session_id  
        return self  
  
    def with_max_tokens(self, max_tokens: int) -> "AgentBuilder":  
        self._max_tokens = max_tokens  
        return self  
  
    def with_engine(  
        self,  
        engine_cls: Type[CognitiveEngine],  
        **engine_kwargs: Any  
    ) -> "AgentBuilder":  
        if not issubclass(engine_cls, CognitiveEngine):  
            raise TypeError("engine_cls 必须继承 CognitiveEngine")  
        self._engine_cls = engine_cls  
        self._engine_kwargs.update(engine_kwargs)  
        return self  
  
    def with_system_prompt(self, prompt: str) -> "AgentBuilder":  
        # 统一放入 engine_kwargs，确保 Engine 可接收  
        self._engine_kwargs["system_prompt"] = prompt  
        return self  
  
    def with_toolbox_config(self, **config: Any) -> "AgentBuilder":  
        """  
        允许调用者自定义 ToolBox 的并发/超时等参数  
        """  
        self._toolbox_config.update(config)  
        return self  
  
    # ---------------- 构建流程 ----------------  
    def build(self) -> Agent:  
        if not self._model:  
            raise ConfigurationError("构建 Agent 前必须调用 with_model 指定模型")  
  
        toolbox = self._build_toolbox()  
        memory = self._build_memory()  
  
        return Agent(  
            model=self._model,  
            toolbox=toolbox,  
            memory=memory,  
            engine_cls=self._engine_cls,  
            event_bus=self._engine_kwargs.pop("event_bus", None),  
            **self._engine_kwargs  # 其余参数直接传给 Engine  
        )  
  
    def _build_toolbox(self) -> ToolBox:  
        # 根据工具名称去重，后注册的同名工具会覆盖前者  
        deduped: dict[str, BaseTool] = {}  
        for tool in self._tools:  
            deduped[tool.name] = tool  
  
        return ToolBox(  
            tools=list(deduped.values()),  
            **self._toolbox_config  
        )  
  
    def _build_memory(self) -> TokenMemory:
        # [优化] 将 self._model (ModelProtocol) 注入到 TokenMemory
        # 这样 Memory 就能使用模型特定的 Tokenizer，而不是硬编码的 tiktoken
        return TokenMemory(
            session_id=self._session_id,
            storage=self._storage,
            max_tokens=self._max_tokens,
            model_driver=self._model  # 依赖注入
        )
```

[10] gecko/core/engine/base.py
```python
# gecko/core/engine/base.py
"""
认知引擎基类

定义 Agent 的推理和执行流程，所有引擎实现（ReAct、Chain、Tree 等）
都应继承此基类。

核心概念：
- CognitiveEngine: 抽象基类，定义引擎接口
- 支持普通推理和流式推理
- 支持结构化输出
- 提供 Hook 机制
- 统一的错误处理

优化点：
1. 强化类型注解（使用 ModelProtocol）
2. 完善抽象方法（step, step_stream, step_structured）
3. 添加 Hook 机制（before_step, after_step）
4. 提供工具方法（validate_input, log_execution）
5. 支持上下文管理器（资源管理）
"""
from __future__ import annotations

import asyncio
import time
from abc import ABC, abstractmethod
from typing import Any, AsyncIterator, Callable, Dict, List, Optional, Type, TypeVar

from pydantic import BaseModel

from gecko.core.exceptions import AgentError, ModelError
from gecko.core.logging import get_logger
from gecko.core.memory import TokenMemory
from gecko.core.message import Message
from gecko.core.output import AgentOutput
from gecko.core.protocols import ModelProtocol, supports_streaming
from gecko.core.toolbox import ToolBox

logger = get_logger(__name__)

T = TypeVar("T", bound=BaseModel)


# ====================== 执行统计 ======================

class ExecutionStats(BaseModel):
    """
    引擎执行统计
    
    用于性能监控和调试。
    """
    total_steps: int = 0
    total_time: float = 0.0
    total_tokens: int = 0
    tool_calls: int = 0
    errors: int = 0
    
    def add_step(self, duration: float, tokens: int = 0, had_error: bool = False):
        """记录一次步骤执行"""
        self.total_steps += 1
        self.total_time += duration
        self.total_tokens += tokens
        if had_error:
            self.errors += 1
    
    def add_tool_call(self):
        """记录一次工具调用"""
        self.tool_calls += 1
    
    def get_avg_step_time(self) -> float:
        """获取平均步骤时间"""
        return self.total_time / self.total_steps if self.total_steps > 0 else 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """转换为字典"""
        return {
            "total_steps": self.total_steps,
            "total_time": self.total_time,
            "avg_step_time": self.get_avg_step_time(),
            "total_tokens": self.total_tokens,
            "tool_calls": self.tool_calls,
            "errors": self.errors,
        }


# ====================== 认知引擎基类 ======================

class CognitiveEngine(ABC):
    """
    认知引擎抽象基类
    
    定义 Agent 的核心推理流程，所有具体引擎实现（ReAct、Chain、Tree 等）
    都应该继承此类。
    
    核心方法：
    - step(): 单次/多轮推理（必需）
    - step_stream(): 流式推理（可选）
    - step_structured(): 结构化输出（可选）
    
    Hook 方法：
    - before_step(): 步骤执行前
    - after_step(): 步骤执行后
    - on_error(): 错误处理
    
    生命周期：
    - initialize(): 初始化
    - cleanup(): 清理资源
    
    示例:
        ```python
        class MyEngine(CognitiveEngine):
            async def step(self, input_messages: List[Message]) -> AgentOutput:
                # 实现推理逻辑
                response = await self.model.acompletion(
                    messages=[m.to_openai_format() for m in input_messages]
                )
                return AgentOutput(content=response.choices[0].message["content"])
        
        # 使用
        engine = MyEngine(model=model, toolbox=toolbox, memory=memory)
        output = await engine.step([Message.user("Hello")])
        ```
    """
    
    def __init__(
        self,
        model: ModelProtocol,
        toolbox: ToolBox,
        memory: TokenMemory,
        max_iterations: int = 10,
        enable_stats: bool = True,
        **kwargs
    ):
        """
        初始化认知引擎
        
        参数:
            model: 语言模型（必须实现 ModelProtocol）
            toolbox: 工具箱
            memory: 记忆管理器
            max_iterations: 最大迭代次数（防止死循环）
            enable_stats: 是否启用统计
            **kwargs: 子类的额外参数
        
        异常:
            TypeError: model 不符合 ModelProtocol
        """
        # 验证模型
        if not isinstance(model, ModelProtocol):
            raise TypeError(
                f"model 必须实现 ModelProtocol，收到类型: {type(model).__name__}"
            )
        
        self.model = model
        self.toolbox = toolbox
        self.memory = memory
        self.max_iterations = max_iterations
        self.enable_stats = enable_stats
        
        # 统计信息
        self.stats = ExecutionStats() if enable_stats else None
        
        # Hook 函数（可由子类或外部设置）
        self.before_step_hook: Optional[Callable] = None
        self.after_step_hook: Optional[Callable] = None
        self.on_error_hook: Optional[Callable] = None
        
        # 存储额外的配置
        self._config = kwargs
        
        logger.debug(
            "Engine initialized",
            engine=self.__class__.__name__,
            model=type(model).__name__,
            max_iterations=max_iterations
        )
    
    # ====================== 核心抽象方法 ======================
    
    @abstractmethod
    async def step(
        self, 
        input_messages: List[Message],
        **kwargs
    ) -> AgentOutput:
        """
        执行推理步骤（必需实现）
        
        这是引擎的核心方法，定义了如何处理输入并生成输出。
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数（如 temperature, max_tokens 等）
        
        返回:
            AgentOutput: 执行结果
        
        异常:
            AgentError: 执行失败
            ModelError: 模型调用失败
        
        实现指南:
            1. 验证输入
            2. 调用 before_step_hook（如果有）
            3. 执行推理逻辑
            4. 调用 after_step_hook（如果有）
            5. 返回结果
        
        示例:
            ```python
            async def step(self, input_messages: List[Message]) -> AgentOutput:
                # 转换为 OpenAI 格式
                messages = [m.to_openai_format() for m in input_messages]
                
                # 调用模型
                response = await self.model.acompletion(messages=messages)
                
                # 构建输出
                return AgentOutput(
                    content=response.choices[0].message["content"],
                    usage=response.usage
                )
            ```
        """
        pass
    
    # ====================== 可选方法 ======================
    
    async def step_stream(
        self, 
        input_messages: List[Message],
        **kwargs
    ) -> AsyncIterator[str]:
        """
        流式推理（可选实现）
        
        如果引擎支持流式输出，应该重写此方法。
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数
        
        返回:
            AsyncIterator[str]: 文本流
        
        异常:
            NotImplementedError: 引擎不支持流式输出
        
        示例:
            ```python
            async def step_stream(self, input_messages: List[Message]):
                if not supports_streaming(self.model):
                    raise NotImplementedError("Model does not support streaming")
                
                messages = [m.to_openai_format() for m in input_messages]
                
                async for chunk in self.model.astream(messages=messages):
                    if chunk.content:
                        yield chunk.content
            ```
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} does not support streaming. "
            f"Override step_stream() to enable this feature."
        )
    
    async def step_structured(
        self,
        input_messages: List[Message],
        response_model: Type[T],
        **kwargs
    ) -> T:
        """
        结构化输出推理（可选实现）
        
        执行推理并将输出解析为 Pydantic 模型。
        
        参数:
            input_messages: 输入消息列表
            response_model: 目标 Pydantic 模型类
            **kwargs: 额外参数
        
        返回:
            T: 解析后的模型实例
        
        异常:
            NotImplementedError: 引擎不支持结构化输出
        
        示例:
            ```python
            from pydantic import BaseModel
            
            class Answer(BaseModel):
                question: str
                answer: str
                confidence: float
            
            result = await engine.step_structured(
                input_messages=[Message.user("What is AI?")],
                response_model=Answer
            )
            print(result.answer)
            ```
        """
        raise NotImplementedError(
            f"{self.__class__.__name__} does not support structured output. "
            f"Override step_structured() to enable this feature."
        )
    
    # ====================== Hook 方法 ======================
    
    async def before_step(
        self, 
        input_messages: List[Message],
        **kwargs
    ) -> None:
        """
        步骤执行前的 Hook
        
        在推理开始前调用，可用于：
        - 日志记录
        - 输入验证
        - 状态初始化
        - 发送事件
        
        参数:
            input_messages: 输入消息列表
            **kwargs: 额外参数
        
        注意:
            此方法不应修改输入，如需修改请在子类中重写
        """
        if self.before_step_hook:
            try:
                if asyncio.iscoroutinefunction(self.before_step_hook):
                    await self.before_step_hook(input_messages, **kwargs)
                else:
                    self.before_step_hook(input_messages, **kwargs)
            except Exception as e:
                logger.warning("before_step_hook failed", error=str(e))
    
    async def after_step(
        self,
        input_messages: List[Message],
        output: AgentOutput,
        **kwargs
    ) -> None:
        """
        步骤执行后的 Hook
        
        在推理完成后调用，可用于：
        - 日志记录
        - 结果验证
        - 统计更新
        - 发送事件
        
        参数:
            input_messages: 输入消息列表
            output: 执行结果
            **kwargs: 额外参数
        """
        if self.after_step_hook:
            try:
                if asyncio.iscoroutinefunction(self.after_step_hook):
                    await self.after_step_hook(input_messages, output, **kwargs)
                else:
                    self.after_step_hook(input_messages, output, **kwargs)
            except Exception as e:
                logger.warning("after_step_hook failed", error=str(e))
    
    async def on_error(
        self,
        error: Exception,
        input_messages: List[Message],
        **kwargs
    ) -> None:
        """
        错误处理 Hook
        
        在推理过程中发生错误时调用，可用于：
        - 错误日志记录
        - 错误恢复
        - 降级处理
        - 发送告警
        
        参数:
            error: 异常对象
            input_messages: 输入消息列表
            **kwargs: 额外参数
        """
        if self.on_error_hook:
            try:
                if asyncio.iscoroutinefunction(self.on_error_hook):
                    await self.on_error_hook(error, input_messages, **kwargs)
                else:
                    self.on_error_hook(error, input_messages, **kwargs)
            except Exception as e:
                logger.error("on_error_hook failed", error=str(e))
    
    # ====================== 工具方法 ======================
    
    def validate_input(self, input_messages: List[Message]) -> None:
        """
        验证输入消息
        
        参数:
            input_messages: 输入消息列表
        
        异常:
            ValueError: 输入无效
        """
        if not input_messages:
            raise ValueError("input_messages 不能为空")
        
        if not all(isinstance(m, Message) for m in input_messages):
            raise TypeError("所有输入必须是 Message 实例")
        
        logger.debug("Input validated", message_count=len(input_messages))
    
    def supports_streaming(self) -> bool:
        """
        检查引擎是否支持流式输出
        
        返回:
            是否支持
        """
        # 检查模型能力
        model_supports = supports_streaming(self.model)
        
        # 检查引擎是否重写了 step_stream
        engine_supports = (
            self.__class__.step_stream != CognitiveEngine.step_stream
        )
        
        return model_supports and engine_supports
    
    def get_config(self, key: str, default: Any = None) -> Any:
        """
        获取配置项
        
        参数:
            key: 配置键
            default: 默认值
        
        返回:
            配置值
        """
        return self._config.get(key, default)
    
    def set_config(self, key: str, value: Any) -> None:
        """
        设置配置项
        
        参数:
            key: 配置键
            value: 配置值
        """
        self._config[key] = value
    
    def get_stats(self) -> Optional[Dict[str, Any]]:
        """
        获取执行统计
        
        返回:
            统计信息字典，如果未启用统计则返回 None
        """
        return self.stats.to_dict() if self.stats else None
    
    def reset_stats(self) -> None:
        """重置统计信息"""
        if self.stats:
            self.stats = ExecutionStats()
            logger.debug("Stats reset")
    
    # ====================== 生命周期管理 ======================
    
    async def initialize(self) -> None:
        """
        初始化引擎
        
        在首次使用前调用，可用于：
        - 加载资源
        - 预热模型
        - 初始化连接
        
        子类可以重写此方法以添加自定义初始化逻辑。
        """
        logger.debug("Engine initialized", engine=self.__class__.__name__)
    
    async def cleanup(self) -> None:
        """
        清理资源
        
        在引擎不再使用时调用，可用于：
        - 关闭连接
        - 释放资源
        - 保存状态
        
        子类可以重写此方法以添加自定义清理逻辑。
        """
        logger.debug("Engine cleanup", engine=self.__class__.__name__)
    
    # ====================== 上下文管理器 ======================
    
    async def __aenter__(self):
        """异步上下文管理器入口"""
        await self.initialize()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """异步上下文管理器出口"""
        await self.cleanup()
        return False
    
    # ====================== 辅助方法 ======================
    
    async def _safe_execute(
        self,
        func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        安全执行函数（带错误处理和统计）
        
        参数:
            func: 要执行的函数
            *args: 位置参数
            **kwargs: 关键字参数
        
        返回:
            函数执行结果
        
        异常:
            原始异常（已记录日志和统计）
        """
        start_time = time.time()
        had_error = False
        
        try:
            if asyncio.iscoroutinefunction(func):
                result = await func(*args, **kwargs)
            else:
                result = func(*args, **kwargs)
            
            return result
        
        except Exception as e:
            had_error = True
            
            # 记录统计
            if self.stats:
                self.stats.errors += 1
            
            # 调用错误 Hook
            await self.on_error(e, kwargs.get("input_messages", []))
            
            # 记录日志
            logger.exception(
                "Engine execution failed",
                engine=self.__class__.__name__,
                error=str(e)
            )
            
            raise
        
        finally:
            # 记录执行时间
            duration = time.time() - start_time
            if self.stats:
                self.stats.add_step(duration, had_error=had_error)
    
    def __repr__(self) -> str:
        """字符串表示"""
        return (
            f"{self.__class__.__name__}("
            f"model={type(self.model).__name__}, "
            f"max_iterations={self.max_iterations}"
            f")"
        )


# ====================== 工具函数 ======================

def create_engine(
    engine_class: Type[CognitiveEngine],
    model: ModelProtocol,
    toolbox: ToolBox,
    memory: TokenMemory,
    **kwargs
) -> CognitiveEngine:
    """
    创建引擎实例（工厂函数）
    
    参数:
        engine_class: 引擎类
        model: 模型
        toolbox: 工具箱
        memory: 记忆
        **kwargs: 额外参数
    
    返回:
        引擎实例
    
    示例:
        ```python
        engine = create_engine(
            ReActEngine,
            model=openai_model,
            toolbox=toolbox,
            memory=memory,
            max_iterations=5
        )
        ```
    """
    if not issubclass(engine_class, CognitiveEngine):
        raise TypeError(
            f"engine_class 必须是 CognitiveEngine 的子类，"
            f"收到: {engine_class.__name__}"
        )
    
    return engine_class(
        model=model,
        toolbox=toolbox,
        memory=memory,
        **kwargs
    )


# ====================== 导出 ======================

__all__ = [
    "CognitiveEngine",
    "ExecutionStats",
    "create_engine",
]
```

[11] gecko/core/engine/react.py
```python
# gecko/core/engine/react.py
"""
ReAct 推理引擎

实现了 ReAct (Reason + Act) 认知架构，负责协调 LLM 与工具箱的交互循环。

核心功能：
1. 推理循环：基于 Thought-Action-Observation 模式的自动执行
2. 流式输出：支持低延迟的 Token 级流式响应，同时保持工具调用的完整性
3. 结构化输出：支持将推理结果解析为强类型的 Pydantic 对象
4. 稳健性设计：内置死循环检测、观测值截断、错误反馈与自动重试

优化日志：
- [Fix] 重构 step_stream: 移除 Peek 机制，显著降低首字延迟 (TTFT)
- [Fix] 修复 Jinja2 模板语法，正确处理字典属性访问
- [Fix] 完善生命周期钩子 (on_turn_start/end) 在流式模式下的覆盖
- [Feat] 增加工具调用死循环检测 (Hash-based loop detection)
- [Feat] 增加工具观测值 (Observation) 智能截断
"""
from __future__ import annotations

import json
import time
from datetime import datetime
from typing import (
    Any,
    AsyncIterator,
    Callable,
    Dict,
    List,
    Optional,
    Type,
    TypeVar,
    Union,
)

from pydantic import BaseModel

from gecko.core.engine.base import CognitiveEngine
from gecko.core.exceptions import AgentError, ModelError
from gecko.core.logging import get_logger
from gecko.core.memory import TokenMemory
from gecko.core.message import Message
from gecko.core.output import AgentOutput
from gecko.core.prompt import PromptTemplate
from gecko.core.structure import StructureEngine, StructureParseError
from gecko.core.toolbox import ToolBox
from gecko.core.utils import ensure_awaitable

logger = get_logger(__name__)

T = TypeVar("T", bound=BaseModel)

# 默认 ReAct 提示词模板
# 包含时间注入和工具列表渲染
DEFAULT_REACT_TEMPLATE = """You are a helpful AI assistant.
Current Time: {{ current_time }}

Available Tools:
{% for tool in tools %}
- {{ tool['function']['name'] }}: {{ tool['function']['description'] }}
{% endfor %}

Answer the user's request. Use tools if necessary.
If you use a tool, just output the tool call format.
"""


class ExecutionContext:
    """
    执行上下文
    
    封装每一轮 ReAct 循环的运行时状态，用于在 Engine 内部传递数据。
    """

    def __init__(self, messages: List[Message]):
        self.messages = messages.copy()  # 浅拷贝，避免污染原始列表
        self.turn = 0
        self.metadata: Dict[str, Any] = {}
        
        # 状态追踪：用于死循环检测
        self.last_tool_calls_hash: Optional[int] = None
        self.consecutive_tool_error_count: int = 0

    def add_message(self, message: Message):
        """追加消息到当前上下文"""
        self.messages.append(message)

    def get_last_message(self) -> Optional[Message]:
        """获取最后一条消息"""
        return self.messages[-1] if self.messages else None


class ReActEngine(CognitiveEngine):
    """
    ReAct (Reason + Act) 引擎实现
    """

    def __init__(
        self,
        model: Any,
        toolbox: ToolBox,
        memory: TokenMemory,
        max_turns: int = 5,
        max_observation_length: int = 2000,
        system_prompt: Union[str, PromptTemplate, None] = None,
        on_turn_start: Optional[Callable[[ExecutionContext], Any]] = None,
        on_turn_end: Optional[Callable[[ExecutionContext], Any]] = None,
        on_tool_execute: Optional[Callable[[str, Dict[str, Any]], Any]] = None,
        **kwargs,
    ):
        """
        初始化 ReAct 引擎
        
        参数:
            model: LLM 模型实例 (需实现 ModelProtocol)
            toolbox: 工具箱实例
            memory: 记忆管理器
            max_turns: 最大思考轮数 (防止无限循环)
            max_observation_length: 工具输出的最大字符数 (防止 Context 爆炸)
            system_prompt: 自定义系统提示词
            on_turn_start: 每一轮开始时的钩子
            on_turn_end: 每一轮结束时的钩子
            on_tool_execute: 工具执行前的钩子
        """
        super().__init__(model, toolbox, memory, **kwargs)
        self.max_turns = max_turns
        self.max_observation_length = max_observation_length
        
        # Hooks
        self.on_turn_start = on_turn_start
        self.on_turn_end = on_turn_end
        self.on_tool_execute = on_tool_execute

        # 初始化 Prompt 模板
        if system_prompt is None:
            self.prompt_template = PromptTemplate(template=DEFAULT_REACT_TEMPLATE)
        elif isinstance(system_prompt, str):
            self.prompt_template = PromptTemplate(template=system_prompt)
        else:
            self.prompt_template = system_prompt

        # 能力检测缓存
        self._supports_functions = self._check_function_calling_support()
        self._supports_stream = self._check_streaming_support()

    def _check_function_calling_support(self) -> bool:
        if hasattr(self.model, "_supports_function_calling"):
            return getattr(self.model, "_supports_function_calling")
        return True  # 默认假设支持，运行时报错再处理

    def _check_streaming_support(self) -> bool:
        return hasattr(self.model, "astream")

    # ===================== 核心接口实现 =====================

    async def step( # type: ignore
        self,
        input_messages: List[Message],
        response_model: Optional[Type[T]] = None,
        strategy: str = "auto",
        max_retries: int = 2,
        **kwargs,
    ) -> Union[AgentOutput, T]:
        """
        执行单次推理任务 (同步等待模式)
        
        流程:
        1. 验证输入 & Hook
        2. 构建上下文 (Context) & 参数
        3. 执行 ReAct 循环 (Reasoning Loop)
        4. 处理结构化输出 (如果需要)
        5. 保存记忆 & Hook
        """
        start_time = time.time()

        # 1. 验证与 Hook
        self.validate_input(input_messages)
        await self.before_step(input_messages, **kwargs)

        logger.info(
            "ReAct execution started",
            input_count=len(input_messages),
            has_structure=response_model is not None,
        )

        try:
            # 2. 预处理与上下文构建
            augmented_messages = self._augment_messages_for_structure(
                input_messages, response_model
            )
            context = await self._build_execution_context(augmented_messages)
            
            llm_params = self._build_llm_params(response_model, strategy)
            llm_params.update(kwargs)

            # 3. 运行推理循环
            final_output = await self._run_reasoning_loop(
                context, llm_params, response_model
            )

            # 4. 结构化输出处理
            result: Union[AgentOutput, T] = final_output
            if response_model:
                result = await self._handle_structured_output(
                    final_output, response_model, context, llm_params, max_retries
                )

            # 5. 保存与收尾
            await self._save_context(context)

            # 触发 after_step (需将 T 转换为 AgentOutput 以兼容 Hook 签名)
            hook_output = (
                result
                if isinstance(result, AgentOutput)
                else AgentOutput(content=str(result), raw=result)
            )
            await self.after_step(input_messages, hook_output, **kwargs)

            # 统计
            if self.stats:
                self.stats.add_step(time.time() - start_time)

            return result

        except Exception as e:
            if self.stats:
                self.stats.errors += 1
            logger.exception("ReAct execution failed")
            await self.on_error(e, input_messages, **kwargs)
            raise

    async def step_stream( # type: ignore
        self, input_messages: List[Message], **kwargs
    ) -> AsyncIterator[str]:
        """
        执行流式推理任务
        
        特点:
        - 立即返回首个 Token (Low Latency)
        - 遇到工具调用时暂停输出，执行工具后继续流式生成
        """
        if not self._supports_stream:
            raise AgentError("当前模型不支持流式输出")

        start_time = time.time()
        await self.before_step(input_messages, **kwargs)

        # 构建上下文
        context = await self._build_execution_context(input_messages)
        llm_params = self._build_llm_params(None, "auto")
        llm_params.update(kwargs)

        try:
            # 进入递归流式循环
            async for token in self._run_streaming_loop(context, llm_params):
                yield token

            # 保存记忆
            await self._save_context(context)

            if self.stats:
                self.stats.add_step(time.time() - start_time)

        except Exception as e:
            if self.stats:
                self.stats.errors += 1
            logger.exception("ReAct stream failed")
            await self.on_error(e, input_messages, **kwargs)
            raise

    # ===================== 推理循环逻辑 =====================

    # [新增辅助方法] 提取通用的回合处理逻辑
    async def _process_turn_results(
        self, 
        context: ExecutionContext, 
        assistant_msg: Message, 
        response_model: Optional[Type[T]] = None
    ) -> bool:
        """
        处理 LLM 生成后的通用逻辑：
        1. 死循环检测
        2. 更新上下文
        3. 判断是否终止 (结构化提取/无工具调用)
        4. 执行工具
        
        返回:
            should_continue: 是否继续下一轮循环 (True=继续, False=终止)
        """
        # 1. 死循环检测
        if self._detect_infinite_loop(assistant_msg, context):
            logger.warning("Detected infinite tool loop, breaking.")
            
            # [关键] 不要把错误的 tool call 加入 context 的历史
            # 如果把死循环的 tool call 加入历史，LLM 在重试时看到历史里自己刚调用了这个工具，
            # 可能会再次困惑。
            # 策略：
            # 1. 记录这个由 Assistant 发出的错误尝试
            context.add_message(assistant_msg) 
            # 2. 紧接着追加 User/System 的警告，强制打断它的思维惯性
            context.add_message(Message.user(
                f"System Alert: Execution stopped because you are"
                f" calling tool '{assistant_msg.tool_calls[0]['function']['name']}' " # type: ignore
                "repeatedly with identical arguments. " # type: ignore
                "Stop looping. Provide the final answer immediately using the correct format."
            ))
            
            return False

        context.add_message(assistant_msg)

        # 2. 终止条件检查
        # 条件 A: 是结构化提取 (Implicit Tool Call)
        if response_model and self._is_structure_extraction(
            assistant_msg, response_model
        ):
            await self._trigger_turn_end(context)
            return False

        # 条件 B: 无工具调用 (纯文本回复)
        if not assistant_msg.tool_calls:
            await self._trigger_turn_end(context)
            return False

        # 3. 执行工具
        if self.stats:
            self.stats.tool_calls += len(assistant_msg.tool_calls)

        await self._execute_tool_calls(assistant_msg.tool_calls, context)

        # Hook: Turn End
        await self._trigger_turn_end(context)
        
        return True

    # [修改方法] 重构同步循环，使用 _process_turn_results
    async def _run_reasoning_loop(
        self,
        context: ExecutionContext,
        llm_params: Dict[str, Any],
        response_model: Optional[Type[T]],
    ) -> AgentOutput:
        """
        ReAct 主循环 (同步模式)
        """
        while context.turn < self.max_turns:
            context.turn += 1

            # Hook: Turn Start
            if self.on_turn_start:
                await ensure_awaitable(self.on_turn_start, context)

            # 1. LLM 推理
            response = await self._call_llm(context, llm_params)
            assistant_msg = self._parse_llm_response(response)
            context.metadata["last_response"] = response

            # 2. 处理回合逻辑 (复用)
            should_continue = await self._process_turn_results(
                context, assistant_msg, response_model
            )
            
            if not should_continue:
                break

        # 循环结束，返回最后结果
        last_msg = context.get_last_message()
        if not last_msg:
            return AgentOutput(content="No response generated.")

        return AgentOutput(
            content=last_msg.content or "", # type: ignore
            raw=context.metadata.get("last_response"),
            tool_calls=last_msg.tool_calls or [],
        )

    # [修改方法] 重构流式循环，使用 _process_turn_results
    async def _run_streaming_loop(
        self, context: ExecutionContext, llm_params: Dict[str, Any]
    ) -> AsyncIterator[str]:
        """
        ReAct 流式循环 (递归模式)
        """
        # 循环控制：只要 turn 未达上限，且 should_continue 为 True，就一直循环
        while context.turn < self.max_turns:
            context.turn += 1
            
            # Hook: Turn Start
            if self.on_turn_start:
                await ensure_awaitable(self.on_turn_start, context)

            messages_payload = [m.to_openai_format() for m in context.messages]
            
            # 状态累积器 (每轮开始前重置)
            collected_content = []
            tool_calls_data: List[Dict[str, Any]] = []

            # 1. 消费流 (Inner Loop: Streaming Consumer)
            # 负责将 LLM 的 Token 实时透传给用户，并累积工具调用信息
            async for chunk in self.model.astream(messages=messages_payload, **llm_params): # type: ignore
                delta = self._extract_delta(chunk)

                # A. 文本内容：实时 Yield
                content = delta.get("content")
                if content:
                    collected_content.append(content)
                    yield content

                # B. 工具调用：后台累积
                if delta.get("tool_calls"):
                    self._accumulate_tool_chunks(tool_calls_data, delta["tool_calls"])

            # 2. 组装完整消息 (Turn Completion)
            final_text = "".join(collected_content)
            assistant_msg = Message.assistant(content=final_text)
            
            # 清洗并组装工具调用
            if tool_calls_data:
                valid_calls = [
                    tc for tc in tool_calls_data 
                    if tc["function"]["name"] or tc["function"]["arguments"]
                ]
                if valid_calls:
                    assistant_msg.tool_calls = valid_calls

            # 3. 处理回合逻辑 (Decision Making)
            # 复用基类的 _process_turn_results 方法
            # 返回 True 表示 "工具已执行完毕，状态已更新，请继续下一轮 LLM 推理"
            # 返回 False 表示 "任务完成" 或 "检测到死循环/无需工具"，应退出循环
            should_continue = await self._process_turn_results(
                context, assistant_msg, response_model=None
            )

            # 如果不需要继续，跳出 while 循环，结束流式生成
            if not should_continue:
                break
            
            # 如果 should_continue 为 True，while 循环会自动进入下一轮
            # context.turn 增加，context.messages 已包含工具结果

    # ===================== 辅助逻辑 =====================
    
    def _normalize_tool_call(self, tool_call: Dict[str, Any]) -> Dict[str, Any]:
        """
        [New Helper] 规范化工具调用数据结构 (Adapter Pattern)
        
        目标：将 LLM 的原始输出统一转换为 ToolBox.execute_many 所需的扁平格式。
        兼容：
        1. OpenAI 嵌套格式: {"function": {"name": "...", "arguments": "..."}}
        2. 扁平格式: {"name": "...", "arguments": {...}}
        3. 参数类型: JSON String 或 Dict
        """
        # 1. 提取 Name 和 Arguments
        func_block = tool_call.get("function")
        
        # 优先尝试 OpenAI 嵌套结构
        if func_block and isinstance(func_block, dict):
            name = func_block.get("name")
            raw_args = func_block.get("arguments", "{}")
        else:
            # 降级尝试扁平结构
            name = tool_call.get("name")
            raw_args = tool_call.get("arguments", "{}")

        # 2. 安全解析参数 (JSON String -> Dict)
        if isinstance(raw_args, str):
            try:
                # 处理常见的 JSON 格式问题 (如包含换行符)
                parsed_args = json.loads(raw_args)
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse tool arguments for '{name}': {raw_args}")
                # 解析失败返回空字典，让 ToolBox 抛出参数校验错误，而不是在这里崩溃
                parsed_args = {} 
        else:
            parsed_args = raw_args if isinstance(raw_args, dict) else {}

        # 3. 返回标准扁平格式
        return {
            "id": tool_call.get("id", ""), # ID 丢失也允许执行
            "name": name,
            "arguments": parsed_args,
        }

    async def _execute_tool_calls(
        self, tool_calls: List[Dict[str, Any]], context: ExecutionContext
    ):
        """
        批量执行工具
        
        改进：使用 _normalize_tool_call 解耦数据清洗逻辑
        """
        # 1. 数据规范化 (Normalization)
        flat_tool_calls = [self._normalize_tool_call(tc) for tc in tool_calls]

        # 2. 批量执行 (Execution)
        results = await self.toolbox.execute_many(flat_tool_calls)

        # 3. 处理结果与副作用 (Side Effects)
        for res in results:
            if self.on_tool_execute:
                await ensure_awaitable(self.on_tool_execute, res.tool_name, {})

            content = res.result
            # 观测值截断
            if len(content) > self.max_observation_length:
                logger.warning(
                    "Truncating tool output",
                    tool=res.tool_name,
                    original_len=len(content),
                    limit=self.max_observation_length,
                )
                content = (
                    content[: self.max_observation_length]
                    + f"\n...(truncated, total {len(content)} chars)"
                )

            tool_msg = Message.tool_result(
                tool_call_id=res.call_id,
                content=content,
                tool_name=res.tool_name,
            )
            context.add_message(tool_msg)

            # 错误反馈策略
            if res.is_error:
                context.consecutive_tool_error_count += 1
                if context.consecutive_tool_error_count >= 3:
                    context.add_message(
                        Message.user(
                            "System: Too many tool errors. Please stop using this tool or change parameters."
                        )
                    )
            else:
                context.consecutive_tool_error_count = 0

    
    def _detect_infinite_loop(
        self, message: Message, context: ExecutionContext
    ) -> bool:
        """检测是否连续以相同参数调用同一工具"""
        if not message.tool_calls:
            return False

        try:
            # 计算工具调用的指纹 (Name + Args)
            calls_dump = json.dumps(
                [
                    {
                        "name": tc["function"]["name"],
                        "args": tc["function"]["arguments"],
                    }
                    for tc in message.tool_calls
                ],
                sort_keys=True,
            )
            current_hash = hash(calls_dump)

            if context.last_tool_calls_hash == current_hash:
                logger.warning("Infinite tool loop detected", calls=calls_dump)
                return True

            context.last_tool_calls_hash = current_hash
            return False
        except Exception:
            # 如果 JSON 解析失败或其他错误，保守放行
            return False

    async def _trigger_turn_end(self, context: ExecutionContext):
        """触发 Turn End Hook"""
        if self.on_turn_end:
            await ensure_awaitable(self.on_turn_end, context)

    def _accumulate_tool_chunks(self, target_list: List[Dict], chunks: List[Dict]):
        """
        合并流式工具调用片段 (OpenAI Protocol)
        """
        for tc_chunk in chunks:
            index = tc_chunk.get("index")
            
            # 确保列表长度足够
            if index is not None:
                while len(target_list) <= index:
                    target_list.append(
                        {
                            "id": "",
                            "type": "function",
                            "function": {"name": "", "arguments": ""},
                        }
                    )
            
            # 获取目标引用
            target = (
                target_list[index]
                if index is not None and index < len(target_list)
                else (target_list[-1] if target_list else None)
            )

            if target:
                if tc_chunk.get("id"):
                    target["id"] += tc_chunk["id"]
                
                func_chunk = tc_chunk.get("function", {})
                if func_chunk.get("name"):
                    target["function"]["name"] += func_chunk["name"]
                if func_chunk.get("arguments"):
                    target["function"]["arguments"] += func_chunk["arguments"]

    # ----------------- 上下文构建 -----------------

    async def _build_execution_context(
        self, input_messages: List[Message]
    ) -> ExecutionContext:
        """加载历史并构建包含 System Prompt 的上下文"""
        history = await self._load_history()

        system_msg = None
        # 检查是否已存在 System Prompt
        has_system = any(m.role == "system" for m in input_messages) or any(
            m.role == "system" for m in history
        )

        if not has_system:
            # 动态渲染 System Prompt
            template_vars = {
                "tools": self.toolbox.to_openai_schema(),
                "current_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }
            system_content = self.prompt_template.format_safe(**template_vars)
            system_msg = Message.system(system_content)

        all_messages = []
        if system_msg:
            all_messages.append(system_msg)
        all_messages.extend(history)
        all_messages.extend(input_messages)

        return ExecutionContext(all_messages)

    async def _load_history(self) -> List[Message]:
        """从 Memory 加载历史记录"""
        if not self.memory.storage:
            return []
        try:
            data = await self.memory.storage.get(self.memory.session_id)
            if data and "messages" in data:
                return await self.memory.get_history(data["messages"])
        except Exception:
            return []
        return []

    async def _save_context(self, context: ExecutionContext):
        """保存上下文到 Memory"""
        if not self.memory.storage:
            return
        try:
            messages_data = [m.to_openai_format() for m in context.messages]
            await self.memory.storage.set(
                self.memory.session_id, {"messages": messages_data}
            )
        except Exception as e:
            logger.warning("Failed to save context", error=str(e))

    # ----------------- LLM 交互 -----------------

    def _build_llm_params(
        self, response_model: Optional[Type[T]], strategy: str
    ) -> Dict[str, Any]:
        """构建传递给 LLM 的参数"""
        params: Dict[str, Any] = {}
        tools_schema = self.toolbox.to_openai_schema()

        if tools_schema and self._supports_functions:
            params["tools"] = tools_schema
            params["tool_choice"] = "auto"

        if response_model:
            if strategy in {"auto", "function_calling"} and self._supports_functions:
                structure_tool = StructureEngine.to_openai_tool(response_model)
                existing_names = {
                    t["function"]["name"] for t in params.get("tools", [])
                }
                if structure_tool["function"]["name"] not in existing_names:
                    params.setdefault("tools", []).append(structure_tool)
                params["tool_choice"] = "auto"
            else:
                params["response_format"] = {"type": "json_object"}

        return params

    def _augment_messages_for_structure(
        self, messages: List[Message], response_model: Optional[Type[T]]
    ) -> List[Message]:
        """注入结构化输出引导指令"""
        if not response_model or not self._supports_functions:
            return messages

        structure_tool_name = StructureEngine.to_openai_tool(response_model)[
            "function"
        ]["name"]
        instruction = (
            f"\nIMPORTANT: You MUST call the '{structure_tool_name}' function to provide your final answer. "
            "Do not reply with text only."
        )

        augmented = messages.copy()
        # 尝试追加到最后一条 User 消息
        for i in range(len(augmented) - 1, -1, -1):
            if augmented[i].role == "user":
                original = augmented[i]
                new_content = str(original.content) + instruction
                augmented[i] = Message(
                    role="user", content=new_content, name=original.name
                )
                return augmented

        # 否则新增一条 User 消息
        augmented.append(Message.user(instruction))
        return augmented

    async def _call_llm(self, context: ExecutionContext, params: Dict[str, Any]) -> Any:
        """调用 LLM API"""
        messages_payload = [m.to_openai_format() for m in context.messages]
        try:
            return await ensure_awaitable(
                self.model.acompletion, messages=messages_payload, **params
            )
        except Exception as e:
            raise ModelError(f"LLM API call failed: {e}") from e

    def _parse_llm_response(self, response: Any) -> Message:
        """解析 LLM 响应为 Message 对象"""
        if hasattr(response, "choices") and response.choices:
            choice = response.choices[0]
            message_data = choice.message
            if hasattr(message_data, "model_dump"):
                message_data = message_data.model_dump()
            elif hasattr(message_data, "to_dict"):
                message_data = message_data.to_dict()
            elif not isinstance(message_data, dict):
                message_data = {
                    "role": getattr(message_data, "role", "assistant"),
                    "content": getattr(message_data, "content", ""),
                    "tool_calls": getattr(message_data, "tool_calls", None),
                }

            if "tool_calls" in message_data and message_data["tool_calls"] is None:
                del message_data["tool_calls"]

            return Message(**message_data)

        raise ModelError("Invalid LLM response format")

    def _extract_delta(self, chunk: Any) -> Dict[str, Any]:
        """从 Stream Chunk 中提取 delta"""
        if hasattr(chunk, "choices") and chunk.choices:
            choice = chunk.choices[0]
            if isinstance(choice, dict):
                return choice.get("delta", {})
            return getattr(choice, "delta", {})
        return {}

    def _is_structure_extraction(self, message: Message, model_class: Type[T]) -> bool:
        """判断当前是否为结构化提取的工具调用"""
        if not message.tool_calls or not self._supports_functions:
            return False
        extraction_tool_name = StructureEngine.to_openai_tool(model_class)["function"][
            "name"
        ]
        return any(
            tc.get("function", {}).get("name") == extraction_tool_name
            for tc in message.tool_calls
        )

    async def _handle_structured_output(
        self,
        output: AgentOutput,
        response_model: Type[T],
        context: ExecutionContext,
        llm_params: Dict[str, Any],
        max_retries: int,
    ) -> T: # type: ignore
        """处理结构化输出解析与自动重试"""
        expected_tool_name = StructureEngine.to_openai_tool(response_model)["function"]["name"]

        for attempt in range(max_retries + 1):
            try:
                # ------------------------------------------------------------------
                # [增强逻辑] 智能定位目标工具调用
                # ------------------------------------------------------------------
                target_tool_call = None
                
                if output.tool_calls and self._supports_functions:
                    # 1. 尝试在所有工具调用中寻找匹配预期的那个
                    for tc in output.tool_calls:
                        name = tc.get("function", {}).get("name")
                        if name == expected_tool_name:
                            target_tool_call = tc
                            break
                    
                    # 2. 如果有工具调用，但没一个是目标工具 -> 判定为推理中断/死循环/逻辑错误
                    if not target_tool_call:
                        # 获取第一个工具名用于报错信息
                        first_tool = output.tool_calls[0].get("function", {}).get("name")
                        raise StructureParseError(
                            f"Incorrect tool used. Expected final tool '{expected_tool_name}', "
                            f"but detected intermediate tool(s) like '{first_tool}'. "
                            "Execution stopped prematurely (e.g., infinite loop or max turns reached). "
                            "Please directly call the final tool with the answer."
                        )
                
                # ------------------------------------------------------------------
                # 解析逻辑 (传入过滤后的 target_tool_call)
                # ------------------------------------------------------------------
                # 注意：如果是纯文本，target_tool_call 为 None，parse 会尝试从 content 提取
                return await StructureEngine.parse(
                    content=output.content,
                    model_class=response_model,
                    # 关键：只传给解析器它关心的那个工具调用，或者 None
                    raw_tool_calls=[target_tool_call] if target_tool_call else None,
                    auto_fix=True,
                )

            except StructureParseError as e:
                if attempt >= max_retries:
                    raise AgentError(f"Failed to parse structured output after {max_retries} retries: {e}")

                # 构造反馈消息 (保持不变)
                feedback_msg = Message.user(
                    f"Error parsing response: {e}. Please ensure you call the '{expected_tool_name}' function correctly."
                )
                context.add_message(feedback_msg)
                
                # 重新生成
                response = await self._call_llm(context, llm_params)
                msg = self._parse_llm_response(response)
                context.add_message(msg)
                output = AgentOutput(
                    content=msg.content or "", tool_calls=msg.tool_calls or [] # type: ignore
                )
```

[12] gecko/core/events/__init__.py
```python
from gecko.core.events.types import BaseEvent
from gecko.core.events.bus import EventBus, EventHandler, Middleware
from gecko.core.events.presets import AgentRunEvent, WorkflowEvent, SessionEvent

__all__ = [
    "EventBus", "BaseEvent", 
    "AgentRunEvent", "WorkflowEvent", "SessionEvent",
    "EventHandler", "Middleware"
]
```

[13] gecko/core/events/bus.py
```python
"""事件总线逻辑"""
from __future__ import annotations
import asyncio
import inspect
from typing import Any, Awaitable, Callable, Dict, List, Optional, Set, Union
from gecko.core.logging import get_logger
from gecko.core.events.types import BaseEvent

logger = get_logger(__name__)

# EventHandler 可以是返回 None 的同步函数，或者返回 Awaitable 的函数
EventHandler = Callable[[BaseEvent], Union[Awaitable[None], None, Any]]
Middleware = Callable[[BaseEvent], Awaitable[Optional[BaseEvent]]]


class EventBus:
    """
    异步事件总线
    """
    
    def __init__(self):
        self._subscribers: Dict[str, List[EventHandler]] = {}
        self._middlewares: List[Middleware] = []
        self._background_tasks: Set[asyncio.Task] = set()
        self._running = True

    # --- 订阅管理 ---
    
    def subscribe(self, event_type: str, handler: EventHandler) -> "EventBus":
        """
        订阅事件
        支持通配符 "*" 订阅所有事件
        """
        if not callable(handler):
            raise TypeError(f"Event handler must be callable, got {type(handler)}")
        
        self._subscribers.setdefault(event_type, []).append(handler)
        logger.debug("Handler subscribed", event_type=event_type, handler=handler)
        return self

    def unsubscribe(self, event_type: str, handler: EventHandler) -> "EventBus":
        """取消订阅"""
        handlers = self._subscribers.get(event_type, [])
        if handler in handlers:
            handlers.remove(handler)
            logger.debug("Handler unsubscribed", event_type=event_type, handler=handler)
        return self

    def add_middleware(self, middleware: Middleware) -> "EventBus":
        """
        添加中间件
        """
        self._middlewares.append(middleware)
        return self

    # --- 发布事件 ---
    async def publish(self, event: BaseEvent, wait: bool = False):
        """
        发布事件
        
        参数:
            event: 事件对象
            wait: 是否等待所有处理器执行完毕
        """
        if not self._running:
            logger.warning("EventBus is shutting down, event ignored", event_type=event.type)
            return

        # 保存原始事件类型用于日志（因为中间件可能返回 None）
        original_type = event.type

        # 1. 执行中间件
        try:
            for mw in self._middlewares:
                event = await mw(event) # type: ignore
                if event is None:
                    logger.debug("Event blocked by middleware", event_type=original_type)
                    return
        except Exception as e:
            logger.error("Middleware error", error=str(e), event_type=original_type)
            return

        # 2. 获取订阅者
        handlers = self._subscribers.get(event.type, []) + self._subscribers.get("*", [])
        if not handlers:
            return

        # 3. 执行处理（去重）
        unique_handlers = list(dict.fromkeys(handlers))
        
        # 创建执行协程
        tasks = [self._execute_handler(h, event) for h in unique_handlers]

        if wait:
            await asyncio.gather(*tasks, return_exceptions=True)
        else:
            for coro in tasks:
                task = asyncio.create_task(coro)
                self._background_tasks.add(task)
                task.add_done_callback(self._background_tasks.discard)
    
    async def _execute_handler(self, handler: EventHandler, event: BaseEvent):
        """
        执行单个处理器（包含错误捕获）
        
        采用统一的调用方式：先调用，再判断返回值是否为 Awaitable。
        这兼容了 async def, def, 以及 async __call__ 对象。
        """
        try:
            result = handler(event)
            if inspect.isawaitable(result):
                await result
        except Exception as e:
            logger.exception(
                "Event handler failed", 
                event_type=event.type, 
                handler=getattr(handler, "__name__", str(handler)),
                error=str(e)
            )

    # --- 生命周期 ---

    async def shutdown(self, wait: bool = True):
        """
        关闭事件总线
        """
        self._running = False
        
        if wait and self._background_tasks:
            count = len(self._background_tasks)
            if count > 0:
                logger.info("Waiting for background tasks to finish", count=count)
                await asyncio.gather(*self._background_tasks, return_exceptions=True)
        
        self._background_tasks.clear()
        logger.info("EventBus shutdown completed")

    async def __aenter__(self):
        self._running = True
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.shutdown()
```

[14] gecko/core/events/presets.py
```python
"""预置系统事件"""
from gecko.core.events.types import BaseEvent

class AgentRunEvent(BaseEvent):
    """Agent 运行事件"""
    pass

class WorkflowEvent(BaseEvent):
    """Workflow 运行事件"""
    pass

class SessionEvent(BaseEvent):
    """会话变更事件"""
    pass
```

[15] gecko/core/events/types.py
```python
"""事件基础类型"""
from __future__ import annotations
import time
from typing import Any, Dict, Optional
from pydantic import BaseModel, Field

class BaseEvent(BaseModel):
    """事件基类"""
    type: str
    timestamp: float = Field(default_factory=time.time)
    data: Dict[str, Any] = Field(default_factory=dict)
    error: Optional[str] = None
    
    model_config = {"arbitrary_types_allowed": True}
```

[16] gecko/core/exceptions.py
```python
# gecko/core/exceptions.py
"""
Gecko 异常体系（改进版）

改进：移除装饰器，提倡显式错误处理
"""
from __future__ import annotations
from typing import Optional, Dict, Any

# ========== 异常基类 ==========

class GeckoError(Exception):
    """
    Gecko 统一异常基类
    
    设计原则：
    1. 包含结构化上下文
    2. 便于日志记录
    3. 支持异常链（from）
    """
    def __init__(
        self,
        message: str,
        *args,
        error_code: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
        **kwargs
    ):
        super().__init__(message, *args, **kwargs)
        self.message = message
        self.error_code = error_code or self.__class__.__name__
        self.context = context or {}
    
    def to_dict(self) -> Dict[str, Any]:
        """转换为字典（便于日志/API 返回）"""
        return {
            "error_type": self.__class__.__name__,
            "error_code": self.error_code,
            "message": self.message,
            "context": self.context,
        }
    
    def __str__(self) -> str:
        if self.context:
            ctx_str = ", ".join(f"{k}={v}" for k, v in self.context.items())
            return f"{self.message} [{ctx_str}]"
        return self.message

# ========== 领域异常 ==========

class AgentError(GeckoError):
    """Agent 执行异常"""
    pass

class ModelError(GeckoError):
    """模型调用异常"""
    pass

class ToolError(GeckoError):
    """工具执行异常"""
    pass

class ToolNotFoundError(ToolError):
    """工具未找到"""
    def __init__(self, tool_name: str):
        super().__init__(
            f"Tool '{tool_name}' not found in registry",
            error_code="TOOL_NOT_FOUND",
            context={"tool_name": tool_name}
        )

class ToolTimeoutError(ToolError):
    """工具超时"""
    def __init__(self, tool_name: str, timeout: float):
        super().__init__(
            f"Tool '{tool_name}' timed out after {timeout}s",
            error_code="TOOL_TIMEOUT",
            context={"tool_name": tool_name, "timeout": timeout}
        )

class WorkflowError(GeckoError):
    """工作流异常"""
    pass

class WorkflowCycleError(WorkflowError):
    """工作流循环依赖"""
    pass

class StorageError(GeckoError):
    """存储异常"""
    pass

class ConfigurationError(GeckoError):
    """配置错误"""
    pass

class ValidationError(GeckoError):
    """验证错误"""
    pass
```

[17] gecko/core/logging.py
```python
# gecko/core/logging.py
"""
Gecko 结构化日志系统（改进版）

改进：使用成熟的 structlog 库，代码减少 80%
"""
from __future__ import annotations
import logging
import sys
import warnings
from typing import Any, Optional

try:
    import structlog
    STRUCTLOG_AVAILABLE = True
except ImportError:
    STRUCTLOG_AVAILABLE = False
    import warnings
    warnings.warn(
        "structlog not installed. Install with: pip install structlog\n"
        "Falling back to standard logging.",
        ImportWarning
    )

from gecko.config import settings

# ========== 日志初始化 ==========

_initialized = False

def setup_logging(
    level: Optional[str] = None,
    force: bool = False
):
    """
    初始化日志系统
    
    改进：
    1. 优先使用 structlog（如果可用）
    2. 降级到标准 logging（如果 structlog 未安装）
    """
    global _initialized
    
    if _initialized and not force:
        return
    
     # [新增] 屏蔽 LiteLLM/Pydantic 的序列化警告
    warnings.filterwarnings("ignore", message=".*Pydantic serializer warnings.*", category=UserWarning)
    # [新增] 屏蔽 DuckDuckGo 的废弃警告
    warnings.filterwarnings("ignore", message=".*backend='api' is deprecated.*", category=UserWarning)
    
    level = level or settings.log_level
    log_level = getattr(logging, level.upper(), logging.INFO)
    
    if STRUCTLOG_AVAILABLE:
        _setup_structlog(log_level)
    else:
        _setup_standard_logging(log_level)
    
    # 降低第三方库日志级别
    for lib in ["httpx", "httpcore", "litellm", "openai"]:
        logging.getLogger(lib).setLevel(logging.WARNING)
    
    _initialized = True

def _setup_structlog(level: int):
    """配置 structlog"""
    structlog.configure( # type: ignore
        processors=[
            structlog.contextvars.merge_contextvars, # type: ignore
            structlog.processors.add_log_level, # type: ignore
            structlog.processors.StackInfoRenderer(), # type: ignore
            structlog.dev.set_exc_info, # type: ignore
            structlog.processors.TimeStamper(fmt="iso", utc=True), # type: ignore
            # 根据配置选择渲染器
            structlog.processors.JSONRenderer() # type: ignore
            if settings.log_format == "json" # type: ignore
            else structlog.dev.ConsoleRenderer(), # type: ignore
        ],
        wrapper_class=structlog.make_filtering_bound_logger(level), # type: ignore
        context_class=dict,
        logger_factory=structlog.PrintLoggerFactory(file=sys.stdout), # type: ignore
        cache_logger_on_first_use=True,
    )

def _setup_standard_logging(level: int):
    """配置标准 logging（降级方案）"""
    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        stream=sys.stdout,
    )

# ========== 获取 Logger ==========

def get_logger(name: str) -> Any:
    """
    获取 Logger 实例
    
    返回：
    - structlog.BoundLogger（如果可用）
    - logging.Logger（降级方案）
    
    使用示例:
        logger = get_logger(__name__)
        logger.info("event happened", user_id=123, action="login")
    """
    if not _initialized:
        setup_logging()
    
    if STRUCTLOG_AVAILABLE:
        return structlog.get_logger(name) # type: ignore
    else:
        return logging.getLogger(name)

# ========== 自动初始化 ==========

setup_logging()
```

[18] gecko/core/memory.py
```python
# gecko/core/memory.py
"""
Token Memory - 上下文记忆管理器

负责管理对话历史的 Token 计数、裁剪与缓存。

核心功能：
1. 精确计数：基于 tiktoken 的模型特定 Token 计算
2. 智能裁剪：基于滑动窗口 (Sliding Window) 的历史记录加载
3. 性能缓存：LRU 缓存 Token 计算结果，减少重复计算开销
4. 多模态支持：估算图片/文件的 Token 占用
5. 完备的工具链：批量计算、统计打印、快速估算

优化日志：
- [Perf] get_history 采用 O(N) 算法 (append + reverse)
- [Perf] 缓存键生成使用 model_dump_json 加速
- [Fix] 补全所有原始方法 (print_cache_stats, batch optimizations)
- [Fix] 修正统计键名以通过单元测试
"""
from __future__ import annotations

import hashlib
import json
from collections import OrderedDict
from typing import Any, Callable, Dict, List, Optional

from gecko.core.logging import get_logger
from gecko.core.message import Message
from gecko.core.prompt import PromptTemplate
from gecko.core.protocols import ModelProtocol
from gecko.plugins.storage.interfaces import SessionInterface

logger = get_logger(__name__)


class TokenMemory:
    """
    Token 感知的记忆管理器
    
    负责在有限的 Context Window 内最大化保留有效对话历史。
    """

    def __init__(
        self,
        session_id: str,
        storage: Optional[SessionInterface] = None,
        max_tokens: int = 4000,
        model_name: str = "gpt-3.5-turbo",
        cache_size: int = 2000,
        max_message_length: int = 20000,
        enable_cache_for_batch: bool = True,
        # [新增] 接收注入的驱动
        model_driver: Optional[ModelProtocol] = None,
    ):
        """
        初始化 Memory
        
        参数:
            session_id: 会话唯一标识
            storage: 持久化存储后端
            max_tokens: 最大上下文 Token 限制
            model_name: 模型名称 (用于加载 tokenizer)
            cache_size: Token 计数缓存大小 (LRU)
            max_message_length: 单条消息最大字符数 (防御性截断)
            enable_cache_for_batch: 批量计算时是否启用缓存
        """
        if max_tokens <= 0:
            raise ValueError(f"max_tokens must be positive, got {max_tokens}")
        if cache_size <= 0:
            raise ValueError(f"cache_size must be positive, got {cache_size}")

        self.session_id = session_id
        self.storage = storage
        self.max_tokens = max_tokens
        self.model_name = model_name
        self.cache_size = cache_size  # 公开属性
        self.max_message_length = max_message_length
        self.enable_cache_for_batch = enable_cache_for_batch
        self.model_driver = model_driver
        
        # LRU 缓存: Hash(Content) -> TokenCount
        self._token_cache: OrderedDict[str, int] = OrderedDict()
        
        # 缓存统计
        self._cache_hits = 0
        self._cache_misses = 0
        self._cache_evictions = 0
        
        # 延迟加载的 tokenizer
        self._encoding = None
        self._tokenizer_failed = False

    # ====================== Tokenizer ======================

    @property
    def tokenizer(self):
        """延迟加载 tiktoken encoder"""
        if self._encoding:
            return self._encoding
        
        if self._tokenizer_failed:
            return None

        try:
            import tiktoken
            try:
                self._encoding = tiktoken.encoding_for_model(self.model_name)
            except KeyError:
                logger.warning(f"Model {self.model_name} not found in tiktoken, using cl100k_base")
                self._encoding = tiktoken.get_encoding("cl100k_base")
        except ImportError:
            logger.warning("tiktoken not installed. Token counting will be estimated by char length.")
            self._tokenizer_failed = True
            return None
        except Exception as e:
            logger.error(f"Failed to load tokenizer: {e}")
            self._tokenizer_failed = True
            return None
            
        return self._encoding

    # ====================== 单条计数 ======================

    def count_message_tokens(self, message: Message) -> int:
        """
        计算单条消息的 Token 数（带缓存）
        """
        # 1. 生成缓存键
        cache_key = self._make_cache_key(message)
        
        # 2. 检查缓存
        if cache_key in self._token_cache:
            self._token_cache.move_to_end(cache_key)
            self._cache_hits += 1
            return self._token_cache[cache_key]
        
        # 3. 计算
        self._cache_misses += 1
        count = self._count_tokens_impl(message)
        
        # 4. 更新缓存 (LRU)
        self._token_cache[cache_key] = count
        self._token_cache.move_to_end(cache_key)
        
        if len(self._token_cache) > self.cache_size:
            self._token_cache.popitem(last=False)
            self._cache_evictions += 1
            
        return count

    def _make_cache_key(self, message: Message) -> str:
        """
        生成消息的缓存键
        
        优化: 使用 Pydantic model_dump_json (Rust) 加速序列化
        """
        # 快速路径: 普通文本消息直接哈希
        if isinstance(message.content, str) and not message.tool_calls:
            raw = f"{message.role}:{message.name}:{message.content}"
            return hashlib.md5(raw.encode("utf-8")).hexdigest()
            
        # 慢速路径: 多模态或工具调用
        # exclude_none=True 减少数据量，sort_keys=True (默认False) 在 dump_json 中不支持，
        # 但 Pydantic 字段顺序通常是固定的。为了绝对安全，可以用 json.dumps(model_dump)
        # 不过对于缓存键，model_dump_json 通常足够稳定且快。
        try:
            raw_json = message.model_dump_json(
                include={"role", "content", "tool_calls", "name"},
                exclude_none=True
            )
            return hashlib.md5(raw_json.encode("utf-8")).hexdigest()
        except Exception:
            # 降级方案
            data = message.model_dump(include={"role", "content", "tool_calls", "name"})
            return hashlib.md5(json.dumps(data, sort_keys=True, default=str).encode("utf-8")).hexdigest()

    # ====================== 批量计数 ======================

    def count_messages_batch(
        self,
        messages: List[Message],
        use_cache: Optional[bool] = None
    ) -> List[int]:
        """
        批量计算消息的 Token 数
        
        优化: 复用 encoder 对象，减少属性查找开销
        """
        if not messages:
            return []
        
        should_use_cache = (
            use_cache if use_cache is not None else self.enable_cache_for_batch
        )
        
        if should_use_cache:
            return [self.count_message_tokens(msg) for msg in messages]
        else:
            # 性能优化: 提取 encode 方法，避免在循环中重复查找 self.tokenizer
            encode_fn = self.tokenizer.encode if self.tokenizer else None
            return [self._count_tokens_impl(msg, encode=encode_fn) for msg in messages]

    def _count_tokens_impl(
        self,
        message: Message,
        encode: Optional[Callable[[str], List[int]]] = None
    ) -> int:
        """
        实际 Token 计算逻辑
        
        参数:
            message: 消息对象
            encode: 可选的编码函数（性能优化用）
        """
        # [优化] 优先委托给模型驱动层计算
        # 这解决了 Tokenizer Mismatch 问题，且利用了 Driver 层的性能优化
        if self.model_driver:
            # 转换为 OpenAI 格式传给 Driver
            return self.model_driver.count_tokens([message.to_openai_format()])
        
        # 以下保留原有的 tiktoken 回退逻辑作为兜底
        if not encode:
            if self.tokenizer:
                encode = self.tokenizer.encode
            else:
                # 降级: 字符估算
                return len(message.get_text_content()) // 4 + 2

        num_tokens = 4  # Per-message overhead
        
        # 1. Content Tokens
        if isinstance(message.content, str):
            num_tokens += len(encode(message.content))
        elif isinstance(message.content, list):
            for block in message.content:
                if block.type == "text" and block.text:
                    num_tokens += len(encode(block.text))
                elif block.type == "image_url":
                    num_tokens += self._estimate_image_tokens(block.image_url)

        # 2. Tool Calls Overhead
        if message.tool_calls:
            try:
                # 使用快速序列化
                dump = self._fast_json_dumps(message.tool_calls)
                num_tokens += len(encode(dump))
            except Exception:
                num_tokens += 100

        # 3. Name Overhead
        if message.name:
            num_tokens += 1

        return num_tokens

    # ====================== 历史加载 ======================

    async def get_history(
        self,
        raw_messages: List[Dict[str, Any]],
        preserve_system: bool = True
    ) -> List[Message]:
        """
        加载并裁剪历史消息 (O(N) 复杂度优化版)
        """
        if not raw_messages:
            logger.debug("No history messages to load")
            return []

        # 1. 解析消息 (单次遍历)
        parsed_messages: List[Message] = []
        for i, raw in enumerate(raw_messages):
            try:
                msg = Message(**raw)
                self._truncate_message_safety(msg)
                parsed_messages.append(msg)
            except Exception as e:
                logger.warning("Skipping invalid message history", index=i, error=str(e))

        if not parsed_messages:
            return []

        # 2. 分离 System Message
        system_msg: Optional[Message] = None
        candidates = parsed_messages
        
        if preserve_system and parsed_messages[0].role == "system":
            system_msg = parsed_messages[0]
            candidates = parsed_messages[1:]

        # 3. 计算 System Token 开销
        current_tokens = 0
        if system_msg:
            sys_tokens = self.count_message_tokens(system_msg)
            if sys_tokens > self.max_tokens:
                logger.warning("System prompt exceeds max_tokens, force truncating")
                self._truncate_to_fit(system_msg, self.max_tokens)
                sys_tokens = self.count_message_tokens(system_msg)
            current_tokens += sys_tokens

        # 4. 反向回填 (Reverse Accumulation - O(N))
        selected_reverse: List[Message] = []
        
        for msg in reversed(candidates):
            tokens = self.count_message_tokens(msg)
            
            if current_tokens + tokens > self.max_tokens:
                logger.debug(
                    "Context limit reached", 
                    current=current_tokens, 
                    limit=self.max_tokens
                )
                break
            
            selected_reverse.append(msg)
            current_tokens += tokens

        # 5. 重组列表
        result = []
        if system_msg:
            result.append(system_msg)
        
        # 再次反转回时间正序
        result.extend(reversed(selected_reverse))
        
        return result

    # ====================== 辅助方法 ======================

    def clear_cache(self):
        """清空计数缓存"""
        cleared_size = len(self._token_cache)
        self._token_cache.clear()
        self._cache_hits = 0
        self._cache_misses = 0
        self._cache_evictions = 0
        logger.info("Token cache cleared", cleared_entries=cleared_size)

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        获取缓存统计信息
        
        [Fix] 键名修正为 'cache_size' 以符合测试预期
        """
        total_requests = self._cache_hits + self._cache_misses
        hit_rate = (self._cache_hits / total_requests) if total_requests > 0 else 0.0
        
        return {
            "cache_size": len(self._token_cache),
            "max_cache_size": self.cache_size,
            "cache_utilization": len(self._token_cache) / self.cache_size,
            "hits": self._cache_hits,
            "misses": self._cache_misses,
            "evictions": self._cache_evictions,
            "total_requests": total_requests,
            "hit_rate": hit_rate,
        }

    def print_cache_stats(self):
        """打印缓存统计信息（格式化输出）"""
        stats = self.get_cache_stats()
        
        print("\n" + "=" * 60)
        print("Token Cache Statistics".center(60))
        print("=" * 60)
        print(f"Cache Size:        {stats['cache_size']} / {stats['max_cache_size']}")
        print(f"Cache Utilization: {stats['cache_utilization']:.1%}")
        print(f"Total Requests:    {stats['total_requests']}")
        print(f"Cache Hits:        {stats['hits']}")
        print(f"Cache Misses:      {stats['misses']}")
        print(f"Cache Evictions:   {stats['evictions']}")
        print(f"Hit Rate:          {stats['hit_rate']:.1%}")
        print("=" * 60 + "\n")

    def estimate_tokens(self, text: str) -> int:
        """快速估算文本的 token 数（不使用缓存）"""
        if not text:
            return 0
        if self.tokenizer:
            return len(self.tokenizer.encode(text))
        return len(text) // 4

    # ====================== 内部工具 ======================

    def _fast_json_dumps(self, obj: Any) -> str:
        """快速 JSON 序列化"""
        try:
            import orjson
            return orjson.dumps(obj).decode("utf-8")
        except ImportError:
            import json
            return json.dumps(obj)

    def _estimate_image_tokens(self, image_resource: Any) -> int:
        """估算图片 Token"""
        if not image_resource:
            return 0
        detail = getattr(image_resource, "detail", "auto")
        if detail == "low":
            return 85
        return 1000 

    def _truncate_message_safety(self, message: Message):
        """防御性截断"""
        if isinstance(message.content, str):
            if len(message.content) > self.max_message_length:
                message.content = message.content[:self.max_message_length]

    def _truncate_to_fit(self, message: Message, limit_tokens: int):
        """强行截断文本"""
        if not isinstance(message.content, str):
            return
        char_limit = int(limit_tokens * 3.5)
        if len(message.content) > char_limit:
            message.content = message.content[:char_limit] + "...(truncated)"

    def __repr__(self) -> str:
        return (
            f"TokenMemory("
            f"session_id='{self.session_id}', "
            f"max_tokens={self.max_tokens}, "
            f"model='{self.model_name}', "
            f"cache_size={len(self._token_cache)}/{self.cache_size}"
            f")"
        )
    
class SummaryTokenMemory(TokenMemory):
    """
    支持自动摘要的记忆管理器
    
    当历史记录超出 max_tokens 时，不是简单丢弃，而是调用 LLM 对早期历史进行摘要。
    """
    
    def __init__(
        self,
        session_id: str,
        model: ModelProtocol,  # 需要传入模型实例用于摘要
        summary_prompt: Optional[str] = None,
        **kwargs
    ):
        super().__init__(session_id, **kwargs)
        self.model = model
        # 默认摘要模板
        self.summary_template = PromptTemplate(
            template=summary_prompt or (
                "Please condense the following conversation history into a concise summary, "
                "preserving key information and context:\n\n{{ history }}\n\nSummary:"
            ),
            input_variables=["history"]
        )
        # 内存中维护当前的摘要
        self.current_summary: str = ""

    async def get_history(
        self,
        raw_messages: List[Dict[str, Any]],
        preserve_system: bool = True
    ) -> List[Message]:
        """
        重写历史加载逻辑：
        1. 加载原始消息
        2. 如果 Token 超限，触发摘要流程
        3. 返回 [System, Summary, ...Recent Messages]
        """
        # 1. 先用父类逻辑加载和清洗基础消息
        messages = await super().get_history(raw_messages, preserve_system)
        
        # 计算总 Token (此时 messages 已经被父类截断过一次，但那是硬截断)
        # 我们需要基于未截断的完整列表来做决策，或者在此处再次检查
        # 这里简化逻辑：如果父类已经截断了，我们可能丢失了信息。
        # 更理想的是完全重写 get_history，但为了复用，我们这里主要处理 "摘要注入"。
        
        # 实际上，TokenMemory.get_history 的逻辑是反向回填直到满。
        # 这意味着早期的消息已经被丢弃了。
        # 为了实现摘要，我们需要改变策略：
        # 不直接丢弃，而是把丢弃的部分拿来做摘要。
        
        # === 重写核心逻辑 ===
        if not raw_messages:
            return []

        parsed_messages = []
        for raw in raw_messages:
            try:
                msg = Message(**raw)
                self._truncate_message_safety(msg)
                parsed_messages.append(msg)
            except Exception:
                pass

        if not parsed_messages:
            return []

        # 分离 System
        system_msg = None
        candidates = parsed_messages
        if preserve_system and parsed_messages[0].role == "system":
            system_msg = parsed_messages[0]
            candidates = parsed_messages[1:]

        # 预留 System 和 Summary 的 Token 空间 (估算 500 tokens)
        reserved_tokens = 500
        if system_msg:
            reserved_tokens += self.count_message_tokens(system_msg)
        
        current_tokens = 0
        recent_messages = []
        to_summarize = []

        # 反向选取最近的消息
        for msg in reversed(candidates):
            tokens = self.count_message_tokens(msg)
            if current_tokens + tokens > (self.max_tokens - reserved_tokens):
                # 超出窗口，归入待摘要队列
                to_summarize.append(msg)
            else:
                recent_messages.append(msg)
                current_tokens += tokens
        
        recent_messages.reverse() # 恢复时间正序
        to_summarize.reverse()    # 恢复时间正序

        # 如果有需要摘要的消息，生成或更新摘要
        if to_summarize:
            await self._update_summary(to_summarize)

        # 组装最终历史
        final_history = []
        if system_msg:
            final_history.append(system_msg)
        
        # 注入摘要 (作为 System 消息或 User 提示)
        if self.current_summary:
            summary_msg = Message.system(f"Previous conversation summary: {self.current_summary}")
            final_history.append(summary_msg)
            
        final_history.extend(recent_messages)
        
        return final_history

    async def _update_summary(self, messages: List[Message]):
        """调用 LLM 更新摘要"""
        if not messages:
            return

        # 将消息转为文本
        history_text = "\n".join([f"{m.role}: {m.get_text_content()}" for m in messages])
        
        # 如果已有摘要，将其合并进去
        if self.current_summary:
            history_text = f"Previous Summary: {self.current_summary}\n\nNew Conversation:\n{history_text}"

        # 构造 Prompt
        prompt = self.summary_template.format(history=history_text)
        
        try:
            # 调用模型生成摘要
            # 注意：这里是一个简单的阻塞调用，生产环境可能需要后台异步更新
            response = await self.model.acompletion([{"role": "user", "content": prompt}])
            new_summary = response.choices[0].message["content"]
            if new_summary:
                self.current_summary = new_summary
                logger.info("Conversation summary updated", summary_len=len(new_summary))
        except Exception as e:
            logger.error("Failed to update summary", error=str(e))
            # 失败时保持原有摘要，或者不做处理
```

[19] gecko/core/message/__init__.py
```python
from gecko.core.message.resources import MediaResource, ContentBlock
from gecko.core.message.model import Message, Role

__all__ = ["Message", "MediaResource", "ContentBlock", "Role"]
```

[20] gecko/core/message/model.py
```python
"""消息主体定义"""
from __future__ import annotations
import asyncio
from typing import Any, Dict, List, Literal, Optional, Union
from pydantic import BaseModel, Field, field_serializer, field_validator
from gecko.core.logging import get_logger
from gecko.core.message.resources import MediaResource, ContentBlock

logger = get_logger(__name__)
Role = Literal["system", "user", "assistant", "tool"]

class Message(BaseModel):
    """
    标准消息对象
    
    兼容 OpenAI Chat Completion API 格式
    
    示例:
        ```python
        # 简单文本消息
        msg = Message.user("Hello!")
        
        # 多模态消息
        msg = Message.user(
            text="What's in this image?",
            images=["./photo.jpg"]
        )
        
        # 助手消息
        msg = Message.assistant("I'm here to help!")
        
        # 工具返回消息
        msg = Message.tool_result(
            tool_call_id="call_123",
            content="Search results: ...",
            tool_name="search"
        )
        
        # 从 OpenAI 格式解析
        msg = Message.from_openai({
            "role": "user",
            "content": "Hello"
        })
        ```
    """
    role: Role
    content: Union[str, List[ContentBlock]] = Field(default="")
    name: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    tool_call_id: Optional[str] = None

    @field_validator("content", mode="before")
    @classmethod
    def validate_content(cls, v):
        """验证并规范化 content"""
        if v is None:
            return ""
        return v

    @field_serializer("content")
    def serialize_content(self, content: Union[str, List[ContentBlock]], _info):
        """序列化 content 为 OpenAI 格式"""
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            return [block.to_openai_format() for block in content]
        return str(content)

    # ===== 工厂方法 =====

    @classmethod
    def user(
        cls,
        text: str = "",
        images: Optional[List[str]] = None,
        name: Optional[str] = None
    ) -> Message:
        """
        创建用户消息
        
        参数:
            text: 文本内容
            images: 图片路径列表（URL 或本地文件）
            name: 用户名称（可选）
        
        返回:
            Message 实例
        
        示例:
            ```python
            # 纯文本
            msg = Message.user("Hello")
            
            # 文本 + 图片
            msg = Message.user(
                text="What's this?",
                images=["./photo.jpg", "https://example.com/img.png"]
            )
            ```
        """
        if not images:
            return cls(role="user", content=text, name=name)
        
        # 构建多模态内容
        blocks: List[ContentBlock] = []
        
        # 添加文本块
        if text:
            blocks.append(ContentBlock(type="text", text=text))
        
        # 添加图片块
        for img in images:
            try:
                # 判断是 URL 还是本地路径
                if img.startswith(("http://", "https://", "data:")):
                    resource = MediaResource(url=img)
                else:
                    resource = MediaResource.from_file(img)
                
                blocks.append(ContentBlock(type="image_url", image_url=resource))
            except Exception as e:
                logger.error("Failed to load image", path=img, error=str(e))
                # 继续处理其他图片
        
        return cls(role="user", content=blocks, name=name)

    @classmethod
    async def user_async(
        cls,
        text: str = "",
        images: Optional[List[str]] = None,
        name: Optional[str] = None
    ) -> Message:
        """
        创建用户消息（异步版本）
        
        对于大量或大文件图片，使用此方法避免阻塞
        """
        if not images:
            return cls(role="user", content=text, name=name)
        
        blocks: List[ContentBlock] = []
        
        if text:
            blocks.append(ContentBlock(type="text", text=text))
        
        # 异步加载所有图片
        async def _load_image(img: str) -> Optional[ContentBlock]:
            try:
                if img.startswith(("http://", "https://", "data:")):
                    resource = MediaResource(url=img)
                else:
                    resource = await MediaResource.from_file_async(img)
                return ContentBlock(type="image_url", image_url=resource)
            except Exception as e:
                logger.error("Failed to load image (async)", path=img, error=str(e))
                return None
        
        # 并发加载所有图片
        image_blocks = await asyncio.gather(*[_load_image(img) for img in images])
        blocks.extend([b for b in image_blocks if b is not None])
        
        return cls(role="user", content=blocks, name=name)

    @classmethod
    def assistant(cls, content: str, name: Optional[str] = None) -> Message:
        """
        创建助手消息
        
        参数:
            content: 回复内容
            name: 助手名称（可选）
        """
        return cls(role="assistant", content=content, name=name)

    @classmethod
    def system(cls, content: str) -> Message:
        """
        创建系统消息
        
        参数:
            content: 系统提示词
        """
        return cls(role="system", content=content)

    @classmethod
    def tool_result(
        cls,
        tool_call_id: str,
        content: Any,
        tool_name: str
    ) -> Message:
        """
        创建工具返回消息
        
        参数:
            tool_call_id: 工具调用 ID
            content: 工具返回结果（任意类型，会自动序列化）
            tool_name: 工具名称
        
        返回:
            Message 实例
        """
        # 序列化 content
        if isinstance(content, str):
            serialized = content
        elif isinstance(content, (dict, list)):
            import json
            serialized = json.dumps(content, ensure_ascii=False, indent=2)
        else:
            serialized = str(content)
        
        return cls(
            role="tool",
            content=serialized,
            tool_call_id=tool_call_id,
            name=tool_name
        )

    @classmethod
    def from_openai(cls, payload: Dict[str, Any]) -> Message:
        """
        从 OpenAI API 格式解析消息
        
        参数:
            payload: OpenAI 格式的消息字典
        
        返回:
            Message 实例
        
        示例:
            ```python
            openai_msg = {
                "role": "assistant",
                "content": "Hello!",
                "tool_calls": [...]
            }
            msg = Message.from_openai(openai_msg)
            ```
        """
        try:
            return cls(**payload)
        except Exception as e:
            logger.error("Failed to parse OpenAI message", error=str(e), payload=payload)
            raise ValueError(f"无效的 OpenAI 消息格式: {e}") from e

    # ===== 转换方法 =====

    def to_openai_format(self) -> Dict[str, Any]:
        """
        转换为 OpenAI API 格式
        
        返回:
            符合 OpenAI 规范的字典
        """
        # 使用 Pydantic 的序列化（会调用 field_serializer）
        data = self.model_dump(exclude_none=True, mode="json")
        
        # 确保必要字段存在
        if "role" not in data:
            raise ValueError("消息缺少 role 字段")
        
        return data

    # ===== 工具方法 =====

    def get_text_content(self) -> str:
        """
        提取文本内容（忽略多模态部分）
        
        返回:
            纯文本内容
        
        用途:
            - 日志记录
            - 文本搜索
            - Token 估算
        """
        if isinstance(self.content, str):
            return self.content
        elif isinstance(self.content, list):
            text_parts = []
            for block in self.content:
                text = block.get_text_content()
                if text:
                    text_parts.append(text)
            return " ".join(text_parts)
        return ""

    def is_empty(self) -> bool:
        """
        检查消息是否为空
        
        返回:
            是否为空消息
        """
        if isinstance(self.content, str):
            return not self.content.strip()
        elif isinstance(self.content, list):
            return len(self.content) == 0
        return True

    def has_images(self) -> bool:
        """
        检查消息是否包含图片
        
        返回:
            是否包含图片
        """
        if isinstance(self.content, list):
            return any(block.type == "image_url" for block in self.content)
        return False

    def get_image_count(self) -> int:
        """
        获取图片数量
        
        返回:
            图片数量
        """
        if isinstance(self.content, list):
            return sum(1 for block in self.content if block.type == "image_url")
        return 0

    def clone(self) -> Message:
        """
        创建消息的深拷贝
        
        返回:
            新的 Message 实例
        """
        return Message.model_validate(self.model_dump())

    def truncate_content(self, max_length: int) -> Message:
        """
        截断消息内容（返回新消息）
        
        参数:
            max_length: 最大字符长度
        
        返回:
            截断后的新消息
        
        注意:
            仅截断文本内容，图片保持不变
        """
        if isinstance(self.content, str):
            if len(self.content) > max_length:
                truncated = self.content[:max_length] + "..."
                return Message(
                    role=self.role,
                    content=truncated,
                    name=self.name,
                    tool_calls=self.tool_calls,
                    tool_call_id=self.tool_call_id
                )
        
        # 多模态消息：截断文本块
        elif isinstance(self.content, list):
            new_blocks = []
            for block in self.content:
                if block.type == "text" and block.text:
                    if len(block.text) > max_length:
                        new_blocks.append(ContentBlock(
                            type="text",
                            text=block.text[:max_length] + "..."
                        ))
                    else:
                        new_blocks.append(block)
                else:
                    new_blocks.append(block)
            
            return Message(
                role=self.role,
                content=new_blocks,
                name=self.name,
                tool_calls=self.tool_calls,
                tool_call_id=self.tool_call_id
            )
        
        return self

    def __str__(self) -> str:
        """字符串表示（用于调试）"""
        text = self.get_text_content()
        preview = text[:50] + "..." if len(text) > 50 else text
        
        extra = []
        if self.has_images():
            extra.append(f"{self.get_image_count()} images")
        if self.tool_calls:
            extra.append(f"{len(self.tool_calls)} tool_calls")
        
        extra_str = f" ({', '.join(extra)})" if extra else ""
        
        return f"Message(role={self.role}, content='{preview}'{extra_str})"

    def __repr__(self) -> str:
        """详细表示"""
        return (
            f"Message("
            f"role={self.role!r}, "
            f"content={self.get_text_content()[:30]!r}, "
            f"has_images={self.has_images()}, "
            f"tool_calls={'Yes' if self.tool_calls else 'No'}"
            f")"
        )
```

[21] gecko/core/message/resources.py
```python
"""多模态资源定义"""
from __future__ import annotations
import asyncio
import base64
import mimetypes
from pathlib import Path
from typing import Any, Dict, Literal, Optional
from pydantic import BaseModel, model_validator
from gecko.core.logging import get_logger

logger = get_logger(__name__)

class MediaResource(BaseModel):
    """
    媒体资源（主要用于图片）
    
    支持：
    - URL（http/https）
    - Base64 编码的数据
    - 本地文件路径（通过工厂方法）
    
    示例:
        ```python
        # 从 URL
        img = MediaResource(url="https://example.com/image.jpg")
        
        # 从本地文件（同步）
        img = MediaResource.from_file("./image.png")
        
        # 从本地文件（异步）
        img = await MediaResource.from_file_async("./large_image.png")
        
        # 从 base64
        img = MediaResource(
            base64_data="iVBORw0KG...",
            mime_type="image/png"
        )
        ```
    """
    url: Optional[str] = None
    base64_data: Optional[str] = None
    mime_type: Optional[str] = None
    detail: Literal["auto", "low", "high"] = "auto"

    @model_validator(mode="after")
    def validate_source(self):
        """验证至少提供了一个数据源"""
        if not self.url and not self.base64_data:
            raise ValueError("必须提供 url 或 base64_data")
        return self

    @classmethod
    def from_file(
        cls,
        path: str,
        mime_type: Optional[str] = None,
        max_size_mb: int = 5,
        detail: Literal["auto", "low", "high"] = "auto"
    ) -> MediaResource:
        """
        从本地文件加载（同步版本）
        
        参数:
            path: 文件路径
            mime_type: MIME 类型（None 则自动推断）
            max_size_mb: 最大文件大小（MB）
            detail: 图片质量（OpenAI API 参数）
        
        返回:
            MediaResource 实例
        
        异常:
            FileNotFoundError: 文件不存在
            ValueError: 文件过大
        
        注意:
            这是同步方法，会阻塞事件循环。
            对于大文件，建议使用 from_file_async()
        """
        p = Path(path)
        
        # 检查文件是否存在
        if not p.exists():
            raise FileNotFoundError(f"文件不存在: {path}")
        
        if not p.is_file():
            raise ValueError(f"路径不是文件: {path}")
        
        # ✅ 优化：先检查文件大小，再读取
        file_size = p.stat().st_size
        max_size_bytes = max_size_mb * 1024 * 1024
        
        if file_size > max_size_bytes:
            raise ValueError(
                f"文件过大: {file_size / 1024 / 1024:.2f} MB "
                f"(最大 {max_size_mb} MB)"
            )
        
        # 读取并编码
        try:
            with open(p, "rb") as f:
                encoded = base64.b64encode(f.read()).decode("utf-8")
        except Exception as e:
            raise IOError(f"文件读取失败: {e}") from e
        
        # 推断 MIME 类型
        mime = mime_type or mimetypes.guess_type(p.name)[0] or "application/octet-stream"
        
        logger.debug(
            "Media loaded from file",
            path=path,
            size_kb=file_size / 1024,
            mime_type=mime
        )
        
        return cls(
            base64_data=encoded,
            mime_type=mime,
            detail=detail
        )

    @classmethod
    async def from_file_async(
        cls,
        path: str,
        mime_type: Optional[str] = None,
        max_size_mb: int = 5,
        detail: Literal["auto", "low", "high"] = "auto"
    ) -> MediaResource:
        """
        从本地文件加载（异步版本）
        
        对于大文件，使用此方法避免阻塞事件循环
        
        参数:
            同 from_file()
        
        返回:
            MediaResource 实例
        """
        p = Path(path)
        
        # 检查文件
        if not p.exists():
            raise FileNotFoundError(f"文件不存在: {path}")
        
        if not p.is_file():
            raise ValueError(f"路径不是文件: {path}")
        
        # 检查大小
        file_size = p.stat().st_size
        max_size_bytes = max_size_mb * 1024 * 1024
        
        if file_size > max_size_bytes:
            raise ValueError(
                f"文件过大: {file_size / 1024 / 1024:.2f} MB "
                f"(最大 {max_size_mb} MB)"
            )
        
        # ✅ 异步读取文件（在线程池中执行）
        def _read_file():
            with open(p, "rb") as f:
                return base64.b64encode(f.read()).decode("utf-8")
        
        try:
            encoded = await asyncio.to_thread(_read_file)
        except Exception as e:
            raise IOError(f"文件读取失败: {e}") from e
        
        # 推断 MIME 类型
        mime = mime_type or mimetypes.guess_type(p.name)[0] or "application/octet-stream"
        
        logger.debug(
            "Media loaded from file (async)",
            path=path,
            size_kb=file_size / 1024,
            mime_type=mime
        )
        
        return cls(
            base64_data=encoded,
            mime_type=mime,
            detail=detail
        )

    def to_openai_image_url(self) -> Dict[str, Any]:
        """
        转换为 OpenAI API 所需的 image_url 格式
        
        返回:
            符合 OpenAI 规范的字典
        """
        # 构建 URL
        if self.url:
            url_value = self.url
        elif self.base64_data:
            mime = self.mime_type or "image/jpeg"
            url_value = f"data:{mime};base64,{self.base64_data}"
        else:
            raise ValueError("MediaResource 缺少 URL 或 base64_data")
        
        return {
            "url": url_value,
            "detail": self.detail
        }

    def get_size_estimate(self) -> int:
        """
        估算数据大小（字节）
        
        返回:
            估算的字节数
        """
        if self.base64_data:
            # Base64 编码后的大小约为原始大小的 4/3
            return int(len(self.base64_data) * 3 / 4)
        elif self.url:
            # URL 无法估算实际大小
            return 0
        return 0


# ===== 内容块 =====

class ContentBlock(BaseModel):
    """
    消息内容块（用于多模态消息）
    
    支持：
    - 文本块
    - 图片块
    
    示例:
        ```python
        # 文本块
        text = ContentBlock(type="text", text="Hello")
        
        # 图片块
        image = ContentBlock(
            type="image_url",
            image_url=MediaResource(url="https://...")
        )
        ```
    """
    type: Literal["text", "image_url"]
    text: Optional[str] = None
    image_url: Optional[MediaResource] = None

    @model_validator(mode="after")
    def ensure_valid(self):
        """验证块的完整性"""
        if self.type == "text":
            if self.text is None:
                raise ValueError("文本块缺少 text 字段")
        elif self.type == "image_url":
            if self.image_url is None:
                raise ValueError("图片块缺少 image_url 字段")
        return self

    def to_openai_format(self) -> Dict[str, Any]:
        """转换为 OpenAI API 格式"""
        if self.type == "text":
            return {"type": "text", "text": self.text}
        elif self.type == "image_url":
            return {
                "type": "image_url",
                "image_url": self.image_url.to_openai_image_url() # type: ignore
            }
        else:
            raise ValueError(f"未知的内容类型: {self.type}")

    def get_text_content(self) -> str:
        """
        提取文本内容（用于调试/日志）
        
        返回:
            文本内容或占位符
        """
        if self.type == "text":
            return self.text or ""
        elif self.type == "image_url":
            return "[image]"
        return ""


# ===== 消息 =====

```

[22] gecko/core/output.py
```python
# gecko/core/output.py
"""
Agent 输出模型

定义 Agent 执行后的标准输出格式，包含：
- 最终回复内容
- 工具调用信息
- Token 使用统计
- 原始模型响应

优化点：
1. 结构化的 Usage 模型
2. 输出验证和后处理
3. 丰富的工具方法
4. 格式化输出
5. 统计信息提取
"""
from __future__ import annotations

from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, field_validator, model_validator

from gecko.core.logging import get_logger

logger = get_logger(__name__)


# ===== Token 使用统计 =====

class TokenUsage(BaseModel):
    """
    Token 使用统计
    
    符合 OpenAI API 的 usage 格式
    
    示例:
        ```python
        usage = TokenUsage(
            prompt_tokens=100,
            completion_tokens=50,
            total_tokens=150
        )
        ```
    """
    prompt_tokens: int = Field(
        default=0,
        ge=0,
        description="提示词（输入）消耗的 tokens"
    )
    completion_tokens: int = Field(
        default=0,
        ge=0,
        description="生成（输出）消耗的 tokens"
    )
    total_tokens: int = Field(
        default=0,
        ge=0,
        description="总消耗 tokens"
    )
    
    # 扩展字段（某些模型提供）
    prompt_tokens_details: Optional[Dict[str, int]] = Field(
        default=None,
        description="提示词 tokens 详细信息"
    )
    completion_tokens_details: Optional[Dict[str, int]] = Field(
        default=None,
        description="生成 tokens 详细信息"
    )

    @model_validator(mode="after")
    def validate_total(self):
        """验证总 tokens 是否正确"""
        calculated_total = self.prompt_tokens + self.completion_tokens
        
        # 如果 total_tokens 为 0，自动计算
        if self.total_tokens == 0:
            self.total_tokens = calculated_total
        
        # 如果不一致，记录警告
        elif self.total_tokens != calculated_total:
            logger.warning(
                "Token usage total mismatch",
                total=self.total_tokens,
                calculated=calculated_total
            )
        
        return self

    def get_cost_estimate(
        self,
        prompt_price_per_1k: float = 0.0,
        completion_price_per_1k: float = 0.0
    ) -> float:
        """
        估算成本（美元）
        
        参数:
            prompt_price_per_1k: 输入 token 每 1000 个的价格
            completion_price_per_1k: 输出 token 每 1000 个的价格
        
        返回:
            估算成本（美元）
        
        示例:
            ```python
            # GPT-4 价格（示例）
            cost = usage.get_cost_estimate(
                prompt_price_per_1k=0.03,      # $0.03/1K tokens
                completion_price_per_1k=0.06   # $0.06/1K tokens
            )
            print(f"Estimated cost: ${cost:.4f}")
            ```
        """
        prompt_cost = (self.prompt_tokens / 1000) * prompt_price_per_1k
        completion_cost = (self.completion_tokens / 1000) * completion_price_per_1k
        return prompt_cost + completion_cost

    def __str__(self) -> str:
        """简洁的字符串表示"""
        return (
            f"TokenUsage("
            f"prompt={self.prompt_tokens}, "
            f"completion={self.completion_tokens}, "
            f"total={self.total_tokens}"
            f")"
        )


# ===== Agent 输出 =====

class AgentOutput(BaseModel):
    """
    Agent 执行结果
    
    包含 Agent 执行后的完整输出信息。
    
    属性:
        content: 最终文本回复（可能为空，如果只有工具调用）
        tool_calls: 工具调用列表
        usage: Token 使用统计（可选）
        raw: 原始模型响应（用于调试）
        metadata: 附加元数据
    
    示例:
        ```python
        # 简单文本输出
        output = AgentOutput(content="Hello, how can I help?")
        
        # 带工具调用的输出
        output = AgentOutput(
            content="I'll search for that information.",
            tool_calls=[
                {
                    "id": "call_1",
                    "function": {
                        "name": "search",
                        "arguments": '{"query": "AI"}'
                    }
                }
            ],
            usage=TokenUsage(
                prompt_tokens=100,
                completion_tokens=50
            )
        )
        
        # 检查输出
        if output.has_tool_calls():
            print(f"需要执行 {output.tool_call_count()} 个工具")
        
        if output.has_content():
            print(f"回复: {output.content}")
        ```
    """
    content: str = Field(
        default="",
        description="最终文本回复"
    )
    tool_calls: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="工具调用列表（OpenAI 格式）"
    )
    usage: Optional[TokenUsage] = Field(
        default=None,
        description="Token 使用统计"
    )
    raw: Any = Field(
        default=None,
        description="原始模型响应（用于调试）"
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="附加元数据"
    )

    model_config = {"arbitrary_types_allowed": True}

    @field_validator("tool_calls", mode="before")
    @classmethod
    def ensure_tool_calls(cls, value):
        """确保 tool_calls 始终是列表"""
        if value is None:
            return []
        if not isinstance(value, list):
            logger.warning("tool_calls should be a list", type=type(value).__name__)
            return []
        return value

    @field_validator("content", mode="before")
    @classmethod
    def ensure_content(cls, value):
        """确保 content 是字符串"""
        if value is None:
            return ""
        if not isinstance(value, str):
            return str(value)
        return value

    # ===== 检查方法 =====

    def has_content(self) -> bool:
        """
        检查是否有文本内容
        
        返回:
            是否有非空文本
        """
        return bool(self.content and self.content.strip())

    def has_tool_calls(self) -> bool:
        """
        检查是否有工具调用
        
        返回:
            是否包含工具调用
        """
        return len(self.tool_calls) > 0

    def tool_call_count(self) -> int:
        """
        获取工具调用数量
        
        返回:
            工具调用的数量
        """
        return len(self.tool_calls)

    def is_empty(self) -> bool:
        """
        检查输出是否完全为空
        
        返回:
            是否既无内容也无工具调用
        """
        return not self.has_content() and not self.has_tool_calls()

    def has_usage(self) -> bool:
        """
        检查是否有 usage 信息
        
        返回:
            是否包含 token 使用统计
        """
        return self.usage is not None

    # ===== 提取方法 =====

    def get_tool_names(self) -> List[str]:
        """
        提取所有被调用的工具名称
        
        返回:
            工具名称列表
        
        示例:
            ```python
            output = AgentOutput(tool_calls=[...])
            tools = output.get_tool_names()
            print(f"调用的工具: {', '.join(tools)}")
            ```
        """
        names = []
        for call in self.tool_calls:
            func = call.get("function", {})
            name = func.get("name")
            if name:
                names.append(name)
        return names

    def get_tool_call_by_id(self, call_id: str) -> Optional[Dict[str, Any]]:
        """
        根据 ID 获取工具调用
        
        参数:
            call_id: 工具调用 ID
        
        返回:
            工具调用字典，如果不存在返回 None
        """
        for call in self.tool_calls:
            if call.get("id") == call_id:
                return call
        return None

    def get_text_preview(self, max_length: int = 100) -> str:
        """
        获取内容预览（用于日志/显示）
        
        参数:
            max_length: 最大长度
        
        返回:
            截断后的文本预览
        """
        if not self.content:
            return ""
        
        if len(self.content) <= max_length:
            return self.content
        
        return self.content[:max_length] + "..."

    # ===== 转换方法 =====

    def to_dict(self) -> Dict[str, Any]:
        """
        转换为字典（便于序列化）
        
        返回:
            包含所有字段的字典
        """
        data = {
            "content": self.content,
            "tool_calls": self.tool_calls,
            "metadata": self.metadata,
        }
        
        if self.usage:
            data["usage"] = self.usage.model_dump()
        
        # raw 字段可能无法序列化，仅在调试模式下包含
        if self.raw and self.metadata.get("include_raw"):
            try:
                data["raw"] = str(self.raw)
            except Exception:
                data["raw"] = "<non-serializable>"
        
        return data

    def to_message_dict(self) -> Dict[str, Any]:
        """
        转换为 OpenAI 消息格式（用于下一轮对话）
        
        返回:
            符合 OpenAI API 的消息字典
        
        示例:
            ```python
            output = AgentOutput(content="Hello", tool_calls=[...])
            msg_dict = output.to_message_dict()
            # 可以直接添加到对话历史
            ```
        """
        msg = {
            "role": "assistant",
            "content": self.content or None,  # OpenAI 允许 null
        }
        
        if self.tool_calls:
            msg["tool_calls"] = self.tool_calls
        
        return msg

    # ===== 格式化输出 =====

    def format(self, include_metadata: bool = False) -> str:
        """
        格式化输出为可读文本
        
        参数:
            include_metadata: 是否包含元数据
        
        返回:
            格式化后的字符串
        
        示例:
            ```python
            output = AgentOutput(...)
            print(output.format())
            ```
        """
        lines = []
        
        # 内容
        if self.has_content():
            lines.append("=== 回复内容 ===")
            lines.append(self.content)
            lines.append("")
        
        # 工具调用
        if self.has_tool_calls():
            lines.append("=== 工具调用 ===")
            for i, call in enumerate(self.tool_calls, 1):
                func = call.get("function", {})
                name = func.get("name", "unknown")
                args = func.get("arguments", "{}")
                lines.append(f"{i}. {name}")
                lines.append(f"   参数: {args}")
            lines.append("")
        
        # Token 使用
        if self.usage:
            lines.append("=== Token 使用 ===")
            lines.append(f"输入: {self.usage.prompt_tokens}")
            lines.append(f"输出: {self.usage.completion_tokens}")
            lines.append(f"总计: {self.usage.total_tokens}")
            lines.append("")
        
        # 元数据
        if include_metadata and self.metadata:
            lines.append("=== 元数据 ===")
            for key, value in self.metadata.items():
                lines.append(f"{key}: {value}")
            lines.append("")
        
        return "\n".join(lines)

    def summary(self) -> str:
        """
        生成简短摘要
        
        返回:
            一行摘要文本
        
        示例:
            ```python
            output = AgentOutput(...)
            print(output.summary())
            # 输出: "回复: Hello... (50 chars) | 工具调用: 2 | Tokens: 150"
            ```
        """
        parts = []
        
        if self.has_content():
            preview = self.get_text_preview(30)
            parts.append(f"回复: {preview}")
        
        if self.has_tool_calls():
            parts.append(f"工具调用: {self.tool_call_count()}")
        
        if self.usage:
            parts.append(f"Tokens: {self.usage.total_tokens}")
        
        if not parts:
            return "空输出"
        
        return " | ".join(parts)

    # ===== 统计方法 =====

    def get_stats(self) -> Dict[str, Any]:
        """
        获取输出统计信息
        
        返回:
            包含各种统计数据的字典
        """
        stats = {
            "content_length": len(self.content),
            "has_content": self.has_content(),
            "tool_call_count": self.tool_call_count(),
            "tool_names": self.get_tool_names(),
            "is_empty": self.is_empty(),
        }
        
        if self.usage:
            stats["usage"] = {
                "prompt_tokens": self.usage.prompt_tokens,
                "completion_tokens": self.usage.completion_tokens,
                "total_tokens": self.usage.total_tokens,
            }
        
        return stats

    # ===== 字符串表示 =====

    def __str__(self) -> str:
        """简洁的字符串表示"""
        return self.summary()

    def __repr__(self) -> str:
        """详细的字符串表示"""
        return (
            f"AgentOutput("
            f"content_length={len(self.content)}, "
            f"tool_calls={self.tool_call_count()}, "
            f"has_usage={self.has_usage()}"
            f")"
        )

    def __bool__(self) -> bool:
        """
        布尔值转换（是否有有效输出）
        
        返回:
            是否不为空
        """
        return not self.is_empty()


# ===== 工具函数 =====

def create_text_output(
    content: str,
    usage: Optional[TokenUsage] = None,
    **metadata
) -> AgentOutput:
    """
    快速创建纯文本输出
    
    参数:
        content: 文本内容
        usage: Token 使用统计（可选）
        **metadata: 附加元数据
    
    返回:
        AgentOutput 实例
    
    示例:
        ```python
        output = create_text_output(
            "Hello, world!",
            usage=TokenUsage(prompt_tokens=10, completion_tokens=5)
        )
        ```
    """
    return AgentOutput(
        content=content,
        usage=usage,
        metadata=metadata
    )


def create_tool_output(
    tool_calls: List[Dict[str, Any]],
    content: str = "",
    usage: Optional[TokenUsage] = None,
    **metadata
) -> AgentOutput:
    """
    快速创建工具调用输出
    
    参数:
        tool_calls: 工具调用列表
        content: 可选的文本内容
        usage: Token 使用统计（可选）
        **metadata: 附加元数据
    
    返回:
        AgentOutput 实例
    """
    return AgentOutput(
        content=content,
        tool_calls=tool_calls,
        usage=usage,
        metadata=metadata
    )


def merge_outputs(outputs: List[AgentOutput]) -> AgentOutput:
    """
    合并多个输出（用于多 Agent 场景）
    
    参数:
        outputs: AgentOutput 列表
    
    返回:
        合并后的 AgentOutput
    
    策略:
        - 内容：用换行符连接
        - 工具调用：合并所有
        - Usage：累加 tokens
        - 元数据：合并（后者覆盖前者）
    
    示例:
        ```python
        output1 = AgentOutput(content="Part 1")
        output2 = AgentOutput(content="Part 2")
        merged = merge_outputs([output1, output2])
        print(merged.content)  # "Part 1\nPart 2"
        ```
    """
    if not outputs:
        return AgentOutput()
    
    if len(outputs) == 1:
        return outputs[0]
    
    # 合并内容
    contents = [o.content for o in outputs if o.has_content()]
    merged_content = "\n".join(contents)
    
    # 合并工具调用
    merged_tool_calls = []
    for output in outputs:
        merged_tool_calls.extend(output.tool_calls)
    
    # 合并 usage
    merged_usage = None
    if any(o.has_usage() for o in outputs):
        total_prompt = sum(
            o.usage.prompt_tokens for o in outputs if o.usage
        )
        total_completion = sum(
            o.usage.completion_tokens for o in outputs if o.usage
        )
        merged_usage = TokenUsage(
            prompt_tokens=total_prompt,
            completion_tokens=total_completion,
            total_tokens=total_prompt + total_completion
        )
    
    # 合并元数据
    merged_metadata = {}
    for output in outputs:
        merged_metadata.update(output.metadata)
    
    return AgentOutput(
        content=merged_content,
        tool_calls=merged_tool_calls,
        usage=merged_usage,
        metadata=merged_metadata
    )
```

[23] gecko/core/prompt.py
```python
# gecko/core/prompt.py
"""
Prompt 模板系统

提供灵活的提示词模板管理，基于 Jinja2 实现。

核心功能：
1. 动态变量替换
2. 模板验证
3. 模板缓存
4. 常用模板库
5. 模板组合

优化点：
1. 更好的错误处理
2. 模板缓存提升性能
3. 预定义模板库
4. 模板组合和继承
5. 安全的沙箱环境
"""
from __future__ import annotations

import re
from typing import Any, Dict, List, Optional, Set

from pydantic import BaseModel, Field, field_validator

from gecko.core.logging import get_logger

logger = get_logger(__name__)

# ===== Jinja2 相关 =====

# 延迟导入 Jinja2（避免强依赖）
_jinja2_available = None
_jinja2_env = None


def _check_jinja2():
    """检查 Jinja2 是否可用"""
    global _jinja2_available
    if _jinja2_available is None:
        try:
            import jinja2
            _jinja2_available = True
        except ImportError:
            _jinja2_available = False
    return _jinja2_available


def _get_jinja2_env():
    """获取 Jinja2 环境（带缓存）"""
    global _jinja2_env
    
    if _jinja2_env is None:
        if not _check_jinja2():
            raise ImportError(
                "PromptTemplate 依赖 jinja2。\n"
                "请安装：pip install jinja2\n"
                "或：rye add jinja2"
            )
        
        from jinja2 import Environment, StrictUndefined
        
        # 创建安全的 Jinja2 环境
        _jinja2_env = Environment(
            # 严格模式：未定义变量会报错
            undefined=StrictUndefined,
            # ✅ 修复1：禁用自动转义（直接设置为 False）
            autoescape=False,
            # 保留换行符
            keep_trailing_newline=True,
            # 启用扩展
            extensions=[]
        )
        
        logger.debug("Jinja2 environment initialized")
    
    return _jinja2_env


# ===== Prompt 模板 =====

class PromptTemplate(BaseModel):
    """
    Prompt 模板
    
    使用 Jinja2 语法，支持动态变量替换、条件判断、循环等。
    
    示例:
        ```python
        # 基础模板
        template = PromptTemplate(
            template="Hello, {{ name }}! You are {{ age }} years old.",
            input_variables=["name", "age"]
        )
        result = template.format(name="Alice", age=25)
        
        # 带条件的模板
        template = PromptTemplate(
            template='''
            {% if tools %}
            You have access to these tools:
            {% for tool in tools %}
            - {{ tool.name }}: {{ tool.description }}
            {% endfor %}
            {% endif %}
            
            User: {{ question }}
            ''',
            input_variables=["tools", "question"]
        )
        
        # 从文件加载
        template = PromptTemplate.from_file("./prompts/system.txt")
        ```
    
    属性:
        template: 模板字符串（Jinja2 语法）
        input_variables: 必需的变量列表
        template_format: 模板格式（默认 'jinja2'）
        validate_template: 是否验证模板语法（默认 True）
    """
    template: str = Field(..., description="模板字符串")
    input_variables: List[str] = Field(
        default_factory=list,
        description="必需的输入变量列表"
    )
    template_format: str = Field(
        default="jinja2",
        description="模板格式（jinja2/f-string）"
    )
    validate_template: bool = Field(
        default=True,
        description="是否验证模板语法"
    )
    
    # 私有字段：缓存编译后的模板
    _compiled_template: Any = None

    @field_validator("template_format")
    @classmethod
    def validate_format(cls, v: str) -> str:
        """验证模板格式"""
        valid_formats = {"jinja2", "f-string"}
        if v not in valid_formats:
            raise ValueError(
                f"不支持的模板格式: {v}。支持的格式: {valid_formats}"
            )
        return v

    def model_post_init(self, __context):
        """初始化后验证"""
        if self.validate_template:
            self._validate_template_syntax()

    def _validate_template_syntax(self):
        """验证模板语法"""
        if self.template_format == "jinja2":
            try:
                env = _get_jinja2_env()
                # 尝试编译模板
                self._compiled_template = env.from_string(self.template)
                logger.debug("Template syntax validated")
            except Exception as e:
                error_msg = self._format_jinja2_error(str(e))
                raise ValueError(f"模板语法错误:\n{error_msg}") from e
        elif self.template_format == "f-string":
            # f-string 格式验证（基础检查）
            self._validate_fstring_syntax()

    def _validate_fstring_syntax(self):
        """验证 f-string 语法（基础）"""
        # 检查是否有未闭合的大括号
        open_count = self.template.count("{")
        close_count = self.template.count("}")
        
        if open_count != close_count:
            raise ValueError(
                f"f-string 语法错误: 大括号不匹配 "
                f"({{ {open_count} 个, }} {close_count} 个)"
            )

    def _format_jinja2_error(self, error: str) -> str:
        """格式化 Jinja2 错误信息"""
        # 提取关键信息
        lines = error.split("\n")
        formatted_lines = []
        
        for line in lines[:5]:  # 只取前 5 行
            if line.strip():
                formatted_lines.append(f"  {line}")
        
        # 添加模板片段
        template_preview = self.template[:100].replace("\n", "\\n")
        formatted_lines.append(f"\n模板片段: {template_preview}...")
        
        return "\n".join(formatted_lines)

    # ===== 格式化方法 =====

    def format(self, **kwargs: Any) -> str:
        """
        格式化模板（填充变量）
        
        参数:
            **kwargs: 模板变量
        
        返回:
            格式化后的字符串
        
        异常:
            ValueError: 缺少必需变量或渲染失败
        
        示例:
            ```python
            template = PromptTemplate(
                template="Hello, {{ name }}!",
                input_variables=["name"]
            )
            result = template.format(name="Alice")
            ```
        """
        # 检查必需变量
        missing = self._check_missing_variables(kwargs)
        if missing:
            raise ValueError(
                f"缺少必需的模板变量: {', '.join(missing)}\n"
                f"需要: {self.input_variables}\n"
                f"提供: {list(kwargs.keys())}"
            )
        
        # 根据格式渲染
        if self.template_format == "jinja2":
            return self._format_jinja2(**kwargs)
        elif self.template_format == "f-string":
            return self._format_fstring(**kwargs)
        else:
            raise ValueError(f"不支持的模板格式: {self.template_format}")

    def _check_missing_variables(self, kwargs: Dict[str, Any]) -> List[str]:
        """检查缺失的变量"""
        provided = set(kwargs.keys())
        required = set(self.input_variables)
        missing = required - provided
        return sorted(missing)

    def _format_jinja2(self, **kwargs: Any) -> str:
        """使用 Jinja2 渲染"""
        try:
            # 使用缓存的编译模板（如果有）
            if self._compiled_template is None:
                env = _get_jinja2_env()
                self._compiled_template = env.from_string(self.template)
            
            result = self._compiled_template.render(**kwargs)
            return result
            
        except Exception as e:
            # 提供更友好的错误信息
            error_msg = str(e)
            
            # 尝试识别具体错误
            if "is undefined" in error_msg:
                # 提取未定义的变量名
                match = re.search(r"'(\w+)' is undefined", error_msg)
                if match:
                    var_name = match.group(1)
                    raise ValueError(
                        f"模板变量 '{var_name}' 未定义。\n"
                        f"可用变量: {list(kwargs.keys())}"
                    ) from e
            
            # 通用错误
            raise ValueError(
                f"模板渲染失败: {error_msg}\n"
                f"模板: {self.template[:100]}..."
            ) from e

    def _format_fstring(self, **kwargs: Any) -> str:
        """使用 f-string 格式化"""
        try:
            return self.template.format(**kwargs)
        except KeyError as e:
            raise ValueError(
                f"缺少变量: {e}\n"
                f"可用变量: {list(kwargs.keys())}"
            ) from e
        except Exception as e:
            raise ValueError(f"f-string 格式化失败: {e}") from e

    def format_safe(self, **kwargs: Any) -> str:
        """
        安全格式化（缺少变量时使用默认值）
        
        缺少的变量会被替换为 "<MISSING: var_name>"
        
        返回:
            格式化后的字符串
        
        注意:
            此方法会自动提取模板中的所有变量，
            不仅仅是 input_variables 中声明的变量。
        """
        try:
            all_vars = self.get_variables_from_template()
        except Exception as e:
            logger.warning("Failed to extract variables", error=str(e))
            all_vars = set(self.input_variables)
        
        safe_kwargs = dict(kwargs)
        for var in all_vars:
            if var not in safe_kwargs:
                # 智能推测默认值
                if var in ('history', 'messages', 'items', 'tools', 'examples'):
                    safe_kwargs[var] = []
                elif var in ('system', 'context', 'prefix', 'suffix'):
                    safe_kwargs[var] = None
                else:
                    safe_kwargs[var] = f"<MISSING: {var}>"
        
        try:
            if self.template_format == "jinja2":
                return self._format_jinja2(**safe_kwargs)
            elif self.template_format == "f-string":
                return self._format_fstring(**safe_kwargs)
            else:
                return f"<TEMPLATE ERROR: 不支持的格式>"
        except Exception as e:
            logger.error("Safe format failed", error=str(e))
            return f"<TEMPLATE ERROR: {e}>"

    # ===== 变量提取 =====

    def get_variables_from_template(self) -> Set[str]:
        """
        从模板中提取所有变量
        
        返回:
            变量名集合
        
        示例:
            ```python
            template = PromptTemplate(template="Hello {{ name }}, you are {{ age }}")
            vars = template.get_variables_from_template()
            # {'name', 'age'}
            ```
        """
        if self.template_format == "jinja2":
            return self._extract_jinja2_variables()
        elif self.template_format == "f-string":
            return self._extract_fstring_variables()
        return set()

    def _extract_jinja2_variables(self) -> Set[str]:
        """从 Jinja2 模板中提取变量"""
        try:
            env = _get_jinja2_env()
            from jinja2 import meta
            
            ast = env.parse(self.template)
            variables = meta.find_undeclared_variables(ast)
            return variables
        except Exception as e:
            logger.warning("Failed to extract Jinja2 variables", error=str(e))
            return set()

    def _extract_fstring_variables(self) -> Set[str]:
        """从 f-string 模板中提取变量"""
        # 简单正则匹配 {var_name}
        pattern = r'\{(\w+)\}'
        matches = re.findall(pattern, self.template)
        return set(matches)

    # ===== 模板操作 =====

    def partial(self, **kwargs: Any) -> "PromptTemplate":
        """
        部分填充变量（返回新模板）
        
        参数:
            **kwargs: 要填充的变量
        
        返回:
            新的 PromptTemplate，已填充部分变量
        
        示例:
            ```python
            template = PromptTemplate(
                template="Hello {{ name }}, you are {{ age }}",
                input_variables=["name", "age"]
            )
            partial = template.partial(name="Alice")
            result = partial.format(age=25)
            ```
        """
        # 填充变量
        partial_result = self.format_safe(**kwargs)
        
        # 计算剩余变量
        remaining_vars = [v for v in self.input_variables if v not in kwargs]
        
        return PromptTemplate(
            template=partial_result,
            input_variables=remaining_vars,
            template_format=self.template_format,
            validate_template=False  # 已经验证过了
        )

    def clone(self) -> "PromptTemplate":
        """
        克隆模板
        
        返回:
            新的 PromptTemplate 实例
        """
        return PromptTemplate(
            template=self.template,
            input_variables=self.input_variables.copy(),
            template_format=self.template_format,
            validate_template=False
        )

    # ===== 工厂方法 =====

    @classmethod
    def from_file(
        cls,
        path: str,
        input_variables: Optional[List[str]] = None,
        encoding: str = "utf-8"
    ) -> "PromptTemplate":
        """
        从文件加载模板
        
        参数:
            path: 文件路径
            input_variables: 变量列表（None 则自动提取）
            encoding: 文件编码
        
        返回:
            PromptTemplate 实例
        
        示例:
            ```python
            template = PromptTemplate.from_file("./prompts/system.txt")
            ```
        """
        from pathlib import Path
        
        file_path = Path(path)
        if not file_path.exists():
            raise FileNotFoundError(f"模板文件不存在: {path}")
        
        try:
            with open(file_path, "r", encoding=encoding) as f:
                template_str = f.read()
        except Exception as e:
            raise IOError(f"读取模板文件失败: {e}") from e
        
        # 创建模板
        prompt = cls(
            template=template_str,
            input_variables=input_variables or []
        )
        
        # 如果未提供变量，自动提取
        if not input_variables:
            detected_vars = prompt.get_variables_from_template()
            prompt.input_variables = sorted(detected_vars)
            logger.info(
                "Auto-detected template variables",
                path=path,
                variables=prompt.input_variables
            )
        
        return prompt

    @classmethod
    def from_examples(
        cls,
        examples: List[Dict[str, str]],
        template: str = "{{ input }}\n{{ output }}\n",
        separator: str = "\n---\n"
    ) -> "PromptTemplate":
        """
        从示例列表创建 few-shot 模板
        
        参数:
            examples: 示例列表 [{"input": "...", "output": "..."}, ...]
            template: 单个示例的模板
            separator: 示例之间的分隔符
        
        返回:
            PromptTemplate 实例
        
        示例:
            ```python
            examples = [
                {"input": "2+2", "output": "4"},
                {"input": "3+5", "output": "8"},
            ]
            template = PromptTemplate.from_examples(examples)
            ```
        """
        # 渲染所有示例
        example_template = cls(template=template, input_variables=[])
        
        rendered_examples = []
        for ex in examples:
            rendered = example_template.format_safe(**ex)
            rendered_examples.append(rendered)
        
        # 合并为完整模板
        full_template = separator.join(rendered_examples)
        
        return cls(
            template=full_template,
            input_variables=[]
        )

    # ===== 字符串表示 =====

    def __str__(self) -> str:
        """简洁表示"""
        preview = self.template[:50].replace("\n", "\\n")
        if len(self.template) > 50:
            preview += "..."
        return f"PromptTemplate('{preview}', vars={self.input_variables})"

    def __repr__(self) -> str:
        """详细表示"""
        return (
            f"PromptTemplate("
            f"template_length={len(self.template)}, "
            f"input_variables={self.input_variables}, "
            f"format={self.template_format}"
            f")"
        )


# ===== 预定义模板库 =====

class PromptLibrary:
    """
    常用 Prompt 模板库
    
    提供预定义的常用模板。
    
    示例:
        ```python
        # 使用预定义模板
        template = PromptLibrary.get_react_prompt()
        prompt = template.format(
            tools=[...],
            question="What is AI?"
        )
        ```
    """
    
    @staticmethod
    def get_react_prompt() -> PromptTemplate:
        """
        获取 ReAct 推理模板
        
        返回:
            ReAct 格式的 PromptTemplate
        """
        template = """You are a helpful AI assistant with access to tools.

{% if tools %}
Available Tools:
{% for tool in tools %}
- {{ tool.name }}: {{ tool.description }}
{% endfor %}
{% endif %}

To use a tool, respond with a tool call in the following format:
Action: tool_name
Action Input: {"param": "value"}

Then wait for the observation before continuing.

Question: {{ question }}

Let's think step by step."""
        
        return PromptTemplate(
            template=template,
            input_variables=["tools", "question"]
        )
    
    @staticmethod
    def get_chat_prompt() -> PromptTemplate:
        """
        获取对话模板
        
        返回:
            对话格式的 PromptTemplate
        """
        template = """{% if system %}{{ system }}

{% endif %}{% for message in history %}{{ message.role }}: {{ message.content }}
{% endfor %}User: {{ user_input }}
Assistant:"""
        
        return PromptTemplate(
            template=template,
            input_variables=["user_input"],
            # system 和 history 是可选的
        )
    
    @staticmethod
    def get_summarization_prompt() -> PromptTemplate:
        """
        获取摘要模板
        
        返回:
            摘要格式的 PromptTemplate
        """
        template = """Please summarize the following text in {{ max_words }} words or less:

{{ text }}

Summary:"""
        
        return PromptTemplate(
            template=template,
            input_variables=["text", "max_words"]
        )
    
    @staticmethod
    def get_extraction_prompt() -> PromptTemplate:
        """
        获取信息提取模板
        
        返回:
            信息提取格式的 PromptTemplate
        """
        template = """Extract the following information from the text:

{% for field in fields %}
- {{ field }}
{% endfor %}

Text: {{ text }}

Respond in JSON format."""
        
        return PromptTemplate(
            template=template,
            input_variables=["fields", "text"]
        )
    
    @staticmethod
    def get_translation_prompt() -> PromptTemplate:
        """
        获取翻译模板
        
        返回:
            翻译格式的 PromptTemplate
        """
        template = """Translate the following text from {{ source_lang }} to {{ target_lang }}:

{{ text }}

Translation:"""
        
        return PromptTemplate(
            template=template,
            input_variables=["source_lang", "target_lang", "text"]
        )


# ===== 原有的默认模板（保持兼容性）=====

DEFAULT_REACT_PROMPT = PromptTemplate(
    template="""You are a helpful AI assistant.
Current time: {{ current_time }}

{% if tools %}
You have access to the following tools:
{% for tool in tools %}
- {{ tool.name }}: {{ tool.description }}
{% endfor %}
{% endif %}

Answer the user's question using the tools if necessary.
""",
    input_variables=["current_time", "tools"],
)
```

[24] gecko/core/protocols/__init__.py
```python
"""
协议模块入口
重新导出所有子模块内容，确保外部引用 gecko.core.protocols.ModelProtocol 依然有效。
"""
from gecko.core.protocols.base import check_protocol, get_missing_methods
from gecko.core.protocols.model import (
    ModelProtocol, StreamableModelProtocol, 
    CompletionResponse, CompletionChoice, CompletionUsage, StreamChunk,
    supports_streaming, supports_function_calling, supports_vision,
    get_model_name, validate_model
)
from gecko.core.protocols.storage import StorageProtocol, validate_storage
from gecko.core.protocols.tool import ToolProtocol, validate_tool
from gecko.core.protocols.embedder import EmbedderProtocol
from gecko.core.protocols.runnable import RunnableProtocol, StreamableRunnableProtocol
from gecko.core.protocols.vector import VectorStoreProtocol

# 类型别名 (保持兼容)
from typing import Dict, List, Any
ModelResponse = CompletionResponse
MessageDict = Dict[str, Any]
MessageList = List[MessageDict]
ToolCall = Dict[str, Any]
ToolCallList = List[ToolCall]
StorageValue = Dict[str, Any]
Vector = List[float]
VectorList = List[Vector]

__all__ = [
    "ModelProtocol", "StreamableModelProtocol",
    "CompletionResponse", "CompletionChoice", "CompletionUsage", "StreamChunk",
    "StorageProtocol", "ToolProtocol", "EmbedderProtocol",
    "RunnableProtocol", "StreamableRunnableProtocol", "VectorStoreProtocol",
    "check_protocol", "validate_model", "validate_storage", "validate_tool",
    "supports_streaming", "supports_function_calling", "supports_vision",
    "get_model_name", "get_missing_methods",
    "ModelResponse", "MessageDict", "ToolCall", "Vector"
]
```

[25] gecko/core/protocols/base.py
```python
"""
协议基础工具
提供运行时检查和验证辅助函数。
"""
from __future__ import annotations
import inspect as insp
from typing import Any, List

def check_protocol(obj: Any, protocol: type) -> bool:
    """
    检查对象是否实现了指定协议
    """
    return isinstance(obj, protocol)

def get_missing_methods(obj: Any, protocol: type) -> List[str]:
    """
    获取对象未实现的协议方法
    """
    missing = []
    # 获取协议的所有成员
    for name, value in insp.getmembers(protocol):
        if name.startswith("_"): continue
        
        if insp.isfunction(value) or insp.ismethod(value):
            if not hasattr(obj, name):
                missing.append(name)
            elif not callable(getattr(obj, name)):
                missing.append(name)
        elif insp.isdatadescriptor(value):
            if not hasattr(obj, name):
                missing.append(name)
    return missing
```

[26] gecko/core/protocols/embedder.py
```python
"""嵌入模型协议"""
from __future__ import annotations
from typing import List, Protocol, runtime_checkable

@runtime_checkable
class EmbedderProtocol(Protocol):
    async def embed(self, texts: List[str]) -> List[List[float]]: ...
    def get_dimension(self) -> int: ...
```

[27] gecko/core/protocols/model.py
```python
"""
LLM 模型相关协议与数据结构
"""
from __future__ import annotations
from typing import Any, Dict, List, Optional, Protocol, AsyncIterator, runtime_checkable
from pydantic import BaseModel, Field
from gecko.core.protocols.base import check_protocol, get_missing_methods

# ====================== 模型响应格式 ======================

class CompletionChoice(BaseModel):
    """单个补全选择"""
    index: int = 0
    message: Dict[str, Any]
    finish_reason: Optional[str] = None
    logprobs: Optional[Dict[str, Any]] = None

class CompletionUsage(BaseModel):
    """Token 使用统计"""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class CompletionResponse(BaseModel):
    """标准的模型补全响应格式"""
    id: str = Field(default="", description="响应 ID")
    object: str = Field(default="chat.completion", description="对象类型")
    created: int = Field(default=0, description="创建时间戳")
    model: str = Field(default="", description="模型名称")
    choices: List[CompletionChoice] = Field(default_factory=list)
    usage: Optional[CompletionUsage] = Field(default=None)
    system_fingerprint: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)

class StreamChunk(BaseModel):
    """流式响应的单个数据块"""
    id: str = ""
    object: str = "chat.completion.chunk"
    created: int = 0
    model: str = ""
    choices: List[Dict[str, Any]] = Field(default_factory=list)
    
    @property
    def delta(self) -> Dict[str, Any]:
        if self.choices:
            return self.choices[0].get("delta", {})
        return {}
    
    @property
    def content(self) -> Optional[str]:
        return self.delta.get("content")

# ====================== 模型协议 ======================

@runtime_checkable
class ModelProtocol(Protocol):
    """LLM 模型核心协议"""
    async def acompletion(self, messages: List[Dict[str, Any]], **kwargs) -> CompletionResponse:
        ...

    # [新增] 同步 Token 计数接口
    # 允许上层模块（如 Memory）在不阻塞 Event Loop 的前提下获取 Token 数
    def count_tokens(self, text_or_messages: str | List[Dict[str, Any]]) -> int:
        """
        计算输入内容的 Token 数量。
        应尽量使用本地 Tokenizer (如 tiktoken) 以保证性能。
        """
        ...

@runtime_checkable
class StreamableModelProtocol(ModelProtocol, Protocol):
    """支持流式输出的模型协议"""
    async def astream(self, messages: List[Dict[str, Any]], **kwargs) -> AsyncIterator[StreamChunk]:
        ...
        yield # type: ignore

# ====================== 工具函数 ======================

def supports_streaming(model: Any) -> bool:
    return isinstance(model, StreamableModelProtocol)

def supports_function_calling(model: Any) -> bool:
    if hasattr(model, "_supports_function_calling"):
        return model._supports_function_calling
    if hasattr(model, "supports_function_calling"):
        method = getattr(model, "supports_function_calling")
        if callable(method):
            return method() # type: ignore
    return False

def supports_vision(model: Any) -> bool:
    if hasattr(model, "_supports_vision"):
        return model._supports_vision
    if hasattr(model, "supports_vision"):
        method = getattr(model, "supports_vision")
        if callable(method):
            return method() # type: ignore
    return False

def get_model_name(model: Any) -> str:
    if hasattr(model, "model_name"): return model.model_name
    if hasattr(model, "name"): return model.name
    return model.__class__.__name__

def validate_model(model: Any) -> None:
    """
    验证模型
    
    修复点：
    调整错误消息格式，包含 "does not implement ModelProtocol" 以匹配测试正则。
    """
    if not isinstance(model, ModelProtocol):
        missing = get_missing_methods(model, ModelProtocol)
        # 之前的写法: raise TypeError(f"Model missing methods: {', '.join(missing)}")
        # 修正后的写法:
        raise TypeError(
            f"Model does not implement ModelProtocol. "
            f"Missing methods: {', '.join(missing) if missing else 'unknown'}"
        )
```

[28] gecko/core/protocols/runnable.py
```python
"""可运行对象协议"""
from __future__ import annotations
from typing import Any, AsyncIterator, Protocol, runtime_checkable

@runtime_checkable
class RunnableProtocol(Protocol):
    async def run(self, input: Any) -> Any: ...

@runtime_checkable
class StreamableRunnableProtocol(RunnableProtocol, Protocol):
    async def stream(self, input: Any) -> AsyncIterator[str]:
        ...
        yield # type: ignore
```

[29] gecko/core/protocols/storage.py
```python
# gecko/core/protocols/storage.py
"""存储相关协议"""
from __future__ import annotations
from typing import Any, Dict, Optional, Protocol, runtime_checkable
from .base import get_missing_methods

@runtime_checkable
class StorageProtocol(Protocol):
    """存储后端协议"""
    async def get(self, key: str) -> Optional[Dict[str, Any]]: ...
    async def set(self, key: str, value: Dict[str, Any], ttl: Optional[int] = None) -> None: ...
    async def delete(self, key: str) -> bool: ...

def validate_storage(storage: Any) -> None:
    """
    验证存储
    
    修复点：
    调整错误消息格式，包含 "does not implement StorageProtocol" 以匹配测试正则。
    """
    if not isinstance(storage, StorageProtocol):
        missing = get_missing_methods(storage, StorageProtocol)
        raise TypeError(
            f"Storage does not implement StorageProtocol. "
            f"Missing methods: {', '.join(missing) if missing else 'unknown'}"
        )
```

[30] gecko/core/protocols/tool.py
```python
# gecko/core/protocols/tool.py
"""工具相关协议"""
from __future__ import annotations
from typing import Any, Dict, Protocol, runtime_checkable
from gecko.core.protocols.base import get_missing_methods

@runtime_checkable
class ToolProtocol(Protocol):
    """工具协议"""
    name: str
    description: str
    parameters: Dict[str, Any]
    async def execute(self, arguments: Dict[str, Any]) -> str: ...

def validate_tool(tool: Any) -> None:
    """
    验证工具是否符合 ToolProtocol
    
    修复点：
    1. 补全 description 和 parameters 的类型/内容检查。
    2. 调整错误提示语序，以匹配单元测试的正则断言 (e.g. "non-empty 'name'").
    """
    # 1. 属性存在性检查
    for attr in ["name", "description", "parameters"]:
        if not hasattr(tool, attr):
            # 测试要求：ValueError(match="'name'") 或 ("Tool must have a 'name' attribute")
            raise ValueError(f"Tool must have a '{attr}' attribute")
    
    # 2. Name 内容检查
    # 测试要求正则: "non-empty 'name'"
    if not isinstance(tool.name, str) or not tool.name.strip():
        raise ValueError("Tool must have a non-empty 'name' attribute")
        
    # 3. Description 内容检查 (之前漏掉了)
    # 测试要求正则: "non-empty 'description'"
    if not isinstance(tool.description, str) or not tool.description.strip():
        raise ValueError("Tool must have a non-empty 'description' attribute")

    # 4. Parameters 类型检查 (之前漏掉了)
    # 测试要求正则: "'parameters'"
    if not isinstance(tool.parameters, dict):
        raise ValueError("Tool must have a 'parameters' dict attribute")

    # 5. 方法检查
    # 测试要求正则: "execute"
    if not hasattr(tool, "execute"):
        raise TypeError("Tool must have an 'execute' method")
    
    if not callable(getattr(tool, "execute")):
        raise TypeError("Tool 'execute' must be callable")
```

[31] gecko/core/protocols/vector.py
```python
"""向量存储协议"""
from __future__ import annotations
from typing import Any, Dict, List, Optional, Protocol, runtime_checkable

@runtime_checkable
class VectorStoreProtocol(Protocol):
    async def add(self, ids: List[str], vectors: List[List[float]], metadata: Optional[List[Dict[str, Any]]] = None) -> None: ...
    async def search(self, query_vector: List[float], top_k: int = 5, filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]: ...
    async def delete(self, ids: List[str]) -> None: ...
```

[32] gecko/core/session/__init__.py
```python
from gecko.core.session.schema import SessionMetadata
from gecko.core.session.entity import Session
from gecko.core.session.manager import SessionManager

__all__ = ["Session", "SessionManager", "SessionMetadata"]
```

[33] gecko/core/session/entity.py
```python
"""会话实体逻辑"""
from __future__ import annotations
import asyncio
import time
import uuid
from typing import Any, Dict, List, Optional
from gecko.core.logging import get_logger
from gecko.core.events import EventBus, SessionEvent
# 引用新的 protocols 路径
from gecko.plugins.storage.interfaces import SessionInterface 
from gecko.core.session.schema import SessionMetadata

logger = get_logger(__name__)

class Session:
    """
    会话对象
    
    管理内存中的会话状态，并提供可选的持久化支持。
    """
    
    def __init__(
        self,
        session_id: Optional[str] = None,
        state: Optional[Dict[str, Any]] = None,
        storage: Optional[SessionInterface] = None,
        ttl: Optional[int] = None,
        event_bus: Optional[EventBus] = None,
        auto_save: bool = False, 
    ):
        self.session_id = session_id or self._generate_id()
        self.state: Dict[str, Any] = state or {}
        self.storage = storage
        self.event_bus = event_bus or EventBus()
        self.auto_save = auto_save
        
        # 元数据
        self.metadata = SessionMetadata(
            session_id=self.session_id,
            ttl=ttl
        )
        
        # 并发锁
        self._lock = asyncio.Lock()
        
        # 标记为已修改
        self._dirty = False
        
        logger.debug("Session created", session_id=self.session_id)
    
    @staticmethod
    def _generate_id() -> str:
        return f"session_{uuid.uuid4().hex[:16]}"
    
    # ===== 状态管理 (同步方法) =====
    
    def get(self, key: str, default: Any = None) -> Any:
        """
        获取状态值（不自动更新访问计数）
        """
        return self.state.get(key, default)
    
    def set(self, key: str, value: Any):
        """
        设置状态值（同步）
        """
        self.state[key] = value
        self.metadata.updated_at = time.time()
        self._dirty = True
        self._try_schedule_auto_save()
    
    def delete(self, key: str) -> bool:
        """删除状态值"""
        if key in self.state:
            del self.state[key]
            self.metadata.updated_at = time.time()
            self._dirty = True
            self._try_schedule_auto_save()
            return True
        return False
    
    def clear(self):
        """清空所有状态"""
        self.state.clear()
        self.metadata.updated_at = time.time()
        self._dirty = True
        self._try_schedule_auto_save()
    
    def update(self, data: Dict[str, Any]):
        """批量更新状态"""
        self.state.update(data)
        self.metadata.updated_at = time.time()
        self._dirty = True
        self._try_schedule_auto_save()
    
    def touch(self):
        """手动更新访问时间和计数"""
        self.metadata.touch()
    
    def _try_schedule_auto_save(self):
        """
        尝试调度自动保存任务
        """
        if not self.auto_save or not self.storage:
            return

        try:
            loop = asyncio.get_running_loop()
            loop.create_task(self._auto_save_task())
        except RuntimeError:
            pass

    async def _auto_save_task(self):
        """后台自动保存任务"""
        try:
            await self.save()
        except Exception as e:
            logger.error("Auto-save failed", session_id=self.session_id, error=str(e))

    # ===== 字典接口 =====
    
    def keys(self) -> List[str]:
        return list(self.state.keys())
    
    def values(self) -> List[Any]:
        return list(self.state.values())
    
    def items(self) -> List[tuple]:
        return list(self.state.items())
    
    def __contains__(self, key: str) -> bool:
        return key in self.state
    
    def __getitem__(self, key: str) -> Any:
        return self.state[key]
    
    def __setitem__(self, key: str, value: Any):
        self.set(key, value)
    
    # ===== 生命周期管理 =====
    
    def is_expired(self) -> bool:
        return self.metadata.is_expired()
    
    def extend_ttl(self, extra_seconds: int):
        if self.metadata.ttl is not None:
            self.metadata.ttl += extra_seconds
            self._dirty = True
    
    def renew(self):
        self.metadata.created_at = time.time()
        self._dirty = True
    
    # ===== 标签管理 =====
    
    def add_tag(self, tag: str):
        self.metadata.tags.add(tag)
        self._dirty = True
    
    def remove_tag(self, tag: str):
        self.metadata.tags.discard(tag)
        self._dirty = True
    
    def has_tag(self, tag: str) -> bool:
        return tag in self.metadata.tags
    
    # ===== 持久化 (异步方法) =====
    
    async def save(self, force: bool = False):
        """
        [优化] 保存会话到存储 (线程安全与一致性保证)
        
        策略：
        1. 持有锁时进行状态检查和数据快照 (Snapshotting)。
        2. 使用 utils.safe_serialize_context 确保数据深拷贝和清洗。
        3. 持有锁进行 IO 操作，确保写入顺序性（针对单个 Session 的并发保护）。
        """
        if not self.storage:
            return
        
        # 全程持有锁，防止在 Snapshotting 和 IO 之间状态发生变更
        # 虽然这会略微增加锁的持有时间，但对于单 Session 粒度来说，一致性优于微小的并发性能提升。
        async with self._lock:
            # 双重检查
            if not force and not self._dirty:
                return

            try:
                # 1. 准备数据快照 (CPU 密集型)
                # 引入 utils 中的序列化工具，它会递归处理并返回纯净的 dict/list 副本
                # 这实际上切断了 storage 数据与内存 self.state 的引用关系
                from gecko.core.utils import safe_serialize_context
                
                # 获取当前状态的字典表示
                raw_data = self.to_dict()
                
                # 清洗并深拷贝
                clean_data = safe_serialize_context(raw_data)
                
                # 2. 执行 IO (IO 密集型)
                # 这里的 await 会释放 GIL，但不会释放 self._lock (协程锁)
                # 因此其他协程无法在此期间修改 self.state 或发起新的 save
                await self.storage.set(self.session_id, clean_data)
                
                # 3. 状态重置
                # 只有成功写入后才清除 dirty 标记
                self._dirty = False
                
                logger.debug("Session saved", session_id=self.session_id)
                await self.event_bus.publish(SessionEvent(
                    type="session_saved",
                    data={"session_id": self.session_id}
                ))
                
            except Exception as e:
                logger.error(
                    "Failed to save session",
                    session_id=self.session_id,
                    error=str(e)
                )
                # 发生异常时，保持 dirty = True，以便下次重试
                # 抛出异常让上层知道保存失败
                raise
    
    async def load(self) -> bool:
        """从存储加载会话"""
        if not self.storage:
            return False
        
        async with self._lock:
            try:
                data = await self.storage.get(self.session_id)
                if not data:
                    return False
                
                self.from_dict(data)
                self._dirty = False
                
                logger.debug("Session loaded", session_id=self.session_id)
                
                await self.event_bus.publish(SessionEvent(
                    type="session_loaded",
                    data={"session_id": self.session_id}
                ))
                
                return True
            except Exception as e:
                logger.error(
                    "Failed to load session",
                    session_id=self.session_id,
                    error=str(e)
                )
                return False
    
    async def destroy(self):
        """销毁会话（从存储中删除）"""
        if self.storage:
            async with self._lock:
                try:
                    await self.storage.delete(self.session_id)
                    logger.info("Session destroyed", session_id=self.session_id)
                    
                    await self.event_bus.publish(SessionEvent(
                        type="session_destroyed",
                        data={"session_id": self.session_id}
                    ))
                except Exception as e:
                    logger.error(
                        "Failed to destroy session",
                        session_id=self.session_id,
                        error=str(e)
                    )
        
        self.state.clear()
    
    # ===== 序列化 =====
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "state": self.state,
            "metadata": self.metadata.model_dump(),
        }
    
    def from_dict(self, data: Dict[str, Any]):
        self.state = data.get("state", {})
        
        metadata_data = data.get("metadata", {})
        if metadata_data:
            self.metadata = SessionMetadata(**metadata_data)
    
    def clone(self, new_id: Optional[str] = None) -> Session:
        cloned = Session(
            session_id=new_id,
            state=self.state.copy(),
            storage=self.storage,
            ttl=self.metadata.ttl,
            event_bus=self.event_bus,
            auto_save=False,
        )
        cloned.metadata.tags = self.metadata.tags.copy()
        cloned.metadata.custom = self.metadata.custom.copy()
        return cloned
    
    def get_info(self) -> Dict[str, Any]:
        """
        [新增] 获取会话的统计信息
        """
        return {
            "session_id": self.session_id,
            "created_at": self.metadata.created_at,
            "updated_at": self.metadata.updated_at,
            "access_count": self.metadata.access_count,
            "state_keys": len(self.state),
            "ttl": self.metadata.ttl,
            "tags": list(self.metadata.tags)
        }
    
    # ===== 上下文管理器支持 =====
    
    async def __aenter__(self):
        await self.load()
        self.touch()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self._dirty:
            await self.save()
    
    def __repr__(self) -> str:
        return (
            f"Session("
            f"id={self.session_id}, "
            f"keys={len(self.state)}, "
            f"dirty={self._dirty}"
            f")"
        )

```

[34] gecko/core/session/manager.py
```python
"""会话管理器"""
from __future__ import annotations
import asyncio
from typing import Dict, List, Optional
from gecko.plugins.storage.interfaces import SessionInterface
from gecko.core.session.entity import Session
from gecko.core.logging import get_logger

logger = get_logger(__name__)

class SessionManager:
    """会话管理器"""
    
    def __init__(
        self,
        storage: Optional[SessionInterface] = None,
        default_ttl: Optional[int] = None,
        auto_cleanup: bool = True,
        cleanup_interval: int = 300,
    ):
        self.storage = storage
        self.default_ttl = default_ttl
        self.auto_cleanup = auto_cleanup
        self.cleanup_interval = cleanup_interval
        
        self._sessions: Dict[str, Session] = {}
        self._lock = asyncio.Lock()
        
        self._cleanup_task: Optional[asyncio.Task] = None
        
        logger.info("SessionManager initialized", default_ttl=default_ttl)
    
    async def start(self):
        """启动管理器（主要是自动清理任务）"""
        if self.auto_cleanup and not self._cleanup_task:
            self._cleanup_task = asyncio.create_task(self._cleanup_loop())
            logger.debug("Cleanup task started", interval=self.cleanup_interval)

    async def _cleanup_loop(self):
        while True:
            try:
                await asyncio.sleep(self.cleanup_interval)
                await self.cleanup_expired()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error("Cleanup task error", error=str(e))
                await asyncio.sleep(60)

    async def create_session(
        self,
        session_id: Optional[str] = None,
        ttl: Optional[int] = None,
        **initial_state
    ) -> Session:
        """创建新会话"""
        async with self._lock:
            session = Session(
                session_id=session_id,
                state=initial_state,
                storage=self.storage,
                ttl=ttl or self.default_ttl,
                auto_save=True, 
            )
            
            self._sessions[session.session_id] = session
            
            if self.storage:
                await session.save()
            
            return session
    
    async def get_session(
        self,
        session_id: str,
        create_if_missing: bool = False
    ) -> Optional[Session]:
        """获取会话"""
        if session_id in self._sessions:
            session = self._sessions[session_id]
            if session.is_expired():
                await self.destroy_session(session_id)
                if create_if_missing:
                    return await self.create_session(session_id=session_id)
                return None
            session.touch()
            return session
        
        if self.storage:
            session = Session(
                session_id=session_id,
                storage=self.storage,
                auto_save=True,
            )
            if await session.load():
                if session.is_expired():
                    await session.destroy()
                    if create_if_missing:
                        return await self.create_session(session_id=session_id)
                    return None
                async with self._lock:
                    self._sessions[session_id] = session
                session.touch()
                return session
        
        if create_if_missing:
            return await self.create_session(session_id=session_id)
        
        return None
    
    async def destroy_session(self, session_id: str) -> bool:
        """销毁会话"""
        async with self._lock:
            session = self._sessions.pop(session_id, None)
            
            if session:
                await session.destroy()
                return True
            elif self.storage:
                try:
                    await self.storage.delete(session_id)
                    return True
                except Exception as e:
                    logger.error("Failed to destroy session", session_id=session_id, error=str(e))
        return False
    
    async def cleanup_expired(self) -> int:
        """清理所有过期会话"""
        expired_ids = []
        async with self._lock:
            for session_id, session in self._sessions.items():
                if session.is_expired():
                    expired_ids.append(session_id)
        
        for session_id in expired_ids:
            await self.destroy_session(session_id)
        
        if expired_ids:
            logger.info("Expired sessions cleaned", count=len(expired_ids))
        return len(expired_ids)
    
    def get_active_count(self) -> int:
        """获取活跃会话数量"""
        return len(self._sessions)
    
    def get_all_sessions(self) -> List[Session]:
        """获取所有活跃会话"""
        return list(self._sessions.values())
    
    async def shutdown(self):
        """关闭管理器"""
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        
        sessions = list(self._sessions.values())
        for session in sessions:
            await session.save(force=True)
        
        logger.info("SessionManager shutdown", sessions_saved=len(sessions))
```

[35] gecko/core/session/schema.py
```python
"""会话元数据定义"""
from __future__ import annotations
import time
from typing import Any, Dict, Optional, Set
from pydantic import BaseModel, Field
from gecko.core.logging import get_logger

logger = get_logger(__name__)

class SessionMetadata(BaseModel):
    """会话元数据"""
    session_id: str = Field(..., description="会话 ID")
    created_at: float = Field(default_factory=time.time, description="创建时间")
    updated_at: float = Field(default_factory=time.time, description="更新时间")
    accessed_at: float = Field(default_factory=time.time, description="访问时间")
    access_count: int = Field(default=0, description="访问次数")
    ttl: Optional[int] = Field(default=None, description="生存时间（秒）")
    tags: Set[str] = Field(default_factory=set, description="标签")
    custom: Dict[str, Any] = Field(default_factory=dict, description="自定义数据")
    
    def is_expired(self) -> bool:
        """检查会话是否过期"""
        if self.ttl is None:
            return False
        age = time.time() - self.created_at
        return age > self.ttl
    
    def time_to_expire(self) -> Optional[float]:
        """获取距离过期的剩余时间（秒）"""
        if self.ttl is None:
            return None
        age = time.time() - self.created_at
        return max(0.0, self.ttl - age)
    
    def touch(self):
        """更新访问时间和计数"""
        self.accessed_at = time.time()
        self.access_count += 1
```

[36] gecko/core/structure.py
```python
# gecko/core/structure.py
"""
结构化输出引擎

负责将 LLM 的文本/工具调用输出解析为 Pydantic 模型。

核心功能：
1. 多策略 JSON 提取（Tool Call、直接 JSON、Markdown、暴力截取）
2. Schema 验证与修复
3. 带反馈的重试机制
4. OpenAI Function Calling Schema 生成

优化点：
1. 改进错误信息收集和报告
2. 更智能的 JSON 提取算法
3. Schema 自动修复
4. 详细的调试日志
5. 可配置的解析策略
"""
from __future__ import annotations

import json
import re
from typing import Any, Dict, List, Optional, Type, TypeVar

from pydantic import BaseModel, ValidationError

from gecko.core.logging import get_logger

logger = get_logger(__name__)

T = TypeVar("T", bound=BaseModel)


# ===== 自定义异常 =====

class StructureParseError(ValueError):
    """
    结构化解析失败异常
    
    属性:
        message: 错误信息
        attempts: 所有尝试的解析策略及其错误
        raw_content: 原始内容
    """
    def __init__(
        self,
        message: str,
        attempts: Optional[List[Dict[str, str]]] = None,
        raw_content: Optional[str] = None
    ):
        super().__init__(message)
        self.attempts = attempts or []
        self.raw_content = raw_content
    
    def get_detailed_error(self) -> str:
        """获取详细错误信息"""
        lines = [f"结构化解析失败: {self.args[0]}"]
        
        if self.attempts:
            lines.append("\n尝试的解析策略:")
            for i, attempt in enumerate(self.attempts, 1):
                strategy = attempt.get("strategy", "unknown")
                error = attempt.get("error", "unknown error")
                lines.append(f"  {i}. {strategy}: {error}")
        
        if self.raw_content:
            preview = self.raw_content[:200].replace("\n", "\\n")
            lines.append(f"\n原始内容预览: {preview}...")
        
        return "\n".join(lines)


# ===== 结构化输出引擎 =====

class StructureEngine:
    """
    结构化输出引擎
    
    提供多种策略将文本解析为 Pydantic 模型。
    
    示例:
        ```python
        from pydantic import BaseModel
        
        class User(BaseModel):
            name: str
            age: int
        
        # 从工具调用解析
        result = await StructureEngine.parse(
            content="",
            model_class=User,
            raw_tool_calls=[{
                "function": {
                    "arguments": '{"name": "Alice", "age": 25}'
                }
            }]
        )
        
        # 从文本解析
        result = await StructureEngine.parse(
            content='{"name": "Bob", "age": 30}',
            model_class=User
        )
        ```
    """
    
    # ===== Schema 生成 =====
    
    @staticmethod
    def to_openai_tool(model: Type[BaseModel]) -> Dict[str, Any]:
        """
        将 Pydantic 模型转换为 OpenAI Function Calling 所需的 schema
        
        参数:
            model: Pydantic 模型类
        
        返回:
            OpenAI tool 定义
        
        示例:
            ```python
            from pydantic import BaseModel, Field
            
            class SearchQuery(BaseModel):
                query: str = Field(description="搜索关键词")
                max_results: int = Field(default=5, description="最大结果数")
            
            tool = StructureEngine.to_openai_tool(SearchQuery)
            # 可用于 OpenAI API 的 tools 参数
            ```
        """
        schema = model.model_json_schema()
        
        # 提取模型名称（移除特殊字符）
        name = re.sub(r"\W+", "_", schema.get("title", "extract_data")).lower()
        
        # 移除 Pydantic 内部字段
        if "title" in schema:
            del schema["title"]
        if "$defs" in schema:
            # 展开 definitions
            schema = StructureEngine._flatten_schema(schema)
        
        return {
            "type": "function",
            "function": {
                "name": name,
                "description": schema.get("description", f"Extract {name} data"),
                "parameters": schema,
            },
        }
    
    @staticmethod
    def _flatten_schema(schema: Dict[str, Any]) -> Dict[str, Any]:
        """
        展开 schema 中的 $ref 引用
        
        简化版实现，处理常见情况
        """
        defs = schema.pop("$defs", {})
        if not defs:
            return schema
            
        def resolve_ref(obj):
            if isinstance(obj, dict):
                if "$ref" in obj:
                    ref_key = obj["$ref"].split("/")[-1]
                    if ref_key in defs:
                        # 递归解析引用
                        return resolve_ref(defs[ref_key])
                return {k: resolve_ref(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [resolve_ref(item) for item in obj]
            return obj

        return resolve_ref(schema) # type: ignore
    
    # ===== 解析方法 =====
    
    @classmethod
    async def parse(
        cls,
        content: str,
        model_class: Type[T],
        raw_tool_calls: Optional[List[Dict[str, Any]]] = None,
        strict: bool = True,
        auto_fix: bool = True,
    ) -> T:
        """
        解析文本为 Pydantic 模型
        
        参数:
            content: 文本内容
            model_class: 目标 Pydantic 模型类
            raw_tool_calls: 原始工具调用列表（优先使用）
            strict: 是否严格模式（False 时会尝试修复）
            auto_fix: 是否自动修复常见问题
        
        返回:
            模型实例
        
        异常:
            StructureParseError: 解析失败
        """
        attempts = []
        
        # 策略 1: 从 Tool Calls 提取
        if raw_tool_calls:
            for idx, call in enumerate(raw_tool_calls):
                try:
                    result = cls._parse_from_tool_call(call, model_class)
                    logger.info(
                        "Parsed from tool call",
                        model=model_class.__name__,
                        tool_call_index=idx
                    )
                    return result
                except Exception as e:
                    attempts.append({
                        "strategy": f"tool_call_{idx}",
                        "error": str(e)
                    })
                    logger.debug("Tool call parse failed", index=idx, error=str(e))
        
        # 策略 2-5: 从文本提取
        try:
            return cls._extract_json(content, model_class, strict=strict, auto_fix=auto_fix)
        except Exception as e:
            # 收集所有尝试的错误
            # [修复] 增加 and e.attempts 判断
            # 如果子异常有 attempts 且不为空，说明是深层策略失败，合并历史
            # 否则（如快速失败），将其视为当前步骤的一个错误记录下来
            if hasattr(e, 'attempts') and e.attempts: # type: ignore
                attempts.extend(e.attempts) # type: ignore
            else:
                attempts.append({
                    "strategy": "text_extraction",
                    "error": str(e)
                })
            
            # 构建详细错误信息
            error_details = "\n".join(
                f"  - {a['strategy']}: {a['error'][:100]}"
                for a in attempts
            )
            
            raise StructureParseError(
                f"无法解析为 {model_class.__name__}。尝试了 {len(attempts)} 种策略:\n{error_details}",
                attempts=attempts,
                raw_content=content
            ) from e
    
    # ===== 内部解析方法 =====
    
    @classmethod
    def _parse_from_tool_call(
        cls,
        call: Dict[str, Any],
        model_class: Type[T]
    ) -> T:
        """从单个工具调用中解析"""
        func = call.get("function", {})
        args = func.get("arguments", "")
        
        # 解析参数
        if isinstance(args, str):
            data = json.loads(args)
        elif isinstance(args, dict):
            data = args
        else:
            raise ValueError(f"Invalid arguments type: {type(args)}")
        
        # 验证并创建模型实例
        return model_class(**data)
    
    @classmethod
    def _extract_json(
        cls,
        text: str,
        model_class: Type[T],
        strict: bool = True,
        auto_fix: bool = True,
    ) -> T:
        """
        从文本中提取 JSON 并解析为模型
        
        尝试多种策略：
        1. 直接解析整个文本
        2. 提取 Markdown 代码块
        3. 暴力括号匹配
        4. 清理并重试
        """
        text = text.strip()
        attempts = []
        
        # 0. 快速失败检查 (Fail-Fast)
        if not text or ('{' not in text and '[' not in text):
            raise StructureParseError(
                "Content does not contain JSON-like structure (missing '{' or '[')",
                raw_content=text
            )

        # 策略 A: 直接解析 (最快)
        try:
            data = json.loads(text)
            return cls.validate(data, model_class)
        except Exception as e:
            attempts.append({"strategy": "direct_json", "error": str(e)})
        
        # 策略 B: Markdown 代码块
        markdown_pattern = r"```(?:\w+)?\s*([\s\S]*?)```"
        for match in re.finditer(markdown_pattern, text):
            candidate = match.group(1).strip()
            try:
                data = json.loads(candidate)
                return cls.validate(data, model_class)
            except Exception as e:
                attempts.append({"strategy": "markdown", "error": str(e)})
        
        # 策略 C: 暴力括号匹配 (限制尝试次数)
        json_candidates = cls._extract_braced_json(text)
        for idx, candidate in enumerate(json_candidates):
            try:
                data = json.loads(candidate)
                return cls.validate(data, model_class)
            except Exception as e:
                if idx < 3: # 仅记录前3次
                    attempts.append({"strategy": f"braced_{idx}", "error": str(e)})
        
        # 策略 D: 清理并重试
        if auto_fix:
            cleaned = cls._clean_json_string(text)
            if cleaned != text:
                try:
                    data = json.loads(cleaned)
                    logger.info("Parsed after cleaning", model=model_class.__name__)
                    return cls.validate(data, model_class)
                except Exception as e:
                    attempts.append({"strategy": "cleaned_json", "error": str(e)})
        
        # 构建错误详情
        error_details = "\n".join(f"  - {a['strategy']}: {a['error'][:100]}" for a in attempts)
        raise StructureParseError(
            f"无法解析为 {model_class.__name__}。尝试了 {len(attempts)} 种策略:\n{error_details}",
            attempts=attempts,
            raw_content=text
        )
    
    @staticmethod
    def _extract_braced_json(text: str) -> List[str]:
        """
        使用栈提取所有 {...} 块
        
        改进：
        - 返回所有可能的 JSON 对象，不仅仅是第一个
        - 按长度排序，优先尝试最长的
        """
        candidates = []
        stack = []
        start = None
        
        # 简单优化：只在看起来像 JSON 的区域搜索
        search_start = text.find('{')
        if search_start == -1:
            return []
            
        for idx, ch in enumerate(text[search_start:], start=search_start):
            if ch == "{":
                if not stack:
                    start = idx
                stack.append(ch)
            elif ch == "}" and stack:
                stack.pop()
                if not stack and start is not None:
                    candidates.append(text[start:idx + 1])
                    start = None
                    # 限制最大候选数量，防止DoS
                    if len(candidates) >= 5:
                        break
        
        # 按长度降序排序
        candidates.sort(key=len, reverse=True)
        return candidates
    
    @staticmethod
    def _clean_json_string(text: str) -> str:
        """
        清理常见的 JSON 格式问题
        
        处理：
        - 单引号 -> 双引号
        - 尾部逗号
        - 注释
        - 控制字符
        """
        # 移除注释
        text = re.sub(r'//.*?\n', '\n', text)
        text = re.sub(r'/\*.*?\*/', '', text, flags=re.DOTALL)
        
        # 尝试修复单引号（简单情况）
        # 注意：这可能会误伤字符串内容，需谨慎
        # text = text.replace("'", '"')
        
        # 移除尾部逗号
        text = re.sub(r',(\s*[}\]])', r'\1', text)
        
        # 移除控制字符
        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
        
        return text.strip()
    
    # ===== 辅助方法 =====
    
    @classmethod
    def validate(cls, data: Dict[str, Any], model_class: Type[T]) -> T:
        """
        验证数据并创建模型实例
        
        参数:
            data: 数据字典
            model_class: 模型类
        
        返回:
            模型实例
        
        异常:
            ValidationError: 验证失败
        """
        try:
            return model_class(**data)
        except ValidationError as e:
            logger.error(
                "Model validation failed",
                model=model_class.__name__,
                errors=e.errors()
            )
            raise
    
    @classmethod
    def get_schema_diff(
        cls,
        data: Dict[str, Any],
        model_class: Type[BaseModel]
    ) -> Dict[str, Any]:
        """
        比较数据与 Schema 的差异
        
        参数:
            data: 实际数据
            model_class: 期望的模型
        
        返回:
            差异信息
        """
        schema = model_class.model_json_schema()
        required = set(schema.get("required", []))
        properties = schema.get("properties", {})
        
        data_keys = set(data.keys())
        schema_keys = set(properties.keys())
        
        return {
            "missing_required": list(required - data_keys),
            "extra_fields": list(data_keys - schema_keys),
            "type_mismatches": cls._check_type_mismatches(data, properties),
        }
    
    @staticmethod
    def _check_type_mismatches(
        data: Dict[str, Any],
        properties: Dict[str, Any]
    ) -> List[Dict[str, str]]:
        """检查类型不匹配"""
        mismatches = []
        
        for key, value in data.items():
            if key not in properties:
                continue
            
            expected_type = properties[key].get("type")
            actual_type = type(value).__name__
            
            # 简化的类型检查
            type_map = {
                "string": str,
                "integer": int,
                "number": (int, float),
                "boolean": bool,
                "array": list,
                "object": dict,
            }
            
            expected_python_type = type_map.get(expected_type)
            if expected_python_type and not isinstance(value, expected_python_type):
                mismatches.append({
                    "field": key,
                    "expected": expected_type,
                    "actual": actual_type,
                })
        
        return mismatches


# ===== 工具函数 =====

def parse_structured_output(
    content: str,
    model_class: Type[T],
    tool_calls: Optional[List[Dict[str, Any]]] = None,
) -> T:
    """
    同步版本的结构化输出解析（便捷函数）
    
    参数:
        content: 文本内容
        model_class: 目标模型类
        tool_calls: 工具调用列表
    
    返回:
        模型实例
    """
    import asyncio
    
    # 创建事件循环执行异步解析
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    return loop.run_until_complete(
        StructureEngine.parse(content, model_class, tool_calls)
    )


def extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
    """
    从文本中提取第一个有效的 JSON 对象
    
    参数:
        text: 文本内容
    
    返回:
        JSON 字典，如果未找到返回 None
    """
    # 尝试直接解析
    try:
        return json.loads(text.strip())
    except json.JSONDecodeError:
        pass
    
    # 尝试 Markdown
    pattern = r"```(?:json)?\s*([\s\S]*?)```"
    for match in re.finditer(pattern, text):
        try:
            return json.loads(match.group(1).strip())
        except json.JSONDecodeError:
            continue
    
    # 尝试括号匹配
    candidates = StructureEngine._extract_braced_json(text)
    for candidate in candidates:
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            continue
    
    return None
```

[37] gecko/core/toolbox.py
```python
# gecko/core/toolbox.py
"""
ToolBox - Agent 工具箱

核心功能：
1. 工具注册与管理（支持实例注入与注册表加载）
2. 单个/批量工具执行
3. 并发控制与超时管理
4. 执行统计与监控
5. OpenAI Function Calling Schema 生成

优化日志：
- [Refactor] 集成 ToolRegistry，支持通过字符串名称加载工具
- [Refactor] 适配新版 BaseTool 接口
- [Fix] 修复并发控制的信号量使用方式
- [Feat] 线程安全的统计数据
- [Fix] 补全 get_summary, reset_stats 及魔术方法
"""
from __future__ import annotations

import asyncio
import threading
import time
from collections import defaultdict
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union

from anyio import create_task_group, fail_after, Semaphore
from anyio import get_cancelled_exc_class

from gecko.config import settings
from gecko.core.exceptions import ToolError, ToolNotFoundError
from gecko.core.logging import get_logger
from gecko.plugins.tools.base import BaseTool, ToolResult
from gecko.plugins.tools.registry import ToolRegistry

logger = get_logger(__name__)


# ===== 返回值模型 =====

@dataclass
class ToolExecutionResult:
    """
    工具执行结果（ToolBox 层面的封装）
    
    包含工具本身的返回内容，以及 ToolBox 记录的执行元数据（耗时、ID等）。
    
    属性:
        tool_name: 工具名称
        call_id: 调用 ID（用于关联请求）
        result: 执行结果（成功时为字符串，失败时为错误信息）
        is_error: 是否执行失败
        duration: 执行耗时（秒）
        metadata: 附加信息
    """
    tool_name: str
    call_id: str
    result: str
    is_error: bool
    duration: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """转换为字典格式"""
        return {
            "tool_name": self.tool_name,
            "call_id": self.call_id,
            "result": self.result,
            "is_error": self.is_error,
            "duration": self.duration,
            "metadata": self.metadata,
        }


# ===== 工具箱主类 =====

class ToolBox:
    """
    Agent 工具箱
    
    负责工具的注册、执行、并发控制和统计。
    既支持直接传入 BaseTool 实例，也支持通过名称从 ToolRegistry 加载。
    
    示例:
        ```python
        # 混合加载工具
        toolbox = ToolBox(
            tools=["calculator", MyCustomTool()],
            max_concurrent=5
        )
        
        # 执行
        result = await toolbox.execute("calculator", {"expression": "1+1"})
        
        # 获取统计
        summary = toolbox.get_summary()
        print(f"Success Rate: {summary['overall_success_rate']:.2%}")
        ```
    """

    def __init__(
        self,
        tools: Optional[List[Union[BaseTool, str]]] = None,
        max_concurrent: int = 5,
        default_timeout: Optional[float] = None,
        enable_retry: bool = False,
        max_retries: int = 2,
    ):
        """
        初始化工具箱
        
        参数:
            tools: 初始工具列表（支持 BaseTool 实例或注册表中的字符串名称）
            max_concurrent: 最大并发执行数
            default_timeout: 默认超时时间（秒）
            enable_retry: 是否启用重试
            max_retries: 最大重试次数
        """
        # 工具存储
        self._tools: Dict[str, BaseTool] = {}
        
        # 配置
        self.max_concurrent = max_concurrent
        self.default_timeout = default_timeout or settings.tool_execution_timeout
        self.enable_retry = enable_retry
        self.max_retries = max_retries
        
        # 统计数据（线程安全）
        self._stats_lock = threading.Lock()
        self._execution_count: Dict[str, int] = defaultdict(int)
        self._error_count: Dict[str, int] = defaultdict(int)
        self._total_time: Dict[str, float] = defaultdict(float)
        
        # 注册初始工具
        if tools:
            for item in tools:
                self.add_tool(item)
    
    # ====================== 工具管理 ======================
    
    def add_tool(self, item: Union[BaseTool, str], **kwargs) -> "ToolBox":
        """
        添加工具（高层接口）
        
        支持：
        1. 字符串：从 ToolRegistry 加载
        2. 实例：直接注册
        
        参数:
            item: 工具名称或实例
            **kwargs: 如果是字符串加载，kwargs 将传递给工具构造函数
        """
        tool_instance: Optional[BaseTool] = None
        
        if isinstance(item, str):
            try:
                tool_instance = ToolRegistry.load_tool(item, **kwargs)
            except Exception as e:
                logger.error(f"Failed to load tool '{item}' from registry: {e}")
                # 此时可以选择抛出异常，或者仅记录错误跳过，这里选择抛出以便尽早发现配置错误
                raise ToolNotFoundError(f"Registry load failed for '{item}': {e}") from e
        elif isinstance(item, BaseTool):
            tool_instance = item
        else:
            raise TypeError(f"Tool must be BaseTool or str, got {type(item)}")
            
        if tool_instance:
            self.register(tool_instance)
            
        return self

    def register(self, tool: BaseTool, replace: bool = True) -> "ToolBox":
        """
        注册工具实例（底层接口）
        
        参数:
            tool: 工具实例
            replace: 是否替换同名工具
        """
        if not isinstance(tool, BaseTool):
            raise TypeError(f"Tool must inherit from BaseTool, got {type(tool)}")
        
        if tool.name in self._tools and not replace:
            raise ValueError(f"Tool '{tool.name}' already registered.")
        
        self._tools[tool.name] = tool
        
        # 初始化统计
        with self._stats_lock:
            if tool.name not in self._execution_count:
                self._execution_count[tool.name] = 0
                self._error_count[tool.name] = 0
                self._total_time[tool.name] = 0.0
        
        logger.debug("Tool registered", tool_name=tool.name)
        return self
    
    def unregister(self, tool_name: str) -> "ToolBox":
        """注销工具"""
        if tool_name in self._tools:
            del self._tools[tool_name]
            logger.info("Tool unregistered", tool_name=tool_name)
        return self
    
    def get(self, name: str) -> Optional[BaseTool]:
        """获取工具实例"""
        return self._tools.get(name)
    
    def list_tools(self) -> List[BaseTool]:
        """获取所有已注册工具"""
        return list(self._tools.values())
    
    def has_tool(self, name: str) -> bool:
        """检查工具是否存在"""
        return name in self._tools
    
    # ====================== Schema 生成 ======================
    
    def to_openai_schema(self) -> List[Dict[str, Any]]:
        """
        生成 OpenAI Function Calling Schema
        
        直接调用 BaseTool.openai_schema 属性
        """
        return [t.openai_schema for t in self._tools.values()]
    
    # ====================== 执行逻辑 ======================
    
    async def execute(
        self,
        name: str,
        arguments: Dict[str, Any],
        timeout: Optional[float] = None,
        call_id: str = "",
    ) -> str:
        """
        执行单个工具（简易版）
        
        返回:
            结果字符串
        """
        result = await self.execute_with_result(name, arguments, timeout, call_id)
        if result.is_error:
            raise ToolError(
                f"Tool execution failed: {result.result}",
                context={"tool": name, "args": arguments}
            )
        return result.result
    
    async def execute_with_result(
        self,
        name: str,
        arguments: Dict[str, Any],
        timeout: Optional[float] = None,
        call_id: str = "",
    ) -> ToolExecutionResult:
        """
        执行单个工具（完整版）
        
        包含：超时控制、重试逻辑、统计记录
        """
        tool = self.get(name)
        if not tool:
            self._update_stats(name, 0, is_error=True)
            raise ToolNotFoundError(name)
        
        actual_timeout = timeout or self.default_timeout
        start_time = time.time()
        
        # 选择执行策略
        if self.enable_retry:
            result_str, is_error = await self._execute_with_retry(
                tool, arguments, actual_timeout
            )
        else:
            result_str, is_error = await self._execute_once(
                tool, arguments, actual_timeout
            )
            
        duration = time.time() - start_time
        self._update_stats(name, duration, is_error)
        
        return ToolExecutionResult(
            tool_name=name,
            call_id=call_id,
            result=result_str,
            is_error=is_error,
            duration=duration
        )

    async def _execute_once(
        self,
        tool: BaseTool,
        arguments: Dict[str, Any],
        timeout: float
    ) -> tuple[str, bool]:
        """
        单次执行封装
        
        处理超时和异常，适配 BaseTool 的 ToolResult 返回值
        """
        try:
            with fail_after(timeout):
                # BaseTool.execute 已经处理了参数校验和内部异常，返回 ToolResult
                res: ToolResult = await tool.execute(arguments)
                return res.content, res.is_error
                
        except TimeoutError:
            return f"Execution timed out after {timeout}s", True
        except get_cancelled_exc_class():
            return "Execution cancelled", True
        except Exception as e:
            logger.exception("Unexpected tool execution error", tool=tool.name)
            return f"System error: {str(e)}", True

    async def _execute_with_retry(
        self,
        tool: BaseTool,
        arguments: Dict[str, Any],
        timeout: float
    ) -> tuple[str, bool]:
        """带重试的执行逻辑"""
        last_result = ""
        
        for attempt in range(self.max_retries + 1):
            content, is_error = await self._execute_once(tool, arguments, timeout)
            
            if not is_error:
                return content, False
            
            last_result = content
            
            # 如果是超时或系统错误，尝试重试
            # 如果是 BaseTool 返回的业务逻辑错误（如参数不对），通常重试无用，但在通用层我们还是给机会
            if attempt < self.max_retries:
                wait_time = 2 ** attempt
                logger.warning(
                    "Tool failed, retrying",
                    tool=tool.name,
                    attempt=attempt + 1,
                    error=content[:100]
                )
                await asyncio.sleep(wait_time)
        
        return last_result, True

    # ====================== 批量执行 ======================
    
    async def execute_many(
        self,
        tool_calls: List[Dict[str, Any]],
        timeout: Optional[float] = None
    ) -> List[ToolExecutionResult]:
        """
        并发批量执行
        
        参数:
            tool_calls: [{"name": "...", "arguments": {...}, "id": "..."}, ...]
            
        返回:
            结果列表（顺序与输入一致）
        """
        if not tool_calls:
            return []
            
        results: List[Optional[ToolExecutionResult]] = [None] * len(tool_calls)
        
        # 使用 anyio 信号量控制并发
        semaphore = Semaphore(self.max_concurrent)
        
        async def _worker(idx: int, call: Dict[str, Any]):
            async with semaphore:
                name = call.get("name", "")
                args = call.get("arguments", {})
                cid = call.get("id", "")
                
                if not name:
                    results[idx] = ToolExecutionResult(
                        tool_name="unknown", call_id=cid, result="Missing tool name", is_error=True
                    )
                    return

                try:
                    # 复用 execute_with_result 以获得完整的统计和重试支持
                    results[idx] = await self.execute_with_result(name, args, timeout, cid)
                except Exception as e:
                    results[idx] = ToolExecutionResult(
                        tool_name=name, call_id=cid, result=f"Batch error: {e}", is_error=True
                    )

        async with create_task_group() as tg:
            for i, call in enumerate(tool_calls):
                tg.start_soon(_worker, i, call)
                
        # 过滤 None (理论上不应该存在)
        return [r for r in results if r is not None]

    # ====================== 统计与辅助 ======================
    
    def _update_stats(self, name: str, duration: float, is_error: bool):
        """更新统计数据（线程安全）"""
        with self._stats_lock:
            self._execution_count[name] += 1
            self._total_time[name] += duration
            if is_error:
                self._error_count[name] += 1
    
    def get_stats(self) -> Dict[str, Dict[str, Any]]:
        """获取详细统计快照"""
        with self._stats_lock:
            stats = {}
            for name in self._tools:
                cnt = self._execution_count[name]
                err = self._error_count[name]
                total = self._total_time[name]
                stats[name] = {
                    "calls": cnt,
                    "errors": err,
                    "avg_time": (total / cnt) if cnt > 0 else 0.0,
                    "success_rate": ((cnt - err) / cnt) if cnt > 0 else 1.0
                }
            return stats
            
    def print_stats(self):
        """打印格式化统计信息"""
        stats = self.get_stats()
        print("\n=== ToolBox Statistics ===")
        if not stats:
            print("No tools executed.")
        for name, data in stats.items():
            print(f"🔧 {name:<15} Calls: {data['calls']:<5} Errors: {data['errors']:<5} "
                  f"Avg: {data['avg_time']:.3f}s Rate: {data['success_rate']:.1%}")
        print("=" * 30 + "\n")
        
    def reset_stats(self):
        """
        重置所有统计数据
        """
        with self._stats_lock:
            self._execution_count.clear()
            self._error_count.clear()
            self._total_time.clear()
        logger.info("Statistics reset")

    def get_summary(self) -> Dict[str, Any]:
        """
        获取工具箱的全局摘要
        """
        stats = self.get_stats()
        
        total_executions = sum(s["calls"] for s in stats.values())
        total_errors = sum(s["errors"] for s in stats.values())
        total_time_sum = sum(
            self._total_time.get(name, 0.0) for name in stats.keys()
        )
        
        return {
            "tool_count": len(self._tools),
            "total_executions": total_executions,
            "total_errors": total_errors,
            "total_time": total_time_sum,
            "overall_success_rate": (
                (total_executions - total_errors) / total_executions
                if total_executions > 0 else 1.0
            ),
            "avg_time_per_call": (
                total_time_sum / total_executions
                if total_executions > 0 else 0.0
            ),
        }

    # ====================== 魔术方法 ======================

    def __repr__(self) -> str:
        return f"ToolBox(tools={len(self._tools)}, concurrent={self.max_concurrent})"
    
    def __len__(self) -> int:
        """返回已注册工具的数量"""
        return len(self._tools)
    
    def __contains__(self, tool_name: str) -> bool:
        """支持 'tool_name' in toolbox 语法"""
        return tool_name in self._tools
```

[38] gecko/core/utils.py
```python
# gecko/core/utils.py
"""
通用工具函数库

提供框架常用的工具函数，包括：
- 异步/同步统一处理
- 重试机制
- 超时控制
- 数据转换
- 字符串处理
- 装饰器

优化点：
1. 扩展工具函数集合
2. 添加超时和重试支持
3. 提供数据转换工具
4. 添加常用装饰器
"""
from __future__ import annotations

import asyncio
import functools
import hashlib
import inspect
import json
import time
from typing import Any, Awaitable, Callable, Dict, List, Optional, TypeVar, Union

from gecko.core.logging import get_logger

logger = get_logger(__name__)

T = TypeVar("T")
R = TypeVar("R")


# ===== 异步/同步统一处理 =====

async def ensure_awaitable(
    func: Callable[..., T | Awaitable[T]],
    *args,
    timeout: Optional[float] = None,
    **kwargs
) -> T:
    """
    统一处理同步/异步函数调用
    
    参数:
        func: 可调用对象（同步或异步）
        *args: 位置参数
        timeout: 超时时间（秒），None 表示无限制
        **kwargs: 关键字参数
    
    返回:
        函数执行结果
    
    异常:
        asyncio.TimeoutError: 超时
        Exception: 函数执行异常
    
    示例:
        ```python
        # 同步函数
        result = await ensure_awaitable(sync_func, arg1, arg2)
        
        # 异步函数
        result = await ensure_awaitable(async_func, arg1, arg2)
        
        # 带超时
        result = await ensure_awaitable(func, timeout=5.0)
        ```
    """
    # 确定是否为协程函数
    if asyncio.iscoroutinefunction(func):
        coro = func(*args, **kwargs)
    else:
        result = func(*args, **kwargs)
        if asyncio.iscoroutine(result):
            coro = result
        else:
            # 同步函数，直接返回结果
            return result # type: ignore
    
    # 执行异步函数（带可选超时）
    if timeout:
        try:
            return await asyncio.wait_for(coro, timeout=timeout)
        except asyncio.TimeoutError:
            logger.warning(
                "Function execution timeout",
                func=getattr(func, "__name__", str(func)),
                timeout=timeout
            )
            raise
    else:
        return await coro


def run_sync(coro: Awaitable[T]) -> T:
    """
    在同步上下文中运行异步函数
    
    参数:
        coro: 协程对象
    
    返回:
        执行结果
    
    示例:
        ```python
        async def async_func():
            return "result"
        
        # 在同步代码中调用
        result = run_sync(async_func())
        ```
    """
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        # 没有运行中的 loop，直接 run
        return asyncio.run(coro) # type: ignore
    
    if loop.is_running():
        # ⚠️ 检测到嵌套循环
        logger.debug(
            "Running async code in sync context with nested loop. "
            "Trying to use nest_asyncio."
        )
        try:
            import nest_asyncio
            nest_asyncio.apply()
            return loop.run_until_complete(coro)
        except ImportError:
            raise RuntimeError(
                "Detected nested event loop. "
                "Please install 'nest_asyncio' to allow nested usage: pip install nest_asyncio"
            )
    else:
        return loop.run_until_complete(coro)


# ===== 重试机制 =====

async def retry_async(
    func: Callable[..., Awaitable[T]],
    *args,
    max_attempts: int = 3,
    delay: float = 1.0,
    backoff: float = 2.0,
    exceptions: tuple = (Exception,),
    on_retry: Optional[Callable[[int, Exception], None]] = None,
    **kwargs
) -> T:
    """
    异步函数重试装饰器
    
    参数:
        func: 异步函数
        *args: 位置参数
        max_attempts: 最大尝试次数
        delay: 初始延迟（秒）
        backoff: 退避倍数
        exceptions: 需要重试的异常类型
        on_retry: 重试时的回调函数
        **kwargs: 关键字参数
    
    返回:
        函数执行结果
    
    异常:
        最后一次尝试的异常
    
    示例:
        ```python
        async def unstable_api_call():
            # 可能失败的 API 调用
            ...
        
        result = await retry_async(
            unstable_api_call,
            max_attempts=5,
            delay=2.0,
            backoff=2.0
        )
        ```
    """
    last_exception = None
    current_delay = delay
    
    func_name = getattr(func, "__name__", str(func))
    
    for attempt in range(1, max_attempts + 1):
        try:
            return await func(*args, **kwargs)
        except exceptions as e:
            last_exception = e
            
            if attempt >= max_attempts:
                logger.error(
                    "All retry attempts failed",
                    func=func_name,
                    attempts=max_attempts,
                    error=str(e)
                )
                raise
            
            logger.warning(
                "Function failed, retrying",
                func=func_name,
                attempt=attempt,
                max_attempts=max_attempts,
                delay=current_delay,
                error=str(e)
            )
            
            if on_retry:
                try:
                    # 新增：检测回调是否为异步函数
                    if inspect.iscoroutinefunction(on_retry):
                        await on_retry(attempt, e)
                    else:
                        on_retry(attempt, e)
                except Exception as cb_err:
                    logger.warning("Retry callback failed", error=str(cb_err))
            
            await asyncio.sleep(current_delay)
            current_delay *= backoff
    
    raise last_exception # type: ignore


def retry(
    max_attempts: int = 3,
    delay: float = 1.0,
    backoff: float = 2.0,
    exceptions: tuple = (Exception,)
):
    """
    重试装饰器（支持同步和异步）
    
    参数:
        max_attempts: 最大尝试次数
        delay: 初始延迟（秒）
        backoff: 退避倍数
        exceptions: 需要重试的异常类型
    
    示例:
        ```python
        @retry(max_attempts=5, delay=2.0)
        async def unstable_function():
            # 可能失败的操作
            ...
        ```
    """
    def decorator(func: Callable) -> Callable:
        if asyncio.iscoroutinefunction(func):
            @functools.wraps(func)
            async def async_wrapper(*args, **kwargs):
                return await retry_async(
                    func,
                    *args,
                    max_attempts=max_attempts,
                    delay=delay,
                    backoff=backoff,
                    exceptions=exceptions,
                    **kwargs
                )
            return async_wrapper
        else:
            @functools.wraps(func)
            def sync_wrapper(*args, **kwargs):
                last_exception = None
                current_delay = delay
                
                for attempt in range(1, max_attempts + 1):
                    try:
                        return func(*args, **kwargs)
                    except exceptions as e:
                        last_exception = e
                        if attempt >= max_attempts:
                            raise
                        time.sleep(current_delay)
                        current_delay *= backoff
                
                raise last_exception # type: ignore
            
            return sync_wrapper
    
    return decorator


# ===== 数据转换 =====

def safe_dict(obj: Any, max_depth: int = 3, _current_depth: int = 0) -> Any:
    """
    安全地将对象转换为字典（递归处理）
    
    参数:
        obj: 要转换的对象
        max_depth: 最大递归深度
        _current_depth: 当前深度（内部使用）
    
    返回:
        字典或可序列化的值
    
    示例:
        ```python
        class MyClass:
            def __init__(self):
                self.name = "test"
                self.value = 123
        
        obj = MyClass()
        data = safe_dict(obj)
        # {"name": "test", "value": 123}
        ```
    """
    if _current_depth >= max_depth:
        return str(obj)[:100]
    
    # Pydantic 模型
    if hasattr(obj, "model_dump"):
        try:
            return obj.model_dump()
        except Exception:
            pass
    
    # 字典
    if isinstance(obj, dict):
        return {
            str(k): safe_dict(v, max_depth, _current_depth + 1)
            for k, v in obj.items()
        }
    
    # 列表/元组
    if isinstance(obj, (list, tuple)):
        return [safe_dict(item, max_depth, _current_depth + 1) for item in obj]
    
    # 基本类型
    if isinstance(obj, (str, int, float, bool, type(None))):
        return obj
    
    # 对象属性
    if hasattr(obj, "__dict__"):
        return {
            k: safe_dict(v, max_depth, _current_depth + 1)
            for k, v in obj.__dict__.items()
            if not k.startswith("_")
        }
    
    # 其他情况：转字符串
    return str(obj)[:200]


def merge_dicts(*dicts: Dict, deep: bool = False) -> Dict:
    """
    合并多个字典
    
    参数:
        *dicts: 要合并的字典
        deep: 是否深度合并
    
    返回:
        合并后的字典
    
    示例:
        ```python
        d1 = {"a": 1, "b": {"x": 1}}
        d2 = {"b": {"y": 2}, "c": 3}
        
        # 浅合并
        result = merge_dicts(d1, d2)
        # {"a": 1, "b": {"y": 2}, "c": 3}
        
        # 深合并
        result = merge_dicts(d1, d2, deep=True)
        # {"a": 1, "b": {"x": 1, "y": 2}, "c": 3}
        ```
    """
    if not dicts:
        return {}
    
    result = {}
    
    for d in dicts:
        if not isinstance(d, dict):
            continue
        
        for key, value in d.items():
            if deep and key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = merge_dicts(result[key], value, deep=True)
            else:
                result[key] = value
    
    return result


# ===== 字符串处理 =====

def truncate(
    text: str,
    max_length: int = 100,
    suffix: str = "..."
) -> str:
    """
    截断文本
    
    参数:
        text: 文本
        max_length: 最大长度
        suffix: 后缀
    
    返回:
        截断后的文本
    """
    if len(text) <= max_length:
        return text
    
    return text[:max_length - len(suffix)] + suffix


def format_size(size_bytes: int) -> str:
    """
    格式化字节大小
    
    参数:
        size_bytes: 字节数
    
    返回:
        可读的大小字符串
    
    示例:
        ```python
        format_size(1024)       # "1.00 KB"
        format_size(1048576)    # "1.00 MB"
        format_size(1073741824) # "1.00 GB"
        ```
    """
    for unit in ["B", "KB", "MB", "GB", "TB"]:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0 # type: ignore
    return f"{size_bytes:.2f} PB"


def format_duration(seconds: float) -> str:
    """
    格式化时长
    
    参数:
        seconds: 秒数
    
    返回:
        可读的时长字符串
    
    示例:
        ```python
        format_duration(65)    # "1m 5s"
        format_duration(3665)  # "1h 1m 5s"
        ```
    """
    if seconds < 60:
        return f"{seconds:.1f}s"
    
    minutes = int(seconds // 60)
    secs = int(seconds % 60)
    
    if minutes < 60:
        return f"{minutes}m {secs}s"
    
    hours = minutes // 60
    minutes = minutes % 60
    
    return f"{hours}h {minutes}m {secs}s"


def compute_hash(text: str, algorithm: str = "md5") -> str:
    """
    计算文本的哈希值
    
    参数:
        text: 文本
        algorithm: 算法（md5/sha1/sha256）
    
    返回:
        哈希值（十六进制字符串）
    
    示例:
        ```python
        hash_value = compute_hash("Hello, World!")
        ```
    """
    if algorithm == "md5":
        hasher = hashlib.md5()
    elif algorithm == "sha1":
        hasher = hashlib.sha1()
    elif algorithm == "sha256":
        hasher = hashlib.sha256()
    else:
        raise ValueError(f"不支持的哈希算法: {algorithm}")
    
    hasher.update(text.encode("utf-8"))
    return hasher.hexdigest()


# ===== 性能监控 =====

class Timer:
    """
    简单的计时器（上下文管理器）
    
    示例:
        ```python
        with Timer("操作名称") as t:
            # 执行耗时操作
            do_something()
        
        print(f"耗时: {t.elapsed:.2f}s")
        ```
    """
    
    def __init__(self, name: str = "Timer", log: bool = True):
        self.name = name
        self.log = log
        self.start_time = None
        self.end_time = None
        self.elapsed = 0.0
    
    def __enter__(self):
        self.start_time = time.time()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.time()
        self.elapsed = self.end_time - self.start_time # type: ignore
        
        if self.log:
            logger.info(
                "Timer completed",
                name=self.name,
                elapsed=f"{self.elapsed:.3f}s"
            )
        
        return False

def safe_json_loads(text: str, default: Any = None) -> Any:
    """
    安全地解析 JSON
    """
    try:
        return json.loads(text)
    except (json.JSONDecodeError, TypeError):
        return default

def timing(func: Callable) -> Callable:
    """
    计时装饰器（支持同步和异步）
    
    示例:
        ```python
        @timing
        async def slow_function():
            await asyncio.sleep(1)
        ```
    """
    if asyncio.iscoroutinefunction(func):
        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            start = time.time()
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                elapsed = time.time() - start
                logger.info(
                    "Function executed",
                    func=func.__name__,
                    elapsed=f"{elapsed:.3f}s"
                )
        return async_wrapper
    else:
        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            start = time.time()
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                elapsed = time.time() - start
                logger.info(
                    "Function executed",
                    func=func.__name__,
                    elapsed=f"{elapsed:.3f}s"
                )
        return sync_wrapper


# ===== 函数签名工具 =====

def get_function_args(func: Callable) -> List[str]:
    """
    获取函数的参数名列表
    
    参数:
        func: 函数对象
    
    返回:
        参数名列表
    """
    sig = inspect.signature(func)
    return [
        name for name, param in sig.parameters.items()
        if param.kind in (
            inspect.Parameter.POSITIONAL_OR_KEYWORD,
            inspect.Parameter.KEYWORD_ONLY,
        )
    ]


def has_argument(func: Callable, arg_name: str) -> bool:
    """
    检查函数是否有指定参数
    
    参数:
        func: 函数对象
        arg_name: 参数名
    
    返回:
        是否存在该参数
    """
    return arg_name in get_function_args(func)


# ===== 其他工具 =====

def chunk_list(lst: List[T], chunk_size: int) -> List[List[T]]:
    """
    将列表分块
    
    参数:
        lst: 列表
        chunk_size: 每块大小
    
    返回:
        分块后的列表
    
    示例:
        ```python
        chunks = chunk_list([1, 2, 3, 4, 5], chunk_size=2)
        # [[1, 2], [3, 4], [5]]
        ```
    """
    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]


def flatten_list(nested: List[List[T]]) -> List[T]:
    """
    展平嵌套列表
    
    参数:
        nested: 嵌套列表
    
    返回:
        展平后的列表
    
    示例:
        ```python
        flat = flatten_list([[1, 2], [3, 4], [5]])
        # [1, 2, 3, 4, 5]
        ```
    """
    return [item for sublist in nested for item in sublist]


def deduplicate(
    items: List[T],
    key: Optional[Callable[[T], Any]] = None
) -> List[T]:
    """
    列表去重（保持顺序）
    
    参数:
        items: 列表
        key: 可选的键函数
    
    返回:
        去重后的列表
    
    示例:
        ```python
        # 简单去重
        unique = deduplicate([1, 2, 2, 3, 1])
        # [1, 2, 3]
        
        # 按属性去重
        users = [{"id": 1, "name": "A"}, {"id": 1, "name": "B"}]
        unique = deduplicate(users, key=lambda u: u["id"])
        ```
    """
    seen = set()
    result = []
    
    for item in items:
        k = key(item) if key else item
        
        # 处理不可哈希的情况
        try:
            if k not in seen:
                seen.add(k)
                result.append(item)
        except TypeError:
            # 不可哈希，使用 == 比较
            if not any(k == s for s in seen):
                seen.add(k)
                result.append(item)
    
    return result

def safe_serialize_context(data: Any) -> Any:
    """
    [新增] 高性能序列化清洗工具
    
    功能：
    1. 递归遍历对象结构 (Dict, List, Pydantic)。
    2. 检测不可序列化的对象 (如 Lock, Socket)。
    3. 将不可序列化对象替换为标记字典，而不是直接抛错。
    4. 纯 Python 操作，便于卸载到 ThreadPool 执行。
    """
    def _clean(obj, depth=0):
        # 防止无限递归
        if depth > 20: 
            return str(obj)

        # 1. 基础类型直通
        if obj is None or isinstance(obj, (str, int, float, bool)):
            return obj
        
        # 2. 列表/元组处理
        if isinstance(obj, (list, tuple)):
            return [_clean(x, depth + 1) for x in obj]
        
        # 3. 字典处理
        if isinstance(obj, dict):
            new_obj = {}
            for k, v in obj.items():
                # 递归清洗 Value
                new_obj[str(k)] = _clean(v, depth + 1)
            return new_obj
        
        # 4. Pydantic 对象 (转为 dict 后继续清洗)
        if hasattr(obj, "model_dump"):
            return _clean(obj.model_dump(mode='python'), depth + 1)
            
        # 5. 其他对象：尝试检测是否可 JSON 序列化
        try:
            json.dumps(obj)
            return obj
        except (TypeError, OverflowError):
            # 6. [关键] 不可序列化对象，转换为特殊标记
            # 这样 Resume 时虽然无法恢复该对象，但至少不会导致整个流程崩溃
            # 同时也给调试留下了线索
            return {
                "__gecko_unserializable__": True,
                "type": type(obj).__name__,
                "repr": str(obj)[:100]
            }

    return _clean(data)


# ===== 向后兼容导出 =====

__all__ = [
    # 异步工具
    "ensure_awaitable",
    "run_sync",
    # 重试
    "retry",
    "retry_async",
    # 数据转换
    "safe_dict",
    "merge_dicts",
    "safe_serialize_context",
    # 字符串
    "truncate",
    "format_size",
    "format_duration",
    "compute_hash",
    # 性能
    "Timer",
    "timing",
    # 函数工具
    "get_function_args",
    "has_argument",
    # 列表工具
    "chunk_list",
    "flatten_list",
    "deduplicate",
    "safe_json_loads",
]
```

[39] gecko/plugins/__init__.py
```python
```

[40] gecko/plugins/base.py
```python
# gecko/plugins/base.py  
  
"""  
占位：未来可在此定义所有插件的抽象基类或通用接口。  
目前实际内容请参考各子目录（tools/ storage/ knowledge 等）。  
"""  
```

[41] gecko/plugins/guardrails/__init__.py
```python
```

[42] gecko/plugins/guardrails/pii.py
```python
```

[43] gecko/plugins/knowledge/__init__.py
```python
# gecko/plugins/knowledge/__init__.py
from gecko.plugins.knowledge.document import Document
from gecko.plugins.knowledge.interfaces import EmbedderProtocol
from gecko.plugins.knowledge.embedders import OpenAIEmbedder, OllamaEmbedder
from gecko.plugins.knowledge.splitters import RecursiveCharacterTextSplitter
from gecko.plugins.knowledge.pipeline import IngestionPipeline
from gecko.plugins.knowledge.tool import RetrievalTool

__all__ = [
    "Document", 
    "EmbedderProtocol", 
    "OpenAIEmbedder", 
    "OllamaEmbedder",
    "RecursiveCharacterTextSplitter",
    "IngestionPipeline",
    "RetrievalTool"
]
```

[44] gecko/plugins/knowledge/base.py
```python
```

[45] gecko/plugins/knowledge/default.py
```python
```

[46] gecko/plugins/knowledge/document.py
```python
# gecko/plugins/knowledge/document.py
from __future__ import annotations
from uuid import uuid4
from typing import Dict, Any, Optional, List
from pydantic import BaseModel, Field

class Document(BaseModel):
    """
    Gecko 标准文档对象
    在 Pipeline 中流转的核心数据结构
    """
    id: str = Field(default_factory=lambda: str(uuid4()))
    text: str
    metadata: Dict[str, Any] = Field(default_factory=dict)
    embedding: Optional[List[float]] = None

    def to_dict(self) -> Dict[str, Any]:
        """转换为存储层所需的字典格式"""
        return {
            "id": self.id,
            "text": self.text,
            "metadata": self.metadata,
            "embedding": self.embedding
        }
```

[47] gecko/plugins/knowledge/embedders.py
```python
# gecko/plugins/knowledge/embedders.py
from __future__ import annotations
import os
from typing import List
import litellm
from gecko.plugins.knowledge.interfaces import EmbedderProtocol

class OpenAIEmbedder(EmbedderProtocol):
    """
    基于 LiteLLM 的通用 Embedder
    支持 OpenAI, Azure, Ollama 等所有 LiteLLM 支持的 embedding 模型
    """
    def __init__(self, model: str = "text-embedding-3-small", dimension: int = 1536, **kwargs):
        self.model = model
        self._dimension = dimension
        self.kwargs = kwargs

    @property
    def dimension(self) -> int:
        return self._dimension

    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        # 替换换行符以提升某些模型的表现
        texts = [t.replace("\n", " ") for t in texts]
        response = await litellm.aembedding(
            model=self.model,
            input=texts,
            **self.kwargs
        )
        return [r["embedding"] for r in response.data]

    async def embed_query(self, text: str) -> List[float]:
        text = text.replace("\n", " ")
        response = await litellm.aembedding(
            model=self.model,
            input=[text],
            **self.kwargs
        )
        return response.data[0]["embedding"]

# 预设 Ollama 配置
class OllamaEmbedder(OpenAIEmbedder):
    """
    Ollama 本地嵌入模型适配器
    """
    def __init__(self, model: str = "ollama/nomic-embed-text", base_url: str = "http://localhost:11434", dimension: int = 768):
        super().__init__(
            model=model, 
            dimension=dimension, 
            api_base=base_url
        )
```

[48] gecko/plugins/knowledge/interfaces.py
```python
# gecko/plugins/knowledge/interfaces.py
from __future__ import annotations
from typing import List, Protocol, runtime_checkable

@runtime_checkable
class EmbedderProtocol(Protocol):
    """
    嵌入模型协议
    负责将文本转换为向量
    """
    @property
    def dimension(self) -> int:
        """返回向量维度 (例如 1536)"""
        ...

    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """批量嵌入文档列表"""
        ...

    async def embed_query(self, text: str) -> List[float]:
        """嵌入单个查询语句"""
        ...

@runtime_checkable
class ReaderProtocol(Protocol):
    """
    文件读取协议
    """
    def load(self, file_path: str) -> str:
        """读取文件内容为字符串"""
        ...
```

[49] gecko/plugins/knowledge/pipeline.py
```python
# gecko/plugins/knowledge/pipeline.py
from typing import List, Optional
from gecko.plugins.knowledge.interfaces import EmbedderProtocol
from gecko.plugins.storage.interfaces import VectorInterface
from gecko.plugins.knowledge.splitters import RecursiveCharacterTextSplitter
from gecko.plugins.knowledge.readers import AutoReader
from gecko.core.utils import ensure_awaitable

class IngestionPipeline:
    """
    RAG 数据入库流水线
    Load -> Split -> Embed -> Store
    """
    def __init__(
        self,
        vector_store: VectorInterface,
        embedder: EmbedderProtocol,
        splitter = None
    ):
        self.vector_store = vector_store
        self.embedder = embedder
        self.splitter = splitter or RecursiveCharacterTextSplitter()

    async def run(self, file_paths: List[str], batch_size: int = 100):
        """
        执行入库流程
        :param file_paths: 文件路径列表
        :param batch_size: 向量库写入批次大小
        """
        print(f"🚀 开始处理 {len(file_paths)} 个文件...")
        
        # 1. Load
        raw_docs = []
        for path in file_paths:
            try:
                docs = AutoReader.read(path)
                raw_docs.extend(docs)
            except Exception as e:
                print(f"⚠️ 读取失败 {path}: {e}")

        # 2. Split
        chunks = self.splitter.split_documents(raw_docs)
        print(f"✂️ 切分为 {len(chunks)} 个片段")

        # 3. Embed & Store (Batch Processing)
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i : i + batch_size]
            texts = [doc.text for doc in batch]
            
            # 生成向量
            embeddings = await ensure_awaitable(self.embedder.embed_documents, texts)
            
            # 注入向量到文档对象
            docs_to_upsert = []
            for doc, emb in zip(batch, embeddings):
                doc.embedding = emb
                docs_to_upsert.append(doc.to_dict())
            
            # 写入数据库
            await self.vector_store.upsert(docs_to_upsert)
            print(f"💾 已存储批次 {i} - {i+len(batch)}")
            
        print("✅ 入库完成")
```

[50] gecko/plugins/knowledge/readers.py
```python
# gecko/plugins/knowledge/readers.py
import os
from pathlib import Path
from typing import List
from gecko.plugins.knowledge.document import Document
from gecko.plugins.knowledge.interfaces import ReaderProtocol

class TextReader(ReaderProtocol):
    """简单文本读取器 (.txt, .md, .py, etc)"""
    def load(self, file_path: str) -> str:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()

class PDFReader(ReaderProtocol):
    """PDF 读取器 (依赖 pypdf)"""
    def load(self, file_path: str) -> str:
        try:
            import pypdf
        except ImportError:
            raise ImportError("请安装 pypdf 以支持 PDF 读取: pip install pypdf")
            
        text = ""
        with open(file_path, "rb") as f:
            reader = pypdf.PdfReader(f)
            for page in reader.pages:
                text += page.extract_text() + "\n"
        return text

class AutoReader:
    """自动分发读取器"""
    _READERS = {
        ".txt": TextReader,
        ".md": TextReader,
        ".py": TextReader,
        ".json": TextReader,
        ".pdf": PDFReader
    }

    @classmethod
    def read(cls, file_path: str) -> List[Document]:
        path = Path(file_path)
        ext = path.suffix.lower()
        
        reader_cls = cls._READERS.get(ext)
        if not reader_cls:
            raise ValueError(f"不支持的文件类型: {ext}")
        
        content = reader_cls().load(str(path))
        return [Document(text=content, metadata={"source": str(path), "filename": path.name})]
```

[51] gecko/plugins/knowledge/splitters.py
```python
# gecko/plugins/knowledge/splitters.py
from typing import List
from gecko.plugins.knowledge.document import Document

class RecursiveCharacterTextSplitter:
    """
    递归字符切分器 (参考 LangChain 逻辑)
    尝试按顺序使用分隔符切分文本，直到块大小符合要求。
    """
    def __init__(
        self, 
        chunk_size: int = 1000, 
        chunk_overlap: int = 200,
        separators: List[str] | None = None
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or ["\n\n", "\n", " ", ""]

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """切分文档列表"""
        final_docs = []
        for doc in documents:
            chunks = self.split_text(doc.text)
            for i, chunk in enumerate(chunks):
                # 继承元数据，并增加切片信息
                new_meta = doc.metadata.copy()
                new_meta.update({"chunk_index": i, "source_id": doc.id})
                final_docs.append(Document(text=chunk, metadata=new_meta))
        return final_docs

    def split_text(self, text: str) -> List[str]:
        """切分单文本核心逻辑"""
        final_chunks = []
        if self._length(text) <= self.chunk_size:
            return [text]
            
        # 找到最优分隔符
        separator = self.separators[-1]
        for sep in self.separators:
            if sep == "":
                separator = ""
                break
            if sep in text:
                separator = sep
                break
                
        # 切分
        splits = text.split(separator) if separator else list(text)
        
        # 合并碎片
        good_splits = []
        current_chunk = ""
        
        for s in splits:
            if self._length(current_chunk) + self._length(s) < self.chunk_size:
                current_chunk += (separator if current_chunk else "") + s
            else:
                if current_chunk:
                    good_splits.append(current_chunk)
                current_chunk = s
        
        if current_chunk:
            good_splits.append(current_chunk)
            
        return good_splits

    def _length(self, text: str) -> int:
        return len(text)
```

[52] gecko/plugins/knowledge/tool.py
```python
# gecko/plugins/knowledge/tool.py
from typing import Type
from pydantic import BaseModel, Field
from gecko.plugins.tools.base import BaseTool
from gecko.plugins.storage.interfaces import VectorInterface
from gecko.plugins.knowledge.interfaces import EmbedderProtocol
from gecko.core.utils import ensure_awaitable

class RetrievalTool(BaseTool):
    name: str = "knowledge_search"
    description: str = "搜索内部知识库以获取相关信息。当问题涉及特定文档、报告或私有数据时使用。"
    parameters: dict = {
        "type": "object",
        "properties": {
            "query": {
                "type": "string", 
                "description": "用于在知识库中检索的查询语句"
            }
        },
        "required": ["query"]
    }

    def __init__(self, vector_store: VectorInterface, embedder: EmbedderProtocol, top_k: int = 3):
        super().__init__()
        # Private attributes are not Pydantic fields
        object.__setattr__(self, "_vector_store", vector_store)
        object.__setattr__(self, "_embedder", embedder)
        object.__setattr__(self, "_top_k", top_k)

    async def execute(self, arguments: dict) -> str:
        query = arguments.get("query")
        if not query:
            return "错误：查询语句为空"

        # 1. Embed Query
        query_vec = await ensure_awaitable(self._embedder.embed_query, query)
        
        # 2. Vector Search
        results = await self._vector_store.search(query_vec, top_k=self._top_k)
        
        if not results:
            return "未在知识库中找到相关内容。"
            
        # 3. Format Results
        context = "找到以下相关内容：\n\n"
        for i, res in enumerate(results, 1):
            source = res['metadata'].get('filename', 'unknown')
            score = f"{res['score']:.2f}" if 'score' in res else 'N/A'
            context += f"--- 文档 {i} (来源: {source}, 相关度: {score}) ---\n{res['text']}\n\n"
            
        return context
```

[53] gecko/plugins/models/__init__.py
```python
# gecko/plugins/models/__init__.py
"""
Gecko Models Plugin

提供统一的模型接入层。
架构：配置 (Config) -> 工厂 (Factory) -> 注册表 (Registry) -> 驱动 (Driver)
"""
import litellm

# ================= Global Configuration =================
# 关闭 LiteLLM 的遥测和冗余打印，保持控制台整洁
litellm.suppress_debug_info = True
litellm.telemetry = False
# ========================================================

from gecko.plugins.models.base import AbstractModel, BaseChatModel, BaseEmbedder
from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.drivers.litellm_driver import LiteLLMDriver
from gecko.plugins.models.embedding import LiteLLMEmbedder
from gecko.plugins.models.factory import create_model
from gecko.plugins.models.presets.ollama import OllamaChat, OllamaEmbedder
from gecko.plugins.models.presets.openai import OpenAIChat, OpenAIEmbedder
from gecko.plugins.models.presets.zhipu import ZhipuChat

# 确保默认驱动被注册
import gecko.plugins.models.drivers.litellm_driver  # noqa: F401

__all__ = [
    "ModelConfig",
    "AbstractModel",
    "BaseChatModel",
    "BaseEmbedder",
    "LiteLLMDriver",
    "LiteLLMEmbedder",
    "create_model",
    "OpenAIChat",
    "OpenAIEmbedder",
    "OllamaChat",
    "OllamaEmbedder",
    "ZhipuChat",
]
```

[54] gecko/plugins/models/adapter.py
```python
# gecko/plugins/models/adapter.py
from __future__ import annotations

from typing import Any, List, Optional

from gecko.core.protocols import (
    CompletionChoice,
    CompletionResponse,
    CompletionUsage,
    StreamChunk,
)


def safe_access(obj: Any, key: str, default: Any = None) -> Any:
    """
    [工具] 通用属性/字典获取 (增强版)
    
    修复了 Pydantic getattr 抛错和 Mock 对象误判的问题。
    """
    if obj is None:
        return default

    # 1. 优先尝试字典访问 (最安全)
    try:
        return obj[key]
    except (TypeError, KeyError, IndexError, AttributeError):
        pass

    # 2. 尝试属性访问 (需捕获 AttributeError)
    try:
        # 注意：不要使用 hasattr，因为它在某些动态代理对象(如 Mock, Pydantic Lazy)上可能误判
        val = getattr(obj, key)
        return val if val is not None else default
    except (AttributeError, TypeError):
        pass

    return default


class LiteLLMAdapter:
    """
    LiteLLM 响应适配器 (Anti-Corruption Layer)
    
    职责：
    将 LiteLLM 返回的异构对象手动映射为 Gecko 的标准协议对象。
    避免调用 model_dump() 从而消除 Pydantic 序列化警告。
    """

    @staticmethod
    def to_gecko_response(resp: Any) -> CompletionResponse:
        """将 LiteLLM 响应转换为 Gecko CompletionResponse"""
        
        # 1. 提取 Choices
        choices: List[CompletionChoice] = []
        raw_choices = safe_access(resp, "choices", [])
        
        if isinstance(raw_choices, list):
            for c in raw_choices:
                # 提取 Message
                raw_msg = safe_access(c, "message", {})
                message_dict = {
                    "role": safe_access(raw_msg, "role", "assistant"),
                    "content": safe_access(raw_msg, "content", None),
                }
                
                # 提取 Tool Calls
                raw_tool_calls = safe_access(raw_msg, "tool_calls", None)
                if raw_tool_calls:
                    sanitized_tool_calls = []
                    for tc in raw_tool_calls:
                        sanitized_tool_calls.append({
                            "id": safe_access(tc, "id", ""),
                            "type": safe_access(tc, "type", "function"),
                            "function": {
                                "name": safe_access(safe_access(tc, "function"), "name", ""),
                                "arguments": safe_access(safe_access(tc, "function"), "arguments", "")
                            }
                        })
                    message_dict["tool_calls"] = sanitized_tool_calls

                choices.append(CompletionChoice(
                    index=safe_access(c, "index", 0),
                    finish_reason=safe_access(c, "finish_reason", None),
                    message=message_dict,
                    logprobs=safe_access(c, "logprobs", None)
                ))

        # 2. 提取 Usage
        usage = None
        raw_usage = safe_access(resp, "usage", None)
        if raw_usage:
            usage = CompletionUsage(
                prompt_tokens=safe_access(raw_usage, "prompt_tokens", 0),
                completion_tokens=safe_access(raw_usage, "completion_tokens", 0),
                total_tokens=safe_access(raw_usage, "total_tokens", 0)
            )

        # 3. 构建最终响应
        return CompletionResponse(
            id=safe_access(resp, "id", ""),
            object=safe_access(resp, "object", "chat.completion"),
            created=safe_access(resp, "created", 0),
            model=safe_access(resp, "model", ""),
            choices=choices,
            usage=usage,
            system_fingerprint=safe_access(resp, "system_fingerprint", None),
            metadata=safe_access(resp, "_hidden_params", {})
        )

    @staticmethod
    def to_gecko_chunk(chunk: Any) -> Optional[StreamChunk]:
        """将 LiteLLM 流式块转换为 StreamChunk"""
        raw_choices = safe_access(chunk, "choices", [])
        
        # 过滤 Keep-Alive 空包
        if not raw_choices and not safe_access(chunk, "id"):
            return None

        mapped_choices = []
        if isinstance(raw_choices, list):
            for c in raw_choices:
                delta = safe_access(c, "delta", {})
                mapped_choices.append({
                    "index": safe_access(c, "index", 0),
                    "delta": {
                        "role": safe_access(delta, "role", None),
                        "content": safe_access(delta, "content", None),
                        "tool_calls": safe_access(delta, "tool_calls", None)
                    },
                    "finish_reason": safe_access(c, "finish_reason", None)
                })

        return StreamChunk(
            id=safe_access(chunk, "id", ""),
            created=safe_access(chunk, "created", 0),
            model=safe_access(chunk, "model", ""),
            choices=mapped_choices
        )
```

[55] gecko/plugins/models/base.py
```python
# gecko/plugins/models/base.py
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any, AsyncIterator, Dict, List

from gecko.core.protocols import (
    CompletionResponse,
    EmbedderProtocol,
    StreamChunk,
    StreamableModelProtocol,
)
from gecko.plugins.models.config import ModelConfig


class AbstractModel(ABC):
    """所有模型的根基类"""

    def __init__(self, config: ModelConfig):
        self.config = config


class BaseChatModel(AbstractModel, StreamableModelProtocol):
    """
    Chat 模型驱动基类
    
    所有具体的驱动器 (Driver) 都必须继承此类并实现 `acompletion` 和 `astream`。
    """

    @abstractmethod
    async def acompletion(self, messages: List[Dict[str, Any]], **kwargs: Any) -> CompletionResponse:
        """单次生成"""
        ...

    @abstractmethod
    async def astream(self, messages: List[Dict[str, Any]], **kwargs: Any) -> AsyncIterator[StreamChunk]: # type: ignore
        """流式生成"""
        ...


class BaseEmbedder(AbstractModel, EmbedderProtocol):
    """
    Embedding 模型基类
    """
    
    def __init__(self, config: ModelConfig, dimension: int):
        super().__init__(config)
        self._dimension = dimension

    @property
    def dimension(self) -> int:
        """返回向量维度"""
        return self._dimension

    @abstractmethod
    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """批量嵌入"""
        ...

    @abstractmethod
    async def embed_query(self, text: str) -> List[float]:
        """单条查询嵌入"""
        ...
```

[56] gecko/plugins/models/chat.py
```python
# gecko/plugins/models/chat.py
from __future__ import annotations

import json
from typing import Any, AsyncIterator, Dict, List

import litellm
from pydantic import ValidationError

from gecko.core.exceptions import ModelError
from gecko.core.logging import get_logger
from gecko.core.protocols import CompletionResponse, StreamChunk
from gecko.plugins.models.base import BaseChatModel

logger = get_logger(__name__)

class LiteLLMChatModel(BaseChatModel):
    """
    基于 LiteLLM 的通用 Chat 模型实现
    """

    def _get_params(self, messages: List[Dict[str, Any]], stream: bool, **kwargs: Any) -> Dict[str, Any]:
        """构造 LiteLLM 调用参数，合并配置与运行时参数"""
        params = {
            "model": self.config.model_name,
            "messages": messages,
            "timeout": self.config.timeout,
            "stream": stream,
            **self.config.extra_kwargs,
            **kwargs,
        }

        if self.config.api_key:
            params["api_key"] = self.config.api_key
        if self.config.base_url:
            params["api_base"] = self.config.base_url

        return params

    def _sanitize_response(self, resp: Any) -> Dict[str, Any]:
        """
        [核心修复] 深度清洗 LiteLLM 响应对象
        
        目标：将不可靠的 Pydantic 对象转换为纯 Python 字典，
        避免因 litellm 内部 Schema 校验失败导致 crash。
        """
        # 策略 1: 尝试通过 JSON 序列化 (通常最稳健，因为会忽略 Pydantic 类型检查)
        try:
            if hasattr(resp, "model_dump_json"):
                # Pydantic v2 JSON 序列化
                return json.loads(resp.model_dump_json())
            if hasattr(resp, "json") and callable(resp.json):
                # Pydantic v1 / 传统 JSON 方法
                return json.loads(resp.json()) # type: ignore
        except Exception:
            pass

        # 策略 2: 尝试标准 model_dump (如果 JSON 失败)
        try:
            if hasattr(resp, "model_dump"):
                return resp.model_dump(mode='json') # mode='json' 强制转换类型
            if hasattr(resp, "dict"):
                return resp.dict()
        except Exception:
            pass

        # 策略 3: 手动暴力提取 (Ultimate Fallback)
        # 当 litellm 对象损坏无法 dump 时，手动提取关键字段
        try:
            data = {
                "id": getattr(resp, "id", ""),
                "object": getattr(resp, "object", "chat.completion"),
                "created": getattr(resp, "created", 0),
                "model": getattr(resp, "model", ""),
                "choices": [],
                "usage": None
            }
            
            # 提取 Choices
            raw_choices = getattr(resp, "choices", [])
            if isinstance(raw_choices, list):
                for c in raw_choices:
                    c_dict = {
                        "index": getattr(c, "index", 0),
                        "finish_reason": getattr(c, "finish_reason", "stop"),
                        "message": {}
                    }
                    # 提取 Message
                    msg = getattr(c, "message", None)
                    if msg:
                        c_dict["message"] = {
                            "role": getattr(msg, "role", "assistant"),
                            "content": getattr(msg, "content", None),
                            "tool_calls": getattr(msg, "tool_calls", None)
                        }
                    data["choices"].append(c_dict)
            
            # 提取 Usage
            usage = getattr(resp, "usage", None)
            if usage:
                data["usage"] = {
                    "prompt_tokens": getattr(usage, "prompt_tokens", 0),
                    "completion_tokens": getattr(usage, "completion_tokens", 0),
                    "total_tokens": getattr(usage, "total_tokens", 0)
                }
            
            return data
            
        except Exception as e:
            logger.error("Manual response extraction failed", error=str(e))
            # 如果到了这一步，说明对象完全不可读，返回空结构防止 crash
            return {"choices": [], "model": "unknown-error"}

    async def acompletion(self, messages: List[Dict[str, Any]], **kwargs: Any) -> CompletionResponse:
        try:
            params = self._get_params(messages, stream=False, **kwargs)
            resp = await litellm.acompletion(**params)
            
            # 使用新的清洗逻辑
            data = self._sanitize_response(resp)
            
            return CompletionResponse(**data)

        except (ValidationError, TypeError) as e:
            logger.error("Failed to parse model response", error=str(e))
            raise ModelError(f"Response parsing failed: {e}") from e
        except Exception as e:
            self._handle_litellm_error(e)
            raise

    async def astream(self, messages: List[Dict[str, Any]], **kwargs: Any) -> AsyncIterator[StreamChunk]: # type: ignore
        try:
            params = self._get_params(messages, stream=True, **kwargs)
            response_iterator = await litellm.acompletion(**params)
            
            async for chunk in response_iterator: # type: ignore
                # 流式 Chunk 同样需要清洗
                data = self._sanitize_response(chunk)
                # 确保 data 不为空且有 choices (防止空包)
                if data and data.get("choices") or data.get("id"):
                    yield StreamChunk(**data)

        except Exception as e:
            self._handle_litellm_error(e)

    def _handle_litellm_error(self, e: Exception) -> None:
        """统一异常映射"""
        msg = str(e)
        error_type = type(e).__name__
        
        if "AuthenticationError" in error_type:
            raise ModelError(f"Authentication failed: {msg}", error_code="AUTH_ERROR") from e
        if "RateLimitError" in error_type:
            raise ModelError(f"Rate limit exceeded: {msg}", error_code="RATE_LIMIT") from e
        if "ContextWindowExceededError" in error_type:
             raise ModelError(f"Context window exceeded: {msg}", error_code="CONTEXT_LIMIT") from e
             
        raise ModelError(f"Model execution failed ({error_type}): {msg}") from e
```

[57] gecko/plugins/models/config.py
```python
# gecko/plugins/models/config.py
from __future__ import annotations

from typing import Any, Dict, Optional

from pydantic import BaseModel, Field


class ModelConfig(BaseModel):
    """
    模型通用配置对象
    """
    model_name: str = Field(..., description="模型名称，如 'gpt-4o', 'ollama/llama3'")
    
    # 驱动类型 (不再是 Enum，支持字符串扩展)
    driver_type: str = Field(default="litellm", description="驱动类型: litellm, openai_native, etc.")
    
    # 连接配置
    api_key: Optional[str] = Field(default=None, description="API Key")
    base_url: Optional[str] = Field(default=None, description="API Base URL (本地模型必填)")
    timeout: float = Field(default=60.0, description="请求超时时间(秒)")
    max_retries: int = Field(default=2, description="最大重试次数")
    
    # 运行时参数透传 (Provider Specific)
    extra_kwargs: Dict[str, Any] = Field(default_factory=dict, description="透传给底层驱动的额外参数")
    
    # 能力标识 (Capability Flags)
    supports_vision: bool = Field(default=False, description="是否支持视觉输入")
    supports_audio: bool = Field(default=False, description="是否支持音频输入")
    supports_function_calling: bool = Field(default=True, description="是否支持工具调用")
```

[58] gecko/plugins/models/drivers/litellm_driver.py
```python
# gecko/plugins/models/drivers/litellm_driver.py
from __future__ import annotations

from typing import Any, AsyncIterator, Dict, List, Union

import litellm
import tiktoken

from gecko.core.exceptions import ModelError
from gecko.core.logging import get_logger
from gecko.core.protocols import CompletionResponse, StreamChunk
from gecko.plugins.models.adapter import LiteLLMAdapter
from gecko.plugins.models.base import BaseChatModel
from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.registry import register_driver

logger = get_logger(__name__)


@register_driver("litellm")
class LiteLLMDriver(BaseChatModel):
    """
    LiteLLM 通用驱动
    
    特点：
    1. 兼容性最强（支持 OpenAI, Zhipu, Ollama 等）。
    2. 使用 LiteLLMAdapter 进行响应清洗，解决 Pydantic 兼容性问题。
    """
    def __init__(self, config: ModelConfig):
        super().__init__(config)
        self._tokenizer = None
        # [优化] 初始化时预加载 Tokenizer，避免在推理热路径中触发文件 IO
        self._preload_tokenizer()

    def _preload_tokenizer(self):
        """预加载 Tokenizer 到内存 (主要针对 OpenAI/Tiktoken 兼容模型)"""
        try:
            # 如果是 GPT 系列或兼容模型，预加载 tiktoken encoding
            if any(k in self.config.model_name.lower() for k in ["gpt", "text-embedding", "claude"]):
                # 注意：Claude 其实不完全用 cl100k_base，但这里仅作示例优化
                # 生产环境可根据 model_name 映射不同的 encoding
                self._tokenizer = tiktoken.encoding_for_model(self.config.model_name)
        except Exception:
            # 加载失败不报错，运行时降级处理
            pass

    # [实现] 高性能、非阻塞的计数方法
    def count_tokens(self, text_or_messages: Union[str, List[Dict[str, Any]]]) -> int:
        try:
            # 1. 统一转换为文本字符串
            text = self._to_text(text_or_messages)

            # 2. 快速路径：如果预加载了 tiktoken，直接在 C++ 层计算 (极快)
            if self._tokenizer:
                return len(self._tokenizer.encode(text))

            # 3. 慢速路径：LiteLLM encode
            # 仅当文本较短 (<10k chars) 时调用，防止大文本触发 heavy computation 阻塞 Loop
            if len(text) < 10000:
                # litellm.encode 内部有缓存，但仍有 Python 层开销
                return len(litellm.encode(model=self.config.model_name, text=text)) # type: ignore
            
            # 4. 兜底策略：字符长度估算 (避免卡死)
            # 中文约 0.6 token/char, 英文约 0.25 -> 平均取 0.33
            return len(text) // 3

        except Exception:
            # 最后的防线
            return len(str(text_or_messages)) // 3

    def _to_text(self, inp: Union[str, List[Dict[str, Any]]]) -> str:
        """辅助函数：将输入转为字符串"""
        if isinstance(inp, str):
            return inp
        if isinstance(inp, list):
            return "".join(str(m.get("content", "")) for m in inp)
        return str(inp)

    def _get_params(self, messages: List[Dict[str, Any]], stream: bool, **kwargs: Any) -> Dict[str, Any]:
        """构造调用参数"""
        params = {
            "model": self.config.model_name,
            "messages": messages,
            "timeout": self.config.timeout,
            "stream": stream,
            **self.config.extra_kwargs,
            **kwargs,
        }
        # 显式传递参数，确保并发安全
        if self.config.api_key:
            params["api_key"] = self.config.api_key
        if self.config.base_url:
            params["api_base"] = self.config.base_url
        return params

    async def acompletion(self, messages: List[Dict[str, Any]], **kwargs: Any) -> CompletionResponse:
        try:
            params = self._get_params(messages, stream=False, **kwargs)
            resp = await litellm.acompletion(**params)
            # 使用适配器清洗
            return LiteLLMAdapter.to_gecko_response(resp)
        except Exception as e:
            self._handle_error(e)
            raise

    async def astream(self, messages: List[Dict[str, Any]], **kwargs: Any) -> AsyncIterator[StreamChunk]: # type: ignore
        try:
            params = self._get_params(messages, stream=True, **kwargs)
            iterator = await litellm.acompletion(**params)
            
            async for chunk in iterator: # type: ignore
                # 使用适配器清洗
                gecko_chunk = LiteLLMAdapter.to_gecko_chunk(chunk)
                if gecko_chunk:
                    yield gecko_chunk
        except Exception as e:
            self._handle_error(e)

    def _handle_error(self, e: Exception) -> None:
        msg = str(e)
        err_name = type(e).__name__
        
        if "AuthenticationError" in err_name:
            raise ModelError(f"Auth failed: {msg}", error_code="AUTH_ERROR") from e
        if "RateLimitError" in err_name:
            raise ModelError(f"Rate limit: {msg}", error_code="RATE_LIMIT") from e
        if "ContextWindowExceededError" in err_name:
             raise ModelError(f"Context limit: {msg}", error_code="CONTEXT_LIMIT") from e
             
        raise ModelError(f"LiteLLM execution failed ({err_name}): {msg}") from e
```

[59] gecko/plugins/models/embedding.py
```python
# gecko/plugins/models/embedding.py
from __future__ import annotations

from typing import List

import litellm

from gecko.core.exceptions import ModelError
from gecko.plugins.models.adapter import safe_access
from gecko.plugins.models.base import BaseEmbedder


class LiteLLMEmbedder(BaseEmbedder):
    """
    基于 LiteLLM 的通用 Embedding 模型实现
    """

    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        try:
            # 预处理：移除换行符
            clean_texts = [t.replace("\n", " ") for t in texts]
            
            params = {
                "model": self.config.model_name,
                "input": clean_texts,
                "timeout": self.config.timeout,
                **self.config.extra_kwargs
            }
            
            if self.config.api_key:
                params["api_key"] = self.config.api_key
            if self.config.base_url:
                params["api_base"] = self.config.base_url

            resp = await litellm.aembedding(**params)
            
            # [修复] 使用 safe_access 进行健壮提取
            embeddings: List[List[float]] = []
            data_items = safe_access(resp, "data", [])
            
            if isinstance(data_items, list):
                 for item in data_items:
                     emb = safe_access(item, "embedding")
                     if emb:
                         embeddings.append(emb)
            
            return embeddings

        except Exception as e:
            raise ModelError(f"Embedding failed: {str(e)}") from e

    async def embed_query(self, text: str) -> List[float]:
        res = await self.embed_documents([text])
        if not res:
            raise ModelError("Embedding returned empty result")
        return res[0]
```

[60] gecko/plugins/models/factory.py
```python
# gecko/plugins/models/factory.py
from gecko.core.exceptions import ConfigurationError
from gecko.plugins.models.base import BaseChatModel
from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.registry import get_driver_class

# 导入默认驱动以触发注册
import gecko.plugins.models.drivers.litellm_driver  # noqa: F401


def create_model(config: ModelConfig) -> BaseChatModel:
    """
    根据配置创建模型实例 (工厂方法)
    """
    driver_cls = get_driver_class(config.driver_type)
    
    if not driver_cls:
        raise ConfigurationError(
            f"Driver '{config.driver_type}' not found. "
            f"Available drivers: {list(gecko.plugins.models.registry._DRIVER_REGISTRY.keys())}. " # type: ignore
            f"Have you registered it?"
        )
    
    return driver_cls(config)
```

[61] gecko/plugins/models/presets/ollama.py
```python
# gecko/plugins/models/presets/ollama.py
from __future__ import annotations

from typing import Any

from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.drivers.litellm_driver import LiteLLMDriver
from gecko.plugins.models.embedding import LiteLLMEmbedder


class OllamaChat(LiteLLMDriver):
    """Ollama 本地 Chat 模型预设"""
    
    def __init__(self, model: str = "llama3", base_url: str = "http://localhost:11434", **kwargs: Any):
        full_model_name = f"ollama/{model}" if not model.startswith("ollama/") else model
        
        config = ModelConfig(
            model_name=full_model_name,
            driver_type="litellm",
            base_url=base_url,
            api_key="ollama",
            timeout=kwargs.pop("timeout", 120.0),
            supports_function_calling=kwargs.pop("supports_function_calling", False),
            **kwargs
        )
        super().__init__(config)


class OllamaEmbedder(LiteLLMEmbedder):
    """Ollama 本地 Embedding 模型预设"""
    
    def __init__(
        self, 
        model: str = "nomic-embed-text", 
        base_url: str = "http://localhost:11434", 
        dimension: int = 768, 
        **kwargs: Any
    ):
        full_model_name = f"ollama/{model}" if not model.startswith("ollama/") else model
        
        config = ModelConfig(
            model_name=full_model_name,
            base_url=base_url,
            api_key="ollama",
            **kwargs
        )
        super().__init__(config, dimension=dimension)
```

[62] gecko/plugins/models/presets/openai.py
```python
# gecko/plugins/models/presets/openai.py
from __future__ import annotations

from typing import Any

from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.drivers.litellm_driver import LiteLLMDriver
from gecko.plugins.models.embedding import LiteLLMEmbedder


class OpenAIChat(LiteLLMDriver):
    """OpenAI Chat 模型预设"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o", **kwargs: Any):
        config = ModelConfig(
            model_name=model,
            driver_type="litellm",
            api_key=api_key,
            supports_vision=True,
            supports_function_calling=True,
            **kwargs
        )
        super().__init__(config)


class OpenAIEmbedder(LiteLLMEmbedder):
    """OpenAI Embedding 模型预设"""
    
    def __init__(self, api_key: str, model: str = "text-embedding-3-small", dimension: int = 1536, **kwargs: Any):
        config = ModelConfig(
            model_name=model,
            api_key=api_key,
            **kwargs
        )
        super().__init__(config, dimension=dimension)
```

[63] gecko/plugins/models/presets/zhipu.py
```python
# gecko/plugins/models/presets/zhipu.py
from __future__ import annotations

from typing import Any, AsyncIterator, Dict, List

from gecko.core.protocols import CompletionResponse, StreamChunk
from gecko.plugins.models.config import ModelConfig
from gecko.plugins.models.drivers.litellm_driver import LiteLLMDriver


class ZhipuChat(LiteLLMDriver):
    """
    智谱 AI (GLM) 预设
    
    继承自 LiteLLMDriver，复用其稳定的清洗逻辑和 OpenAI 协议。
    """
    
    def __init__(self, api_key: str, model: str = "glm-4-plus", **kwargs: Any):
        # 自动判断视觉支持
        is_vision = "v" in model.lower() or "vision" in model.lower()
        
        config = ModelConfig(
            model_name=model,
            # 显式指定使用 litellm 驱动
            driver_type="litellm",
            api_key=api_key,
            # 智谱官方 OpenAI 兼容接口
            base_url="https://open.bigmodel.cn/api/paas/v4/",
            # 强制 LiteLLM 使用 openai 协议处理
            extra_kwargs={"custom_llm_provider": "openai"},
            supports_vision=is_vision,
            supports_function_calling=True,
            **kwargs
        )
        super().__init__(config)
```

[64] gecko/plugins/models/registry.py
```python
# gecko/plugins/models/registry.py
from __future__ import annotations

from typing import Callable, Dict, Optional, Type, TypeVar

from gecko.core.logging import get_logger
from gecko.plugins.models.base import BaseChatModel

logger = get_logger(__name__)

_DRIVER_REGISTRY: Dict[str, Type[BaseChatModel]] = {}

# [修复] 定义一个泛型变量，限定范围是 BaseChatModel 的子类
T = TypeVar("T", bound=Type[BaseChatModel])

def register_driver(name: str) -> Callable[[T], T]:
    """
    装饰器：注册模型驱动
    
    使用泛型 T 确保类型推断能够透传：
    输入是具体的 Driver 类，返回的也是具体的 Driver 类（保留了具体实现信息）。
    """
    def decorator(cls: T) -> T:
        if name in _DRIVER_REGISTRY:
            logger.warning(f"Driver '{name}' already registered, overwriting with {cls.__name__}")
        _DRIVER_REGISTRY[name] = cls
        logger.debug(f"Registered model driver: {name}")
        return cls
    return decorator


def get_driver_class(name: str) -> Optional[Type[BaseChatModel]]:
    """获取驱动类"""
    return _DRIVER_REGISTRY.get(name)
```

[65] gecko/plugins/registry.py
```python
# gecko/plugins/registry.py  
  
"""  
占位：预留统一插件注册机制的位置。  
  
TODO:  
    - 整合 tools/storage 等注册器  
    - 支持入口点加载第三方插件  
"""  
```

[66] gecko/plugins/storage/__init__.py
```python
# gecko/plugins/storage/__init__.py
"""
Gecko Storage 插件系统

提供统一的接口用于访问 Session (KV) 和 Vector (RAG) 存储。
"""
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.factory import create_storage
from gecko.plugins.storage.interfaces import SessionInterface, VectorInterface
from gecko.plugins.storage.registry import register_storage

__all__ = [
    "AbstractStorage",
    "create_storage",
    "SessionInterface",
    "VectorInterface",
    "register_storage",
]
```

[67] gecko/plugins/storage/abc.py
```python
# gecko/plugins/storage/abc.py
"""
存储抽象基类

定义所有存储后端必须实现的生命周期方法。
确保所有插件都有统一的初始化和关闭流程。
"""
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any, Dict


class AbstractStorage(ABC):
    """
    存储后端抽象基类
    
    所有具体的存储实现（SQLite, Redis, Chroma 等）都必须继承此类，
    并实现异步的生命周期管理方法。
    """
    
    def __init__(self, url: str, **kwargs: Any):
        """
        初始化存储后端配置
        
        参数:
            url: 连接字符串 (例如: sqlite:///./data.db)
            **kwargs: 额外的配置参数
        """
        self.url = url
        self.config = kwargs
        self._is_initialized = False

    @abstractmethod
    async def initialize(self) -> None:
        """
        异步初始化
        
        用于建立数据库连接、创建表结构、检查索引等耗时 IO 操作。
        必须确保此方法是幂等的（多次调用不会出错）。
        """
        pass

    @abstractmethod
    async def shutdown(self) -> None:
        """
        异步关闭
        
        用于释放连接池、关闭文件句柄、清理临时资源。
        """
        pass
    
    @property
    def is_initialized(self) -> bool:
        """检查是否已初始化"""
        return self._is_initialized

    async def __aenter__(self):
        """支持上下文管理器"""
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """退出上下文时自动关闭"""
        await self.shutdown()
```

[68] gecko/plugins/storage/backends/__init__.py
```python
```

[69] gecko/plugins/storage/backends/chroma.py
```python
# gecko/plugins/storage/backends/chroma.py
"""
ChromaDB 存储后端

更新日志：
- [Robustness] 增加 metadata 为 None 的防御性处理。
- [Robustness] 所有操作统一抛出 StorageError，屏蔽底层异常。
- [Feature] 实现 search 方法的 filters 参数支持 (Mapping dict to Chroma `where` clause)。
"""
from __future__ import annotations

from typing import Any, Dict, List, Optional, TYPE_CHECKING

# 仅用于类型检查的导入
if TYPE_CHECKING:
    from chromadb import ClientAPI, Collection # type: ignore

from gecko.core.exceptions import StorageError
from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.interfaces import SessionInterface, VectorInterface
from gecko.plugins.storage.mixins import (
    JSONSerializerMixin,
    ThreadOffloadMixin,
)
from gecko.plugins.storage.registry import register_storage
from gecko.plugins.storage.utils import parse_storage_url

logger = get_logger(__name__)


class IdentityEmbeddingFunction:
    """
    空实现的 Embedding Function。
    用于禁用 Chroma 内置的模型加载，提升性能。
    """
    def __call__(self, input: Any) -> Any:
        return [[0.0] for _ in input]

    def name(self) -> str:
        return "gecko_identity"


@register_storage("chroma")
class ChromaStorage(
    AbstractStorage,
    VectorInterface,
    SessionInterface,
    ThreadOffloadMixin,
    JSONSerializerMixin
):
    def __init__(self, url: str, **kwargs):
        super().__init__(url, **kwargs)
        scheme, path, params = parse_storage_url(url)
        
        self.persist_path = path
        self.collection_name = params.get("collection", "gecko_default")
        
        self.client: Optional[ClientAPI] = None
        self.vector_col: Optional[Collection] = None
        self.session_col: Optional[Collection] = None

    async def initialize(self) -> None:
        if self.is_initialized:
            return

        def _init_sync():
            try:
                import chromadb
                from chromadb.config import Settings

                logger.info("Initializing ChromaDB", path=self.persist_path)
                
                client = chromadb.PersistentClient(
                    path=self.persist_path,
                    settings=Settings(anonymized_telemetry=False)
                )
                
                ef : Any = IdentityEmbeddingFunction()
                
                v_col = client.get_or_create_collection(
                    name=self.collection_name,
                    metadata={"hnsw:space": "cosine"},
                    embedding_function=ef
                )
                
                s_col = client.get_or_create_collection(
                    name=f"{self.collection_name}_sessions",
                    embedding_function=ef
                )
                
                return client, v_col, s_col
            except Exception as e:
                raise StorageError(f"Failed to initialize ChromaDB: {e}") from e

        self.client, self.vector_col, self.session_col = await self._run_sync(_init_sync)
        self._is_initialized = True

    async def shutdown(self) -> None:
        self.client = None
        self.vector_col = None
        self.session_col = None
        self._is_initialized = False

    # ==================== VectorInterface 实现 ====================

    async def upsert(self, documents: List[Dict[str, Any]]) -> None:
        if not documents or not self.vector_col:
            return

        def _sync_upsert():
            try:
                if self.vector_col: 
                    # [修复] Chroma 不接受空字典作为 metadata，必须为 None 或 非空字典
                    metadatas = []
                    for d in documents:
                        m = d.get("metadata")
                        # 如果 m 是 None 或 {}，都转为 None
                        metadatas.append(m if m else None)
                    
                    self.vector_col.upsert(
                        ids=[d["id"] for d in documents],
                        embeddings=[d["embedding"] for d in documents],
                        metadatas=metadatas, # type: ignore
                        documents=[d.get("text", "") for d in documents]
                    )
            except Exception as e:
                raise StorageError(f"Chroma upsert failed: {e}") from e

        await self._run_sync(_sync_upsert)

    async def search(
        self, 
        query_embedding: List[float], 
        top_k: int = 5,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        if not self.vector_col:
            return []

        def _sync_search():
            try:
                if not self.vector_col:
                    return []
                
                # [Feature] 构造 Chroma filter
                # Chroma where 语法: {"field": "value"} 或 {"$and": [...]}
                chroma_filter = None
                if filters:
                    if len(filters) == 1:
                        chroma_filter = filters
                    else:
                        # 多个条件默认为 AND
                        chroma_filter = {"$and": [{k: v} for k, v in filters.items()]}

                results = self.vector_col.query(
                    query_embeddings=[query_embedding],
                    n_results=top_k,
                    where=chroma_filter, # type: ignore
                    include=["metadatas", "documents", "distances"]
                )
                
                parsed_results = []
                if not results["ids"]:
                    return []

                count = len(results["ids"][0])
                for i in range(count):
                    dist = results["distances"][0][i] # type: ignore
                    score = max(0.0, 1.0 - dist)

                    # [Fix] 处理 metadata 为 None 的情况，统一返回 {}
                    raw_meta = results["metadatas"][0][i] # type: ignore
                    meta = raw_meta if raw_meta is not None else {}
                    
                    parsed_results.append({
                        "id": results["ids"][0][i],
                        "text": results["documents"][0][i], # type: ignore
                        "metadata": meta,
                        "score": score
                    })
                
                return parsed_results
            except Exception as e:
                raise StorageError(f"Chroma search failed: {e}") from e

        return await self._run_sync(_sync_search)

    # ==================== SessionInterface 实现 ====================

    async def get(self, session_id: str) -> Optional[Dict[str, Any]]:
        if not self.session_col:
            return None

        def _sync_get():
            try:
                if not self.session_col:
                    return None
                result = self.session_col.get(
                    ids=[session_id],
                    include=["documents"] # type: ignore
                )
                if result["ids"] and result["documents"]:
                    return result["documents"][0]
                return None
            except Exception as e:
                raise StorageError(f"Chroma session get failed: {e}") from e

        json_str = await self._run_sync(_sync_get)
        return self._deserialize(json_str)

    async def set(self, session_id: str, state: Dict[str, Any]) -> None:
        if not self.session_col:
            return

        json_str = self._serialize(state)

        def _sync_set():
            try:
                if not self.session_col:
                    return
                self.session_col.upsert(
                    ids=[session_id],
                    documents=[json_str],
                    metadatas=[{"type": "session_state"}]
                )
            except Exception as e:
                raise StorageError(f"Chroma session set failed: {e}") from e

        await self._run_sync(_sync_set)

    async def delete(self, session_id: str) -> None:
        if not self.session_col:
            return

        def _sync_delete():
            try:
                if not self.session_col:
                    return
                self.session_col.delete(ids=[session_id])
            except Exception as e:
                raise StorageError(f"Chroma session delete failed: {e}") from e
        
        await self._run_sync(_sync_delete)
```

[70] gecko/plugins/storage/backends/lancedb.py
```python
# gecko/plugins/storage/backends/lancedb.py
"""
LanceDB 存储后端

LanceDB 是一个无服务器、基于 Arrow 的高性能向量数据库。
本实现通过 ThreadOffloadMixin 处理文件 I/O。

核心特性：
1. **自动建表**：首次 Upsert 时根据数据结构自动创建表。
2. **线程安全**：I/O 操作隔离。

更新日志：
- [Robustness] 增加 metadata 为 None 的防御性处理。
- [Robustness] 所有操作统一抛出 StorageError。
- [Feature] 实现 search 方法的 filters 参数支持 (SQL-like string construction)。
"""
from __future__ import annotations

from typing import Any, Dict, List, Optional

from gecko.core.exceptions import StorageError
from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.interfaces import VectorInterface
from gecko.plugins.storage.mixins import ThreadOffloadMixin
from gecko.plugins.storage.registry import register_storage
from gecko.plugins.storage.utils import parse_storage_url

logger = get_logger(__name__)


@register_storage("lancedb")
class LanceDBStorage(
    AbstractStorage,
    VectorInterface,
    ThreadOffloadMixin
):
    def __init__(self, url: str, **kwargs):
        super().__init__(url, **kwargs)
        scheme, path, params = parse_storage_url(url)
        
        self.db_path = path
        self.table_name = params.get("table", "gecko_vectors")
        self.embedding_dim = int(params.get("dim", 1536))
        
        self.db: Any = None
        self.table: Any = None

    async def initialize(self) -> None:
        if self.is_initialized:
            return

        def _init_sync():
            try:
                import lancedb
                logger.info("Connecting to LanceDB", path=self.db_path)
                
                self.db = lancedb.connect(self.db_path)
                
                if self.table_name in self.db.table_names():
                    self.table = self.db.open_table(self.table_name)
                    logger.debug(f"Opened existing table: {self.table_name}")
                else:
                    self.table = None
                    logger.debug(f"Table {self.table_name} will be created on first upsert")
            except Exception as e:
                raise StorageError(f"Failed to initialize LanceDB: {e}") from e

        await self._run_sync(_init_sync)
        self._is_initialized = True

    async def shutdown(self) -> None:
        self.db = None
        self.table = None
        self._is_initialized = False

    async def upsert(self, documents: List[Dict[str, Any]]) -> None:
        if not documents:
            return

        def _sync_upsert():
            try:
                data = []
                for doc in documents:
                    # [防御性处理] 确保 metadata 是字典
                    meta = doc.get("metadata")
                    if meta is None:
                        meta = {}
                    
                    item = {
                        "id": doc["id"],
                        "vector": doc["embedding"],
                        "text": doc.get("text", ""),
                        "metadata": meta
                    }
                    data.append(item)

                if self.table is None:
                    try:
                        self.table = self.db.create_table(self.table_name, data=data)
                        logger.info(f"Created LanceDB table: {self.table_name}")
                    except Exception as e:
                        # 处理并发创建冲突
                        if self.table_name in self.db.table_names():
                            self.table = self.db.open_table(self.table_name)
                            self.table.add(data)
                        else:
                            raise e
                else:
                    self.table.add(data)
            except Exception as e:
                raise StorageError(f"LanceDB upsert failed: {e}") from e

        await self._run_sync(_sync_upsert)

    async def search(
        self, 
        query_embedding: List[float], 
        top_k: int = 5,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        if self.table is None:
            return []

        def _sync_search():
            try:
                if self.table is None:
                    return []
                
                # 构建查询
                query = self.table.search(query_embedding).limit(top_k)
                
                # [Feature] 添加 where 子句
                # LanceDB metadata 存储为 struct，查询语法: "metadata.key = 'value'"
                if filters:
                    clauses = []
                    for k, v in filters.items():
                        if isinstance(v, str):
                            clauses.append(f"metadata.{k} = '{v}'")
                        else:
                            clauses.append(f"metadata.{k} = {v}")
                    
                    if clauses:
                        where_str = " AND ".join(clauses)
                        query = query.where(where_str)

                results = query.to_list()
                
                parsed_results = []
                for r in results:
                    distance = r.get("_distance", 0.0)
                    
                    parsed_results.append({
                        "id": r["id"],
                        "text": r["text"],
                        "metadata": r["metadata"],
                        "score": 1.0 / (1.0 + distance) 
                    })
                return parsed_results
            except Exception as e:
                raise StorageError(f"LanceDB search failed: {e}") from e

        return await self._run_sync(_sync_search)
```

[71] gecko/plugins/storage/backends/redis.py
```python
# gecko/plugins/storage/backends/redis.py
"""
Redis 存储后端

基于 redis-py (asyncio) 实现的高性能会话存储。
由于 Redis 客户端原生支持 asyncio，因此不需要使用 ThreadOffloadMixin。

核心特性：
1. **原生异步**：直接利用 asyncio Event Loop，性能极高。
2. **TTL 管理**：支持会话自动过期。

更新日志：
- [Robustness] 初始化失败时自动清理资源。
- [Robustness] 所有操作统一抛出 StorageError，屏蔽底层 redis 异常。
"""
from __future__ import annotations

from typing import Any, Dict, Optional

try:
    import redis.asyncio as redis
except ImportError:
    redis = None  # type: ignore

from gecko.core.exceptions import StorageError
from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.interfaces import SessionInterface
from gecko.plugins.storage.mixins import JSONSerializerMixin
from gecko.plugins.storage.registry import register_storage
from gecko.plugins.storage.utils import parse_storage_url

logger = get_logger(__name__)


@register_storage("redis")
class RedisStorage(
    AbstractStorage,
    SessionInterface,
    JSONSerializerMixin
):
    def __init__(self, url: str, **kwargs):
        super().__init__(url, **kwargs)
        if redis is None:
            raise ImportError(
                "Redis client not installed. Please install: pip install redis"
            )
        
        scheme, path, params = parse_storage_url(url)
        
        try:
            self.ttl = int(params.get("ttl", 3600 * 24 * 7))
        except ValueError:
            self.ttl = 3600 * 24 * 7
            
        self.prefix = kwargs.get("prefix", "gecko:session:")
        self.client: Optional[redis.Redis] = None # type: ignore

    async def initialize(self) -> None:
        if self.is_initialized:
            return

        logger.info("Connecting to Redis", url=self.url)
        
        try:
            self.client = redis.from_url( # type: ignore
                self.url,
                decode_responses=True,
                encoding="utf-8"
            )
            if self.client:
                await self.client.ping()
            self._is_initialized = True
            logger.debug("Redis connected successfully")
        except Exception as e:
            logger.error("Failed to connect to Redis", error=str(e))
            # 初始化失败时尝试清理
            await self.shutdown()
            raise StorageError(f"Failed to connect to Redis: {e}") from e

    async def shutdown(self) -> None:
        if self.client:
            try:
                await self.client.aclose()
            except Exception as e:
                logger.warning(f"Error closing Redis connection: {e}")
            finally:
                self.client = None
        self._is_initialized = False

    async def get(self, session_id: str) -> Optional[Dict[str, Any]]:
        if not self.client:
            raise StorageError("RedisStorage not initialized")
            
        key = f"{self.prefix}{session_id}"
        try:
            data = await self.client.get(key)
            return self._deserialize(data)
        except Exception as e:
            logger.error("Redis get failed", session_id=session_id, error=str(e))
            raise StorageError(f"Redis get failed: {e}") from e

    async def set(self, session_id: str, state: Dict[str, Any]) -> None:
        if not self.client:
            raise StorageError("RedisStorage not initialized")
            
        key = f"{self.prefix}{session_id}"
        json_str = self._serialize(state)
        
        try:
            await self.client.setex(key, self.ttl, json_str)
        except Exception as e:
            logger.error("Redis set failed", session_id=session_id, error=str(e))
            raise StorageError(f"Redis set failed: {e}") from e

    async def delete(self, session_id: str) -> None:
        if not self.client:
            raise StorageError("RedisStorage not initialized")
            
        key = f"{self.prefix}{session_id}"
        try:
            await self.client.delete(key)
        except Exception as e:
            logger.error("Redis delete failed", session_id=session_id, error=str(e))
            raise StorageError(f"Redis delete failed: {e}") from e
```

[72] gecko/plugins/storage/backends/sqlite.py
```python
# gecko/plugins/storage/backends/sqlite.py
"""
SQLite 存储后端 (高并发优化版)

优化策略：
1. Mixin 组合：继承 ThreadOffloadMixin 防止阻塞 Loop，继承 AtomicWriteMixin 防止写冲突。
2. WAL 模式：开启 Write-Ahead Logging，大幅提升读写并发性能。
3. 线程安全：配置 check_same_thread=False 以支持在线程池中使用连接。
4. 进程安全：启用 FileLock 防止多进程环境下的 Database is locked 错误。

更新日志：
- [Arch] 初始化时配置 FileLock。
- [Robustness] 所有操作统一抛出 StorageError。
- [Perf] 保持 WAL 模式优化。
"""
from __future__ import annotations

import os
from pathlib import Path
from typing import Any, Dict, Optional

from sqlalchemy import text
from sqlmodel import Field, Session, SQLModel, create_engine, select

from gecko.core.exceptions import StorageError
from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
from gecko.plugins.storage.interfaces import SessionInterface
from gecko.plugins.storage.mixins import (
    AtomicWriteMixin,
    JSONSerializerMixin,
    ThreadOffloadMixin,
)
from gecko.plugins.storage.utils import parse_storage_url

logger = get_logger(__name__)


class SessionModel(SQLModel, table=True):
    """SQLModel 表定义"""
    __tablename__ = "gecko_sessions" # type: ignore
    session_id: str = Field(primary_key=True)
    state_json: str = Field(default="{}")


class SQLiteStorage(
    AbstractStorage,
    SessionInterface,
    ThreadOffloadMixin,
    AtomicWriteMixin,
    JSONSerializerMixin
):
    def __init__(self, url: str, **kwargs):
        super().__init__(url, **kwargs)
        
        scheme, path, params = parse_storage_url(url)
        
        self.db_path = path
        self.is_memory = path == ":memory:"
        
        try:
            if not self.is_memory:
                # 确保父目录存在
                db_file = Path(path)
                if not db_file.parent.exists():
                    db_file.parent.mkdir(parents=True, exist_ok=True)
                
                # [架构优化] 配置跨进程文件锁
                self.setup_multiprocess_lock(self.db_path)
            
            if self.is_memory:
                sqlalchemy_url = "sqlite:///:memory:"
            else:
                sqlalchemy_url = f"sqlite:///{self.db_path}"

            self.engine = create_engine(
                sqlalchemy_url,
                connect_args={"check_same_thread": False, "timeout": 30},
            )
        except Exception as e:
            raise StorageError(f"Failed to configure SQLite: {e}") from e

    async def initialize(self) -> None:
        if self.is_initialized:
            return

        try:
            await self._run_sync(SQLModel.metadata.create_all, self.engine)
            
            if not self.is_memory:
                def _enable_wal():
                    with self.engine.connect() as conn:
                        conn.execute(text("PRAGMA journal_mode=WAL;"))
                        conn.execute(text("PRAGMA synchronous=NORMAL;"))
                
                await self._run_sync(_enable_wal)
                logger.debug("SQLite WAL mode enabled", path=self.db_path)
                
            self._is_initialized = True
            logger.info("SQLite storage initialized", url=self.url)
        except Exception as e:
            raise StorageError(f"Failed to initialize SQLite: {e}") from e

    async def shutdown(self) -> None:
        try:
            self.engine.dispose()
        except Exception as e:
            logger.warning(f"Error during SQLite shutdown: {e}")
        finally:
            self._is_initialized = False

    async def get(self, session_id: str) -> Optional[Dict[str, Any]]:
        def _sync_get():
            try:
                with Session(self.engine) as session:
                    statement = select(SessionModel).where(
                        SessionModel.session_id == session_id
                    )
                    result = session.exec(statement).first()
                    return result.state_json if result else None
            except Exception as e:
                raise e

        try:
            json_str = await self._run_sync(_sync_get)
            return self._deserialize(json_str)
        except Exception as e:
            raise StorageError(f"SQLite get failed: {e}") from e

    async def set(self, session_id: str, state: Dict[str, Any]) -> None:
        json_str = self._serialize(state)

        def _sync_set():
            # [修复] 在 Worker 线程内部获取文件锁
            with self.file_lock_guard():
                try:
                    with Session(self.engine) as session:
                        statement = select(SessionModel).where(
                            SessionModel.session_id == session_id
                        )
                        existing = session.exec(statement).first()
                        
                        if existing:
                            existing.state_json = json_str
                            session.add(existing)
                        else:
                            new_rec = SessionModel(
                                session_id=session_id, 
                                state_json=json_str
                            )
                            session.add(new_rec)
                        session.commit()
                except Exception as e:
                    raise e

        try:
            # 获取协程锁 -> 切换线程 -> 获取文件锁 -> 操作 -> 释放
            async with self.write_guard():
                await self._run_sync(_sync_set)
        except Exception as e:
            raise StorageError(f"SQLite set failed: {e}") from e

    async def delete(self, session_id: str) -> None:
        def _sync_delete():
            # [修复] 在 Worker 线程内部获取文件锁
            with self.file_lock_guard():
                try:
                    with Session(self.engine) as session:
                        statement = select(SessionModel).where(
                            SessionModel.session_id == session_id
                        )
                        result = session.exec(statement).first()
                        if result:
                            session.delete(result)
                            session.commit()
                except Exception as e:
                    raise e
        
        try:
            async with self.write_guard():
                await self._run_sync(_sync_delete)
        except Exception as e:
            raise StorageError(f"SQLite delete failed: {e}") from e
```

[73] gecko/plugins/storage/factory.py
```python
# gecko/plugins/storage/factory.py
"""
存储工厂

负责解析 URL，自动加载对应的后端模块，实例化并初始化存储对象。

更新日志：
- [Arch] 引入 importlib.metadata (entry_points) 支持第三方插件自动发现。
- [Refactor] 优化加载顺序：Entry Points -> Built-in -> Registry Check。
"""
from __future__ import annotations

import importlib
import inspect
import sys
# 引入 metadata 用于发现插件
from importlib.metadata import entry_points
from typing import Any, Optional, Type

from gecko.core.exceptions import ConfigurationError
from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage
# [重要] 导入模块本身，防止闭包引用陈旧的 _STORAGE_REGISTRY 字典
import gecko.plugins.storage.registry as storage_registry
from gecko.plugins.storage.utils import parse_storage_url

logger = get_logger(__name__)

# 模块映射表：Scheme -> Module Path (内置后端)
_BACKEND_MODULES = {
    "sqlite": "gecko.plugins.storage.backends.sqlite",
    "redis": "gecko.plugins.storage.backends.redis",
    "chroma": "gecko.plugins.storage.backends.chroma",
    "lancedb": "gecko.plugins.storage.backends.lancedb",
    "postgres": "gecko.plugins.storage.backends.postgres",
    "qdrant": "gecko.plugins.storage.backends.qdrant",
    "milvus": "gecko.plugins.storage.backends.milvus",
}

# 插件组名称
PLUGIN_GROUP = "gecko.storage.backends"


def _load_from_entry_point(scheme: str) -> bool:
    """
    尝试从 Entry Points 加载插件
    
    返回: 是否成功加载
    """
    # 兼容 Python 3.10+ 和旧版本的 entry_points API
    try:
        # Python 3.10+
        eps = entry_points(group=PLUGIN_GROUP)
    except TypeError:
        # Python 3.8/3.9: entry_points() 返回字典
        eps = entry_points().get(PLUGIN_GROUP, []) # type: ignore

    # 查找匹配 scheme 的插件
    # 假设 entry point name 即为 scheme (例如: "mongo = my_package.mongo:MongoStorage")
    target_ep = next((ep for ep in eps if ep.name == scheme), None)

    if target_ep:
        try:
            logger.info(f"Loading storage plugin via entry point: {scheme}")
            # load() 会导入模块并返回对象 (类或函数)
            # 导入模块时，其内部的 @register_storage 装饰器应该会被触发
            target_ep.load()
            return True
        except Exception as e:
            logger.error(f"Failed to load plugin for '{scheme}': {e}")
            # 这里不抛出异常，允许回退到内置模块
            return False
    
    return False


async def create_storage(url: str, **kwargs: Any) -> AbstractStorage:
    """
    创建并初始化存储后端
    
    流程：
    1. 解析 URL 获取协议 (scheme)。
    2. 检查注册表是否已有对应类。
    3. 如果没有，按优先级尝试动态加载：
       a. Entry Points (第三方插件)
       b. Built-in Modules (内置支持)
    4. 再次检查注册表。
    5. 回退机制：手动扫描模块。
    6. 实例化并异步初始化。
    
    参数:
        url: 存储 URL (e.g., "sqlite:///data.db")
        **kwargs: 传递给存储后端的额外参数
    
    返回:
        已初始化的存储对象
        
    异常:
        ConfigurationError: 无法加载或初始化存储后端
    """
    try:
        scheme, _, _ = parse_storage_url(url)
        # 处理特殊变体 (如 postgres+pgvector -> postgres)
        clean_scheme = scheme.split("+")[0]
    except Exception as e:
        raise ConfigurationError(f"Invalid storage URL: {e}") from e

    # 1. 尝试从注册表获取
    cls: Optional[Type[AbstractStorage]] = storage_registry.get_storage_class(scheme)

    # 2. 如果未注册，尝试动态加载
    if not cls:
        module: Any = None
        loaded = False

        # 2a. 优先尝试 Entry Points (允许插件覆盖内置)
        if _load_from_entry_point(clean_scheme):
            loaded = True
        
        # 2b. 如果插件未处理，尝试内置模块
        if not loaded:
            module_path = _BACKEND_MODULES.get(clean_scheme)
            if not module_path:
                # 如果既没有插件也没有内置支持
                raise ConfigurationError(
                    f"Unknown storage scheme: '{scheme}'. "
                    f"Supported built-ins: {list(_BACKEND_MODULES.keys())}, "
                    f"Plugins: Check '{PLUGIN_GROUP}' entry points."
                )
            
            try:
                logger.debug("Lazy loading built-in backend", module=module_path)
                
                # [Fix] 增加对 None 的检查。
                # 如果测试 Mock 将模块置为 None，或者模块加载失败残留为 None，应视为未加载，
                # 从而进入 import_module 尝试加载（并触发预期的 ImportError）。
                if module_path in sys.modules and sys.modules[module_path] is not None:
                    module = importlib.reload(sys.modules[module_path])
                else:
                    module = importlib.import_module(module_path)
                
                loaded = True
            except ImportError as e:
                raise ConfigurationError(
                    f"Failed to load built-in backend for '{scheme}'.\n"
                    f"Missing dependency? Try installing: pip install gecko-ai[{clean_scheme}]\n"
                    f"Error: {e}"
                ) from e
        
        # 3. 再次尝试从注册表获取
        cls = storage_registry.get_storage_class(scheme)
        
        # 4. 回退机制：手动扫描模块 (针对内置模块加载后装饰器失效的罕见情况)
        if not cls and module:
            logger.warning(
                f"Registry lookup failed for {scheme}, scanning module..."
            )
            for name, obj in inspect.getmembers(module):
                if (inspect.isclass(obj) 
                    and issubclass(obj, AbstractStorage) 
                    and obj is not AbstractStorage):
                    
                    cls = obj
                    # 手动补注册
                    storage_registry._STORAGE_REGISTRY[scheme] = cls
                    logger.info(f"Manually registered {name} for {scheme}")
                    break
            
        if not cls:
            current_keys = list(storage_registry._STORAGE_REGISTRY.keys())
            raise ConfigurationError(
                f"Backend loaded but no storage class registered for '{scheme}'.\n"
                f"Current registry keys: {current_keys}"
            )

    # 5. 实例化与初始化
    instance: Optional[AbstractStorage] = None
    
    try:
        instance = cls(url, **kwargs)
        await instance.initialize()
        return instance
    except Exception as e:
        if instance:
            await instance.shutdown()
        raise ConfigurationError(f"Failed to initialize {cls.__name__}: {e}") from e
```

[74] gecko/plugins/storage/interfaces.py
```python
# gecko/plugins/storage/interfaces.py
"""
业务接口定义

定义 Session（会话存储）和 Vector（向量存储）的标准行为。

更新日志：
- [Feat] VectorInterface.search 增加 filters 参数，支持元数据过滤。
"""
from __future__ import annotations

from abc import abstractmethod
from typing import Any, Dict, List, Optional, Protocol, runtime_checkable


@runtime_checkable
class SessionInterface(Protocol):
    """
    Session 存储接口协议
    
    负责 Agent 的短期记忆（Conversation History）和状态（State）的持久化。
    """
    
    @abstractmethod
    async def get(self, session_id: str) -> Optional[Dict[str, Any]]:
        """
        获取会话状态
        
        参数:
            session_id: 会话唯一标识
            
        返回:
            状态字典，如果不存在返回 None
        """
        ...

    @abstractmethod
    async def set(self, session_id: str, state: Dict[str, Any]) -> None:
        """
        设置/更新会话状态
        
        参数:
            session_id: 会话唯一标识
            state: 要保存的状态字典
        """
        ...

    @abstractmethod
    async def delete(self, session_id: str) -> None:
        """
        删除会话
        
        参数:
            session_id: 会话唯一标识
        """
        ...


@runtime_checkable
class VectorInterface(Protocol):
    """
    Vector 存储接口协议 (RAG 用)
    
    负责文档的向量存储与检索。
    """
    
    @abstractmethod
    async def upsert(self, documents: List[Dict[str, Any]]) -> None:
        """
        插入或更新向量文档
        
        参数:
            documents: 文档列表，每项需包含 id, embedding, text, metadata
        """
        ...

    @abstractmethod
    async def search(
        self, 
        query_embedding: List[float], 
        top_k: int = 5,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        向量相似度搜索
        
        参数:
            query_embedding: 查询向量
            top_k: 返回结果数量
            filters: 元数据过滤条件 (Key-Value 精确匹配)
            
        返回:
            包含 text, metadata, score 的结果列表
        """
        ...
```

[75] gecko/plugins/storage/mixins.py
```python
# gecko/plugins/storage/mixins.py
"""
存储功能混入类 (Mixins)

包含解决异步阻塞、并发冲突和数据序列化的通用逻辑。

更新日志：
- [Arch] 引入 FileLock 实现跨进程文件锁，解决多进程环境下 SQLite/文件存储的并发安全问题。
- [Fix] 分离 asyncio.Lock (write_guard) 和 FileLock (file_lock_guard) 以避免死锁。
"""
from __future__ import annotations

import asyncio
import json
from contextlib import asynccontextmanager, contextmanager
from functools import partial
from typing import Any, Callable, TypeVar, Optional

from anyio import to_thread

from gecko.core.logging import get_logger

logger = get_logger(__name__)

# 尝试导入 filelock，用于跨进程文件锁
try:
    from filelock import FileLock
    FILELOCK_AVAILABLE = True
except ImportError:
    FILELOCK_AVAILABLE = False

T = TypeVar("T")


class ThreadOffloadMixin:
    """
    [核心] 线程卸载混入类
    
    将同步的 IO 操作（如 sqlite3, chromadb, pandas 操作）卸载到
    独立的线程池中执行，防止阻塞主线程的 Event Loop。
    
    原理: 使用 anyio.to_thread.run_sync
    """
    
    async def _run_sync(self, func: Callable[..., T], *args: Any, **kwargs: Any) -> T:
        """
        在线程池中执行同步函数
        
        参数:
            func: 同步函数
            *args, **kwargs: 传递给函数的参数
            
        返回:
            函数执行结果
        """
        if kwargs:
            func = partial(func, **kwargs)
        
        return await to_thread.run_sync(func, *args)


class AtomicWriteMixin:
    """
    [核心] 原子写混入类 (增强版)
    
    提供双层锁机制：
    1. asyncio.Lock: 保证单进程内协程的互斥 (write_guard)。
    2. FileLock: 保证跨进程（如 Gunicorn Worker）对同一文件的互斥访问 (file_lock_guard)。
    
    使用方法：
        1. 在子类 __init__ 中调用 self.setup_multiprocess_lock(path) 启用文件锁。
        2. 在 async 方法中由于 await _run_sync 调用前使用 async with self.write_guard()。
        3. 在 _run_sync 调用的同步函数内部使用 with self.file_lock_guard()。
    """
    
    _write_lock: Optional[asyncio.Lock] = None
    _file_lock: Any = None

    @property
    def write_lock(self) -> asyncio.Lock:
        """懒加载获取协程锁"""
        if getattr(self, "_write_lock", None) is None:
            self._write_lock = asyncio.Lock()
        return self._write_lock # type: ignore

    def setup_multiprocess_lock(self, lock_path: str) -> None:
        """
        配置跨进程文件锁
        
        参数:
            lock_path: 锁文件路径（通常是数据库文件路径）
        """
        if not FILELOCK_AVAILABLE:
            logger.warning(
                "filelock module not installed. Cross-process safety is NOT guaranteed. "
                "Install with: pip install filelock"
            )
            return

        try:
            # 创建 .lock 文件
            self._file_lock = FileLock(f"{lock_path}.lock") # type: ignore
            logger.debug("FileLock initialized", path=f"{lock_path}.lock")
        except Exception as e:
            logger.error("Failed to initialize FileLock", error=str(e))

    @asynccontextmanager
    async def write_guard(self):
        """
        写操作保护上下文 (Async)
        
        只负责进程内协程互斥。
        """
        async with self.write_lock:
            yield

    @contextmanager
    def file_lock_guard(self):
        """
        跨进程文件锁 (Sync)
        
        必须在 Worker 线程（_run_sync 调用的函数）内部使用。
        确保 acquire/release 在同一个线程中执行，避免 Reentrancy 问题。
        """
        if self._file_lock:
            with self._file_lock:
                yield
        else:
            yield


class JSONSerializerMixin:
    """
    JSON 序列化混入类
    
    提供标准化的数据序列化/反序列化方法。
    """
    
    def _serialize(self, data: Any) -> str:
        """序列化为 JSON 字符串"""
        try:
            # ensure_ascii=False 减少体积并保持中文可读性
            return json.dumps(data, ensure_ascii=False)
        except (TypeError, ValueError) as e:
            logger.error("Serialization failed", error=str(e))
            raise

    def _deserialize(self, data: str | bytes | None) -> Any:
        """从 JSON 字符串反序列化"""
        if not data:
            return None
        try:
            return json.loads(data)
        except json.JSONDecodeError as e:
            logger.error("Deserialization failed", error=str(e))
            return None
```

[76] gecko/plugins/storage/registry.py
```python
# gecko/plugins/storage/registry.py
"""
存储插件注册器

负责管理 URL Scheme 到存储后端类的映射。
采用装饰器模式进行注册。
"""
from __future__ import annotations

from typing import Callable, Dict, Type

from gecko.core.logging import get_logger
from gecko.plugins.storage.abc import AbstractStorage

logger = get_logger(__name__)

# 存储后端类注册表
# Key: URL scheme (e.g., "sqlite", "redis")
# Value: Storage Class
_STORAGE_REGISTRY: Dict[str, Type[AbstractStorage]] = {}


def register_storage(scheme: str) -> Callable[[Type[AbstractStorage]], Type[AbstractStorage]]:
    """
    装饰器：注册存储后端实现
    
    参数:
        scheme: URL 协议前缀 (如 'sqlite', 'redis')
    
    示例:
        @register_storage("redis")
        class RedisStorage(AbstractStorage):
            ...
    """
    def decorator(cls: Type[AbstractStorage]) -> Type[AbstractStorage]:
        if scheme in _STORAGE_REGISTRY:
            logger.warning(
                "Storage scheme already registered, overwriting",
                scheme=scheme,
                existing=_STORAGE_REGISTRY[scheme].__name__,
                new=cls.__name__
            )
        
        _STORAGE_REGISTRY[scheme] = cls
        logger.debug("Registered storage backend", scheme=scheme, cls=cls.__name__)
        return cls
    
    return decorator


def get_storage_class(scheme: str) -> Type[AbstractStorage] | None:
    """获取已注册的存储类"""
    return _STORAGE_REGISTRY.get(scheme)
```

[77] gecko/plugins/storage/utils.py
```python
# gecko/plugins/storage/utils.py
"""
Storage 插件工具函数

提供统一的 URL 解析和验证逻辑。
"""
from __future__ import annotations

from urllib.parse import parse_qs, urlparse
from typing import Dict, Tuple, Optional


def parse_storage_url(url: str) -> Tuple[str, str, Dict[str, str]]:
    """
    解析存储 URL
    
    格式：scheme://path?param1=value1&param2=value2
    
    返回：(scheme, path, params)
    
    示例:
        "sqlite:///./data.db" -> ("sqlite", "./data.db", {})
        "sqlite://:memory:" -> ("sqlite", ":memory:", {})
    """
    if "://" not in url:
        raise ValueError(f"Invalid storage URL: '{url}'. Must include scheme.")
    
    parsed = urlparse(url)
    scheme = parsed.scheme
    
    # 解析路径
    if scheme == "sqlite":
        # 特殊处理 sqlite
        # sqlite:///foo.db -> path = /foo.db (urlparse behavior)
        # 我们需要去掉开头的 / 变成相对路径，或者保留绝对路径
        if parsed.netloc:
            # sqlite://:memory: -> netloc=':memory:'
            path = parsed.netloc
        else:
            # sqlite:///./data.db -> path='/./data.db'
            path = parsed.path
            if path.startswith("/") and len(path) > 1 and path[1] == ".":
                # 修正相对路径 /./data.db -> ./data.db
                path = path[1:]
            elif path.startswith("/"):
                 # 绝对路径保持不变，或者根据 OS 调整
                 # 这里简化处理，对于 Windows 可能需要更复杂的逻辑
                 pass
    else:
        path = f"{parsed.netloc}{parsed.path}"

    # 解析参数
    params: Dict[str, str] = {}
    if parsed.query:
        query_dict = parse_qs(parsed.query)
        params = {k: v[0] for k, v in query_dict.items()}
    
    return scheme, path, params
```

[78] gecko/plugins/tools/__init__.py
```python
# gecko/plugins/tools/__init__.py
"""
Gecko 工具插件系统

提供工具的定义、注册和发现机制。

核心组件：
- BaseTool: 所有工具的基类
- ToolResult: 标准化执行结果
- register_tool: 工具注册装饰器
- load_tool: 工具加载工厂
- ToolRegistry: 注册表管理类
"""
from gecko.plugins.tools.base import BaseTool, ToolResult
from gecko.plugins.tools.registry import ToolRegistry, load_tool, register_tool

__all__ = [
    "BaseTool",
    "ToolResult",
    "ToolRegistry",
    "register_tool",
    "load_tool",
]
```

[79] gecko/plugins/tools/base.py
```python
# gecko/plugins/tools/base.py
"""
工具基类定义

核心功能：
1. 定义统一的工具接口 BaseTool
2. 强制参数校验 (Pydantic)
3. 自动生成 OpenAI Function Schema
4. 统一的错误处理与结果封装
"""
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any, Dict, Type, Union

from pydantic import BaseModel, Field, ValidationError

from gecko.core.utils import ensure_awaitable


class ToolResult(BaseModel):
    """工具执行结果封装"""
    content: str = Field(..., description="工具执行的文本输出")
    is_error: bool = Field(default=False, description="是否执行出错")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="额外的元数据")


class BaseTool(BaseModel, ABC):
    """
    工具抽象基类
    
    所有自定义工具必须继承此类，并提供 args_schema 用于参数校验。
    
    示例:
        ```python
        class MyArgs(BaseModel):
            query: str
            
        class MyTool(BaseTool):
            name = "my_tool"
            description = "My awesome tool"
            args_schema = MyArgs
            
            async def _run(self, args: MyArgs) -> ToolResult:
                return ToolResult(content=f"Echo: {args.query}")
        ```
    """
    name: str = Field(..., description="工具唯一标识名称 (e.g., 'calculator')")
    description: str = Field(..., description="工具功能描述，用于 LLM 决策")
    
    # 排除 args_schema 不参与 BaseTool 本身的序列化，仅用于元编程
    args_schema: Type[BaseModel] = Field(..., exclude=True, description="参数定义的 Pydantic 模型类")

    @property
    def openai_schema(self) -> Dict[str, Any]:
        """
        自动生成 OpenAI Function Calling Schema
        """
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": self.args_schema.model_json_schema(),
            }
        }
    
    # 兼容旧代码的属性
    @property
    def parameters(self) -> Dict[str, Any]:
        return self.args_schema.model_json_schema()

    async def execute(self, arguments: Dict[str, Any]) -> ToolResult:
        """
        执行工具（模板方法）
        
        负责：
        1. 参数校验
        2. 异常捕获
        3. 调用具体的 _run 实现
        """
        try:
            # 1. Pydantic 校验
            validated_args = self.args_schema(**arguments)
        except ValidationError as e:
            return ToolResult(
                content=f"参数校验错误: {str(e)}",
                is_error=True
            )
        except Exception as e:
            return ToolResult(
                content=f"参数解析错误: {str(e)}",
                is_error=True
            )

        try:
            # 2. 执行业务逻辑 (支持同步/异步)
            result = await ensure_awaitable(self._run, validated_args)
            
            # 3. 结果标准化
            if isinstance(result, ToolResult):
                return result
            return ToolResult(content=str(result))
            
        except Exception as e:
            return ToolResult(
                content=f"工具执行内部错误: {str(e)}",
                is_error=True
            )

    @abstractmethod
    async def _run(self, args: BaseModel) -> Union[ToolResult, str]:
        """
        工具的具体实现逻辑
        
        参数:
            args: 已校验的 Pydantic 对象
        """
        pass
```

[80] gecko/plugins/tools/registry.py
```python
# gecko/plugins/tools/registry.py
"""
工具注册表

提供工具的自动发现、注册与工厂化创建能力。
"""
from __future__ import annotations

from typing import Dict, Type, List, Any, Optional

from gecko.core.logging import get_logger
from gecko.plugins.tools.base import BaseTool

logger = get_logger(__name__)


class ToolRegistry:
    """全局工具注册中心"""
    
    _registry: Dict[str, Type[BaseTool]] = {}

    @classmethod
    def register(cls, name: str):
        """
        装饰器：注册工具类
        
        示例:
            @register_tool("my_tool")
            class MyTool(BaseTool): ...
        """
        def decorator(tool_cls: Type[BaseTool]):
            if not issubclass(tool_cls, BaseTool):
                raise TypeError(f"Registered class {tool_cls.__name__} must inherit from BaseTool")
            
            if name in cls._registry:
                logger.warning(f"Tool '{name}' is being overwritten in registry.")
            
            cls._registry[name] = tool_cls
            logger.debug(f"Tool registered: {name} -> {tool_cls.__name__}")
            return tool_cls
        return decorator

    @classmethod
    def load_tool(cls, name: str, **kwargs: Any) -> BaseTool:
        """
        工厂方法：根据名称加载并实例化工具
        
        参数:
            name: 工具名称
            **kwargs: 传递给工具构造函数的参数
            
        异常:
            ValueError: 工具未找到
        """
        if name not in cls._registry:
            raise ValueError(f"Tool '{name}' not found in registry. Available: {list(cls._registry.keys())}")
        
        tool_cls = cls._registry[name]
        try:
            # 实例化工具
            # 注意：BaseTool 是 Pydantic 模型，kwargs 会被 validate
            return tool_cls(**kwargs)
        except Exception as e:
            raise ValueError(f"Failed to instantiate tool '{name}': {e}") from e

    @classmethod
    def list_tools(cls) -> List[str]:
        """列出所有已注册工具"""
        return list(cls._registry.keys())


# 便捷导出
register_tool = ToolRegistry.register
load_tool = ToolRegistry.load_tool
```

[81] gecko/plugins/tools/standard/__init__.py
```python
# gecko/plugins/tools/standard/__init__.py
"""
Gecko 标准工具库

包含一组经过安全审查和优化的内置工具。
导入此模块时，会自动将工具注册到 ToolRegistry。
"""
from gecko.plugins.tools.standard.calculator import CalculatorTool
from gecko.plugins.tools.standard.duckduckgo import DuckDuckGoSearchTool

# 可以在此定义 lazy_load 逻辑，目前为了简单直接导入以触发注册
__all__ = [
    "CalculatorTool",
    "DuckDuckGoSearchTool",
]
```

[82] gecko/plugins/tools/standard/calculator.py
```python
# gecko/plugins/tools/standard/calculator.py
"""
安全计算器工具

基于 AST 解析的安全数学表达式计算，防止代码注入。
"""
from __future__ import annotations

import ast
import math
import operator
from typing import Type, Union, Dict

from pydantic import BaseModel, Field

from gecko.plugins.tools.base import BaseTool, ToolResult
from gecko.plugins.tools.registry import register_tool


class CalculatorArgs(BaseModel):
    expression: str = Field(
        ..., 
        description="数学表达式，支持加减乘除、幂运算及常用数学函数 (sqrt, log, sin, etc.)。例如: '2 + 2 * sqrt(4)'"
    )


@register_tool("calculator")
class CalculatorTool(BaseTool):
    name: str = "calculator"
    description: str = "用于执行精确的数学计算。"
    args_schema: Type[BaseModel] = CalculatorArgs

    async def _run(self, args: CalculatorArgs) -> ToolResult: # type: ignore
        expr = args.expression.strip()
        
        # 长度限制防止 DoS
        if len(expr) > 500:
             return ToolResult(content="错误：表达式过长", is_error=True)

        try:
            result = self._safe_eval(expr)
            return ToolResult(content=str(result))
        except Exception as e:
            return ToolResult(content=f"计算错误: {str(e)}", is_error=True)

    def _safe_eval(self, expr: str) -> Union[int, float]:
        """
        基于 AST 的安全求值
        仅允许特定的节点类型和函数。
        """
        # 支持的操作符
        operators = {
            ast.Add: operator.add,
            ast.Sub: operator.sub,
            ast.Mult: operator.mul,
            ast.Div: operator.truediv,
            ast.Pow: operator.pow,
            ast.BitXor: operator.xor,
            ast.USub: operator.neg,
        }

        # 支持的函数
        functions = {
            "sqrt": math.sqrt,
            "log": math.log,
            "ln": math.log,
            "sin": math.sin,
            "cos": math.cos,
            "tan": math.tan,
            "abs": abs,
            "round": round,
            "ceil": math.ceil,
            "floor": math.floor,
            "pi": math.pi,
            "e": math.e,
        }

        def _eval(node):
            # 1. 数字
            if isinstance(node, ast.Constant): 
                if isinstance(node.value, (int, float)):
                    return node.value
                raise ValueError(f"不支持的常量类型: {type(node.value)}")

            # 2. 二元运算 (a + b)
            elif isinstance(node, ast.BinOp):
                op_type = type(node.op)
                if op_type in operators:
                    left = _eval(node.left)
                    right = _eval(node.right)

                    # [新增] 安全防御：限制幂运算的指数大小
                    if op_type == ast.Pow:
                        # 检查指数是否为数字且过大 (例如限制为 1000)
                        if isinstance(right, (int, float)) and right > 1000:
                            raise ValueError(f"指数过大: {right} (最大允许 1000)")

                    return operators[op_type](left, right)
            
                raise ValueError(f"不支持的操作符: {op_type}")

            # 3. 一元运算 (-a)
            elif isinstance(node, ast.UnaryOp):
                op_type = type(node.op)
                if op_type in operators:
                    return operators[op_type](_eval(node.operand))
                raise ValueError(f"不支持的一元操作符: {op_type}")

            # 4. 函数调用 (sqrt(4))
            elif isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name):
                    func_name = node.func.id
                    if func_name in functions:
                        # 递归计算参数
                        args = [_eval(arg) for arg in node.args]
                        return functions[func_name](*args)
                    raise ValueError(f"禁止调用的函数: {func_name}")
                raise ValueError("不支持的复杂函数调用")

            # 5. 变量名 (pi, e)
            elif isinstance(node, ast.Name):
                if node.id in functions:
                    val = functions[node.id]
                    if isinstance(val, (int, float)):
                        return val
                raise ValueError(f"未知变量: {node.id}")
            
            # [优化建议] 显式拦截属性访问和下标访问，给出更明确的提示
            elif isinstance(node, (ast.Attribute, ast.Subscript, ast.List, ast.Dict, ast.Tuple)):
                raise ValueError("出于安全考虑，禁止使用属性访问、下标或复杂数据结构")

            raise ValueError(f"非法表达式结构: {type(node)}")

        # 解析并求值
        tree = ast.parse(expr, mode='eval')
        return _eval(tree.body)
```

[83] gecko/plugins/tools/standard/duckduckgo.py
```python
# gecko/plugins/tools/standard/duckduckgo.py
from __future__ import annotations

from typing import Any, Dict, List, Type

from anyio.to_thread import run_sync
from pydantic import BaseModel, Field

from gecko.core.logging import get_logger
from gecko.plugins.tools.base import BaseTool, ToolResult
from gecko.plugins.tools.registry import register_tool

logger = get_logger(__name__)


class DuckDuckGoArgs(BaseModel):
    query: str = Field(..., description="搜索关键词", min_length=1, max_length=200)
    max_results: int = Field(default=5, description="返回的最大结果数量 (1-10)", ge=1, le=10)


@register_tool("duckduckgo_search")
class DuckDuckGoSearchTool(BaseTool):
    name: str = "duckduckgo_search"
    description: str = (
        "使用 DuckDuckGo 搜索引擎搜索互联网信息。"
        "当需要获取实时新闻、具体事实或不知道的信息时使用。"
        "无需 API Key。"
    )
    args_schema: Type[BaseModel] = DuckDuckGoArgs

    async def _run(self, args: DuckDuckGoArgs) -> ToolResult: # type: ignore
        # 检查依赖
        try:
            from duckduckgo_search import DDGS
        except ImportError:
            return ToolResult(
                content=(
                    "错误：未安装 duckduckgo_search 库。\n"
                    "请运行: pip install duckduckgo-search"
                ),
                is_error=True
            )

        query = args.query.strip()
        
        # 定义同步执行函数
        def _search_sync() -> List[Dict[str, str]]:
            results = []
            try:
                # 使用上下文管理器确保 session 关闭
                with DDGS() as ddgs:
                    # [修改] 移除 backend='api'，让库自动选择 (通常是 html 或 api)
                    # text() 方法在新版中可能返回 None 或生成器
                    raw_results = ddgs.text(
                        keywords=query,
                        max_results=args.max_results
                    )
                    if raw_results:
                        results = list(raw_results)
            except Exception as e:
                logger.error("DuckDuckGo search failed", error=str(e))
                raise e
            return results

        try:
            # 异步非阻塞执行
            raw_data = await run_sync(_search_sync)

            if not raw_data:
                return ToolResult(content=f"未找到关于 '{query}' 的相关结果，或搜索服务暂时不可用。")

            return self._format_results(raw_data)

        except Exception as e:
            return ToolResult(
                content=f"搜索工具执行异常: {str(e)}",
                is_error=True
            )

    def _format_results(self, results: List[Dict[str, Any]]) -> ToolResult:
        """格式化搜索结果为易读文本"""
        lines = []
        for i, r in enumerate(results, 1):
            title = r.get('title', 'No Title')
            link = r.get('href', '#')
            body = r.get('body', '')
            
            lines.append(f"[{i}] {title}")
            lines.append(f"    Link: {link}")
            lines.append(f"    Snippet: {body}\n")
        
        formatted_text = "DuckDuckGo 搜索结果:\n" + "\n".join(lines)
        
        return ToolResult(
            content=formatted_text,
            metadata={"source": "duckduckgo", "count": len(results)}
        )
```

[84] gecko/utils/cleanup.py
```python
# gecko/utils/cleanup.py
import atexit
import asyncio

def register_litellm_cleanup():
    """在进程退出时优雅关闭 LiteLLM 异步客户端，避免 RuntimeWarning"""
    def _cleanup():
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # 如果循环还在运行，调度清理任务
                loop.create_task(_close_clients())
            else:
                # 循环已关闭，直接新开一个临时循环执行清理
                asyncio.run(_close_clients())
        except Exception:
            pass  # 防止清理本身抛错

    async def _close_clients():
        try:
            import litellm
            # LiteLLM 官方提供的异步关闭方法（v1.40+ 支持）
            if hasattr(litellm, "async_http_handler"):
                if litellm.async_http_handler:
                    await litellm.async_http_handler.client.close()
            # 兼容旧版本
            if hasattr(litellm, "http_client"):
                if litellm.http_client:
                    await litellm.http_client.close()
        except Exception:
            pass

    atexit.register(_cleanup)

# 自动注册（模块导入即生效）
register_litellm_cleanup()
```

